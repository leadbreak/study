{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swish\n",
    "- 2017년 Google Brain 팀에서 공개(Searching for Activation Functions)\n",
    "- ReLU 함수와 모양이 비슷하지만 함수가 매끄러워 그레디언트 소실 문제를 완화하는데 도움이 됨\n",
    "\n",
    "$ f(x) = x \\cdot \\sigma(\\beta x) $\n",
    "\n",
    "여기서:\n",
    "- $ x $ 는 입력 신호,\n",
    "- $ \\sigma $ 는 시그모이드 함수 $ \\sigma(x) = \\frac{1}{1 + e^{-x}} $,\n",
    "- $ \\beta $ 는 학습 가능한 매개변수 또는 설정된 상수. $ \\beta = 1 $ 일 때 표준 Swish 함수가 됨.\n",
    "\n",
    "### 특징\n",
    "- **비선형성**: Swish 함수는 비선형 함수이므로, 복잡한 패턴과 관계를 모델링하는 데 유용.\n",
    "- **부드러운 곡선**: Swish 함수는 매끄러운 곡선을 가지고 있어, 경사 하강법과 같은 최적화 알고리즘에서 안정적인 훈련을 도울 수 있음.\n",
    "- **값 범위**: Swish 함수는 입력 값이 매우 크거나 작을 때 ReLU보다 정보 손실이 적음. 음수 입력에 대해 작은 값이 활성화되어, 네트워크가 더 다양한 정보를 학습할 수 있게 함."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
