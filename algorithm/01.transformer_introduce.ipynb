{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Scaled dot Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "# 임의의 입력 벡터(임베딩)를 가정\n",
    "input_vectors = np.array([[1, 0, 1], [0, 2, 0], [1, 1, 1]])\n",
    "\n",
    "# 가중치 행렬 (임의로 설정)\n",
    "W_q = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])  # Q에 대한 가중치\n",
    "W_k = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])  # K에 대한 가중치\n",
    "W_v = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])  # V에 대한 가중치\n",
    "\n",
    "# Q, K, V 계산\n",
    "Q = np.dot(input_vectors, W_q)  # 쿼리\n",
    "K = np.dot(input_vectors, W_k)  # 키\n",
    "V = np.dot(input_vectors, W_v)  # 밸류\n",
    "\n",
    "# 스케일드 닷-프로덕트 어텐션 계산\n",
    "dk = K.shape[1]  # 키 벡터의 차원\n",
    "attention_scores = np.matmul(Q, K.T) / np.sqrt(dk) # Scaled Dot-Product\n",
    "attention_weights = softmax(attention_scores, axis=1) # softmax -> attention weight\n",
    "output = np.matmul(attention_weights, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1, 0, 1],\n",
       "        [0, 2, 0],\n",
       "        [1, 1, 1]]),\n",
       " array([[1, 0, 1],\n",
       "        [0, 2, 0],\n",
       "        [1, 1, 1]]),\n",
       " array([[1, 0, 1],\n",
       "        [0, 2, 0],\n",
       "        [1, 1, 1]]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 결과 확인\n",
    "Q, K, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4319371 , 0.1361258 , 0.4319371 ],\n",
       "       [0.07021749, 0.70697728, 0.22280523],\n",
       "       [0.26445846, 0.26445846, 0.47108308]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8638742 , 0.7041887 , 0.8638742 ],\n",
       "       [0.29302272, 1.63675979, 0.29302272],\n",
       "       [0.73554154, 1.        , 0.73554154]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8638742 , 0.7041887 , 0.8638742 ],\n",
       "       [0.29302272, 1.63675979, 0.29302272],\n",
       "       [0.73554154, 1.        , 0.73554154]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights@V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4319371 , 0.1361258 , 0.4319371 ],\n",
       "       [0.07021749, 0.70697728, 0.22280523],\n",
       "       [0.26445846, 0.26445846, 0.47108308]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(attention_scores) / np.sum(np.exp(attention_scores), axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadAttentionWithMask(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttentionWithMask, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.depth = d_model // num_heads\n",
    "\n",
    "        self.wq = nn.Linear(d_model, d_model)\n",
    "        self.wk = nn.Linear(d_model, d_model)\n",
    "        self.wv = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    # 마스크 영역 생성을 위한 함수\n",
    "    def create_look_ahead_mask(self, size):\n",
    "        mask = torch.triu(torch.ones(size, size), diagonal=1)\n",
    "        return mask  \n",
    "    \n",
    "    # Multi-Head Attention을 위한 함수\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.depth)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "    \n",
    "    def forward(self, q, k, v):\n",
    "        batch_size = q.size(0)\n",
    "\n",
    "        q = self.split_heads(self.wq(q), batch_size)\n",
    "        k = self.split_heads(self.wk(k), batch_size)\n",
    "        v = self.split_heads(self.wv(v), batch_size)\n",
    "\n",
    "        matmul_qk = torch.matmul(q, k.transpose(-2, -1))\n",
    "\n",
    "        # 마스크 적용 전 스코어\n",
    "        dk = torch.tensor(self.depth, dtype=torch.float32)\n",
    "        attention_before_mask = matmul_qk / torch.sqrt(dk)\n",
    "\n",
    "        # 마스크 영역\n",
    "        mask_area = self.create_look_ahead_mask(q.size(2))\n",
    "        matmul_qk += (mask_area * -1e9)\n",
    "\n",
    "        # 마스크 적용 후 스코어\n",
    "        attention_after_mask = matmul_qk / torch.sqrt(dk)\n",
    "\n",
    "        attention_weights = nn.functional.softmax(attention_after_mask, dim=-1)\n",
    "\n",
    "        output = torch.matmul(attention_weights, v)\n",
    "\n",
    "        output = output.permute(0, 2, 1, 3).contiguous()\n",
    "        output = output.view(batch_size, -1, self.d_model)\n",
    "\n",
    "        return attention_before_mask, attention_after_mask, mask_area, output\n",
    "\n",
    "# 모델 초기화\n",
    "d_model = 64\n",
    "num_heads = 4\n",
    "\n",
    "# 더미 입력 데이터 생성 (q, k, v)\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "q = torch.rand(batch_size, seq_len, d_model)\n",
    "k = torch.rand(batch_size, seq_len, d_model)\n",
    "v = torch.rand(batch_size, seq_len, d_model)\n",
    "\n",
    "# 멀티헤드 어텐션 모델 초기화 (마스크 포함)\n",
    "multi_head_attn_with_mask = MultiHeadAttentionWithMask(d_model, num_heads)\n",
    "\n",
    "# 멀티헤드 어텐션에 마스크 적용하여 수행\n",
    "scores_before_mask, scores_after_mask, mask_area, output = multi_head_attn_with_mask(q, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 출력 길이 설정\n",
    "torch.set_printoptions(linewidth=140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5456e-02,  4.1445e-02,  3.5639e-04, -4.4191e-02, -3.3841e-03,  7.5077e-02,  6.1469e-02, -5.1129e-02,  2.8336e-02, -2.6438e-02],\n",
       "        [-7.7577e-02, -1.1581e-02, -6.0852e-02, -8.6383e-02, -6.6699e-02,  1.4562e-02, -2.7794e-03, -1.0405e-01, -2.9481e-02, -7.7760e-02],\n",
       "        [-1.8181e-02,  3.8231e-02, -5.2480e-04,  1.3417e-02, -1.0679e-05,  3.1950e-02,  4.4274e-02, -3.6149e-02,  4.6171e-02,  5.4952e-03],\n",
       "        [-1.9081e-01, -1.4831e-01, -1.5368e-01, -2.3596e-01, -1.9699e-01, -5.2835e-02, -7.0669e-02, -2.3505e-01, -1.1223e-01, -1.6724e-01],\n",
       "        [-8.6314e-03,  3.8139e-02, -2.5298e-02, -4.7316e-04, -8.1381e-02,  4.9788e-02,  4.4812e-02, -4.4572e-02,  9.3385e-03, -6.0012e-02],\n",
       "        [-1.4960e-01, -8.4267e-02, -1.3960e-01, -1.3799e-01, -1.1551e-01, -9.7467e-03, -3.0405e-02, -1.5251e-01, -6.4841e-02, -1.2215e-01],\n",
       "        [-1.7652e-01, -7.3223e-02, -1.5140e-01, -1.8178e-01, -1.2315e-01, -3.0111e-02, -1.1649e-01, -1.8518e-01, -1.1341e-01, -1.4087e-01],\n",
       "        [-2.1848e-02,  4.1641e-02, -4.1888e-02, -4.6538e-04, -4.6424e-02,  1.1684e-01,  3.5187e-02, -1.1062e-01,  6.3995e-02, -2.8281e-02],\n",
       "        [-5.9225e-02, -4.4329e-02, -5.9218e-02, -9.5778e-02, -1.0607e-01, -1.7878e-02, -3.0971e-02, -1.0823e-01, -4.1539e-02, -1.1044e-01],\n",
       "        [-9.3825e-02, -8.2601e-02, -8.0836e-02, -1.0074e-01, -1.6536e-01,  3.3626e-02,  2.8884e-02, -1.0452e-01, -2.8437e-02, -9.0287e-02]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_before_mask[0][0] # mask가 적용되지 않은 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5456e-02, -2.5000e+08, -2.5000e+08, -2.5000e+08, -2.5000e+08, -2.5000e+08, -2.5000e+08, -2.5000e+08, -2.5000e+08, -2.5000e+08],\n",
       "        [-7.7577e-02, -1.1581e-02, -2.5000e+08, -2.5000e+08, -2.5000e+08, -2.5000e+08, -2.5000e+08, -2.5000e+08, -2.5000e+08, -2.5000e+08],\n",
       "        [-1.8181e-02,  3.8231e-02, -5.2480e-04, -2.5000e+08, -2.5000e+08, -2.5000e+08, -2.5000e+08, -2.5000e+08, -2.5000e+08, -2.5000e+08],\n",
       "        [-1.9081e-01, -1.4831e-01, -1.5368e-01, -2.3596e-01, -2.5000e+08, -2.5000e+08, -2.5000e+08, -2.5000e+08, -2.5000e+08, -2.5000e+08],\n",
       "        [-8.6314e-03,  3.8139e-02, -2.5298e-02, -4.7316e-04, -8.1381e-02, -2.5000e+08, -2.5000e+08, -2.5000e+08, -2.5000e+08, -2.5000e+08],\n",
       "        [-1.4960e-01, -8.4267e-02, -1.3960e-01, -1.3799e-01, -1.1551e-01, -9.7467e-03, -2.5000e+08, -2.5000e+08, -2.5000e+08, -2.5000e+08],\n",
       "        [-1.7652e-01, -7.3223e-02, -1.5140e-01, -1.8178e-01, -1.2315e-01, -3.0111e-02, -1.1649e-01, -2.5000e+08, -2.5000e+08, -2.5000e+08],\n",
       "        [-2.1848e-02,  4.1641e-02, -4.1888e-02, -4.6538e-04, -4.6424e-02,  1.1684e-01,  3.5187e-02, -1.1062e-01, -2.5000e+08, -2.5000e+08],\n",
       "        [-5.9225e-02, -4.4329e-02, -5.9218e-02, -9.5778e-02, -1.0607e-01, -1.7878e-02, -3.0971e-02, -1.0823e-01, -4.1539e-02, -2.5000e+08],\n",
       "        [-9.3825e-02, -8.2601e-02, -8.0836e-02, -1.0074e-01, -1.6536e-01,  3.3626e-02,  2.8884e-02, -1.0452e-01, -2.8437e-02, -9.0287e-02]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_after_mask[0][0]  # mask가 적용된 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(linewidth=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  1.        ,  0.        ,  1.        ,  0.        ,  1.        ,  0.        ,  1.        ],\n",
       "       [ 0.84147098,  0.73176098,  0.53316844,  0.91239586,  0.31098359,  0.97201445,  0.17689219,  0.99112177],\n",
       "       [ 0.90929743,  0.07094825,  0.90213071,  0.66493241,  0.59112712,  0.88962418,  0.34820528,  0.96464473],\n",
       "       [ 0.14112001, -0.62792665,  0.99325317,  0.30096729,  0.8126489 ,  0.75744066,  0.50853613,  0.92103902],\n",
       "       [-0.7568025 , -0.98993269,  0.77847174, -0.11572978,  0.95358074,  0.58286235,  0.652828  ,  0.86107891],\n",
       "       [-0.95892427, -0.82086157,  0.3239352 , -0.51215004,  0.99994652,  0.37566059,  0.77652998,  0.7858291 ],\n",
       "       [-0.2794155 , -0.21141624, -0.23036747, -0.81883737,  0.94714816,  0.1474327 ,  0.87574057,  0.69662574],\n",
       "       [ 0.6569866 ,  0.51144927, -0.71372117, -0.98205762,  0.80042165, -0.08904716,  0.94733071,  0.59505278],\n",
       "       [ 0.98935825,  0.95993347, -0.97726175, -0.97321324,  0.57431777, -0.32054296,  0.98904248,  0.48291379],\n",
       "       [ 0.41211849,  0.89343443, -0.93982351, -0.79385383,  0.29125912, -0.53409761,  0.99956032,  0.36219997]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def positional_encoding(max_len, d_model):\n",
    "    \"\"\"\n",
    "    주어진 최대 길이와 모델 차원에 대한 위치 인코딩을 생성합니다.\n",
    "    \n",
    "    :param max_len: 시퀀스의 최대 길이.\n",
    "    :param d_model: 모델의 차원.\n",
    "    :return: (max_len, d_model) 형태의 numpy 배열로, 위치 인코딩을 포함합니다.\n",
    "    \"\"\"\n",
    "    pos_enc = np.zeros((max_len, d_model)) # max_len x d_model 사이즈의 영행렬 생성\n",
    "    for pos in range(max_len):\n",
    "        for i in range(0, d_model, 2):\n",
    "            # 사인 함수를 사용하여 짝수 인덱스의 위치 인코딩을 계산\n",
    "            pos_enc[pos, i] = np.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "            # 코사인 함수를 사용하여 홀수 인덱스의 위치 인코딩을 계산\n",
    "            if i + 1 < d_model:\n",
    "                pos_enc[pos, i + 1] = np.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
    "    \n",
    "    return pos_enc\n",
    "\n",
    "# 최대 길이 10과 모델 차원 64를 가진 시퀀스에 대한 위치 인코딩\n",
    "pos_encoding = positional_encoding(10, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 64),\n",
       " array([[ 0.        ,  1.        ,  0.        ,  1.        ,  0.        ,  1.        ,  0.        ,  1.        ],\n",
       "        [ 0.84147098,  0.73176098,  0.53316844,  0.91239586,  0.31098359,  0.97201445,  0.17689219,  0.99112177],\n",
       "        [ 0.90929743,  0.07094825,  0.90213071,  0.66493241,  0.59112712,  0.88962418,  0.34820528,  0.96464473],\n",
       "        [ 0.14112001, -0.62792665,  0.99325317,  0.30096729,  0.8126489 ,  0.75744066,  0.50853613,  0.92103902],\n",
       "        [-0.7568025 , -0.98993269,  0.77847174, -0.11572978,  0.95358074,  0.58286235,  0.652828  ,  0.86107891],\n",
       "        [-0.95892427, -0.82086157,  0.3239352 , -0.51215004,  0.99994652,  0.37566059,  0.77652998,  0.7858291 ],\n",
       "        [-0.2794155 , -0.21141624, -0.23036747, -0.81883737,  0.94714816,  0.1474327 ,  0.87574057,  0.69662574],\n",
       "        [ 0.6569866 ,  0.51144927, -0.71372117, -0.98205762,  0.80042165, -0.08904716,  0.94733071,  0.59505278],\n",
       "        [ 0.98935825,  0.95993347, -0.97726175, -0.97321324,  0.57431777, -0.32054296,  0.98904248,  0.48291379],\n",
       "        [ 0.41211849,  0.89343443, -0.93982351, -0.79385383,  0.29125912, -0.53409761,  0.99956032,  0.36219997]]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# positional encoding 출력결과\n",
    "pos_encoding.shape, pos_encoding[:, :8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  1.00000000e+00],\n",
       "       [ 8.41470985e-01,  7.31760976e-01,  5.33168440e-01,  9.12395860e-01,  3.10983593e-01,  9.72014449e-01,  1.76892186e-01,  9.91121771e-01,  9.98334166e-02,  9.97189611e-01,  5.62044992e-02,\n",
       "         9.99110992e-01,  3.16175064e-02,  9.99718843e-01,  1.77818569e-02,  9.99911087e-01,  9.99983333e-03,  9.99971883e-01,  5.62338361e-03,  9.99991109e-01,  3.16227239e-03,  9.99997188e-01,\n",
       "         1.77827847e-03,  9.99999111e-01,  9.99999833e-04,  9.99999719e-01,  5.62341296e-04,  9.99999911e-01,  3.16227761e-04,  9.99999972e-01,  1.77827940e-04,  9.99999991e-01,  9.99999998e-05,\n",
       "         9.99999997e-01,  5.62341325e-05,  9.99999999e-01,  3.16227766e-05,  1.00000000e+00,  1.77827941e-05,  1.00000000e+00,  1.00000000e-05,  1.00000000e+00,  5.62341325e-06,  1.00000000e+00,\n",
       "         3.16227766e-06,  1.00000000e+00,  1.77827941e-06,  1.00000000e+00,  1.00000000e-06,  1.00000000e+00,  5.62341325e-07,  1.00000000e+00,  3.16227766e-07,  1.00000000e+00,  1.77827941e-07,\n",
       "         1.00000000e+00,  1.00000000e-07,  1.00000000e+00,  5.62341325e-08,  1.00000000e+00,  3.16227766e-08,  1.00000000e+00,  1.77827941e-08,  1.00000000e+00],\n",
       "       [ 9.09297427e-01,  7.09482514e-02,  9.02130715e-01,  6.64932409e-01,  5.91127117e-01,  8.89624176e-01,  3.48205276e-01,  9.64644731e-01,  1.98669331e-01,  9.88774240e-01,  1.12231311e-01,\n",
       "         9.96445549e-01,  6.32033979e-02,  9.98875528e-01,  3.55580908e-02,  9.99644365e-01,  1.99986667e-02,  9.99887534e-01,  1.12465894e-02,  9.99964435e-01,  6.32451316e-03,  9.99988753e-01,\n",
       "         3.55655132e-03,  9.99996443e-01,  1.99999867e-03,  9.99998875e-01,  1.12468241e-03,  9.99999644e-01,  6.32455490e-04,  9.99999888e-01,  3.55655875e-04,  9.99999964e-01,  1.99999999e-04,\n",
       "         9.99999989e-01,  1.12468265e-04,  9.99999996e-01,  6.32455532e-05,  9.99999999e-01,  3.55655882e-05,  1.00000000e+00,  2.00000000e-05,  1.00000000e+00,  1.12468265e-05,  1.00000000e+00,\n",
       "         6.32455532e-06,  1.00000000e+00,  3.55655882e-06,  1.00000000e+00,  2.00000000e-06,  1.00000000e+00,  1.12468265e-06,  1.00000000e+00,  6.32455532e-07,  1.00000000e+00,  3.55655882e-07,\n",
       "         1.00000000e+00,  2.00000000e-07,  1.00000000e+00,  1.12468265e-07,  1.00000000e+00,  6.32455532e-08,  1.00000000e+00,  3.55655882e-08,  1.00000000e+00],\n",
       "       [ 1.41120008e-01, -6.27926652e-01,  9.93253167e-01,  3.00967295e-01,  8.12648897e-01,  7.57440658e-01,  5.08536134e-01,  9.21039018e-01,  2.95520207e-01,  9.74801187e-01,  1.67903310e-01,\n",
       "         9.92008410e-01,  9.47260913e-02,  9.97470531e-01,  5.33230805e-02,  9.99199881e-01,  2.99955002e-02,  9.99746957e-01,  1.68694395e-02,  9.99919978e-01,  9.48669068e-03,  9.99974695e-01,\n",
       "         5.33481292e-03,  9.99991998e-01,  2.99999550e-03,  9.99997469e-01,  1.68702318e-03,  9.99999200e-01,  9.48683156e-04,  9.99999747e-01,  5.33483798e-04,  9.99999920e-01,  2.99999995e-04,\n",
       "         9.99999975e-01,  1.68702397e-04,  9.99999992e-01,  9.48683297e-05,  9.99999997e-01,  5.33483823e-05,  9.99999999e-01,  3.00000000e-05,  1.00000000e+00,  1.68702398e-05,  1.00000000e+00,\n",
       "         9.48683298e-06,  1.00000000e+00,  5.33483823e-06,  1.00000000e+00,  3.00000000e-06,  1.00000000e+00,  1.68702398e-06,  1.00000000e+00,  9.48683298e-07,  1.00000000e+00,  5.33483823e-07,\n",
       "         1.00000000e+00,  3.00000000e-07,  1.00000000e+00,  1.68702398e-07,  1.00000000e+00,  9.48683298e-08,  1.00000000e+00,  5.33483823e-08,  1.00000000e+00],\n",
       "       [-7.56802495e-01, -9.89932691e-01,  7.78471741e-01, -1.15729782e-01,  9.53580740e-01,  5.82862351e-01,  6.52828002e-01,  8.61078914e-01,  3.89418342e-01,  9.55348994e-01,  2.23044492e-01,\n",
       "         9.85807464e-01,  1.26154067e-01,  9.95504641e-01,  7.10712085e-02,  9.98577714e-01,  3.99893342e-02,  9.99550161e-01,  2.24917562e-02,  9.99857741e-01,  1.26487733e-02,  9.99955013e-01,\n",
       "         7.11305766e-03,  9.99985774e-01,  3.99998933e-03,  9.99995501e-01,  2.24936340e-03,  9.99998577e-01,  1.26491073e-03,  9.99999550e-01,  7.11311704e-04,  9.99999858e-01,  3.99999989e-04,\n",
       "         9.99999955e-01,  2.24936528e-04,  9.99999986e-01,  1.26491106e-04,  9.99999996e-01,  7.11311763e-05,  9.99999999e-01,  4.00000000e-05,  1.00000000e+00,  2.24936530e-05,  1.00000000e+00,\n",
       "         1.26491106e-05,  1.00000000e+00,  7.11311764e-06,  1.00000000e+00,  4.00000000e-06,  1.00000000e+00,  2.24936530e-06,  1.00000000e+00,  1.26491106e-06,  1.00000000e+00,  7.11311764e-07,\n",
       "         1.00000000e+00,  4.00000000e-07,  1.00000000e+00,  2.24936530e-07,  1.00000000e+00,  1.26491106e-07,  1.00000000e+00,  7.11311764e-08,  1.00000000e+00],\n",
       "       [-9.58924275e-01, -8.20861572e-01,  3.23935204e-01, -5.12150043e-01,  9.99946517e-01,  3.75660595e-01,  7.76529980e-01,  7.85829100e-01,  4.79425539e-01,  9.30526995e-01,  2.77480531e-01,\n",
       "         9.77853736e-01,  1.57455898e-01,  9.92978965e-01,  8.87968624e-02,  9.97777974e-01,  4.99791693e-02,  9.99297156e-01,  2.81133617e-02,  9.99777723e-01,  1.58107295e-02,  9.99929708e-01,\n",
       "         8.89127990e-03,  9.99977772e-01,  4.99997917e-03,  9.99992971e-01,  2.81170292e-03,  9.99997777e-01,  1.58113817e-03,  9.99999297e-01,  8.89139588e-04,  9.99999778e-01,  4.99999979e-04,\n",
       "         9.99999930e-01,  2.81170659e-04,  9.99999978e-01,  1.58113882e-04,  9.99999993e-01,  8.89139704e-05,  9.99999998e-01,  5.00000000e-05,  9.99999999e-01,  2.81170663e-05,  1.00000000e+00,\n",
       "         1.58113883e-05,  1.00000000e+00,  8.89139705e-06,  1.00000000e+00,  5.00000000e-06,  1.00000000e+00,  2.81170663e-06,  1.00000000e+00,  1.58113883e-06,  1.00000000e+00,  8.89139705e-07,\n",
       "         1.00000000e+00,  5.00000000e-07,  1.00000000e+00,  2.81170663e-07,  1.00000000e+00,  1.58113883e-07,  1.00000000e+00,  8.89139705e-08,  1.00000000e+00],\n",
       "       [-2.79415498e-01, -2.11416238e-01, -2.30367475e-01, -8.18837375e-01,  9.47148158e-01,  1.47432701e-01,  8.75740567e-01,  6.96625745e-01,  5.64642473e-01,  9.00474710e-01,  3.31039330e-01,\n",
       "         9.68161370e-01,  1.88600287e-01,  9.89894921e-01,  1.06494437e-01,  9.96800804e-01,  5.99640065e-02,  9.98987956e-01,  3.37340781e-02,  9.99679927e-01,  1.89725276e-02,  9.99898780e-01,\n",
       "         1.06694740e-02,  9.99967991e-01,  5.99996400e-03,  9.99989878e-01,  3.37404155e-03,  9.99996799e-01,  1.89736546e-03,  9.99998988e-01,  1.06696744e-03,  9.99999680e-01,  5.99999964e-04,\n",
       "         9.99999899e-01,  3.37404789e-04,  9.99999968e-01,  1.89736658e-04,  9.99999990e-01,  1.06696764e-04,  9.99999997e-01,  6.00000000e-05,  9.99999999e-01,  3.37404795e-05,  1.00000000e+00,\n",
       "         1.89736660e-05,  1.00000000e+00,  1.06696765e-05,  1.00000000e+00,  6.00000000e-06,  1.00000000e+00,  3.37404795e-06,  1.00000000e+00,  1.89736660e-06,  1.00000000e+00,  1.06696765e-06,\n",
       "         1.00000000e+00,  6.00000000e-07,  1.00000000e+00,  3.37404795e-07,  1.00000000e+00,  1.89736660e-07,  1.00000000e+00,  1.06696765e-07,  1.00000000e+00],\n",
       "       [ 6.56986599e-01,  5.11449266e-01, -7.13721168e-01, -9.82057618e-01,  8.00421646e-01, -8.90471635e-02,  9.47330707e-01,  5.95052784e-01,  6.44217687e-01,  8.65361056e-01,  3.83551568e-01,\n",
       "         9.56747597e-01,  2.19556091e-01,  9.86254244e-01,  1.24158336e-01,  9.95646378e-01,  6.99428473e-02,  9.98622580e-01,  3.93537277e-02,  9.99564353e-01,  2.21341359e-02,  9.99862230e-01,\n",
       "         1.24476344e-02,  9.99956432e-01,  6.99994283e-03,  9.99986223e-01,  3.93637911e-03,  9.99995643e-01,  2.21359255e-03,  9.99998622e-01,  1.24479527e-03,  9.99999564e-01,  6.99999943e-04,\n",
       "         9.99999862e-01,  3.93638917e-04,  9.99999956e-01,  2.21359434e-04,  9.99999986e-01,  1.24479558e-04,  9.99999996e-01,  6.99999999e-05,  9.99999999e-01,  3.93638928e-05,  1.00000000e+00,\n",
       "         2.21359436e-05,  1.00000000e+00,  1.24479559e-05,  1.00000000e+00,  7.00000000e-06,  1.00000000e+00,  3.93638928e-06,  1.00000000e+00,  2.21359436e-06,  1.00000000e+00,  1.24479559e-06,\n",
       "         1.00000000e+00,  7.00000000e-07,  1.00000000e+00,  3.93638928e-07,  1.00000000e+00,  2.21359436e-07,  1.00000000e+00,  1.24479559e-07,  1.00000000e+00],\n",
       "       [ 9.89358247e-01,  9.59933466e-01, -9.77261746e-01, -9.73213235e-01,  5.74317769e-01, -3.20542960e-01,  9.89042481e-01,  4.82913794e-01,  7.17356091e-01,  8.25383399e-01,  4.34851228e-01,\n",
       "         9.43632711e-01,  2.50292358e-01,  9.82058982e-01,  1.41782974e-01,  9.94314901e-01,  7.99146940e-02,  9.98201047e-01,  4.49721329e-02,  9.99431005e-01,  2.52955229e-02,  9.99820056e-01,\n",
       "         1.42257554e-02,  9.99943096e-01,  7.99991467e-03,  9.99982005e-01,  4.49871543e-03,  9.99994310e-01,  2.52981943e-03,  9.99998201e-01,  1.42262305e-03,  9.99999431e-01,  7.99999915e-04,\n",
       "         9.99999820e-01,  4.49873045e-04,  9.99999943e-01,  2.52982210e-04,  9.99999982e-01,  1.42262352e-04,  9.99999994e-01,  7.99999999e-05,  9.99999998e-01,  4.49873060e-05,  9.99999999e-01,\n",
       "         2.52982213e-05,  1.00000000e+00,  1.42262353e-05,  1.00000000e+00,  8.00000000e-06,  1.00000000e+00,  4.49873060e-06,  1.00000000e+00,  2.52982213e-06,  1.00000000e+00,  1.42262353e-06,\n",
       "         1.00000000e+00,  8.00000000e-07,  1.00000000e+00,  4.49873060e-07,  1.00000000e+00,  2.52982213e-07,  1.00000000e+00,  1.42262353e-07,  1.00000000e+00],\n",
       "       [ 4.12118485e-01,  8.93434434e-01, -9.39823513e-01, -7.93853834e-01,  2.91259121e-01, -5.34097614e-01,  9.99560318e-01,  3.62199965e-01,  7.83326910e-01,  7.80766445e-01,  4.84776130e-01,\n",
       "         9.28840031e-01,  2.80778353e-01,  9.77311494e-01,  1.59362777e-01,  9.92806609e-01,  8.98785492e-02,  9.97723382e-01,  5.05891159e-02,  9.99279883e-01,  2.84566569e-02,  9.99772260e-01,\n",
       "         1.60038315e-02,  9.99927981e-01,  8.99987850e-03,  9.99977225e-01,  5.06105032e-03,  9.99992798e-01,  2.84604605e-03,  9.99997723e-01,  1.60045079e-03,  9.99999280e-01,  8.99999879e-04,\n",
       "         9.99999772e-01,  5.06107171e-04,  9.99999928e-01,  2.84604986e-04,  9.99999977e-01,  1.60045146e-04,  9.99999993e-01,  8.99999999e-05,  9.99999998e-01,  5.06107192e-05,  9.99999999e-01,\n",
       "         2.84604989e-05,  1.00000000e+00,  1.60045147e-05,  1.00000000e+00,  9.00000000e-06,  1.00000000e+00,  5.06107193e-06,  1.00000000e+00,  2.84604989e-06,  1.00000000e+00,  1.60045147e-06,\n",
       "         1.00000000e+00,  9.00000000e-07,  1.00000000e+00,  5.06107193e-07,  1.00000000e+00,  2.84604989e-07,  1.00000000e+00,  1.60045147e-07,  1.00000000e+00]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Cross Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def cross_attention(decoder_output, encoder_output, mask=None):\n",
    "    \"\"\"\n",
    "    Cross Attention을 수행하는 함수.\n",
    "\n",
    "    :param decoder_output: Decoder에서 나온 Query 행렬 (batch_size, target_seq_len, d_model)\n",
    "    :param encoder_output: Encoder에서 나온 Key, Value 행렬 (batch_size, source_seq_len, d_model)\n",
    "    :param mask: 선택적 Mask 행렬 (batch_size, 1, target_seq_len, source_seq_len)\n",
    "    :return: Attention을 적용한 결과와 attention weights\n",
    "    \"\"\"\n",
    "    d_model = decoder_output.size(-1)\n",
    "\n",
    "    # Decoder 출력을 Query로, Encoder 출력을 Key와 Value로 사용\n",
    "    query = decoder_output\n",
    "    key = value = encoder_output\n",
    "\n",
    "    # Scaled Dot-Product Attention\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_model, dtype=torch.float32))\n",
    "\n",
    "    # Mask가 제공된 경우 적용\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "    # Softmax를 적용하여 확률 분포 얻기\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "    # Attention weights를 Value에 적용\n",
    "    output = torch.matmul(attention_weights, value)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
