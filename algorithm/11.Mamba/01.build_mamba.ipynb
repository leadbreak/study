{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20ec8bf0",
   "metadata": {
    "id": "20ec8bf0"
   },
   "outputs": [],
   "source": [
    "## Importing Libraries\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3390f4",
   "metadata": {
    "id": "5a3390f4"
   },
   "source": [
    "## Hyperparameters and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c61f36",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1732703737281,
     "user": {
      "displayName": "Hugman Sangkeun Jung",
      "userId": "08689291704194029524"
     },
     "user_tz": -540
    },
    "id": "d0c61f36",
    "outputId": "45d9894c-a14d-4164-9f2d-699025751e72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Modified hyperparameters\n",
    "SEQUENCE_LENGTH = 64\n",
    "\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = EMBEDDING_DIM*2\n",
    "NUM_LAYERS = 4\n",
    "\n",
    "BATCH_SIZE = 2048\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 1e-4\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c9b962",
   "metadata": {
    "id": "89c9b962"
   },
   "source": [
    "## Data Preparation\n",
    "\n",
    "We are using the TinyShakespeare dataset, a small character-level text corpus consisting of a subset of Shakespeare's plays. It's often used for testing sequence models, as it includes a rich set of vocabulary and provides a challenging task for next-character prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "089830e2",
   "metadata": {
    "id": "089830e2"
   },
   "outputs": [],
   "source": [
    "## Utility Functions\n",
    "\n",
    "def load_data(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "def create_char_mappings(text):\n",
    "    chars = sorted(list(set(text)))\n",
    "    char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "    idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "    return chars, char_to_idx, idx_to_char"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fc6452",
   "metadata": {
    "id": "30fc6452"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f07670d9",
   "metadata": {
    "id": "f07670d9"
   },
   "outputs": [],
   "source": [
    "class CharDataset(Dataset):\n",
    "    def __init__(self, text, seq_length, char_to_idx):\n",
    "        self.text = text\n",
    "        self.seq_length = seq_length\n",
    "        self.char_to_idx = char_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text) - self.seq_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = [self.char_to_idx[ch] for ch in self.text[idx:idx+self.seq_length]]\n",
    "        y = [self.char_to_idx[ch] for ch in self.text[idx+1:idx+self.seq_length+1]]\n",
    "        return torch.tensor(x), torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9172da13",
   "metadata": {
    "id": "9172da13"
   },
   "outputs": [],
   "source": [
    "def prepare_data(text, seq_length, batch_size, val_split):\n",
    "    chars, char_to_idx, idx_to_char = create_char_mappings(text)\n",
    "\n",
    "    # Split data into train and validation\n",
    "    val_size = int(len(text) * val_split)\n",
    "    train_text, val_text = text[:-val_size], text[-val_size:]\n",
    "\n",
    "    train_dataset = CharDataset(train_text, seq_length, char_to_idx)\n",
    "    val_dataset = CharDataset(val_text, seq_length, char_to_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=12, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=12, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader, chars, char_to_idx, idx_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "nEwKFB_8L6AG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3706,
     "status": "ok",
     "timestamp": 1732704246464,
     "user": {
      "displayName": "Hugman Sangkeun Jung",
      "userId": "08689291704194029524"
     },
     "user_tz": -540
    },
    "id": "nEwKFB_8L6AG",
    "outputId": "e75cdff9-3775-461e-d930-0ced534bf74d"
   },
   "outputs": [],
   "source": [
    "# !wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=19zosLuU0z4MxIMKbGVYEGlg52QyfbTIy' -O input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47d03398",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 668,
     "status": "ok",
     "timestamp": 1732704255324,
     "user": {
      "displayName": "Hugman Sangkeun Jung",
      "userId": "08689291704194029524"
     },
     "user_tz": -540
    },
    "id": "47d03398",
    "outputId": "f247f5b3-88ff-4ecb-e8a3-4b2e42a2820b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 1115394\n",
      "Vocabulary size: 65\n",
      "Train dataset size: 1003791\n",
      "Validation dataset size: 111475\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "text = load_data('./input.txt')\n",
    "train_loader, val_loader, chars, char_to_idx, idx_to_char = prepare_data(text, SEQUENCE_LENGTH, BATCH_SIZE, VALIDATION_SPLIT)\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(f\"Total characters: {len(text)}\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Train dataset size: {len(train_loader.dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9442b104",
   "metadata": {
    "id": "9442b104"
   },
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1f0a6a1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 916,
     "status": "ok",
     "timestamp": 1732704257555,
     "user": {
      "displayName": "Hugman Sangkeun Jung",
      "userId": "08689291704194029524"
     },
     "user_tz": -540
    },
    "id": "f1f0a6a1",
    "outputId": "9ef6eb5a-0015-40ad-f143-236e3d38fb7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2048, 64])\n",
      "Target shape: torch.Size([2048, 64])\n",
      "Sample 1: ------------------------------\n",
      "Input sequence : st out, like to itself,No father owning it,--which is, indeed,\n",
      "Target sequence: t out, like to itself,No father owning it,--which is, indeed,M\n",
      "\n",
      "Sample 2: ------------------------------\n",
      "Input sequence : ill should live! 'True, noble prince!'Cousin, thou wert not won\n",
      "Target sequence: ll should live! 'True, noble prince!'Cousin, thou wert not wont\n",
      "\n",
      "Sample 3: ------------------------------\n",
      "Input sequence : ep you!Both Tribunes:Farewell, farewell.SICINIUS:This is a\n",
      "Target sequence: p you!Both Tribunes:Farewell, farewell.SICINIUS:This is a \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to convert index sequence to character sequence\n",
    "def indices_to_text(indices, idx_to_char):\n",
    "    return ''.join([idx_to_char[idx.item()] for idx in indices])\n",
    "\n",
    "# Get a batch of data\n",
    "dataiter = iter(train_loader)\n",
    "batch_x, batch_y = next(dataiter)\n",
    "\n",
    "print(f\"Input shape: {batch_x.shape}\")\n",
    "print(f\"Target shape: {batch_y.shape}\")\n",
    "\n",
    "# Print a few samples from the batch\n",
    "num_samples = 3\n",
    "for i in range(num_samples):\n",
    "    print(f\"Sample {i+1}: ------------------------------\" )\n",
    "    print(\"Input sequence :\", indices_to_text(batch_x[i], idx_to_char).replace('\\n',''))\n",
    "    print(\"Target sequence:\", indices_to_text(batch_y[i], idx_to_char).replace('\\n',''))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5486f73f",
   "metadata": {
    "id": "5486f73f"
   },
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61a52f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_vram_usage(device=\"cuda\"):\n",
    "    allocated = torch.cuda.memory_allocated(device) / (1024**2)  # in MB\n",
    "    reserved = torch.cuda.memory_reserved(device) / (1024**2)    # in MB\n",
    "    max_allocated = torch.cuda.max_memory_allocated(device) / (1024**2)  # in MB\n",
    "    print(f\"Allocated: {allocated:.2f} MB, Reserved: {reserved:.2f} MB, Max Allocated: {max_allocated:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59178a11",
   "metadata": {
    "id": "59178a11"
   },
   "outputs": [],
   "source": [
    "def validate(model, dataloader, criterion, device, epoch, step):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            output, _ = model(x)\n",
    "            loss = criterion(output.view(-1, output.size(-1)), y.view(-1))\n",
    "            losses.append((step, epoch, loss.item()))\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea5e61f5",
   "metadata": {
    "id": "ea5e61f5"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, dataloader, criterion, optimizer, device, epoch, step):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    vram_usage = []\n",
    "    scaler = torch.amp.GradScaler('cuda')\n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch}\", leave=False)\n",
    "    for batch, (x, y) in enumerate(pbar):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.amp.autocast('cuda'):\n",
    "            output, _ = model(x)\n",
    "            loss = criterion(output.view(-1, output.size(-1)), y.view(-1))\n",
    "            \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        step += 1\n",
    "        losses.append((step, epoch, loss.item()))\n",
    "        \n",
    "        # VRAM 사용량을 progress bar의 postfix로 업데이트\n",
    "        allocated = torch.cuda.memory_allocated(device) / (1024**2)\n",
    "        vram_usage.append(allocated)\n",
    "        pbar.set_postfix(loss=f'{loss.item():.4f}', step=step, vram=f'{allocated:.2f} MB')\n",
    "    return losses, step, vram_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbedfd61",
   "metadata": {
    "id": "cbedfd61"
   },
   "source": [
    "## Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e91075e8",
   "metadata": {
    "id": "e91075e8"
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, device, epochs):\n",
    "    all_train_losses = []\n",
    "    all_val_losses = []\n",
    "    all_vram_usages = []\n",
    "    step = 0\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        # Training phase with tqdm updates\n",
    "        epoch_train_losses, step, vram_usage = train(model, train_loader, criterion, optimizer, device, epoch, step)\n",
    "        all_train_losses.extend(epoch_train_losses)\n",
    "        all_vram_usages.append(vram_usage)\n",
    "        \n",
    "        # Validation phase\n",
    "        epoch_val_losses = validate(model, val_loader, criterion, device, epoch, step)\n",
    "        all_val_losses.extend(epoch_val_losses)\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        print(f'Epoch {epoch}/{epochs}, Train Loss: {epoch_train_losses[-1][2]:.4f}, '\n",
    "              f'Val Loss: {epoch_val_losses[-1][2]:.4f}, Epoch Time: {epoch_time:.2f}s',\n",
    "              f'Average Vram Usage: {np.mean(vram_usage):.2f}MB')\n",
    "\n",
    "    train_losses_df = pd.DataFrame(all_train_losses, columns=['step', 'epoch', 'loss_value'])\n",
    "    val_losses_df = pd.DataFrame(all_val_losses, columns=['step', 'epoch', 'loss_value'])\n",
    "    # average_vram_usage = np.mean(all_vram_usages)\n",
    "    return model, train_losses_df, val_losses_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4535c886",
   "metadata": {
    "id": "4535c886"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, char_to_idx, idx_to_char, start_text, device, max_length=500):\n",
    "    model.eval()\n",
    "    current_text = start_text\n",
    "    hidden = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            x = torch.tensor([[char_to_idx[ch] for ch in current_text[-SEQUENCE_LENGTH:]]]).to(device)\n",
    "            output, hidden = model(x, hidden)\n",
    "            probs = torch.softmax(output[0, -1], dim=0)\n",
    "            next_char_idx = torch.multinomial(probs, 1).item()\n",
    "            next_char = idx_to_char[next_char_idx]\n",
    "            current_text += next_char\n",
    "\n",
    "    return current_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6aef1cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_comparison_dict = {}\n",
    "\n",
    "def add_loss_to_comparison(model_name, train_losses_df, val_losses_df):\n",
    "    \"\"\"\n",
    "    Adds training and validation losses from a model to the comparison dictionary.\n",
    "    \"\"\"\n",
    "    loss_comparison_dict[model_name] = {\n",
    "        'train': train_losses_df,\n",
    "        'val': val_losses_df\n",
    "    }\n",
    "\n",
    "def print_final_losses(loss_dict):\n",
    "    for model_name, losses in loss_dict.items():\n",
    "        train_df = losses['train']\n",
    "        val_df = losses['val']\n",
    "        final_train = train_df.groupby('epoch')['loss_value'].last().iloc[-1]\n",
    "        final_val = val_df.groupby('epoch')['loss_value'].last().iloc[-1]\n",
    "        print(f\"{model_name}: Final Train Loss: {final_train:.4f}, Final Val Loss: {final_val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "973b4a80",
   "metadata": {
    "id": "973b4a80"
   },
   "outputs": [],
   "source": [
    "# Function to plot loss curves\n",
    "def plot_loss(train_losses_df, val_losses_df):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    # Plot training losses\n",
    "    for epoch in train_losses_df['epoch'].unique():\n",
    "        epoch_train_losses = train_losses_df[train_losses_df['epoch'] == epoch]\n",
    "        plt.plot(epoch_train_losses['step'], epoch_train_losses['loss_value'],\n",
    "                 color='blue', alpha=0.3)\n",
    "\n",
    "    # scatter training loss at the end of each epoch\n",
    "    last_train_losses = train_losses_df.groupby('epoch').last().reset_index()\n",
    "    plt.scatter(last_train_losses['step'], last_train_losses['loss_value'],\n",
    "                color='blue')\n",
    "\n",
    "    # Plot and scatter validation loss at the end of each epoch\n",
    "    last_val_losses = val_losses_df.groupby('epoch').last().reset_index()\n",
    "    plt.plot(last_val_losses['step'], last_val_losses['loss_value'],\n",
    "             color='orange', label='Validation Loss')\n",
    "    plt.scatter(last_val_losses['step'], last_val_losses['loss_value'],\n",
    "                color='orange')\n",
    "\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Function to print final loss values\n",
    "def print_final_losses(train_losses_df, val_losses_df):\n",
    "    print(\"Final Training Loss:\", train_losses_df.groupby('epoch')['loss_value'].last().iloc[-1])\n",
    "    print(\"Final Validation Loss:\", val_losses_df.groupby('epoch')['loss_value'].last().iloc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5001e203",
   "metadata": {
    "id": "5001e203"
   },
   "outputs": [],
   "source": [
    "# Function to plot loss curves for multiple models stored in loss_comparison_dict\n",
    "def plot_loss_comparisons():\n",
    "    \"\"\"\n",
    "    Plots the training loss curves and average validation loss per epoch for multiple models added to the loss comparison dictionary.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    # Get the last model in the dictionary (for special final-point highlighting)\n",
    "    last_model_name = list(loss_comparison_dict.keys())[-1]\n",
    "\n",
    "    # Loop through each model in the loss dictionary\n",
    "    for model_name, losses in loss_comparison_dict.items():\n",
    "        train_losses_df = losses['train']\n",
    "        val_losses_df = losses['val']\n",
    "\n",
    "        # Plot training losses for each model\n",
    "        plt.plot(train_losses_df['step'], train_losses_df['loss_value'],\n",
    "                 label=f'{model_name} train', linestyle='-', alpha=0.7)\n",
    "\n",
    "        # Scatter training loss at the end of each epoch\n",
    "        last_train_losses = train_losses_df.groupby('epoch').last().reset_index()\n",
    "        plt.scatter(last_train_losses['step'], last_train_losses['loss_value'], marker='o', s=50)\n",
    "\n",
    "        # Compute average validation loss per epoch (using the last step of each epoch for x-axis)\n",
    "        avg_val_losses = val_losses_df.groupby('epoch').agg({'loss_value': 'mean', 'step': 'last'}).reset_index()\n",
    "        # Scatter the average validation loss for each epoch\n",
    "        plt.scatter(avg_val_losses['step'], avg_val_losses['loss_value'], marker='s', s=50,\n",
    "                    label=f'{model_name} val avg')\n",
    "\n",
    "        # For the last model, highlight the final training loss with a star\n",
    "        if model_name == last_model_name:\n",
    "            final_step = train_losses_df['step'].iloc[-1]\n",
    "            final_loss = train_losses_df['loss_value'].iloc[-1]\n",
    "            plt.scatter(final_step, final_loss, marker='*', s=100, color='red', zorder=5)\n",
    "\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Comparison')\n",
    "    plt.legend()  # Legend shows both training and validation average labels\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16777ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_separate_train_val(loss_dict):\n",
    "    \"\"\"\n",
    "    모델별 Training Loss와 Validation Loss를 각각 별도의 그래프로 그립니다.\n",
    "    단, Validation Loss는 에포크별 평균으로 계산합니다.\n",
    "    \"\"\"\n",
    "    # 1. Training Loss Plot (원본 그대로)\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    plt.subplot(1, 2, 1)  # 1행 2열 중 첫 번째\n",
    "    for model_name, losses in loss_dict.items():\n",
    "        train_df = losses['train']\n",
    "        steps_train = train_df['step'].values\n",
    "        loss_train = train_df['loss_value'].values\n",
    "        plt.plot(steps_train, loss_train, label=f'{model_name} Train')\n",
    "    plt.title('Training Loss Comparison')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # 2. Validation Loss Plot (에포크별 평균 처리)\n",
    "    plt.subplot(1, 2, 2)  # 1행 2열 중 두 번째\n",
    "    for model_name, losses in loss_dict.items():\n",
    "        val_df = losses['val']\n",
    "        # 에포크별 평균 loss와 마지막 step을 계산\n",
    "        val_avg = val_df.groupby('epoch').agg({'loss_value': 'mean', 'step': 'last'}).reset_index()\n",
    "        plt.plot(val_avg['step'], val_avg['loss_value'], label=f'{model_name} Val')\n",
    "    plt.title('Validation Loss (Epoch Avg) Comparison')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70664998",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 489,
     "status": "ok",
     "timestamp": 1732704395512,
     "user": {
      "displayName": "Hugman Sangkeun Jung",
      "userId": "08689291704194029524"
     },
     "user_tz": -540
    },
    "id": "70664998",
    "outputId": "a970b003-a23c-47b7-900e-5ed705866838"
   },
   "outputs": [],
   "source": [
    "# Text generation using validation data\n",
    "val_sample, _ = next(iter(val_loader))\n",
    "start_text = ''.join([idx_to_char[idx.item()] for idx in val_sample[0][:SEQUENCE_LENGTH]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2d8a352",
   "metadata": {
    "id": "a2d8a352"
   },
   "outputs": [],
   "source": [
    "def generate_text_attention(model, char_to_idx, idx_to_char, start_text, device, max_length=500):\n",
    "    model.eval()\n",
    "    current_text = start_text\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            # Convert the last sequence of characters to indices and feed it to the model\n",
    "            x = torch.tensor([[char_to_idx[ch] for ch in current_text[-SEQUENCE_LENGTH:]]]).to(device)\n",
    "            output = model(x)[0]  # No hidden state needed for attention-based models\n",
    "            probs = torch.softmax(output[0, -1], dim=0)\n",
    "            next_char_idx = torch.multinomial(probs, 1).item()\n",
    "            next_char = idx_to_char[next_char_idx]\n",
    "            current_text += next_char\n",
    "\n",
    "    return current_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32c936b1",
   "metadata": {
    "id": "32c936b1"
   },
   "outputs": [],
   "source": [
    "def train_and_test(model_desc, model, start_text):\n",
    "    # Initialize the model\n",
    "    model = model.to(device)\n",
    "    # Use the same optimizer and criterion\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    # Train the model\n",
    "    trained_model, train_losses_df, val_losses_df = train_model(\n",
    "        model, train_loader, val_loader, criterion, optimizer, device, EPOCHS\n",
    "    )\n",
    "\n",
    "    # Generate text\n",
    "    generated_text = generate_text_attention(trained_model, char_to_idx, idx_to_char, start_text, device)\n",
    "    print(f\"Generated text [{start_text}]:\")\n",
    "    print(\"-\"*50)\n",
    "    print(generated_text)\n",
    "    \n",
    "    add_loss_to_comparison(model_desc, train_losses_df, val_losses_df)\n",
    "\n",
    "    # Plot loss comparisons including this model\n",
    "    plot_loss_comparisons()\n",
    "    \n",
    "    plot_separate_train_val(loss_comparison_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd160c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.nn import functional as F\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0ffa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x = x.to(torch.float32) # RMSNorm 계산은 float32에서 수행하는 것이 안정적일 수 있음\n",
    "        norm = self._norm(x.to(torch.float32))\n",
    "        return self.weight * norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6296069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수정된 SSM 모듈\n",
    "class SSM(nn.Module):\n",
    "    # --- 3.1 초기화 (__init__) ---\n",
    "    def __init__(self, d_inner, state_size, device='cuda'): # seq_len 불필요\n",
    "        super(SSM, self).__init__()\n",
    "        self.d_inner = d_inner         # 모델 내부 차원 (입력/출력 벡터 u, y의 차원)\n",
    "        self.state_size = state_size     # 상태 벡터 h의 차원 (종종 N으로 표기)\n",
    "        self.device = device\n",
    "\n",
    "        # 입력 x (d_inner) -> dt, B, C 계산용 프로젝션 (Mamba 스타일)\n",
    "        # dt_rank 는 보통 d_inner / 16 정도 사용\n",
    "        dt_rank = d_inner // 16\n",
    "        # 이 Linear 레이어는 입력 x로부터 Δ, B, C를 '선택적'으로 계산하기 위한 중간 값을 만듭니다.\n",
    "        # 출력 크기: dt 계산용(dt_rank) + B 계산용(state_size) + C 계산용(state_size)\n",
    "        self.x_proj = nn.Linear(d_inner, dt_rank + state_size * 2, bias=False, device=device)\n",
    "\n",
    "        # dt_rank -> d_inner 로 확장\n",
    "        # Δ는 입력 x의 각 채널(d_inner)마다 다른 값을 가져야 하므로, dt_rank에서 d_inner로 확장합니다.\n",
    "        self.dt_proj = nn.Linear(dt_rank, d_inner, bias=True, device=device)\n",
    "\n",
    "        # --- 파라미터 A ---\n",
    "        # Mamba는 A를 직접 학습하지 않고, A_log를 학습합니다.\n",
    "        # A는 일반적으로 음수 값을 가져야 안정적인 시스템이 되므로, -exp(A_log) 형태로 사용합니다.\n",
    "        # S4D 스타일: A를 state_size(N) 크기의 벡터로 파라미터화하고, d_inner 채널에 걸쳐 반복하여 사용합니다.\n",
    "        # 이는 A가 대각 행렬(diagonal matrix)임을 의미하며, 각 상태 h의 요소는 독립적으로 업데이트됩니다.\n",
    "        A = torch.arange(1, state_size + 1, dtype=torch.float32, device=device).repeat(d_inner, 1)\n",
    "        # A_log를 Parameter로 등록하여 학습 대상임을 명시합니다. log 공간에서 학습합니다.\n",
    "        self.A_log = nn.Parameter(torch.log(A)) # Shape: [d_inner, state_size]\n",
    "        # 보통 음수로 초기화하는 것이 좋습니다 (예: nn.init.normal_(self.A_log, mean=-1, std=0.5))\n",
    "\n",
    "        # 파라미터 D (피드스루) - Mamba는 보통 학습 가능한 D 사용\n",
    "        # D는 입력 x가 출력 y에 직접 영향을 미치는 skip connection 역할을 합니다.\n",
    "        self.D = nn.Parameter(torch.ones(d_inner, device=device)) # Shape: [d_inner]\n",
    "\n",
    "        # 수치 안정성 위한 값\n",
    "        self.log_eps = torch.log(torch.tensor(1e-7)).to(device) # log(0) 방지\n",
    "        self.exp_clamp_val = 20.0 # exp() 결과가 너무 커지는 것 방지 (overflow 방지)\n",
    "\n",
    "    # --- 3.2 이산화 (discretization) ---\n",
    "    def discretization(self, delta, B):\n",
    "        # 입력:\n",
    "        # delta (Δ): 시간 간격 파라미터 [B, L, d_inner] - 입력 x로부터 계산됨\n",
    "        # B: 입력 행렬 파라미터 [B, L, state_size] - 입력 x로부터 계산됨\n",
    "        # 출력:\n",
    "        # delta_A (Ā): 이산화된 상태 행렬 [B, L, d_inner, state_size]\n",
    "        # delta_B (B̄): 이산화된 입력 행렬 [B, L, d_inner, state_size]\n",
    "\n",
    "        # A 계산 (로그 공간 -> 실제 공간)\n",
    "        # A는 일반적으로 음수 값을 가짐 (안정적 시스템)\n",
    "        A = -torch.exp(self.A_log.float()) # Shape: [d_inner, state_size]\n",
    "\n",
    "        # ΔA 계산 (Ā ≈ exp(ΔA))\n",
    "        # broadcasting: delta [B, L, D, 1] * A [1, 1, D, N] -> [B, L, D, N]\n",
    "        # .to(A.device) 추가하여 device 일치 보장\n",
    "        # log(ΔA) = log(Δ) + log(A) 인데, 여기서는 Δ * A 를 사용합니다. (선형 근사?)\n",
    "        # Mamba 논문에서는 ZOH 이산화를 사용: Ā = exp(ΔA)\n",
    "        log_delta_A = torch.clamp(delta.unsqueeze(-1) * A.unsqueeze(0).unsqueeze(0), min=self.log_eps.to(A.device), max=self.exp_clamp_val)\n",
    "        delta_A = torch.exp(log_delta_A) # Shape: [B, L, d_inner, state_size] (Ā)\n",
    "\n",
    "        # ΔB 계산 (B̄ ≈ ΔB 또는 ZOH: B̄ = (exp(ΔA) - 1) A⁻¹ B)\n",
    "        # 코드에서는 간단한 근사 B̄ ≈ ΔB 를 사용합니다.\n",
    "        # broadcasting: delta [B, L, D, 1] * B [B, L, 1, N] -> [B, L, D, N]\n",
    "        delta_B = delta.unsqueeze(-1) * B.unsqueeze(2) # Shape: [B, L, d_inner, state_size] (B̄)\n",
    "\n",
    "        return delta_A, delta_B\n",
    "\n",
    "    # --- 3.3 순방향 전파 (forward) ---\n",
    "    def forward(self, x):\n",
    "        # x (u_k 역할): 입력 시퀀스 [B, L, d_inner] (Batch, Sequence Length, Inner Dimension)\n",
    "        B, L, d_inner = x.shape\n",
    "\n",
    "        # --- 3.3.1 입력으로부터 Δ, B, C 계산 (선택적 메커니즘) ---\n",
    "        x_proj_out = self.x_proj(x) # [B, L, dt_rank + 2 * state_size]\n",
    "        # 결과를 dt 계산용, B 계산용, C 계산용으로 분리\n",
    "        dt_inter, B_ssm, C_ssm = torch.split(x_proj_out, [self.dt_proj.in_features, self.state_size, self.state_size], dim=-1)\n",
    "        # dt_inter: [B, L, dt_rank]\n",
    "        # B_ssm (B 역할): [B, L, state_size]\n",
    "        # C_ssm (C 역할): [B, L, state_size]\n",
    "\n",
    "        # Δ 계산: dt_inter를 dt_proj로 d_inner 차원으로 확장 후 softplus 적용\n",
    "        dt = self.dt_proj(dt_inter) # [B, L, d_inner]\n",
    "        # softplus(x) = log(1 + exp(x)) 를 사용하여 Δ가 항상 양수가 되도록 보장\n",
    "        delta = F.softplus(dt)      # [B, L, d_inner], (Δ 역할)\n",
    "\n",
    "        # --- 3.3.2 이산화 수행 ---\n",
    "        # .to(x.device) 추가하여 device 일치 보장\n",
    "        # 위에서 계산한 Δ와 B_ssm을 사용하여 이산화된 파라미터 Ā, B̄ 계산\n",
    "        delta_A, delta_B = self.discretization(delta.to(x.device), B_ssm.to(x.device)) # Ā:[B,L,D,N], B̄:[B,L,D,N]\n",
    "\n",
    "        # --- 3.3.3 Scan 연산 (Vectorized Recurrence) ---\n",
    "        # 목표: h_k = Ā_k * h_{k-1} + B̄_k * x_k 계산\n",
    "        # 실제 Mamba는 효율적인 병렬 스캔 알고리즘(CUDA 커널)을 사용하지만,\n",
    "        # 여기서는 PyTorch의 `cumsum`을 이용한 벡터화된 형태로 근사 계산합니다.\n",
    "        # (주의: 이 방식은 수치적으로 불안정하거나 메모리 사용량이 많을 수 있습니다)\n",
    "\n",
    "        # 단계 1: B̄_k * x_k 계산\n",
    "        # delta_B (B̄): [B, L, D, N], x: [B, L, D] -> x.unsqueeze(-1): [B, L, D, 1]\n",
    "        delta_B_u = delta_B * x.unsqueeze(-1) # Shape: [B, L, D, N] (B̄_k * x_k 에 해당)\n",
    "\n",
    "        # 단계 2: Ā의 누적 곱 계산 (로그 공간에서 수행 후 exp 변환)\n",
    "        # R_k = Ā_k * Ā_{k-1} * ... * Ā_1\n",
    "        log_delta_A = torch.log(torch.clamp(delta_A, min=1e-7)) # log(Ā_k) 계산, 0 방지\n",
    "        # log(R_k) = log(Ā_k) + log(Ā_{k-1}) + ... + log(Ā_1)\n",
    "        log_R = torch.cumsum(log_delta_A, dim=1) # Shape: [B, L, D, N] (log(R_k))\n",
    "        R = torch.exp(torch.clamp(log_R, max=self.exp_clamp_val)) # Shape: [B, L, D, N] (R_k)\n",
    "\n",
    "        # 단계 3: 스캔 계산 (h_k = Σ_{i=1 to k} (Π_{j=i+1 to k} Ā_j) * B̄_i * x_i)\n",
    "        #       = R_k * Σ_{i=1 to k} (B̄_i * x_i) / R_i\n",
    "        # 위 공식을 벡터화하여 계산합니다.\n",
    "\n",
    "        # 1/R_k 계산 (로그 공간에서 -log_R 계산 후 exp)\n",
    "        exp_neg_log_R = torch.exp(torch.clamp(-log_R, max=self.exp_clamp_val)) # Shape: [B, L, D, N] (1/R_k)\n",
    "\n",
    "        # S_term = (B̄_k * x_k) / R_k 계산\n",
    "        S_term = delta_B_u * exp_neg_log_R # Shape: [B, L, D, N]\n",
    "\n",
    "        # S_k = Σ_{i=1 to k} S_term_i 계산\n",
    "        S = torch.cumsum(S_term, dim=1)    # Shape: [B, L, D, N] (S_k)\n",
    "\n",
    "        # h_k = R_k * S_k 계산\n",
    "        h = R * S # Shape: [B, L, d_inner, state_size] (h_k)\n",
    "\n",
    "        # --- 3.3.4 출력 계산 ---\n",
    "        # y_k = C_k * h_k + D * x_k\n",
    "        # C_ssm: [B, L, N], h: [B, L, D, N] -> einsum -> y: [B, L, D]\n",
    "        # 'bln,bldn->bld'는 각 배치(b), 각 시퀀스 위치(l)에 대해 C 벡터와 h 행렬의 내적(dot product)을 수행합니다.\n",
    "        y = torch.einsum('bln,bldn->bld', C_ssm, h) # Shape: [B, L, d_inner] (C_k * h_k 부분)\n",
    "\n",
    "        # 피드스루 D 추가 (Skip connection)\n",
    "        # D: [D] -> unsqueeze -> [1, 1, D]\n",
    "        # .to(x.device) 추가하여 device 일치 보장\n",
    "        # x: [B, L, D] 와 브로드캐스팅되어 요소별 곱셈 후 더해짐\n",
    "        y = y + x * self.D.unsqueeze(0).unsqueeze(0).to(x.device) # Shape: [B, L, d_inner] (최종 y_k)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f44661e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MambaBlock(nn.Module):\n",
    "    def __init__(self, d_model, state_size, d_conv=4, expand=2, dropout_prob=0.1, device='cuda'): \n",
    "        super(MambaBlock, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_inner = int(expand * d_model) \n",
    "        self.state_size = state_size\n",
    "        self.d_conv = d_conv\n",
    "        self.device = device\n",
    "\n",
    "        # 입력 프로젝션 (x -> xz) 및 분기 (x -> x_for_ssm, z -> gate)\n",
    "        self.in_proj = nn.Linear(d_model, 2 * self.d_inner, bias=False, device=device)\n",
    "\n",
    "        # 컨볼루션 브랜치 (Depthwise Conv1d)\n",
    "        self.conv1d = nn.Conv1d(\n",
    "            in_channels=self.d_inner,\n",
    "            out_channels=self.d_inner,\n",
    "            bias=True,\n",
    "            kernel_size=d_conv,\n",
    "            groups=self.d_inner, # Depthwise 설정\n",
    "            padding=d_conv - 1, # Causal padding을 위한 설정\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        # SSM 모듈 (d_inner 차원 사용)\n",
    "        self.ssm = SSM(self.d_inner, state_size, device=device)\n",
    "\n",
    "        # 출력 프로젝션\n",
    "        self.out_proj = nn.Linear(self.d_inner, d_model, bias=False, device=device)\n",
    "\n",
    "        # Layer Normalization (RMSNorm 사용)\n",
    "        self.norm = RMSNorm(d_model, eps=1e-5) # RMSNorm 사용 시 eps는 1e-5가 종종 사용됨\n",
    "\n",
    "        # Residual Dropout 추가 (Llama의 res_dropout과 유사)\n",
    "        self.dropout_res = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, L, d_model]\n",
    "        B, L, D = x.shape\n",
    "\n",
    "        # Residual connection 저장\n",
    "        residual = x\n",
    "\n",
    "        # Layer Norm 적용\n",
    "        x_norm = self.norm(x)\n",
    "\n",
    "        # Input projection & split (x -> x_in, z)\n",
    "        xz = self.in_proj(x_norm) # [B, L, 2 * d_inner]\n",
    "        x_in, z = xz.chunk(2, dim=-1) # 각 [B, L, d_inner]\n",
    "\n",
    "        # Conv branch\n",
    "        # .to(x_in.device) 추가하여 device 일치 보장\n",
    "        x_conv = rearrange(x_in, 'b l d -> b d l') # [B, d_inner, L]\n",
    "        # Conv1D 적용 시 causal padding 효과를 위해 마지막 d_conv-1 제거\n",
    "        x_conv = self.conv1d(x_conv)[:, :, :L]\n",
    "        x_conv = rearrange(x_conv, 'b d l -> b l d') # [B, L, d_inner]\n",
    "        x_conv_act = F.silu(x_conv) # Conv 후 Activation\n",
    "\n",
    "        # SSM branch\n",
    "        y_ssm = self.ssm(x_conv_act) # [B, L, d_inner]\n",
    "\n",
    "        # Gating (z * y_ssm)\n",
    "        y_gated = y_ssm * F.silu(z) # [B, L, d_inner]\n",
    "\n",
    "        # Output projection\n",
    "        output = self.out_proj(y_gated) # [B, L, d_model]\n",
    "\n",
    "        # Residual connection 추가 전에 Dropout 적용\n",
    "        # dropout은 training 중에만 활성화됨 (model.eval() 시 자동 비활성화)\n",
    "        output = residual + self.dropout_res(output) # [B, L, d_model]\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccc68d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Mamba(nn.Module):\n",
    "    # dropout_prob 인수 추가\n",
    "    def __init__(self, d_model, n_layers, vocab_size, state_size=16, d_conv=4, expand=2, dropout_prob=0.1, device='cuda'): # seq_len 불필요\n",
    "        super(Mamba, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_layers = n_layers\n",
    "        self.vocab_size = vocab_size\n",
    "        self.device = device\n",
    "        self.dropout_prob = dropout_prob # dropout 확률 저장\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, device=device)\n",
    "        # Embedding Dropout 추가\n",
    "        self.dropout_emb = nn.Dropout(dropout_prob)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            MambaBlock(\n",
    "                d_model=d_model,\n",
    "                state_size=state_size,\n",
    "                d_conv=d_conv,\n",
    "                expand=expand,\n",
    "                # dropout_prob 전달\n",
    "                dropout_prob=self.dropout_prob,\n",
    "                device=device\n",
    "            )\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.norm_f = RMSNorm(d_model, eps=1e-5) # 최종 Norm\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False, device=device)\n",
    "\n",
    "        # Weight tying (optional but common)\n",
    "        self.lm_head.weight = self.embedding.weight\n",
    "\n",
    "        # Weight initialization (optional)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "            # 가중치 초기화는 모델 성능에 중요할 수 있음\n",
    "            # nn.init.xavier_uniform_(module.weight) 또는 다른 방법 사용\n",
    "            # Mamba 논문에서는 특별한 초기화를 제안할 수 있으므로 확인 필요\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02) # 예시 초기화\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, std=0.02)\n",
    "        elif isinstance(module, nn.Conv1d):\n",
    "            # Conv1d 가중치 초기화 (SiLU 사용 고려)\n",
    "            # std = math.sqrt((4 * (1.0 - self.dropout_prob)) / (self.d_conv * self.d_inner)) # 예시 (정확하지 않을 수 있음)\n",
    "            # nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            nn.init.kaiming_normal_(module.weight, nonlinearity='leaky_relu') # SiLU 사용시 kaiming_normal도 고려 가능\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, RMSNorm):\n",
    "             nn.init.ones_(module.weight) # RMSNorm 가중치는 1로 초기화\n",
    "        # Add specific init for SSM parameters if needed (e.g., A_log, D)\n",
    "        # 예를 들어 SSM의 A_log는 음수로 시작하는 것이 안정적일 수 있음\n",
    "        if hasattr(module, 'A_log'):\n",
    "             nn.init.normal_(module.A_log, mean=-1.0, std=0.5) # 음수 초기화 예시\n",
    "        if hasattr(module, 'D'):\n",
    "             nn.init.ones_(module.D) # D는 1로 시작하는 경우가 많음\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, labels=None): # labels는 loss 계산 시 사용 (여기서는 무시)\n",
    "        # input_ids: [B, L]\n",
    "        x = self.embedding(input_ids) # [B, L, d_model]\n",
    "\n",
    "        # Embedding Dropout 적용\n",
    "        x = self.dropout_emb(x)\n",
    "\n",
    "        # Mamba 블록 통과 (Residual connection 및 Dropout은 블록 내부에 구현됨)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        # Final normalization\n",
    "        x = self.norm_f(x)\n",
    "\n",
    "        # LM Head\n",
    "        logits = self.lm_head(x) # [B, L, vocab_size]\n",
    "\n",
    "        return logits, _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf6a7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2048, 64, 8])\n",
      "x.shape=torch.Size([2048, 64, 8]), self.d_model=8\n",
      "torch.Size([2048, 64, 8])\n",
      "x.shape=torch.Size([2048, 64, 8]), self.d_model=8\n",
      "torch.Size([2048, 64, 8])\n",
      "x.shape=torch.Size([2048, 64, 8]), self.d_model=8\n",
      "test_output.shape = torch.Size([2048, 64, 8])\n"
     ]
    }
   ],
   "source": [
    "STATE_SIZE = 8\n",
    "\n",
    "x = torch.randint(0, vocab_size, (BATCH_SIZE, SEQUENCE_LENGTH)).to(device)\n",
    "\n",
    "mamba = Mamba(HIDDEN_DIM, NUM_LAYERS, vocab_size, STATE_SIZE, d_conv=4, expand=3).to(device)\n",
    "\n",
    "test_output, _ = mamba(x)\n",
    "print(f\"test_output.shape = {test_output.shape}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e31528f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2048, 64, 8])\n",
      "x.shape=torch.Size([2048, 64, 8]), self.d_model=8\n",
      "torch.Size([2048, 64, 8])\n",
      "x.shape=torch.Size([2048, 64, 8]), self.d_model=8\n",
      "torch.Size([2048, 64, 8])\n",
      "x.shape=torch.Size([2048, 64, 8]), self.d_model=8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Mamba                                    [2048, 64, 8]             --\n",
       "├─ModuleList: 1-1                        --                        --\n",
       "│    └─MambaBlock: 2-1                   [2048, 64, 8]             --\n",
       "│    │    └─RMSNorm: 3-1                 [2048, 64, 8]             8\n",
       "│    │    └─Linear: 3-2                  [2048, 64, 16]            144\n",
       "│    │    └─Conv1d: 3-3                  [2048, 64, 16]            12,352\n",
       "│    │    └─Linear: 3-4                  [2048, 64, 16]            272\n",
       "│    │    └─S6: 3-5                      [2048, 64, 16]            6,672\n",
       "│    │    └─Linear: 3-6                  [2048, 64, 16]            144\n",
       "│    │    └─Linear: 3-7                  [2048, 64, 8]             136\n",
       "│    └─MambaBlock: 2-2                   [2048, 64, 8]             --\n",
       "│    │    └─RMSNorm: 3-8                 [2048, 64, 8]             8\n",
       "│    │    └─Linear: 3-9                  [2048, 64, 16]            144\n",
       "│    │    └─Conv1d: 3-10                 [2048, 64, 16]            12,352\n",
       "│    │    └─Linear: 3-11                 [2048, 64, 16]            272\n",
       "│    │    └─S6: 3-12                     [2048, 64, 16]            6,672\n",
       "│    │    └─Linear: 3-13                 [2048, 64, 16]            144\n",
       "│    │    └─Linear: 3-14                 [2048, 64, 8]             136\n",
       "│    └─MambaBlock: 2-3                   [2048, 64, 8]             --\n",
       "│    │    └─RMSNorm: 3-15                [2048, 64, 8]             8\n",
       "│    │    └─Linear: 3-16                 [2048, 64, 16]            144\n",
       "│    │    └─Conv1d: 3-17                 [2048, 64, 16]            12,352\n",
       "│    │    └─Linear: 3-18                 [2048, 64, 16]            272\n",
       "│    │    └─S6: 3-19                     [2048, 64, 16]            6,672\n",
       "│    │    └─Linear: 3-20                 [2048, 64, 16]            144\n",
       "│    │    └─Linear: 3-21                 [2048, 64, 8]             136\n",
       "==========================================================================================\n",
       "Total params: 59,184\n",
       "Trainable params: 59,184\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 1.25\n",
       "==========================================================================================\n",
       "Input size (MB): 4.19\n",
       "Forward/backward pass size (MB): 1107.30\n",
       "Params size (MB): 0.21\n",
       "Estimated Total Size (MB): 1111.70\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(mamba.to(device), input_size=(batch_size, seq_len, d_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "51ce9296",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2048, 64])\n",
      "x.shape=torch.Size([2048, 64]), self.d_model=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (64) must match the size of tensor b (8) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_and_test\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmamba\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmamba\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_text\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 9\u001b[0m, in \u001b[0;36mtrain_and_test\u001b[0;34m(model_desc, model, start_text)\u001b[0m\n\u001b[1;32m      6\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mLEARNING_RATE)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m trained_model, train_losses_df, val_losses_df \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Generate text\u001b[39;00m\n\u001b[1;32m     14\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m generate_text_attention(trained_model, char_to_idx, idx_to_char, start_text, device)\n",
      "Cell \u001b[0;32mIn[12], line 10\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, device, epochs)\u001b[0m\n\u001b[1;32m      8\u001b[0m epoch_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Training phase with tqdm updates\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m epoch_train_losses, step, vram_usage \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m all_train_losses\u001b[38;5;241m.\u001b[39mextend(epoch_train_losses)\n\u001b[1;32m     12\u001b[0m all_vram_usages\u001b[38;5;241m.\u001b[39mappend(vram_usage)\n",
      "Cell \u001b[0;32mIn[11], line 14\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, criterion, optimizer, device, epoch, step)\u001b[0m\n\u001b[1;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 14\u001b[0m     output, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, output\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), y\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     17\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[27], line 9\u001b[0m, in \u001b[0;36mMamba.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 9\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[26], line 35\u001b[0m, in \u001b[0;36mMambaBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03mx_proj.shape = torch.Size([batch_size, seq_len, 2*d_model])\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03mx_conv.shape = torch.Size([batch_size, seq_len, 2*d_model])\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03mx_conv_act.shape = torch.Size([batch_size, seq_len, 2*d_model])\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Refer to Figure 3 in the MAMBA paper\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m x_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minp_proj(x)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m#print(f\"x_proj.shape = {x_proj.shape}\")\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Add 1D convolution with kernel size 3\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[25], line 15\u001b[0m, in \u001b[0;36mRMSNorm.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m---> 15\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrsqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (64) must match the size of tensor b (8) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "train_and_test(\"mamba\", mamba, start_text)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
