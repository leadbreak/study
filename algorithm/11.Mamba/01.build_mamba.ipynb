{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88571d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class SSM(nn.Module):\n",
    "    def __init__(self, d_inner, state_size, device='cuda'):\n",
    "        \"\"\"\n",
    "        SSM 레이어 초기화 (논문 수식 기반)\n",
    "\n",
    "        Args:\n",
    "            d_inner (int): 내부 차원 크기 (D)\n",
    "            state_size (int): 상태 공간의 크기 (N)\n",
    "            device (str): 모델 파라미터를 로드할 장치\n",
    "        \"\"\"\n",
    "        super(SSM, self).__init__()\n",
    "        self.d_inner = d_inner\n",
    "        self.state_size = state_size\n",
    "        self.device = device\n",
    "\n",
    "        # 입력 x -> Δ, B, C 계산용 프로젝션 정의\n",
    "        dt_rank = math.ceil(d_inner / 16)\n",
    "        self.x_proj = nn.Linear(d_inner, dt_rank + state_size * 2, bias=False, device=device)\n",
    "        self.dt_proj = nn.Linear(dt_rank, d_inner, bias=True, device=device)\n",
    "\n",
    "        # 연속 시간 상태 행렬 A (A_log) 정의 - 학습 가능 파라미터\n",
    "        # 논문 Eq (1a)의 A에 해당 (실제로는 이산화에 사용됨)\n",
    "        A = torch.arange(1, state_size + 1, dtype=torch.float32, device=device).repeat(d_inner, 1)\n",
    "        self.A_log = nn.Parameter(torch.log(A)) # shape: (d_inner, N)\n",
    "\n",
    "        # 피드스루 D 정의 - 학습 가능 파라미터\n",
    "        # Mamba의 출력 수식 y_t = C_t h_t + D x_t 에 사용됨\n",
    "        self.D = nn.Parameter(torch.ones(d_inner, device=device)) # shape: (d_inner,)\n",
    "\n",
    "        print(f\"SSM Layer Initialized: d_inner={d_inner}, state_size={state_size}, dt_rank={dt_rank}\")\n",
    "        \n",
    "    # 단계 2: 이산화 (Discretization) - 논문 Eq (4) 구현\n",
    "    def discretization(self, delta, B):\n",
    "        \"\"\"\n",
    "        연속 시간 파라미터(A, B)와 시간 스텝(delta)을 사용하여\n",
    "        이산 시간 파라미터(Ā, B̄)를 계산합니다. (ZOH 방식 근사 - Eq 4)\n",
    "\n",
    "        Args:\n",
    "            delta (torch.Tensor): 시간 스텝 Δ. shape: [B, L, d_inner]\n",
    "            B (torch.Tensor): 연속 시간 입력 행렬 B (입력 의존적). shape: [B, L, state_size]\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]:\n",
    "                - delta_A (torch.Tensor): 이산화된 상태 전이 행렬 Ā. shape: [B, L, d_inner, state_size]\n",
    "                - delta_B (torch.Tensor): 이산화된 입력 행렬 B̄. shape: [B, L, d_inner, state_size]\n",
    "        \"\"\"\n",
    "        # 연속 시간 파라미터 A 계산\n",
    "        A = -torch.exp(self.A_log.float()) # shape: (d_inner, state_size)\n",
    "\n",
    "        # --- Ā = exp(ΔA) 계산 --- (Eq 4 첫 부분)\n",
    "        # Broadcasting 사용: delta (B, L, D, 1) * A (1, 1, D, N) -> (B, L, D, N)\n",
    "        dA = delta.unsqueeze(-1) * A.unsqueeze(0).unsqueeze(0) # ΔA 계산\n",
    "        delta_A = torch.exp(dA) # shape: (B, L, D, N)\n",
    "\n",
    "        # --- B̄ = (ΔA)⁻¹ (exp(ΔA) - I) B 계산 --- (Eq 4 두 번째 부분, B 사용 버전)\n",
    "        # (exp(ΔA) - 1) 계산\n",
    "        delta_A_minus_1 = delta_A - 1.0 # exp(ΔA) - I 부분\n",
    "\n",
    "        # (ΔA)⁻¹ 부분 계산 (A가 0에 가까울 때 수치 안정성 중요)\n",
    "        # 여기서는 간단히 분모에 작은 값(1e-10)을 더하여 0 나누기 방지\n",
    "        dA_inv = 1.0 / (dA + 1e-10)\n",
    "\n",
    "        # B̄ 계산\n",
    "        delta_B = dA_inv * delta_A_minus_1 * B.unsqueeze(2) # shape: (B, L, D, N)\n",
    "\n",
    "        return delta_A, delta_B\n",
    "    \n",
    "    # 단계 3: 순전파 (Forward Pass) - 논문 Eq (2a), (2b) + Mamba 특징 적용\n",
    "    def forward(self, x):\n",
    "        \"\"\" SSM 레이어 순전파 연산 수행 \"\"\"\n",
    "        B, L, D = x.shape\n",
    "        N = self.state_size\n",
    "\n",
    "        # 3-1. 입력 x로부터 Δ, B, C 동적 계산\n",
    "        x_proj_out = self.x_proj(x) # (B, L, dt_rank + 2*N)\n",
    "        dt_inter, B_ssm, C_ssm = torch.split(\n",
    "            x_proj_out, [self.dt_proj.in_features, N, N], dim=-1\n",
    "        )\n",
    "        # B_ssm: 연속 시간 B (입력 의존적), shape: (B, L, N)\n",
    "        # C_ssm: 이산 시간 C (입력 의존적), shape: (B, L, N) - Eq (2b)의 C 역할\n",
    "\n",
    "        dt = self.dt_proj(dt_inter) # (B, L, D)\n",
    "        delta = F.softplus(dt)      # Δ 계산, shape: (B, L, D)\n",
    "\n",
    "        # 3-2. 이산화 (Discretization) - Eq (4) 호출\n",
    "        delta_A, delta_B = self.discretization(delta, B_ssm) # Ā, B̄ 계산\n",
    "        # delta_A (Ā): (B, L, D, N), delta_B (B̄): (B, L, D, N)\n",
    "\n",
    "        # 3-3. Scan 연산 (상태 h 계산) - Eq (2a) 구현\n",
    "        delta_B_u = delta_B * x.unsqueeze(-1) # 입력 항 B̄*x 계산, shape: (B, L, D, N)\n",
    "\n",
    "        h = torch.zeros(B, L, D, N, device=x.device, dtype=x.dtype) # 상태 h 저장 공간\n",
    "        h_prev = torch.zeros(B, D, N, device=x.device, dtype=x.dtype) # 초기 상태 h_0 = 0\n",
    "        for t in range(L):\n",
    "            # 상태 업데이트: h_t = Ā_t * h_{t-1} + B̄_t * x_t (Eq 2a)\n",
    "            # delta_A[:, t] 는 t 시점의 Ā_t 역할\n",
    "            h_t = delta_A[:, t] * h_prev + delta_B_u[:, t] # shape: (B, D, N)\n",
    "            h[:, t] = h_t\n",
    "            h_prev = h_t\n",
    "\n",
    "        # 3-4. 출력 계산 - Eq (2b) 기반 + Mamba 특징 (C_t, D)\n",
    "        # y_t = C_t * h_t + D * x_t\n",
    "        y_state_contribution = torch.einsum('bln,bldn->bld', C_ssm, h) # C_t * h_t 부분\n",
    "        y = y_state_contribution + x * self.D # + D * x_t 부분\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "629e9763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "B = 4  # 배치 크기\n",
    "L = 512 # 시퀀스 길이 (순차 루프 때문에 너무 길면 매우 느려짐)\n",
    "D = 128 # 내부 차원 (d_inner)\n",
    "N = 64  # 상태 공간 크기 (state_size)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4332ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSM Layer Initialized: d_inner=128, state_size=64, dt_rank=8\n"
     ]
    }
   ],
   "source": [
    "ssm_model = SSM(d_inner=D, state_size=N, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1172fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape (x): torch.Size([4, 512, 128])\n",
      "\n",
      "Sample output values (first batch, first sequence element):\n",
      "tensor([ 0.4276, -0.3772, -0.4217,  0.0248, -0.2643,  0.2048, -0.6259,  0.6269,\n",
      "        -0.1294, -0.3750], device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x_sample = torch.randn(B, L, D, device=device)\n",
    "print(f\"Input shape (x): {x_sample.shape}\")\n",
    "\n",
    "\n",
    "y_output = ssm_model(x_sample)\n",
    "print(\"\\nSample output values (first batch, first sequence element):\")\n",
    "print(y_output[0, 0, :10]) # 첫 번째 배치, 첫 번째 시퀀스 요소의 앞 10개 값 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e9b626",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
