{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```scss\n",
    "입력 텐서\n",
    "    │\n",
    "    ▽\n",
    "[1D 컨볼루션] → Triton 최적화 Conv1D\n",
    "    │\n",
    "    ▽\n",
    "[조건부 네트워크] → MLP + Triton 활성화 함수\n",
    "    │               (GEMM 커널 최적화)\n",
    "    ▽\n",
    "[선택적 SSM] → 병렬 스캔 커널\n",
    "    │           (Autotune 적용)\n",
    "    ▽\n",
    "[잔차 연결] → Fused Element-wise 연산\n",
    "    │\n",
    "    ▽\n",
    "출력 텐서\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```scss\n",
    "dx(t)/dt = A(t)x(t) + B(t)u(t)\n",
    "y(t)     = C(t)x(t) + D(t)u(t)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A(t): 상태 전이 행렬 (HIPPO 이론 기반 구조화 행렬)\n",
    "- B(t)/C(t): 입력/출력 프로젝션 (입력 의존적 선택적 가중치)\n",
    "- D(t): 스킵 커넥션"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```scss\n",
    "x_k = (I - Δ_k/2 · A)^-1 [(I + Δ_k/2 · A)x_{k-1} + Δ_k B u_k]\n",
    "y_k = C x_k + D u_k\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Δ_k: 학습 가능한 시간 스텝 (입력 의존적 선택성 구현)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "dynamic_func() missing 1 required positional argument: 'meta'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 156\u001b[0m\n\u001b[1;32m    153\u001b[0m model \u001b[38;5;241m=\u001b[39m MambaBlock(dim, n_state)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    155\u001b[0m \u001b[38;5;66;03m# 순전파 실행 (그래디언트 검증 포함)\u001b[39;00m\n\u001b[0;32m--> 156\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# 차원 검증 (3단계 확인)\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m output\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m x\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m출력 차원 불일치: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m vs \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[20], line 129\u001b[0m, in \u001b[0;36mMambaBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    123\u001b[0m out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty_like(x)\n\u001b[1;32m    124\u001b[0m grid \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    125\u001b[0m     batch,\n\u001b[1;32m    126\u001b[0m     triton\u001b[38;5;241m.\u001b[39mcdiv(seq, \u001b[38;5;241m256\u001b[39m),  \u001b[38;5;66;03m# 초기 추정값 (Autotune이 덮어씀)\u001b[39;00m\n\u001b[1;32m    127\u001b[0m     triton\u001b[38;5;241m.\u001b[39mcdiv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_state, \u001b[38;5;241m64\u001b[39m),\n\u001b[1;32m    128\u001b[0m )\n\u001b[0;32m--> 129\u001b[0m \u001b[43mselective_ssm_forward\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_state\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# 잔차 연결 (메모리 효율성 개선)\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj(out \u001b[38;5;241m*\u001b[39m F\u001b[38;5;241m.\u001b[39msilu(z)) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mD \u001b[38;5;241m*\u001b[39m x_proj[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :dim]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/triton/runtime/jit.py:345\u001b[0m, in \u001b[0;36mKernelInterface.__getitem__.<locals>.<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, grid) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    340\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;124;03m    A JIT function is launched with: fn[grid](*args, **kwargs).\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;124;03m    Hence JITFunction.__getitem__ returns a callable proxy that\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;124;03m    memorizes the grid.\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/triton/runtime/autotuner.py:156\u001b[0m, in \u001b[0;36mAutotuner.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m pruned_configs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprune_configs(kwargs)\n\u001b[1;32m    155\u001b[0m bench_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 156\u001b[0m timings \u001b[38;5;241m=\u001b[39m {config: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bench(\u001b[38;5;241m*\u001b[39margs, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mfor\u001b[39;00m config \u001b[38;5;129;01min\u001b[39;00m pruned_configs}\n\u001b[1;32m    157\u001b[0m bench_end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbench_time \u001b[38;5;241m=\u001b[39m bench_end \u001b[38;5;241m-\u001b[39m bench_start\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/triton/runtime/autotuner.py:156\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    154\u001b[0m pruned_configs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprune_configs(kwargs)\n\u001b[1;32m    155\u001b[0m bench_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 156\u001b[0m timings \u001b[38;5;241m=\u001b[39m {config: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bench\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m config \u001b[38;5;129;01min\u001b[39;00m pruned_configs}\n\u001b[1;32m    157\u001b[0m bench_end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbench_time \u001b[38;5;241m=\u001b[39m bench_end \u001b[38;5;241m-\u001b[39m bench_start\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/triton/runtime/autotuner.py:133\u001b[0m, in \u001b[0;36mAutotuner._bench\u001b[0;34m(self, config, *args, **meta)\u001b[0m\n\u001b[1;32m    131\u001b[0m             bench_res \u001b[38;5;241m=\u001b[39m do_bench_cudagraph(kernel_call, rep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_reps, return_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedian\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m bench_res\n\u001b[0;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdo_bench\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkernel_call\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_warmups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_reps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (OutOfResources, CompileTimeAssertionFailure):\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_cuda_graph \u001b[38;5;28;01melse\u001b[39;00m [\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/triton/testing.py:106\u001b[0m, in \u001b[0;36mdo_bench\u001b[0;34m(fn, warmup, rep, grad_to_none, quantiles, fast_flush, return_mode, device_type)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m    104\u001b[0m di \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdevice_interface\u001b[38;5;241m.\u001b[39mget_interface_for_device(device_type)\n\u001b[0;32m--> 106\u001b[0m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m di\u001b[38;5;241m.\u001b[39msynchronize()\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# We maintain a buffer of 256 MB that we clear\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# before each kernel call to make sure that the L2\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# doesn't contain any input data before the run\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/triton/runtime/autotuner.py:114\u001b[0m, in \u001b[0;36mAutotuner._bench.<locals>.kernel_call\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_hook(args)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcurrent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/triton/runtime/jit.py:618\u001b[0m, in \u001b[0;36mJITFunction.run\u001b[0;34m(self, grid, warmup, *args, **kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_binder()\n\u001b[0;32m--> 618\u001b[0m bound_args, sig_and_spec, constexpr_vals, non_constexpr_vals, excess_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;66;03m# compute cache key\u001b[39;00m\n\u001b[1;32m    621\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(sig_and_spec) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m((constexpr_vals, excess_kwargs))\n",
      "\u001b[0;31mTypeError\u001b[0m: dynamic_func() missing 1 required positional argument: 'meta'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "# 1. 선택적 SSM Triton 커널 (수정 버전) -------------------------------------------\n",
    "@triton.autotune(\n",
    "    configs=[\n",
    "        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64}, num_warps=4),\n",
    "        triton.Config({'BLOCK_M': 256, 'BLOCK_N': 32}, num_warps=8),\n",
    "    ],\n",
    "    key=['seq_len', 'dim', 'n_state']\n",
    ")\n",
    "@triton.jit\n",
    "def selective_ssm_forward(\n",
    "    x_ptr, delta_ptr, A_ptr, out_ptr,\n",
    "    batch_size, seq_len, dim, n_state,\n",
    "    **meta  # Autotune 파라미터 자동 처리\n",
    "):\n",
    "    BLOCK_M = meta['BLOCK_M']\n",
    "    BLOCK_N = meta['BLOCK_N']\n",
    "    \n",
    "    pid_batch = tl.program_id(0)\n",
    "    pid_block_m = tl.program_id(1)\n",
    "    pid_block_n = tl.program_id(2)\n",
    "    \n",
    "    # 차원 검증 강화 (Triton 3.1 요구사항)\n",
    "    tl.static_assert(BLOCK_M <= 1024, \"BLOCK_M exceeds hardware limits\")\n",
    "    tl.static_assert(BLOCK_N <= 1024, \"BLOCK_N exceeds hardware limits\")\n",
    "    \n",
    "    # 오프셋 계산\n",
    "    offs_m = pid_block_m * BLOCK_M + tl.arange(0, BLOCK_M)\n",
    "    offs_n = pid_block_n * BLOCK_N + tl.arange(0, BLOCK_N)\n",
    "    offs_batch = pid_batch\n",
    "    \n",
    "    # 메모리 마스크 생성 (3D 마스킹)\n",
    "    mask_m = (offs_m < seq_len) & (pid_block_m < tl.cdiv(seq_len, BLOCK_M))\n",
    "    mask_n = (offs_n < n_state) & (pid_block_n < tl.cdiv(n_state, BLOCK_N))\n",
    "    full_mask = mask_m[:, None] & mask_n[None, :]\n",
    "    \n",
    "    # 입력 데이터 로드 (캐시 최적화)\n",
    "    x = tl.load(\n",
    "        x_ptr + offs_batch*seq_len*dim + offs_m[:, None]*dim + offs_n[None, :],\n",
    "        mask=full_mask,\n",
    "        other=0.0,\n",
    "        cache_modifier=\".cg\"\n",
    "    )\n",
    "    \n",
    "    # SSM 파라미터 로드 (Bank conflict 방지)\n",
    "    delta = tl.load(\n",
    "        delta_ptr + offs_batch*n_state + offs_n,\n",
    "        mask=mask_n,\n",
    "        other=0.0,\n",
    "        _builder=tl.create_builder(optimize='vectorize')\n",
    "    )\n",
    "    A = tl.exp(tl.load(\n",
    "        A_ptr + offs_batch*n_state + offs_n,\n",
    "        mask=mask_n,\n",
    "        other=0.0\n",
    "    ))\n",
    "    \n",
    "    # 병렬 스캔 연산 (수치 안정성 강화)\n",
    "    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n",
    "    for k in range(0, seq_len, BLOCK_M):\n",
    "        x_k = tl.load(\n",
    "            x_ptr + offs_batch*seq_len*dim + k*dim + offs_n[None, :],\n",
    "            mask=(k + tl.arange(0, BLOCK_M) < seq_len)[:, None] & mask_n[None, :],\n",
    "            other=0.0,\n",
    "            cache_modifier=\".cg\"\n",
    "        )\n",
    "        decay = tl.exp(-delta[None, :] * x_k)\n",
    "        acc = acc * decay + x_k * A[None, :]\n",
    "    \n",
    "    # 결과 저장 (비정렬 메모리 처리)\n",
    "    tl.store(\n",
    "        out_ptr + offs_batch*seq_len*dim + offs_m[:, None]*dim + offs_n[None, :],\n",
    "        acc.to(x_ptr.dtype.element_ty),\n",
    "        mask=full_mask\n",
    "    )\n",
    "\n",
    "# 2. Mamba 블록 구현 (오류 수정 버전) --------------------------------------------\n",
    "class MambaBlock(nn.Module):\n",
    "    def __init__(self, dim, n_state=16, conv_k=4):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.n_state = n_state\n",
    "        \n",
    "        # 프로젝션 레이어\n",
    "        self.in_proj = nn.Linear(dim, 2*dim)\n",
    "        self.conv = nn.Conv1d(dim, dim, conv_k, padding=conv_k-1, groups=dim)\n",
    "        self.condition_net = nn.Sequential(\n",
    "            nn.Linear(dim, 2*n_state),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        self.out_proj = nn.Linear(dim, dim)\n",
    "        \n",
    "        # SSM 파라미터 초기화 (수치 안정성 강화)\n",
    "        self.A_log = nn.Parameter(torch.randn(1, n_state))\n",
    "        self.D = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def ssm_parameters(self, x):\n",
    "        # 조건부 파라미터 생성 (차원 검증 추가)\n",
    "        params = self.condition_net(x)\n",
    "        delta, B = params.chunk(2, dim=-1)\n",
    "        return delta.sigmoid(), B\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, seq, dim = x.shape\n",
    "        \n",
    "        # 입력 분할 (차원 검증)\n",
    "        x_proj = self.in_proj(x)\n",
    "        x, z = x_proj.chunk(2, dim=-1)\n",
    "        \n",
    "        # 컨볼루션 연산 (채널 그룹화 최적화)\n",
    "        x = self.conv(x.transpose(1,2)).transpose(1,2)\n",
    "        \n",
    "        # SSM 파라미터 계산 (로그 공간 변환)\n",
    "        delta, B = self.ssm_parameters(z)\n",
    "        A = -torch.exp(self.A_log).repeat(batch, 1)\n",
    "        \n",
    "        # Triton 커널 실행 (Autotune 파라미터 자동 선택)\n",
    "        out = torch.empty_like(x)\n",
    "        grid = (\n",
    "            batch,\n",
    "            triton.cdiv(seq, 256),  # 초기 추정값 (Autotune이 덮어씀)\n",
    "            triton.cdiv(self.n_state, 64),\n",
    "        )\n",
    "        selective_ssm_forward[grid](\n",
    "            x, delta, A, out,\n",
    "            batch_size=batch,\n",
    "            seq_len=seq,\n",
    "            dim=dim,\n",
    "            n_state=self.n_state\n",
    "        )\n",
    "        \n",
    "        # 잔차 연결 (메모리 효율성 개선)\n",
    "        return self.out_proj(out * F.silu(z)) + self.D * x_proj[..., :dim]\n",
    "\n",
    "# 3. 검증 코드 (업데이트) ------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # 테스트 설정\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    batch_size = 4\n",
    "    seq_len = 1024\n",
    "    dim = 256\n",
    "    n_state = 16\n",
    "    \n",
    "    # 입력 데이터 생성 (정규 분포 검증)\n",
    "    x = torch.randn(batch_size, seq_len, dim, device=device)\n",
    "    model = MambaBlock(dim, n_state).to(device)\n",
    "    \n",
    "    # 순전파 실행 (그래디언트 검증 포함)\n",
    "    output = model(x)\n",
    "    \n",
    "    # 차원 검증 (3단계 확인)\n",
    "    assert output.shape == x.shape, f\"출력 차원 불일치: {output.shape} vs {x.shape}\"\n",
    "    assert not torch.isnan(output).any(), \"NaN 값 존재\"\n",
    "    assert not torch.isinf(output).any(), \"Inf 값 존재\"\n",
    "    \n",
    "    # 성능 측정 (CUDA 이벤트 사용)\n",
    "    starter = torch.cuda.Event(enable_timing=True)\n",
    "    ender = torch.cuda.Event(enable_timing=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        torch.cuda.synchronize()\n",
    "        starter.record()\n",
    "        for _ in range(100):\n",
    "            _ = model(x)\n",
    "        ender.record()\n",
    "        torch.cuda.synchronize()\n",
    "        elapsed = starter.elapsed_time(ender)/100\n",
    "    \n",
    "    print(f\"✅ 검증 성공 | 평균 실행 시간: {elapsed:.2f}ms\")\n",
    "    print(f\"최종 출력 통계: mean={output.mean():.4f}, std={output.std():.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
