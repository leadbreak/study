{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f9ed9f03070>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn import functional as F\n",
    "from einops import rearrange\n",
    "from tqdm import tqdm\n",
    "\n",
    "import math\n",
    "import os\n",
    "import urllib.request\n",
    "from zipfile import ZipFile\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration flags and hyperparameters\n",
    "USE_MAMBA = 1\n",
    "DIFFERENT_H_STATES_RECURRENT_UPDATE_MECHANISM = 0\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-defined hyperparameters\n",
    "d_model = 8\n",
    "state_size = 128  # Example state size\n",
    "seq_len = 100  # Example sequence length\n",
    "batch_size = 256  # Example batch size\n",
    "last_batch_size = 81  # only for the very last batch of the dataset\n",
    "current_batch_size = batch_size\n",
    "different_batch_size = False\n",
    "h_new = None\n",
    "temp_buffer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class S6(nn.Module):\n",
    "    def __init__(self, seq_len, d_model, state_size, device='cuda'):\n",
    "        super(S6, self).__init__()\n",
    "\n",
    "        self.fc_delta = nn.Linear(d_model, d_model, device=device)\n",
    "        self.fc_B = nn.Linear(d_model, state_size, device=device)\n",
    "        self.fc_C = nn.Linear(d_model, state_size, device=device)\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.d_model = d_model\n",
    "        self.state_size = state_size\n",
    "\n",
    "        self.A = nn.Parameter(torch.empty(d_model, state_size, device=device))\n",
    "        nn.init.xavier_uniform_(self.A)\n",
    "\n",
    "    def discretization(self, delta, B):\n",
    "        dB = torch.einsum(\"bld,bln->bldn\", delta, B)\n",
    "        dA = torch.exp(torch.clamp(torch.einsum(\"bld,dn->bldn\", delta, self.A), max=10.0))\n",
    "        return dA, dB\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = self.fc_B(x)\n",
    "        C = self.fc_C(x)\n",
    "        delta = F.softplus(self.fc_delta(x))\n",
    "\n",
    "        dA, dB = self.discretization(delta, B)\n",
    "\n",
    "        h = torch.zeros(x.size(0), self.seq_len, self.d_model, self.state_size, device=x.device)\n",
    "        h = torch.einsum('bldn,bldn->bldn', dA, h) + rearrange(x, \"b l d -> b l d 1\") * dB\n",
    "\n",
    "        y = torch.einsum('bln,bldn->bld', C, h)\n",
    "\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MambaBlock(nn.Module):\n",
    "    def __init__(self, seq_len, d_model, state_size, device='cuda'):\n",
    "        super(MambaBlock, self).__init__()\n",
    "\n",
    "        self.inp_proj = nn.Linear(d_model, 2*d_model, device=device)\n",
    "        self.out_proj = nn.Linear(2*d_model, d_model, device=device)\n",
    "        self.D = nn.Linear(d_model, 2*d_model, device=device)\n",
    "        nn.init.constant_(self.out_proj.bias, 1.0)\n",
    "\n",
    "        self.S6 = S6(seq_len, 2*d_model, state_size, device)\n",
    "\n",
    "        self.conv = nn.Conv1d(2*d_model, 2*d_model, kernel_size=3, padding=1, device=device)\n",
    "        nn.init.xavier_uniform_(self.conv.weight)\n",
    "        if self.conv.bias is not None:\n",
    "            nn.init.constant_(self.conv.bias, 0.0)\n",
    "        self.norm = RMSNorm(d_model, device=device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.norm(x)\n",
    "\n",
    "        x_proj = self.inp_proj(x)\n",
    "        x_proj = rearrange(x_proj, 'b l d -> b d l')  # (batch, channels, seq_len)\n",
    "        x_conv = F.silu(self.conv(x_proj))\n",
    "        x_conv = rearrange(x_conv, 'b d l -> b l d')\n",
    "\n",
    "        x_ssm = self.S6(x_conv)\n",
    "        x_act = F.silu(x_ssm)\n",
    "\n",
    "        x_residual = F.silu(self.D(residual))\n",
    "\n",
    "        x_combined = x_act * x_residual\n",
    "        return self.out_proj(x_combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mamba(nn.Module):\n",
    "    def __init__(self, seq_len, d_model, state_size, num_layers=3, device='cuda'):\n",
    "        super(Mamba, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            MambaBlock(seq_len, d_model, state_size, device)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-5, device='cuda'):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(d_model, device=device))\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm = torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "        return x * norm * self.weight.view(1, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_output.shape = torch.Size([256, 100, 8])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(batch_size, seq_len, d_model, device=device)\n",
    "# Create the Mamba model\n",
    "num_layers = 3\n",
    "mamba = Mamba(seq_len, d_model, state_size, num_layers, device)\n",
    "\n",
    "# rmsnorm\n",
    "norm = RMSNorm(d_model)\n",
    "x = norm(x)\n",
    "\n",
    "# Forward pass\n",
    "test_output = mamba(x)\n",
    "print(f\"test_output.shape = {test_output.shape}\")  # Should be [batch_size, seq_len, d_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Enwiki8Dataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data['input_ids'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.data.items()}\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for padding\n",
    "def pad_sequences_3d(sequences, max_len=None, pad_value=0):\n",
    "    # Assuming sequences is a tensor of shape (batch_size, seq_len, feature_size)\n",
    "    batch_size, seq_len, feature_size = sequences.shape\n",
    "\n",
    "    if max_len is None:\n",
    "        max_len = seq_len + 1\n",
    "\n",
    "\n",
    "    # Initialize padded_sequences with the pad_value\n",
    "    padded_sequences = torch.full((batch_size, max_len, feature_size), fill_value=pad_value, dtype=sequences.dtype, device=sequences.device)\n",
    "    # Pad each sequence to the max_len\n",
    "    padded_sequences[:, :seq_len, :] = sequences\n",
    "\n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, tokenizer, data_loader, optimizer, scheduler, criterion, device, max_grad_norm=1.0, DEBUGGING_IS_ON=False):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    scaler = torch.amp.GradScaler('cuda')\n",
    "    pbar = tqdm(data_loader, leave=False)\n",
    "\n",
    "    for step, batch in enumerate(pbar):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_data = batch['input_ids'].clone().to(device)\n",
    "        attention_mask = batch['attention_mask'].clone().to(device)\n",
    "\n",
    "        target = input_data[:, 1:]\n",
    "        input_data = input_data[:, :-1]\n",
    "\n",
    "        input_data = pad_sequences_3d(input_data, pad_value=tokenizer.pad_token_id)\n",
    "        target = pad_sequences_3d(target, max_len=input_data.size(1), pad_value=tokenizer.pad_token_id)\n",
    "\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            output = model(input_data)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        scaler.unscale_(optimizer)\n",
    "        parameters_to_clip = [\n",
    "            param for name, param in model.named_parameters()\n",
    "            if param.grad is not None and 'out_proj.bias' not in name\n",
    "        ]\n",
    "        torch.nn.utils.clip_grad_norm_(parameters_to_clip, max_norm=max_grad_norm)\n",
    "\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if DEBUGGING_IS_ON:\n",
    "            for name, parameter in model.named_parameters():\n",
    "                if parameter.grad is None:\n",
    "                    print(f\"{name} has no gradient\")\n",
    "\n",
    "        if USE_MAMBA and DIFFERENT_H_STATES_RECURRENT_UPDATE_MECHANISM:\n",
    "            model.S6.h[:current_batch_size, ...].copy_(temp_buffer)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        current_lr = scheduler.get_last_lr()[0]  # 현재 learning rate 가져오기\n",
    "        pbar.set_postfix(loss=f'{loss.item():.4f}', step=step, lr=f'{current_lr:.8f}')\n",
    "\n",
    "\n",
    "    return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_data = batch['input_ids'].clone().detach().to(device)\n",
    "            attention_mask = batch['attention_mask'].clone().detach().to(device)\n",
    "\n",
    "            # In most sequence modeling tasks, like language modeling, the target should be the next token\n",
    "            # in the sequence rather than the input token itself.\n",
    "            # This is because the model's goal is to predict the next word given the previous words.\n",
    "            # Shift the input data by one position to get the target, so that each target token\n",
    "            # is the next token following the input token.\n",
    "            target = input_data[:, 1:]\n",
    "            input_data = input_data[:, :-1]\n",
    "\n",
    "            # Pad all the sequences in the batch:\n",
    "            input_data = pad_sequences_3d(input_data, pad_value=tokenizer.pad_token_id)\n",
    "            target = pad_sequences_3d(target, max_len=input_data.size(1), pad_value=tokenizer.pad_token_id)\n",
    "\n",
    "            if USE_MAMBA:\n",
    "                output = model(input_data)\n",
    "                loss = criterion(output, target)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(loss):\n",
    "    return math.exp(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_enwiki8_dataset():\n",
    "    print(f\"Download and extract enwiki8 data\")\n",
    "    url = \"http://mattmahoney.net/dc/enwik8.zip\"\n",
    "    urllib.request.urlretrieve(url, \"enwik8.zip\")\n",
    "\n",
    "    with ZipFile(\"enwik8.zip\") as f:\n",
    "        data = f.read(\"enwik8\").decode(\"utf-8\")\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and encode the dataset\n",
    "def encode_dataset(tokenizer, text_data):\n",
    "    def batch_encode(tokenizer, text_data, batch_size=1000):\n",
    "        # Tokenize in batches\n",
    "        batched_input_ids = []\n",
    "        for i in range(0, len(text_data), batch_size):\n",
    "            batch = text_data[i:i+batch_size]\n",
    "            inputs = tokenizer(batch, add_special_tokens=True, truncation=True,\n",
    "                               padding='max_length', max_length=seq_len,\n",
    "                               return_tensors='pt')\n",
    "            batched_input_ids.append(inputs['input_ids'])\n",
    "        return torch.cat(batched_input_ids)\n",
    "\n",
    "    # Assuming enwiki8_data is a list of sentences\n",
    "    input_ids = batch_encode(tokenizer, enwiki8_data)\n",
    "\n",
    "    # vocab_size is the number of unique tokens in the tokenizer's vocabulary\n",
    "    global vocab_size\n",
    "    vocab_size = len(tokenizer.vocab)  # Note that for some tokenizers, we might access the vocab directly\n",
    "    print(f\"vocab_size = {vocab_size}\")\n",
    "\n",
    "    # Create an embedding layer\n",
    "    # embedding_dim is the size of the embedding vectors (MAMBA model's D)\n",
    "    embedding_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)\n",
    "\n",
    "    # Pass `input_ids` through the embedding layer\n",
    "    # This will change `input_ids` from shape [B, L] to [B, L, D]\n",
    "    #encoded_input = embedding_layer(input_ids)   ## this eats memory, so use batched_embedding_calls instead\n",
    "    def batch_embedding_calls(input_ids, embedding_layer, batch_size=256):\n",
    "        # Check if input_ids is already a tensor, if not convert it\n",
    "        if not isinstance(input_ids, torch.Tensor):\n",
    "            input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "\n",
    "        # Calculate the number of batches needed\n",
    "        num_batches = math.ceil(input_ids.size(0) / batch_size)\n",
    "\n",
    "        # List to hold the output embeddings\n",
    "        output_embeddings = []\n",
    "\n",
    "        # Process each batch\n",
    "        for i in range(num_batches):\n",
    "            # Calculate start and end indices for the current batch\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = start_idx + batch_size\n",
    "\n",
    "            # Get the batch\n",
    "            input_id_batch = input_ids[start_idx:end_idx]\n",
    "\n",
    "            # Call the embedding layer\n",
    "            with torch.no_grad():  # No need gradients for this operation\n",
    "                batch_embeddings = embedding_layer(input_id_batch)\n",
    "\n",
    "            # Append the result to the list\n",
    "            output_embeddings.append(batch_embeddings)\n",
    "\n",
    "        # Concatenate the embeddings from each batch into a single tensor\n",
    "        all_embeddings = torch.cat(output_embeddings, dim=0)\n",
    "\n",
    "        return all_embeddings\n",
    "\n",
    "    # `input_ids` is a list or tensor of the input IDs and `embedding_layer` is model's embedding layer\n",
    "    if USE_MAMBA:\n",
    "        # Set `batch_size` to a value that works for memory constraints\n",
    "        encoded_inputs = batch_embedding_calls(input_ids, embedding_layer, batch_size=1).float()\n",
    "\n",
    "    attention_mask = (input_ids != tokenizer.pad_token_id).type(input_ids.dtype)\n",
    "\n",
    "    return encoded_inputs, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load a pretrained tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1842709/3960179654.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  saved_data = torch.load(encoded_inputs_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-tokenized data...\n",
      "[2025-03-25 09:33:01,535] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "2025-03-25 09:33:02.961088: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-25 09:33:03.769616: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 3.8879, Validation Loss: 3.3162, Validation Perplexity: 27.5562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss: 3.8804, Validation Loss: 3.3121, Validation Perplexity: 27.4414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss: 3.8749, Validation Loss: 3.2806, Validation Perplexity: 26.5923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss: 3.8032, Validation Loss: 3.1869, Validation Perplexity: 24.2129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Training Loss: 3.6967, Validation Loss: 3.0699, Validation Perplexity: 21.5404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Training Loss: 3.5888, Validation Loss: 2.9510, Validation Perplexity: 19.1253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Training Loss: 3.4634, Validation Loss: 2.8547, Validation Perplexity: 17.3696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Training Loss: 3.3960, Validation Loss: 2.7959, Validation Perplexity: 16.3778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Training Loss: 3.3531, Validation Loss: 2.7728, Validation Perplexity: 16.0033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Training Loss: 3.3457, Validation Loss: 2.7693, Validation Perplexity: 15.9479\n"
     ]
    }
   ],
   "source": [
    "# Assuming encoded_inputs is a preprocessed tensor of shape [num_samples, seq_len, d_model]\n",
    "encoded_inputs_file = 'encoded_inputs_mamba.pt'\n",
    "\n",
    "if os.path.exists(encoded_inputs_file):\n",
    "    print(\"Loading pre-tokenized data...\")\n",
    "    saved_data = torch.load(encoded_inputs_file)\n",
    "    encoded_inputs = saved_data['input_ids']\n",
    "    attention_mask = saved_data['attention_mask']\n",
    "else:\n",
    "    print(\"Tokenizing raw data...\")\n",
    "    enwiki8_data = load_enwiki8_dataset()\n",
    "    encoded_inputs, attention_mask = encode_dataset(tokenizer, enwiki8_data)\n",
    "    torch.save({'input_ids': encoded_inputs, 'attention_mask': attention_mask}, encoded_inputs_file)\n",
    "    print(\"Finished tokenizing data\")\n",
    "\n",
    "# Combine into a single dictionary\n",
    "data = {\n",
    "    'input_ids': encoded_inputs,\n",
    "    'attention_mask': attention_mask\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Combine into a single dictionary\n",
    "data = {\n",
    "    'input_ids': encoded_inputs,\n",
    "    'attention_mask': attention_mask\n",
    "}\n",
    "\n",
    "# Split the data into train and validation sets\n",
    "total_size = len(data['input_ids'])\n",
    "train_size = int(total_size * 0.8)\n",
    "\n",
    "train_data = {key: val[:train_size] for key, val in data.items()}\n",
    "val_data = {key: val[train_size:] for key, val in data.items()}\n",
    "\n",
    "train_dataset = Enwiki8Dataset(train_data)\n",
    "val_dataset = Enwiki8Dataset(val_data)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "num_layers = 3\n",
    "model = Mamba(seq_len, d_model, state_size, num_layers, device).to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "learning_rate = 3e-6\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10  \n",
    "\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "total_steps = len(train_loader) * num_epochs\n",
    "warmup_steps = int(0.1 * total_steps) \n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, tokenizer, train_loader, optimizer, scheduler, criterion, device, max_grad_norm=1.0, DEBUGGING_IS_ON=True)\n",
    "    val_loss = evaluate(model, val_loader, criterion, device)\n",
    "    val_perplexity = calculate_perplexity(val_loss)\n",
    "    print(f'Epoch: {epoch+1}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation Perplexity: {val_perplexity:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
