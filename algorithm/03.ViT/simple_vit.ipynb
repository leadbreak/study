{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, dropout_rate, channels=3):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        # 이미지는 패치로 분할되고, 각 패치는 Transformer에 입력될 수 있도록 임베딩되어야 합니다.\n",
    "        num_patches = (image_size // patch_size) ** 2\n",
    "        patch_dim = channels * patch_size ** 2\n",
    "        self.patch_to_embedding = nn.Linear(patch_dim, dim)\n",
    "\n",
    "        # 클래스 토큰을 추가합니다. 이 토큰은 분류를 위해 사용됩니다.\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "\n",
    "        # 위치 임베딩은 Transformer 모델에 시퀀스의 순서 정보를 제공합니다.\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "\n",
    "        # Transformer 인코더를 정의합니다.\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=dim, nhead=heads, dim_feedforward=mlp_dim, dropout=dropout_rate, activation='gelu'),\n",
    "            num_layers=depth\n",
    "        )\n",
    "\n",
    "        # 분류를 위한 MLP 헤드입니다.\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.Linear(dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(mlp_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "\n",
    "        # 이미지를 패치로 분할하고 임베딩합니다.\n",
    "        x = x.reshape(b, c, h // self.patch_size, self.patch_size, w // self.patch_size, self.patch_size)\n",
    "        x = x.transpose(2, 4).flatten(2)\n",
    "        x = self.patch_to_embedding(x)\n",
    "\n",
    "        # 클래스 토큰과 위치 임베딩을 추가합니다.\n",
    "        cls_tokens = self.cls_token.expand(b, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x += self.pos_embedding\n",
    "\n",
    "        # Transformer를 통과시킵니다.\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        # 분류를 위해 첫 번째 토큰 (클래스 토큰)만 사용합니다.\n",
    "        x = x[:, 0]\n",
    "\n",
    "        return self.mlp_head(x)\n",
    "\n",
    "# 예시 사용\n",
    "vit = ViT(image_size=256, patch_size=32, num_classes=10, dim=1024, depth=6, heads=8, mlp_dim=2048, dropout_rate=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
