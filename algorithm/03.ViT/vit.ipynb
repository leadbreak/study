{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "num_workers = 8\n",
    "\n",
    "model_name = 'vit_base_patch16_224'\n",
    "pretrained = False\n",
    "num_classes = 10\n",
    "\n",
    "device_gpu = 'cuda:4'\n",
    "\n",
    "label_smoothing = 0.1\n",
    "learning_rate = 0.001\n",
    "epochs = 20\n",
    "\n",
    "model_path = 'best_model3.pth'  # 모델 저장 경로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 데이터 증강을 위한 전처리\n",
    "# transform_train = transforms.Compose([\n",
    "#     transforms.RandomResizedCrop(224),  # 무작위 크기 및 비율로 자르기\n",
    "#     transforms.RandomHorizontalFlip(),  # 50% 확률로 수평 뒤집기\n",
    "#     transforms.RandomRotation(15),      # -15도에서 15도 사이로 무작위 회전\n",
    "#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # 색상 변경\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "# ])\n",
    "\n",
    "# transform_test = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "# ])\n",
    "\n",
    "# trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "# trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "# testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "# testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # 이미지 크기 조정\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(196, 40)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainloader), len(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 768, 14, 14]         590,592\n",
      "          Identity-2             [-1, 196, 768]               0\n",
      "        PatchEmbed-3             [-1, 196, 768]               0\n",
      "           Dropout-4             [-1, 197, 768]               0\n",
      "          Identity-5             [-1, 197, 768]               0\n",
      "          Identity-6             [-1, 197, 768]               0\n",
      "         LayerNorm-7             [-1, 197, 768]           1,536\n",
      "            Linear-8            [-1, 197, 2304]       1,771,776\n",
      "          Identity-9          [-1, 12, 197, 64]               0\n",
      "         Identity-10          [-1, 12, 197, 64]               0\n",
      "           Linear-11             [-1, 197, 768]         590,592\n",
      "          Dropout-12             [-1, 197, 768]               0\n",
      "        Attention-13             [-1, 197, 768]               0\n",
      "         Identity-14             [-1, 197, 768]               0\n",
      "         Identity-15             [-1, 197, 768]               0\n",
      "        LayerNorm-16             [-1, 197, 768]           1,536\n",
      "           Linear-17            [-1, 197, 3072]       2,362,368\n",
      "             GELU-18            [-1, 197, 3072]               0\n",
      "          Dropout-19            [-1, 197, 3072]               0\n",
      "         Identity-20            [-1, 197, 3072]               0\n",
      "           Linear-21             [-1, 197, 768]       2,360,064\n",
      "          Dropout-22             [-1, 197, 768]               0\n",
      "              Mlp-23             [-1, 197, 768]               0\n",
      "         Identity-24             [-1, 197, 768]               0\n",
      "         Identity-25             [-1, 197, 768]               0\n",
      "            Block-26             [-1, 197, 768]               0\n",
      "        LayerNorm-27             [-1, 197, 768]           1,536\n",
      "           Linear-28            [-1, 197, 2304]       1,771,776\n",
      "         Identity-29          [-1, 12, 197, 64]               0\n",
      "         Identity-30          [-1, 12, 197, 64]               0\n",
      "           Linear-31             [-1, 197, 768]         590,592\n",
      "          Dropout-32             [-1, 197, 768]               0\n",
      "        Attention-33             [-1, 197, 768]               0\n",
      "         Identity-34             [-1, 197, 768]               0\n",
      "         Identity-35             [-1, 197, 768]               0\n",
      "        LayerNorm-36             [-1, 197, 768]           1,536\n",
      "           Linear-37            [-1, 197, 3072]       2,362,368\n",
      "             GELU-38            [-1, 197, 3072]               0\n",
      "          Dropout-39            [-1, 197, 3072]               0\n",
      "         Identity-40            [-1, 197, 3072]               0\n",
      "           Linear-41             [-1, 197, 768]       2,360,064\n",
      "          Dropout-42             [-1, 197, 768]               0\n",
      "              Mlp-43             [-1, 197, 768]               0\n",
      "         Identity-44             [-1, 197, 768]               0\n",
      "         Identity-45             [-1, 197, 768]               0\n",
      "            Block-46             [-1, 197, 768]               0\n",
      "        LayerNorm-47             [-1, 197, 768]           1,536\n",
      "           Linear-48            [-1, 197, 2304]       1,771,776\n",
      "         Identity-49          [-1, 12, 197, 64]               0\n",
      "         Identity-50          [-1, 12, 197, 64]               0\n",
      "           Linear-51             [-1, 197, 768]         590,592\n",
      "          Dropout-52             [-1, 197, 768]               0\n",
      "        Attention-53             [-1, 197, 768]               0\n",
      "         Identity-54             [-1, 197, 768]               0\n",
      "         Identity-55             [-1, 197, 768]               0\n",
      "        LayerNorm-56             [-1, 197, 768]           1,536\n",
      "           Linear-57            [-1, 197, 3072]       2,362,368\n",
      "             GELU-58            [-1, 197, 3072]               0\n",
      "          Dropout-59            [-1, 197, 3072]               0\n",
      "         Identity-60            [-1, 197, 3072]               0\n",
      "           Linear-61             [-1, 197, 768]       2,360,064\n",
      "          Dropout-62             [-1, 197, 768]               0\n",
      "              Mlp-63             [-1, 197, 768]               0\n",
      "         Identity-64             [-1, 197, 768]               0\n",
      "         Identity-65             [-1, 197, 768]               0\n",
      "            Block-66             [-1, 197, 768]               0\n",
      "        LayerNorm-67             [-1, 197, 768]           1,536\n",
      "           Linear-68            [-1, 197, 2304]       1,771,776\n",
      "         Identity-69          [-1, 12, 197, 64]               0\n",
      "         Identity-70          [-1, 12, 197, 64]               0\n",
      "           Linear-71             [-1, 197, 768]         590,592\n",
      "          Dropout-72             [-1, 197, 768]               0\n",
      "        Attention-73             [-1, 197, 768]               0\n",
      "         Identity-74             [-1, 197, 768]               0\n",
      "         Identity-75             [-1, 197, 768]               0\n",
      "        LayerNorm-76             [-1, 197, 768]           1,536\n",
      "           Linear-77            [-1, 197, 3072]       2,362,368\n",
      "             GELU-78            [-1, 197, 3072]               0\n",
      "          Dropout-79            [-1, 197, 3072]               0\n",
      "         Identity-80            [-1, 197, 3072]               0\n",
      "           Linear-81             [-1, 197, 768]       2,360,064\n",
      "          Dropout-82             [-1, 197, 768]               0\n",
      "              Mlp-83             [-1, 197, 768]               0\n",
      "         Identity-84             [-1, 197, 768]               0\n",
      "         Identity-85             [-1, 197, 768]               0\n",
      "            Block-86             [-1, 197, 768]               0\n",
      "        LayerNorm-87             [-1, 197, 768]           1,536\n",
      "           Linear-88            [-1, 197, 2304]       1,771,776\n",
      "         Identity-89          [-1, 12, 197, 64]               0\n",
      "         Identity-90          [-1, 12, 197, 64]               0\n",
      "           Linear-91             [-1, 197, 768]         590,592\n",
      "          Dropout-92             [-1, 197, 768]               0\n",
      "        Attention-93             [-1, 197, 768]               0\n",
      "         Identity-94             [-1, 197, 768]               0\n",
      "         Identity-95             [-1, 197, 768]               0\n",
      "        LayerNorm-96             [-1, 197, 768]           1,536\n",
      "           Linear-97            [-1, 197, 3072]       2,362,368\n",
      "             GELU-98            [-1, 197, 3072]               0\n",
      "          Dropout-99            [-1, 197, 3072]               0\n",
      "        Identity-100            [-1, 197, 3072]               0\n",
      "          Linear-101             [-1, 197, 768]       2,360,064\n",
      "         Dropout-102             [-1, 197, 768]               0\n",
      "             Mlp-103             [-1, 197, 768]               0\n",
      "        Identity-104             [-1, 197, 768]               0\n",
      "        Identity-105             [-1, 197, 768]               0\n",
      "           Block-106             [-1, 197, 768]               0\n",
      "       LayerNorm-107             [-1, 197, 768]           1,536\n",
      "          Linear-108            [-1, 197, 2304]       1,771,776\n",
      "        Identity-109          [-1, 12, 197, 64]               0\n",
      "        Identity-110          [-1, 12, 197, 64]               0\n",
      "          Linear-111             [-1, 197, 768]         590,592\n",
      "         Dropout-112             [-1, 197, 768]               0\n",
      "       Attention-113             [-1, 197, 768]               0\n",
      "        Identity-114             [-1, 197, 768]               0\n",
      "        Identity-115             [-1, 197, 768]               0\n",
      "       LayerNorm-116             [-1, 197, 768]           1,536\n",
      "          Linear-117            [-1, 197, 3072]       2,362,368\n",
      "            GELU-118            [-1, 197, 3072]               0\n",
      "         Dropout-119            [-1, 197, 3072]               0\n",
      "        Identity-120            [-1, 197, 3072]               0\n",
      "          Linear-121             [-1, 197, 768]       2,360,064\n",
      "         Dropout-122             [-1, 197, 768]               0\n",
      "             Mlp-123             [-1, 197, 768]               0\n",
      "        Identity-124             [-1, 197, 768]               0\n",
      "        Identity-125             [-1, 197, 768]               0\n",
      "           Block-126             [-1, 197, 768]               0\n",
      "       LayerNorm-127             [-1, 197, 768]           1,536\n",
      "          Linear-128            [-1, 197, 2304]       1,771,776\n",
      "        Identity-129          [-1, 12, 197, 64]               0\n",
      "        Identity-130          [-1, 12, 197, 64]               0\n",
      "          Linear-131             [-1, 197, 768]         590,592\n",
      "         Dropout-132             [-1, 197, 768]               0\n",
      "       Attention-133             [-1, 197, 768]               0\n",
      "        Identity-134             [-1, 197, 768]               0\n",
      "        Identity-135             [-1, 197, 768]               0\n",
      "       LayerNorm-136             [-1, 197, 768]           1,536\n",
      "          Linear-137            [-1, 197, 3072]       2,362,368\n",
      "            GELU-138            [-1, 197, 3072]               0\n",
      "         Dropout-139            [-1, 197, 3072]               0\n",
      "        Identity-140            [-1, 197, 3072]               0\n",
      "          Linear-141             [-1, 197, 768]       2,360,064\n",
      "         Dropout-142             [-1, 197, 768]               0\n",
      "             Mlp-143             [-1, 197, 768]               0\n",
      "        Identity-144             [-1, 197, 768]               0\n",
      "        Identity-145             [-1, 197, 768]               0\n",
      "           Block-146             [-1, 197, 768]               0\n",
      "       LayerNorm-147             [-1, 197, 768]           1,536\n",
      "          Linear-148            [-1, 197, 2304]       1,771,776\n",
      "        Identity-149          [-1, 12, 197, 64]               0\n",
      "        Identity-150          [-1, 12, 197, 64]               0\n",
      "          Linear-151             [-1, 197, 768]         590,592\n",
      "         Dropout-152             [-1, 197, 768]               0\n",
      "       Attention-153             [-1, 197, 768]               0\n",
      "        Identity-154             [-1, 197, 768]               0\n",
      "        Identity-155             [-1, 197, 768]               0\n",
      "       LayerNorm-156             [-1, 197, 768]           1,536\n",
      "          Linear-157            [-1, 197, 3072]       2,362,368\n",
      "            GELU-158            [-1, 197, 3072]               0\n",
      "         Dropout-159            [-1, 197, 3072]               0\n",
      "        Identity-160            [-1, 197, 3072]               0\n",
      "          Linear-161             [-1, 197, 768]       2,360,064\n",
      "         Dropout-162             [-1, 197, 768]               0\n",
      "             Mlp-163             [-1, 197, 768]               0\n",
      "        Identity-164             [-1, 197, 768]               0\n",
      "        Identity-165             [-1, 197, 768]               0\n",
      "           Block-166             [-1, 197, 768]               0\n",
      "       LayerNorm-167             [-1, 197, 768]           1,536\n",
      "          Linear-168            [-1, 197, 2304]       1,771,776\n",
      "        Identity-169          [-1, 12, 197, 64]               0\n",
      "        Identity-170          [-1, 12, 197, 64]               0\n",
      "          Linear-171             [-1, 197, 768]         590,592\n",
      "         Dropout-172             [-1, 197, 768]               0\n",
      "       Attention-173             [-1, 197, 768]               0\n",
      "        Identity-174             [-1, 197, 768]               0\n",
      "        Identity-175             [-1, 197, 768]               0\n",
      "       LayerNorm-176             [-1, 197, 768]           1,536\n",
      "          Linear-177            [-1, 197, 3072]       2,362,368\n",
      "            GELU-178            [-1, 197, 3072]               0\n",
      "         Dropout-179            [-1, 197, 3072]               0\n",
      "        Identity-180            [-1, 197, 3072]               0\n",
      "          Linear-181             [-1, 197, 768]       2,360,064\n",
      "         Dropout-182             [-1, 197, 768]               0\n",
      "             Mlp-183             [-1, 197, 768]               0\n",
      "        Identity-184             [-1, 197, 768]               0\n",
      "        Identity-185             [-1, 197, 768]               0\n",
      "           Block-186             [-1, 197, 768]               0\n",
      "       LayerNorm-187             [-1, 197, 768]           1,536\n",
      "          Linear-188            [-1, 197, 2304]       1,771,776\n",
      "        Identity-189          [-1, 12, 197, 64]               0\n",
      "        Identity-190          [-1, 12, 197, 64]               0\n",
      "          Linear-191             [-1, 197, 768]         590,592\n",
      "         Dropout-192             [-1, 197, 768]               0\n",
      "       Attention-193             [-1, 197, 768]               0\n",
      "        Identity-194             [-1, 197, 768]               0\n",
      "        Identity-195             [-1, 197, 768]               0\n",
      "       LayerNorm-196             [-1, 197, 768]           1,536\n",
      "          Linear-197            [-1, 197, 3072]       2,362,368\n",
      "            GELU-198            [-1, 197, 3072]               0\n",
      "         Dropout-199            [-1, 197, 3072]               0\n",
      "        Identity-200            [-1, 197, 3072]               0\n",
      "          Linear-201             [-1, 197, 768]       2,360,064\n",
      "         Dropout-202             [-1, 197, 768]               0\n",
      "             Mlp-203             [-1, 197, 768]               0\n",
      "        Identity-204             [-1, 197, 768]               0\n",
      "        Identity-205             [-1, 197, 768]               0\n",
      "           Block-206             [-1, 197, 768]               0\n",
      "       LayerNorm-207             [-1, 197, 768]           1,536\n",
      "          Linear-208            [-1, 197, 2304]       1,771,776\n",
      "        Identity-209          [-1, 12, 197, 64]               0\n",
      "        Identity-210          [-1, 12, 197, 64]               0\n",
      "          Linear-211             [-1, 197, 768]         590,592\n",
      "         Dropout-212             [-1, 197, 768]               0\n",
      "       Attention-213             [-1, 197, 768]               0\n",
      "        Identity-214             [-1, 197, 768]               0\n",
      "        Identity-215             [-1, 197, 768]               0\n",
      "       LayerNorm-216             [-1, 197, 768]           1,536\n",
      "          Linear-217            [-1, 197, 3072]       2,362,368\n",
      "            GELU-218            [-1, 197, 3072]               0\n",
      "         Dropout-219            [-1, 197, 3072]               0\n",
      "        Identity-220            [-1, 197, 3072]               0\n",
      "          Linear-221             [-1, 197, 768]       2,360,064\n",
      "         Dropout-222             [-1, 197, 768]               0\n",
      "             Mlp-223             [-1, 197, 768]               0\n",
      "        Identity-224             [-1, 197, 768]               0\n",
      "        Identity-225             [-1, 197, 768]               0\n",
      "           Block-226             [-1, 197, 768]               0\n",
      "       LayerNorm-227             [-1, 197, 768]           1,536\n",
      "          Linear-228            [-1, 197, 2304]       1,771,776\n",
      "        Identity-229          [-1, 12, 197, 64]               0\n",
      "        Identity-230          [-1, 12, 197, 64]               0\n",
      "          Linear-231             [-1, 197, 768]         590,592\n",
      "         Dropout-232             [-1, 197, 768]               0\n",
      "       Attention-233             [-1, 197, 768]               0\n",
      "        Identity-234             [-1, 197, 768]               0\n",
      "        Identity-235             [-1, 197, 768]               0\n",
      "       LayerNorm-236             [-1, 197, 768]           1,536\n",
      "          Linear-237            [-1, 197, 3072]       2,362,368\n",
      "            GELU-238            [-1, 197, 3072]               0\n",
      "         Dropout-239            [-1, 197, 3072]               0\n",
      "        Identity-240            [-1, 197, 3072]               0\n",
      "          Linear-241             [-1, 197, 768]       2,360,064\n",
      "         Dropout-242             [-1, 197, 768]               0\n",
      "             Mlp-243             [-1, 197, 768]               0\n",
      "        Identity-244             [-1, 197, 768]               0\n",
      "        Identity-245             [-1, 197, 768]               0\n",
      "           Block-246             [-1, 197, 768]               0\n",
      "       LayerNorm-247             [-1, 197, 768]           1,536\n",
      "        Identity-248                  [-1, 768]               0\n",
      "         Dropout-249                  [-1, 768]               0\n",
      "          Linear-250                   [-1, 10]           7,690\n",
      "================================================================\n",
      "Total params: 85,654,282\n",
      "Trainable params: 85,654,282\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 479.03\n",
      "Params size (MB): 326.75\n",
      "Estimated Total Size (MB): 806.35\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "model = timm.create_model(model_name=model_name, \n",
    "                          pretrained=pretrained, \n",
    "                          num_classes=num_classes)\n",
    "\n",
    "device = torch.device(device_gpu if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "summary(model.to('cuda'), (3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (patch_drop): Identity()\n",
       "  (norm_pre): Identity()\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (fc_norm): Identity()\n",
       "  (head_drop): Dropout(p=0.0, inplace=False)\n",
       "  (head): Linear(in_features=768, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion1 = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
    "criterion2 = nn.CrossEntropyLoss(label_smoothing=0.0)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "# scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, max_lr=learning_rate*2, total_steps=epochs*len(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss == None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/196 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 196/196 [05:30<00:00,  1.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.9136067409904634, Val Loss: 2.885424870252609, Duration: 354.37 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  15%|█▌        | 30/196 [00:53<04:55,  1.78s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/root/study/algorithm/03.ViT/vit.ipynb 셀 8\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bqscar:192.168.10.110/root/study/algorithm/03.ViT/vit.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bqscar:192.168.10.110/root/study/algorithm/03.ViT/vit.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m     scheduler\u001b[39m.\u001b[39mstep()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bqscar:192.168.10.110/root/study/algorithm/03.ViT/vit.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m     running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bqscar:192.168.10.110/root/study/algorithm/03.ViT/vit.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m     lrs\u001b[39m.\u001b[39mappend(optimizer\u001b[39m.\u001b[39mparam_groups[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bqscar:192.168.10.110/root/study/algorithm/03.ViT/vit.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m epoch_loss \u001b[39m=\u001b[39m running_loss \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(trainloader)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "training_time = 0\n",
    "early_stopping = EarlyStopping(patience=5)\n",
    "losses = []\n",
    "val_losses = []\n",
    "lrs = []\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "model_save = False\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(enumerate(trainloader), total=len(trainloader), desc=f\"Epoch {epoch + 1}\")\n",
    "    for i, data in pbar:\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        running_loss += loss.item()\n",
    "        lrs.append(optimizer.param_groups[0][\"lr\"])\n",
    "    epoch_loss = running_loss / len(trainloader)\n",
    "    losses.append(epoch_loss)\n",
    "\n",
    "    # 검증 손실 계산\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(testloader)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    # 모델 저장\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        model_save = True\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_duration = time.time() - start_time\n",
    "    training_time += epoch_duration\n",
    "    if model_save:\n",
    "        print(f'Epoch {epoch + 1}, Loss: {epoch_loss}, Val Loss: {val_loss}, Duration: {epoch_duration:.2f} sec - model saved!')\n",
    "        model_save = False\n",
    "    else :\n",
    "        print(f'Epoch {epoch + 1}, Loss: {epoch_loss}, Val Loss: {val_loss}, Duration: {epoch_duration:.2f} sec')\n",
    "\n",
    "    # Early Stopping 체크\n",
    "    early_stopping(val_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "# 학습 및 검증 손실 시각화\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.title('Training & Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(lrs, label='Learning Rate')\n",
    "plt.title('Learning Rate')\n",
    "plt.xlabel('Batch number')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 모델 및 옵티마이저 설정\n",
    "model = torch.nn.Linear(10, 2)  # 예시 모델\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 총 스텝 수\n",
    "total_steps = 100\n",
    "\n",
    "# 코사인 학습률 감소 스케줄러 설정\n",
    "scheduler_cosine = CosineAnnealingLR(optimizer, T_max=total_steps//2)\n",
    "\n",
    "# 학습률 감소 추적을 위한 리스트\n",
    "lrs_cosine = []\n",
    "\n",
    "# 코사인 스케줄러에 대해 학습률 기록\n",
    "for step in range(total_steps):\n",
    "    optimizer.step()\n",
    "    lrs_cosine.append(scheduler_cosine.get_last_lr()[0])\n",
    "    scheduler_cosine.step()\n",
    "\n",
    "# 그래프 그리기\n",
    "steps = np.arange(total_steps)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(steps, lrs_cosine, label='Cosine Decay')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Cosine Learning Rate Decay Schedule')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "import math\n",
    "\n",
    "class WarmupCosineAnnealingLR(_LRScheduler):\n",
    "    def __init__(self, optimizer, warmup_steps, total_steps, last_epoch=-1):\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.total_steps = total_steps\n",
    "        super(WarmupCosineAnnealingLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch < self.warmup_steps:\n",
    "            return [base_lr * self.last_epoch / self.warmup_steps for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            cosine_decay = 0.5 * (1 + math.cos(math.pi * (self.last_epoch - self.warmup_steps) / (self.total_steps - self.warmup_steps)))\n",
    "            return [base_lr * cosine_decay for base_lr in self.base_lrs]\n",
    "\n",
    "# 모델 및 옵티마이저 설정\n",
    "model = torch.nn.Linear(10, 2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "# 스케줄러 초기화\n",
    "total_steps = 100\n",
    "warmup_steps = 10  # 예시로 10 스텝의 warmup 설정\n",
    "scheduler = WarmupCosineAnnealingLR(optimizer, warmup_steps, total_steps)\n",
    "\n",
    "# 학습률 기록 및 업데이트\n",
    "lrs = []\n",
    "for epoch in range(total_steps):\n",
    "    optimizer.step()\n",
    "    lrs.append(scheduler.get_last_lr())\n",
    "    scheduler.step()\n",
    "\n",
    "# 그래프 그리기\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "steps = np.arange(total_steps)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(steps, lrs, label='Warmup with Cosine Decay')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Learning Rate Schedule: Warmup with Cosine Decay')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
