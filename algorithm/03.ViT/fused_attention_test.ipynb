{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case1: 일반적인 경우(without fused attention & amp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|                                           | 0/37 [00:01<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/study/algorithm/03.ViT/model/train.py\", line 165, in <module>\n",
      "    main()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1157, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1078, in main\n",
      "    rv = self.invoke(ctx)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1434, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 783, in invoke\n",
      "    return __callback(*args, **kwargs)\n",
      "  File \"/root/study/algorithm/03.ViT/model/train.py\", line 120, in main\n",
      "    outputs = model(inputs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/root/study/algorithm/03.ViT/model/vit_better.py\", line 264, in forward\n",
      "    x = layer(x)  # 각 Transformer Encoder 레이어 적용\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/root/study/algorithm/03.ViT/model/vit_better.py\", line 192, in forward\n",
      "    x = x + self.drop_path(self.ls1(self.attn(x)))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/root/study/algorithm/03.ViT/model/vit_better.py\", line 101, in forward\n",
      "    q, k = self.q_norm(q), self.k_norm(k) # Norm\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/normalization.py\", line 196, in forward\n",
      "    return F.layer_norm(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 2543, in layer_norm\n",
      "    return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 232.00 MiB. GPU 5 has a total capacty of 79.15 GiB of which 26.19 MiB is free. Process 2739290 has 79.11 GiB memory in use. Of the allocated memory 77.96 GiB is allocated by PyTorch, and 668.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "!cd model && python3 train.py --epochs 3 --device cuda:5 --fused_attention False --batch_size 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████████████████████████████| 57/57 [02:17<00:00,  2.41s/it]\n",
      "\tLoss: 1.1457473593845702, Val Loss: 1.0618762530778583, LR: 0.001, Duration: 186.06 sec - model saved!\n",
      "Epoch 2: 100%|██████████████████████████████████| 57/57 [02:16<00:00,  2.39s/it]\n",
      "\tLoss: 1.0479048019961308, Val Loss: 1.0600337699839943, LR: 0.001, Duration: 185.55 sec - model saved!\n",
      "Epoch 3: 100%|██████████████████████████████████| 57/57 [02:16<00:00,  2.40s/it]\n",
      "\tLoss: 1.0435521278465003, Val Loss: 1.059691859964739, LR: 0.001, Duration: 187.27 sec - model saved!\n",
      "Epoch 당 평균 소요시간 : 186.29초\n"
     ]
    }
   ],
   "source": [
    "!cd model && python3 train.py --epochs 3 --device cuda:5 --fused_attention False --batch_size 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### case2: fused attention without amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|                                           | 0/29 [00:01<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/study/algorithm/03.ViT/model/train.py\", line 165, in <module>\n",
      "    main()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1157, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1078, in main\n",
      "    rv = self.invoke(ctx)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1434, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 783, in invoke\n",
      "    return __callback(*args, **kwargs)\n",
      "  File \"/root/study/algorithm/03.ViT/model/train.py\", line 120, in main\n",
      "    outputs = model(inputs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/root/study/algorithm/03.ViT/model/vit_better.py\", line 264, in forward\n",
      "    x = layer(x)  # 각 Transformer Encoder 레이어 적용\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/root/study/algorithm/03.ViT/model/vit_better.py\", line 197, in forward\n",
      "    x = x + self.drop_path(self.ls2(self.mlp(x2)))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.15 GiB. GPU 5 has a total capacty of 79.15 GiB of which 430.19 MiB is free. Process 2743736 has 78.72 GiB memory in use. Of the allocated memory 78.15 GiB is allocated by PyTorch, and 69.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "!cd model && python3 train.py --epochs 3 --device cuda:5 --fused_attention True --batch_size 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████████████████████████████| 37/37 [02:24<00:00,  3.90s/it]\n",
      "\tLoss: 1.2121397707913373, Val Loss: 1.1046851518991831, LR: 0.001, Duration: 197.32 sec - model saved!\n",
      "Epoch 2: 100%|██████████████████████████████████| 37/37 [02:19<00:00,  3.78s/it]\n",
      "\tLoss: 1.0434243936796446, Val Loss: 1.0968107919435244, LR: 0.001, Duration: 187.50 sec - model saved!\n",
      "Epoch 3: 100%|██████████████████████████████████| 37/37 [02:14<00:00,  3.62s/it]\n",
      "\tLoss: 1.045310196038839, Val Loss: 1.0994582756145581, LR: 0.001, Duration: 181.45 sec\n",
      "\u001b[36m\u001b[1mEpoch 당 평균 소요시간 : 188.76초\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!cd model && python3 train.py --epochs 3 --device cuda:5 --fused_attention True --batch_size 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████████████████████████████| 57/57 [02:16<00:00,  2.40s/it]\n",
      "\tLoss: 1.1571151185453983, Val Loss: 1.0732111021092063, LR: 0.001, Duration: 187.78 sec - model saved!\n",
      "Epoch 2: 100%|██████████████████████████████████| 57/57 [02:15<00:00,  2.38s/it]\n",
      "\tLoss: 1.0437934534591542, Val Loss: 1.0585989000504477, LR: 0.001, Duration: 185.10 sec - model saved!\n",
      "Epoch 3: 100%|██████████████████████████████████| 57/57 [02:24<00:00,  2.54s/it]\n",
      "\tLoss: 1.0442616667663842, Val Loss: 1.0574433312081455, LR: 0.001, Duration: 199.40 sec - model saved!\n",
      "Epoch 당 평균 소요시간 : 190.76초\n"
     ]
    }
   ],
   "source": [
    "!cd model && python3 train.py --epochs 3 --device cuda:5 --fused_attention True --batch_size 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### case3: amp without fused attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|                                           | 0/29 [00:01<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/study/algorithm/03.ViT/model/train_better.py\", line 174, in <module>\n",
      "    main()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1157, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1078, in main\n",
      "    rv = self.invoke(ctx)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1434, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 783, in invoke\n",
      "    return __callback(*args, **kwargs)\n",
      "  File \"/root/study/algorithm/03.ViT/model/train_better.py\", line 126, in main\n",
      "    outputs = model(inputs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/root/study/algorithm/03.ViT/model/vit_better.py\", line 264, in forward\n",
      "    x = layer(x)  # 각 Transformer Encoder 레이어 적용\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/root/study/algorithm/03.ViT/model/vit_better.py\", line 192, in forward\n",
      "    x = x + self.drop_path(self.ls1(self.attn(x)))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/root/study/algorithm/03.ViT/model/vit_better.py\", line 113, in forward\n",
      "    attn = self.attn_drop(attn)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/dropout.py\", line 58, in forward\n",
      "    return F.dropout(input, self.p, self.training, self.inplace)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 1266, in dropout\n",
      "    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 910.00 MiB. GPU 5 has a total capacty of 79.15 GiB of which 342.19 MiB is free. Process 2733207 has 78.80 GiB memory in use. Of the allocated memory 77.91 GiB is allocated by PyTorch, and 394.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "!cd model && python3 train_better.py --epochs 3 --device cuda:5 --fused_attention False --batch_size 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|                                           | 0/37 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "Epoch 1: 100%|██████████████████████████████████| 37/37 [01:09<00:00,  1.88s/it]\n",
      "\tLoss: 3.0528077048224374, Val Loss: 1.1772976549896035, LR: 0.001, Duration: 127.30 sec - model saved!\n",
      "Epoch 2: 100%|██████████████████████████████████| 37/37 [01:09<00:00,  1.88s/it]\n",
      "\tLoss: 1.0705030963227555, Val Loss: 1.1126448463749241, LR: 0.001, Duration: 126.60 sec - model saved!\n",
      "Epoch 3: 100%|██████████████████████████████████| 37/37 [01:11<00:00,  1.94s/it]\n",
      "\tLoss: 1.1106478813532237, Val Loss: 1.1457491839254224, LR: 0.001, Duration: 128.79 sec\n",
      "\u001b[36m\u001b[1mEpoch 당 평균 소요시간 : 127.57초\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!cd model && python3 train_better.py --epochs 3 --device cuda:5 --fused_attention False --batch_size 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### case4: both(fused attention & amp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|                                           | 0/29 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "Epoch 1: 100%|██████████████████████████████████| 29/29 [01:04<00:00,  2.22s/it]\n",
      "\tLoss: 3.322332299988845, Val Loss: 1.1255637900582676, LR: 0.001, Duration: 118.36 sec - model saved!\n",
      "Epoch 2: 100%|██████████████████████████████████| 29/29 [01:03<00:00,  2.18s/it]\n",
      "\tLoss: 1.0643817350782196, Val Loss: 1.1067666966339638, LR: 0.001, Duration: 119.32 sec - model saved!\n",
      "Epoch 3: 100%|██████████████████████████████████| 29/29 [01:03<00:00,  2.19s/it]\n",
      "\tLoss: 1.251329701522301, Val Loss: 1.262545844604229, LR: 0.001, Duration: 120.67 sec\n",
      "\u001b[36m\u001b[1mEpoch 당 평균 소요시간 : 119.45초\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!cd model && python3 train_better.py --epochs 3 --device cuda:5 --fused_attention True --batch_size 512"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
