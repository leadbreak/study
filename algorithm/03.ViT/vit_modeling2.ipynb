{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    이미지를 패치로 분할하고, 각 패치를 임베딩하는 클래스입니다.\n",
    "\n",
    "    Attributes:\n",
    "    patch_size (int): 패치의 크기 (예: 16x16).\n",
    "    n_patches (int): 이미지당 생성되는 패치의 총 수.\n",
    "    projection (nn.Conv2d): 패치를 임베딩 벡터로 변환하는 컨볼루션 레이어.\n",
    "\n",
    "    Args:\n",
    "    img_size (int): 입력 이미지의 크기 (예: 32x32).\n",
    "    patch_size (int): 패치의 크기 (예: 16x16).\n",
    "    in_channels (int): 입력 이미지의 채널 수 (RGB의 경우 3).\n",
    "    embed_dim (int): 임베딩 차원의 크기.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size:int=32, patch_size:int=2, in_channels:int=3, embed_dim:int=768):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Patch 정보 인식\n",
    "        self.patch_size = patch_size\n",
    "        assert img_size % patch_size == 0, f'img size({img_size})는 patch size({patch_size})로 나뉘어야 합니다.'\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        # 컨볼루션을 사용하여 패치를 임베딩 벡터로 변환\n",
    "        self.projection = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        \n",
    "        # class token 추가\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1,1,embed_dim))        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [배치 크기, 채널 수, 높이, 너비]\n",
    "        x = self.projection(x)  # 컨볼루션을 통한 임베딩: [배치 크기, 임베딩 차원, 패치 수, _]\n",
    "        x = x.flatten(2)        # 평탄화: [배치 크기, 임베딩 차원, 패치 수]\n",
    "        x = x.transpose(1, 2)   # 변환: [배치 크기, 패치 수, 임베딩 차원]\n",
    "        \n",
    "        cls_tokens = self.cls_token.expand(x.size(0), -1, -1) # [배치크기, 1, 임베딩 차원]\n",
    "        x = torch.cat((cls_tokens, x), dim=1) # cls_token 추가: [배치크기, 패치 수+1, 임베딩 차원]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    위치 임베딩을 추가하는 클래스입니다. 각 패치에 대한 위치 정보를 제공합니다.\n",
    "\n",
    "    Attributes:\n",
    "    scale (torch.Tensor): 스케일링 펙터.\n",
    "    position_embedding (torch.nn.Parameter): 학습 가능한 위치 인코딩.\n",
    "\n",
    "    Args:\n",
    "    num_patches (int): 이미지당 생성되는 패치의 수.\n",
    "    embed_dim (int): 임베딩 차원의 크기.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_patches:int, embed_dim:int):\n",
    "        super().__init__()\n",
    "        self.scale = torch.sqrt(torch.tensor(embed_dim, dtype=torch.float32))\n",
    "        self.position_embedding = nn.Parameter(torch.zeros(1, num_patches+1, embed_dim)) # [1, 패치 수+1, 임베딩 차원]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.scale*x + self.position_embedding # scaled x에 위치 정보를 임베딩에 더함 \n",
    "        # x += self.position_embedding  \n",
    "        return x # [배치 크기, 패치 수+1, 임베딩 차원]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터 수 비교를 위해 가져온 이전 구현체\n",
    "class MHA(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        assert d_model % n_heads == 0, f'd_model ({d_model})은 n_heads ({n_heads})로 나누어 떨어져야 합니다.'\n",
    "\n",
    "        self.head_dim = d_model // n_heads  # int 형변환 제거\n",
    "\n",
    "        # 쿼리, 키, 값에 대한 선형 변환\n",
    "        self.fc_q = nn.Linear(d_model, d_model) \n",
    "        self.fc_k = nn.Linear(d_model, d_model) \n",
    "        self.fc_v = nn.Linear(d_model, d_model)\n",
    "        self.fc_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # 어텐션 점수를 위한 스케일 요소\n",
    "        self.scale = torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))\n",
    "        \n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        batch_size = Q.shape[0]\n",
    "\n",
    "        # 쿼리, 키, 값에 대한 선형 변환 수행\n",
    "        Q = self.fc_q(Q) \n",
    "        K = self.fc_k(K)\n",
    "        V = self.fc_v(V)\n",
    "\n",
    "        # 멀티 헤드 어텐션을 위해 텐서 재구성 및 순서 변경\n",
    "        Q = Q.reshape(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.reshape(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.reshape(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "        # 스케일드 닷-프로덕트 어텐션 계산\n",
    "        attention_score = Q @ K.permute(0, 1, 3, 2) / self.scale\n",
    "\n",
    "        # 소프트맥스를 사용하여 어텐션 확률 계산\n",
    "        attention_dist = torch.softmax(attention_score, dim=-1)\n",
    "\n",
    "        # 어텐션 결과\n",
    "        attention = attention_dist @ V\n",
    "\n",
    "        # 어텐션 헤드 재조립\n",
    "        x = attention.permute(0, 2, 1, 3).reshape(batch_size, -1, self.d_model)\n",
    "\n",
    "        # 최종 선형 변환\n",
    "        x = self.fc_o(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer Encoder 레이어를 정의하는 클래스입니다.\n",
    "\n",
    "    Attributes:\n",
    "    norm1, norm2 (nn.LayerNorm): 정규화 레이어.\n",
    "    attn (nn.MultiheadAttention): 멀티헤드 어텐션 레이어.\n",
    "    mlp (nn.Sequential): 피드포워드 네트워크.\n",
    "\n",
    "    Args:\n",
    "    embed_dim (int): 임베딩 차원의 크기.\n",
    "    num_heads (int): 멀티헤드 어텐션에서의 헤드 수.\n",
    "    mlp_ratio (float): 첫 번째 선형 레이어의 출력 차원을 결정하는 비율.\n",
    "    dropout (float): 드롭아웃 비율.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim:int, num_heads:int, mlp_ratio:float=4.0, dropout:float=0.1, estimate_params:bool=False):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.estimate_params = estimate_params\n",
    "        if estimate_params:\n",
    "            self.attn = MHA(embed_dim, num_heads)\n",
    "        else :\n",
    "            self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout) # [attention_output, attention weights]        \n",
    "\n",
    "        mlp_hidden_dim = int(mlp_ratio * embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_hidden_dim, embed_dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 멀티헤드 어텐션과 피드포워드 네트워크를 적용\n",
    "        if self.estimate_params:\n",
    "            x = self.norm(x)\n",
    "            x = x + self.attn(x, x, x)\n",
    "        else :\n",
    "            x = x + self.attn(x, x, x)[0] # attention output만 사용\n",
    "        \n",
    "        x2 = self.norm(x)\n",
    "        x = x + self.mlp(x2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    전체 Vision Transformer 모델을 정의하는 클래스입니다.\n",
    "\n",
    "    Attributes:\n",
    "    patch_embed (PatchEmbedding): 이미지를 패치로 분할하고 임베딩하는 레이어.\n",
    "    pos_embed (PositionalEncoding): 위치 인코딩 레이어.\n",
    "    transformer_encoders (nn.ModuleList): Transformer Encoder 레이어들의 리스트.\n",
    "    norm (nn.LayerNorm): 정규화 레이어.\n",
    "    head (nn.Linear): 최종 분류를 위한 선형 레이어.\n",
    "\n",
    "    Args:\n",
    "    img_size (int): 입력 이미지의 크기 (예: 32x32).\n",
    "    patch_size (int): 패치의 크기 (예: 16x16).\n",
    "    in_channels (int): 입력 이미지의 채널 수 (RGB의 경우 3).\n",
    "    num_classes (int): 분류할 클래스의 수 (CIFAR-10의 경우 10).\n",
    "    embed_dim (int): 임베딩 차원의 크기.\n",
    "    num_heads (int): 멀티헤드 어텐션에서의 헤드 수.\n",
    "    num_layers (int): Transformer Encoder 레이어의 수.\n",
    "    mlp_ratio (float): 피드포워드 네트워크의 차원 확장 비율.\n",
    "    dropout (float): 드롭아웃 비율.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size:int=32, patch_size:int=4, in_channels:int=3, \n",
    "                 num_classes:int=100, embed_dim:int=768, num_heads:int=12, \n",
    "                 num_layers:int=12, mlp_ratio:float=4., dropout:float=0.1,\n",
    "                 estimate_params:bool=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        num_patches = self.patch_embed.n_patches\n",
    "        self.pos_embed = PositionalEmbedding(num_patches, embed_dim)\n",
    "\n",
    "        self.transformer_encoders = nn.ModuleList([\n",
    "            TransformerEncoderLayer(embed_dim, num_heads, mlp_ratio, dropout,estimate_params) \n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)  # 이미지를 패치로 분할하고 임베딩\n",
    "        x = self.pos_embed(x)    # 위치 인코딩 적용\n",
    "        x = self.dropout(x)      # 임베딩 작업 후 dropout\n",
    "\n",
    "        for layer in self.transformer_encoders:\n",
    "            x = layer(x)  # 각 Transformer Encoder 레이어 적용\n",
    "\n",
    "        x = self.norm(x)        # 정규화\n",
    "        x = self.dropout(x)     # dropout 적용\n",
    "        \n",
    "        # cls_token의 출력을 사용하여 분류\n",
    "        cls_token_output = x[:, 0]  # 첫 번째 토큰 (cls_token) 추출\n",
    "        x = self.head(cls_token_output)  # 최종 분류를 위한 선형 레이어\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 100])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 테스트 - 임의의 이미지 데이터 생성 (B, C, W, H)\n",
    "test_img = torch.randn(2, 3, 224, 224) \n",
    "\n",
    "# 모델 초기화\n",
    "vit = VisionTransformer(img_size=224, \n",
    "                        patch_size=16, \n",
    "                        num_classes=100, \n",
    "                        dropout=0.0,\n",
    "                        estimate_params=True) # 파라미터 수를 측정하고 싶으면 True\n",
    "output = vit(test_img)     # 테스트 이미지를 모델에 통과\n",
    "\n",
    "output.shape  # 결과의 형태 확인 ([배치 크기, 클래스 수])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 768, 14, 14]         590,592\n",
      "    PatchEmbedding-2             [-1, 197, 768]               0\n",
      "PositionalEmbedding-3             [-1, 197, 768]               0\n",
      "           Dropout-4             [-1, 197, 768]               0\n",
      "         LayerNorm-5             [-1, 197, 768]           1,536\n",
      "            Linear-6             [-1, 197, 768]         590,592\n",
      "            Linear-7             [-1, 197, 768]         590,592\n",
      "            Linear-8             [-1, 197, 768]         590,592\n",
      "            Linear-9             [-1, 197, 768]         590,592\n",
      "              MHA-10             [-1, 197, 768]               0\n",
      "        LayerNorm-11             [-1, 197, 768]           1,536\n",
      "           Linear-12            [-1, 197, 3072]       2,362,368\n",
      "             GELU-13            [-1, 197, 3072]               0\n",
      "          Dropout-14            [-1, 197, 3072]               0\n",
      "           Linear-15             [-1, 197, 768]       2,360,064\n",
      "          Dropout-16             [-1, 197, 768]               0\n",
      "TransformerEncoderLayer-17             [-1, 197, 768]               0\n",
      "        LayerNorm-18             [-1, 197, 768]           1,536\n",
      "           Linear-19             [-1, 197, 768]         590,592\n",
      "           Linear-20             [-1, 197, 768]         590,592\n",
      "           Linear-21             [-1, 197, 768]         590,592\n",
      "           Linear-22             [-1, 197, 768]         590,592\n",
      "              MHA-23             [-1, 197, 768]               0\n",
      "        LayerNorm-24             [-1, 197, 768]           1,536\n",
      "           Linear-25            [-1, 197, 3072]       2,362,368\n",
      "             GELU-26            [-1, 197, 3072]               0\n",
      "          Dropout-27            [-1, 197, 3072]               0\n",
      "           Linear-28             [-1, 197, 768]       2,360,064\n",
      "          Dropout-29             [-1, 197, 768]               0\n",
      "TransformerEncoderLayer-30             [-1, 197, 768]               0\n",
      "        LayerNorm-31             [-1, 197, 768]           1,536\n",
      "           Linear-32             [-1, 197, 768]         590,592\n",
      "           Linear-33             [-1, 197, 768]         590,592\n",
      "           Linear-34             [-1, 197, 768]         590,592\n",
      "           Linear-35             [-1, 197, 768]         590,592\n",
      "              MHA-36             [-1, 197, 768]               0\n",
      "        LayerNorm-37             [-1, 197, 768]           1,536\n",
      "           Linear-38            [-1, 197, 3072]       2,362,368\n",
      "             GELU-39            [-1, 197, 3072]               0\n",
      "          Dropout-40            [-1, 197, 3072]               0\n",
      "           Linear-41             [-1, 197, 768]       2,360,064\n",
      "          Dropout-42             [-1, 197, 768]               0\n",
      "TransformerEncoderLayer-43             [-1, 197, 768]               0\n",
      "        LayerNorm-44             [-1, 197, 768]           1,536\n",
      "           Linear-45             [-1, 197, 768]         590,592\n",
      "           Linear-46             [-1, 197, 768]         590,592\n",
      "           Linear-47             [-1, 197, 768]         590,592\n",
      "           Linear-48             [-1, 197, 768]         590,592\n",
      "              MHA-49             [-1, 197, 768]               0\n",
      "        LayerNorm-50             [-1, 197, 768]           1,536\n",
      "           Linear-51            [-1, 197, 3072]       2,362,368\n",
      "             GELU-52            [-1, 197, 3072]               0\n",
      "          Dropout-53            [-1, 197, 3072]               0\n",
      "           Linear-54             [-1, 197, 768]       2,360,064\n",
      "          Dropout-55             [-1, 197, 768]               0\n",
      "TransformerEncoderLayer-56             [-1, 197, 768]               0\n",
      "        LayerNorm-57             [-1, 197, 768]           1,536\n",
      "           Linear-58             [-1, 197, 768]         590,592\n",
      "           Linear-59             [-1, 197, 768]         590,592\n",
      "           Linear-60             [-1, 197, 768]         590,592\n",
      "           Linear-61             [-1, 197, 768]         590,592\n",
      "              MHA-62             [-1, 197, 768]               0\n",
      "        LayerNorm-63             [-1, 197, 768]           1,536\n",
      "           Linear-64            [-1, 197, 3072]       2,362,368\n",
      "             GELU-65            [-1, 197, 3072]               0\n",
      "          Dropout-66            [-1, 197, 3072]               0\n",
      "           Linear-67             [-1, 197, 768]       2,360,064\n",
      "          Dropout-68             [-1, 197, 768]               0\n",
      "TransformerEncoderLayer-69             [-1, 197, 768]               0\n",
      "        LayerNorm-70             [-1, 197, 768]           1,536\n",
      "           Linear-71             [-1, 197, 768]         590,592\n",
      "           Linear-72             [-1, 197, 768]         590,592\n",
      "           Linear-73             [-1, 197, 768]         590,592\n",
      "           Linear-74             [-1, 197, 768]         590,592\n",
      "              MHA-75             [-1, 197, 768]               0\n",
      "        LayerNorm-76             [-1, 197, 768]           1,536\n",
      "           Linear-77            [-1, 197, 3072]       2,362,368\n",
      "             GELU-78            [-1, 197, 3072]               0\n",
      "          Dropout-79            [-1, 197, 3072]               0\n",
      "           Linear-80             [-1, 197, 768]       2,360,064\n",
      "          Dropout-81             [-1, 197, 768]               0\n",
      "TransformerEncoderLayer-82             [-1, 197, 768]               0\n",
      "        LayerNorm-83             [-1, 197, 768]           1,536\n",
      "           Linear-84             [-1, 197, 768]         590,592\n",
      "           Linear-85             [-1, 197, 768]         590,592\n",
      "           Linear-86             [-1, 197, 768]         590,592\n",
      "           Linear-87             [-1, 197, 768]         590,592\n",
      "              MHA-88             [-1, 197, 768]               0\n",
      "        LayerNorm-89             [-1, 197, 768]           1,536\n",
      "           Linear-90            [-1, 197, 3072]       2,362,368\n",
      "             GELU-91            [-1, 197, 3072]               0\n",
      "          Dropout-92            [-1, 197, 3072]               0\n",
      "           Linear-93             [-1, 197, 768]       2,360,064\n",
      "          Dropout-94             [-1, 197, 768]               0\n",
      "TransformerEncoderLayer-95             [-1, 197, 768]               0\n",
      "        LayerNorm-96             [-1, 197, 768]           1,536\n",
      "           Linear-97             [-1, 197, 768]         590,592\n",
      "           Linear-98             [-1, 197, 768]         590,592\n",
      "           Linear-99             [-1, 197, 768]         590,592\n",
      "          Linear-100             [-1, 197, 768]         590,592\n",
      "             MHA-101             [-1, 197, 768]               0\n",
      "       LayerNorm-102             [-1, 197, 768]           1,536\n",
      "          Linear-103            [-1, 197, 3072]       2,362,368\n",
      "            GELU-104            [-1, 197, 3072]               0\n",
      "         Dropout-105            [-1, 197, 3072]               0\n",
      "          Linear-106             [-1, 197, 768]       2,360,064\n",
      "         Dropout-107             [-1, 197, 768]               0\n",
      "TransformerEncoderLayer-108             [-1, 197, 768]               0\n",
      "       LayerNorm-109             [-1, 197, 768]           1,536\n",
      "          Linear-110             [-1, 197, 768]         590,592\n",
      "          Linear-111             [-1, 197, 768]         590,592\n",
      "          Linear-112             [-1, 197, 768]         590,592\n",
      "          Linear-113             [-1, 197, 768]         590,592\n",
      "             MHA-114             [-1, 197, 768]               0\n",
      "       LayerNorm-115             [-1, 197, 768]           1,536\n",
      "          Linear-116            [-1, 197, 3072]       2,362,368\n",
      "            GELU-117            [-1, 197, 3072]               0\n",
      "         Dropout-118            [-1, 197, 3072]               0\n",
      "          Linear-119             [-1, 197, 768]       2,360,064\n",
      "         Dropout-120             [-1, 197, 768]               0\n",
      "TransformerEncoderLayer-121             [-1, 197, 768]               0\n",
      "       LayerNorm-122             [-1, 197, 768]           1,536\n",
      "          Linear-123             [-1, 197, 768]         590,592\n",
      "          Linear-124             [-1, 197, 768]         590,592\n",
      "          Linear-125             [-1, 197, 768]         590,592\n",
      "          Linear-126             [-1, 197, 768]         590,592\n",
      "             MHA-127             [-1, 197, 768]               0\n",
      "       LayerNorm-128             [-1, 197, 768]           1,536\n",
      "          Linear-129            [-1, 197, 3072]       2,362,368\n",
      "            GELU-130            [-1, 197, 3072]               0\n",
      "         Dropout-131            [-1, 197, 3072]               0\n",
      "          Linear-132             [-1, 197, 768]       2,360,064\n",
      "         Dropout-133             [-1, 197, 768]               0\n",
      "TransformerEncoderLayer-134             [-1, 197, 768]               0\n",
      "       LayerNorm-135             [-1, 197, 768]           1,536\n",
      "          Linear-136             [-1, 197, 768]         590,592\n",
      "          Linear-137             [-1, 197, 768]         590,592\n",
      "          Linear-138             [-1, 197, 768]         590,592\n",
      "          Linear-139             [-1, 197, 768]         590,592\n",
      "             MHA-140             [-1, 197, 768]               0\n",
      "       LayerNorm-141             [-1, 197, 768]           1,536\n",
      "          Linear-142            [-1, 197, 3072]       2,362,368\n",
      "            GELU-143            [-1, 197, 3072]               0\n",
      "         Dropout-144            [-1, 197, 3072]               0\n",
      "          Linear-145             [-1, 197, 768]       2,360,064\n",
      "         Dropout-146             [-1, 197, 768]               0\n",
      "TransformerEncoderLayer-147             [-1, 197, 768]               0\n",
      "       LayerNorm-148             [-1, 197, 768]           1,536\n",
      "          Linear-149             [-1, 197, 768]         590,592\n",
      "          Linear-150             [-1, 197, 768]         590,592\n",
      "          Linear-151             [-1, 197, 768]         590,592\n",
      "          Linear-152             [-1, 197, 768]         590,592\n",
      "             MHA-153             [-1, 197, 768]               0\n",
      "       LayerNorm-154             [-1, 197, 768]           1,536\n",
      "          Linear-155            [-1, 197, 3072]       2,362,368\n",
      "            GELU-156            [-1, 197, 3072]               0\n",
      "         Dropout-157            [-1, 197, 3072]               0\n",
      "          Linear-158             [-1, 197, 768]       2,360,064\n",
      "         Dropout-159             [-1, 197, 768]               0\n",
      "TransformerEncoderLayer-160             [-1, 197, 768]               0\n",
      "       LayerNorm-161             [-1, 197, 768]           1,536\n",
      "         Dropout-162             [-1, 197, 768]               0\n",
      "          Linear-163                  [-1, 100]          76,900\n",
      "================================================================\n",
      "Total params: 85,723,492\n",
      "Trainable params: 85,723,492\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 311.66\n",
      "Params size (MB): 327.01\n",
      "Estimated Total Size (MB): 639.24\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(vit.to('cuda'), (3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "num_workers = 8\n",
    "num_classes = 10\n",
    "\n",
    "# label_smoothing = 0.1\n",
    "learning_rate = 0.001\n",
    "epochs = 20\n",
    "\n",
    "device = 'cuda:3'\n",
    "model_path = 'best_model2.pth'  # 모델 저장 경로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# 데이터 증강을 위한 전처리\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),  # 50% 확률로 수평 뒤집기\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # 색상 변경\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarmupCosineAnnealingLR(_LRScheduler):\n",
    "    def __init__(self, optimizer, warmup_steps, total_steps, last_epoch=-1):\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.total_steps = total_steps\n",
    "        super(WarmupCosineAnnealingLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch < self.warmup_steps:\n",
    "            return [base_lr * self.last_epoch / self.warmup_steps for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            cosine_decay = 0.5 * (1 + math.cos(math.pi * (self.last_epoch - self.warmup_steps) / (self.total_steps - self.warmup_steps)))\n",
    "            return [base_lr * cosine_decay for base_lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_steps = len(trainloader) * epochs\n",
    "warmup_steps = total_steps * 0.1\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(vit.parameters(), lr=learning_rate, betas=[0.9,0.999], weight_decay=0.03)\n",
    "# scheduler = CosineAnnealingLR(optimizer, T_max=total_steps, eta_min=0)\n",
    "scheduler = WarmupCosineAnnealingLR(optimizer, warmup_steps, total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss == None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 196/196 [05:54<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.425742443726987, Val Loss: 2.328350293636322, LR: 0.001, Duration: 379.85 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 196/196 [05:56<00:00,  1.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3268476870595194, Val Loss: 2.350078117847443, LR: 0.001, Duration: 380.35 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 196/196 [05:57<00:00,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3168082358885784, Val Loss: 2.3138040244579314, LR: 0.001, Duration: 382.91 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4:  39%|███▉      | 77/196 [02:21<03:36,  1.82s/it]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "training_time = 0\n",
    "early_stopping = EarlyStopping(patience=10)\n",
    "losses = []\n",
    "val_losses = []\n",
    "lrs = []\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "vit_save = False\n",
    "vit.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    vit.train()\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(enumerate(trainloader), total=len(trainloader), desc=f\"Epoch {epoch + 1}\")\n",
    "    for i, data in pbar:\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = vit(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        running_loss += loss.item()\n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        lrs.append(lr)\n",
    "    epoch_loss = running_loss / len(trainloader)\n",
    "    losses.append(epoch_loss)\n",
    "\n",
    "    # 검증 손실 계산\n",
    "    vit.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = vit(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(testloader)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    # 모델 저장\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        vit_save = True\n",
    "        torch.save(vit.state_dict(), model_path)\n",
    "\n",
    "    epoch_duration = time.time() - start_time\n",
    "    training_time += epoch_duration\n",
    "    if vit_save:\n",
    "        print(f'\\tLoss: {epoch_loss}, Val Loss: {val_loss}, LR: {lr}, Duration: {epoch_duration:.2f} sec - model saved!')\n",
    "        vit_save = False\n",
    "    else :\n",
    "        print(f'\\tLoss: {epoch_loss}, Val Loss: {val_loss}, LR: {lr}, Duration: {epoch_duration:.2f} sec')\n",
    "\n",
    "    # Early Stopping 체크\n",
    "    early_stopping(val_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "# 학습 및 검증 손실 시각화\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.title('Training & Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(lrs, label='Learning Rate')\n",
    "plt.title('Learning Rate')\n",
    "plt.xlabel('Batch number')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "# Hook 함수 정의 및 등록\n",
    "attention_maps = []\n",
    "def get_attention_map(module, input, output):\n",
    "    global attention_maps\n",
    "    attention_maps.append(output[1].detach())\n",
    "hook = vit.transformer_encoders[0].attn.register_forward_hook(get_attention_map)\n",
    "\n",
    "# 모델을 통한 예측 및 Attention Map 추출\n",
    "original_images, labels = next(iter(testloader))\n",
    "outputs = vit(original_images.to(device))\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "hook.remove()\n",
    "\n",
    "# CIFAR-10 클래스 레이블\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# Attention Map 가공 및 시각화 개선 함수\n",
    "def show_attention_on_image(img, attention_map, label, pred, threshold=0.0):\n",
    "    # 이미지 크기에 맞게 Attention Map 조정\n",
    "    attention_map = attention_map.cpu().numpy()\n",
    "    attention_map = attention_map - np.min(attention_map)\n",
    "    attention_map = attention_map / np.max(attention_map)\n",
    "\n",
    "    # 임계값 이하의 Attention 값 제거\n",
    "    # attention_map[attention_map < threshold] = 0\n",
    "\n",
    "    # 원본 이미지 크기로 Attention Map 업샘플링\n",
    "    attention_map_resized = np.resize(attention_map, (img.shape[1], img.shape[2]))\n",
    "\n",
    "    # 원본 이미지와 Attention Map 겹쳐서 표시\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axs[0].imshow(img.permute(1, 2, 0))\n",
    "    axs[0].axis('off')\n",
    "    axs[0].set_title(f'Original Image\\nTrue Label: {label}')\n",
    "\n",
    "    axs[1].imshow(img.permute(1, 2, 0))\n",
    "    axs[1].imshow(attention_map_resized, cmap='jet', alpha=0.6)  # 투명도 조절\n",
    "    axs[1].axis('off')\n",
    "    axs[1].set_title(f'Image with Attention Map\\nPredicted: {pred}')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# 각 이미지에 대해 시각화 수행\n",
    "for i in range(4):\n",
    "    show_attention_on_image(original_images[i], attention_maps[0][i], classes[labels[i]], classes[predicted[i]])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
