{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms.autoaugment import AutoAugmentPolicy\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.optim.lr_scheduler import _LRScheduler, CosineAnnealingLR\n",
    "\n",
    "from PIL import Image\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 112\n",
    "patch_size = 8\n",
    "num_classes = 200\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1)Patch Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    이미지를 패치로 분할하고, 각 패치를 임베딩하는 클래스입니다.\n",
    "\n",
    "    Attributes:\n",
    "    patch_size (int): 패치의 크기 (예: 16x16).\n",
    "    n_patches (int): 이미지당 생성되는 패치의 총 수.\n",
    "    projection (nn.Conv2d): 패치를 임베딩 벡터로 변환하는 컨볼루션 레이어.\n",
    "\n",
    "    Args:\n",
    "    img_size (int): 입력 이미지의 크기 (예: 32x32).\n",
    "    patch_size (int): 패치의 크기 (예: 16x16).\n",
    "    in_channels (int): 입력 이미지의 채널 수 (RGB의 경우 3).\n",
    "    embed_dim (int): 임베딩 차원의 크기.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size:int=32, patch_size:int=2, in_channels:int=3, embed_dim:int=768):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Patch 정보 인식\n",
    "        self.patch_size = patch_size\n",
    "        assert img_size % patch_size == 0, f'img size({img_size})는 patch size({patch_size})로 나뉘어야 합니다.'\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        # 컨볼루션을 사용하여 패치를 임베딩 벡터로 변환\n",
    "        self.projection = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        \n",
    "        # class token 추가\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1,1,embed_dim))        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [배치 크기, 채널 수, 높이, 너비]\n",
    "        x = self.projection(x)  # 컨볼루션을 통한 임베딩: [배치 크기, 임베딩 차원, 패치 수, _]\n",
    "        x = x.flatten(2)        # 평탄화: [배치 크기, 임베딩 차원, 패치 수]\n",
    "        x = x.transpose(1, 2)   # 변환: [배치 크기, 패치 수, 임베딩 차원]\n",
    "        \n",
    "        cls_tokens = self.cls_token.expand(x.size(0), -1, -1) # [배치크기, 1, 임베딩 차원]\n",
    "        x = torch.cat((cls_tokens, x), dim=1) # cls_token 추가: [배치크기, 패치 수+1, 임베딩 차원]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2)Positional Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    위치 임베딩을 추가하는 클래스입니다. 각 패치에 대한 위치 정보를 제공합니다.\n",
    "\n",
    "    Attributes:\n",
    "    scale (torch.Tensor): 스케일링 펙터.\n",
    "    position_embedding (torch.nn.Parameter): 학습 가능한 위치 인코딩.\n",
    "\n",
    "    Args:\n",
    "    num_patches (int): 이미지당 생성되는 패치의 수.\n",
    "    embed_dim (int): 임베딩 차원의 크기.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_patches:int, embed_dim:int):\n",
    "        super().__init__()\n",
    "        self.scale = torch.sqrt(torch.tensor(embed_dim, dtype=torch.float32))\n",
    "        self.position_embedding = nn.Parameter(torch.zeros(1, num_patches+1, embed_dim)) # [1, 패치 수+1, 임베딩 차원]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.scale*x + self.position_embedding # scaled x에 위치 정보를 임베딩에 더함 \n",
    "        # x += self.position_embedding  \n",
    "        return x # [배치 크기, 패치 수+1, 임베딩 차원]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3)MHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터 수 비교를 위해 가져온 이전 구현체\n",
    "class MHA(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        assert d_model % n_heads == 0, f'd_model ({d_model})은 n_heads ({n_heads})로 나누어 떨어져야 합니다.'\n",
    "\n",
    "        self.head_dim = d_model // n_heads  # int 형변환 제거\n",
    "\n",
    "        # 쿼리, 키, 값에 대한 선형 변환\n",
    "        self.fc_q = nn.Linear(d_model, d_model) \n",
    "        self.fc_k = nn.Linear(d_model, d_model) \n",
    "        self.fc_v = nn.Linear(d_model, d_model)\n",
    "        self.fc_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # 어텐션 점수를 위한 스케일 요소\n",
    "        self.scale = torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))\n",
    "        \n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        batch_size = Q.shape[0]\n",
    "\n",
    "        # 쿼리, 키, 값에 대한 선형 변환 수행\n",
    "        Q = self.fc_q(Q) \n",
    "        K = self.fc_k(K)\n",
    "        V = self.fc_v(V)\n",
    "\n",
    "        # 멀티 헤드 어텐션을 위해 텐서 재구성 및 순서 변경\n",
    "        Q = Q.reshape(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.reshape(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.reshape(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "        # 스케일드 닷-프로덕트 어텐션 계산\n",
    "        attention_score = Q @ K.permute(0, 1, 3, 2) / self.scale\n",
    "\n",
    "        # 소프트맥스를 사용하여 어텐션 확률 계산\n",
    "        attention_dist = torch.softmax(attention_score, dim=-1)\n",
    "\n",
    "        # 어텐션 결과\n",
    "        attention = attention_dist @ V\n",
    "\n",
    "        # 어텐션 헤드 재조립\n",
    "        x = attention.permute(0, 2, 1, 3).reshape(batch_size, -1, self.d_model)\n",
    "\n",
    "        # 최종 선형 변환\n",
    "        x = self.fc_o(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4)Transformer Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer Encoder 레이어를 정의하는 클래스입니다.\n",
    "\n",
    "    Attributes:\n",
    "    norm1, norm2 (nn.LayerNorm): 정규화 레이어.\n",
    "    attn (nn.MultiheadAttention): 멀티헤드 어텐션 레이어.\n",
    "    mlp (nn.Sequential): 피드포워드 네트워크.\n",
    "\n",
    "    Args:\n",
    "    embed_dim (int): 임베딩 차원의 크기.\n",
    "    num_heads (int): 멀티헤드 어텐션에서의 헤드 수.\n",
    "    mlp_ratio (float): 첫 번째 선형 레이어의 출력 차원을 결정하는 비율.\n",
    "    dropout (float): 드롭아웃 비율.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim:int, num_heads:int, mlp_ratio:float=4.0, dropout:float=0.1, estimate_params:bool=False):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.estimate_params = estimate_params\n",
    "        if estimate_params:\n",
    "            self.attn = MHA(embed_dim, num_heads)\n",
    "        else :\n",
    "            self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout) # [attention_output, attention weights]        \n",
    "\n",
    "        mlp_hidden_dim = int(mlp_ratio * embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_hidden_dim, embed_dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 멀티헤드 어텐션과 피드포워드 네트워크를 적용\n",
    "        if self.estimate_params:\n",
    "            x = self.norm(x)\n",
    "            x = x + self.attn(x, x, x)\n",
    "        else :\n",
    "            x = x + self.attn(x, x, x)[0] # attention output만 사용\n",
    "        \n",
    "        x2 = self.norm(x)\n",
    "        x = x + self.mlp(x2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5)Vision Transformer(ViT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    전체 Vision Transformer 모델을 정의하는 클래스입니다.\n",
    "\n",
    "    Attributes:\n",
    "    patch_embed (PatchEmbedding): 이미지를 패치로 분할하고 임베딩하는 레이어.\n",
    "    pos_embed (PositionalEncoding): 위치 인코딩 레이어.\n",
    "    transformer_encoders (nn.ModuleList): Transformer Encoder 레이어들의 리스트.\n",
    "    norm (nn.LayerNorm): 정규화 레이어.\n",
    "    head (nn.Linear): 최종 분류를 위한 선형 레이어.\n",
    "\n",
    "    Args:\n",
    "    img_size (int): 입력 이미지의 크기 (예: 32x32).\n",
    "    patch_size (int): 패치의 크기 (예: 16x16).\n",
    "    in_channels (int): 입력 이미지의 채널 수 (RGB의 경우 3).\n",
    "    num_classes (int): 분류할 클래스의 수 (CIFAR-10의 경우 10).\n",
    "    embed_dim (int): 임베딩 차원의 크기.\n",
    "    num_heads (int): 멀티헤드 어텐션에서의 헤드 수.\n",
    "    num_layers (int): Transformer Encoder 레이어의 수.\n",
    "    mlp_ratio (float): 피드포워드 네트워크의 차원 확장 비율.\n",
    "    dropout (float): 드롭아웃 비율.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size:int=32, patch_size:int=4, in_channels:int=3, \n",
    "                 num_classes:int=100, embed_dim:int=768, num_heads:int=12, \n",
    "                 num_layers:int=12, mlp_ratio:float=4., dropout:float=0.1,\n",
    "                 estimate_params:bool=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        num_patches = self.patch_embed.n_patches\n",
    "        self.pos_embed = PositionalEmbedding(num_patches, embed_dim)\n",
    "\n",
    "        self.transformer_encoders = nn.ModuleList([\n",
    "            TransformerEncoderLayer(embed_dim, num_heads, mlp_ratio, dropout,estimate_params) \n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # 파라미터 초기화\n",
    "        for m in self.modules():\n",
    "            if hasattr(m, 'weight') and m.weight.dim() > 1: \n",
    "                nn.init.xavier_uniform_(m.weight) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)  # 이미지를 패치로 분할하고 임베딩\n",
    "        x = self.pos_embed(x)    # 위치 인코딩 적용\n",
    "        x = self.dropout(x)      # 임베딩 작업 후 dropout\n",
    "\n",
    "        for layer in self.transformer_encoders:\n",
    "            x = layer(x)  # 각 Transformer Encoder 레이어 적용\n",
    "\n",
    "        x = self.norm(x)        # 정규화\n",
    "        x = self.dropout(x)     # dropout 적용\n",
    "        \n",
    "        # cls_token의 출력을 사용하여 분류\n",
    "        cls_token_output = x[:, 0]  # 첫 번째 토큰 (cls_token) 추출\n",
    "        x = self.head(cls_token_output)  # 최종 분류를 위한 선형 레이어\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6)Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 테스트 - 임의의 이미지 데이터 생성 (B, C, W, H)\n",
    "test_img = torch.randn(2, 3, img_size, img_size) \n",
    "\n",
    "# 모델 초기화\n",
    "vit = VisionTransformer(img_size=img_size, \n",
    "                        patch_size=patch_size, \n",
    "                        num_classes=num_classes, \n",
    "                        dropout=dropout,\n",
    "                        estimate_params=True) # 파라미터 수를 측정하고 싶으면 True\n",
    "output = vit(test_img)     # 테스트 이미지를 모델에 통과\n",
    "\n",
    "output.shape  # 결과의 형태 확인 ([배치 크기, 클래스 수])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7)Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "# torch summary는 무조건 cuda:0에서만 가능\n",
    "summary(vit.to('cuda'), (3,img_size,img_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Prework of Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1)Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "num_workers = 16\n",
    "\n",
    "label_smoothing = 0.1\n",
    "learning_rate = 0.003\n",
    "epochs = 10\n",
    "\n",
    "device = 'cuda:3'\n",
    "model_path = 'tiny_imageNet.pth'  # 모델 저장 경로"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2)Load Data & Construct Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiny ImageNet 데이터셋 다운로드 URL\n",
    "url = 'http://cs231n.stanford.edu/tiny-imagenet-200.zip'\n",
    "filename = url.split('/')[-1]  # 파일명 추출\n",
    "folder_name = './data/tiny-imagenet-200'\n",
    "\n",
    "if not os.path.isdir(folder_name):\n",
    "    # 데이터셋을 다운로드합니다.\n",
    "    r = requests.get(url, stream=True)\n",
    "    with open(filename, 'wb') as f:\n",
    "        for chunk in r.iter_content(chunk_size=8192): \n",
    "            if chunk:  # 필터링된 청크 데이터를 파일에 씁니다.\n",
    "                f.write(chunk)\n",
    "    \n",
    "    # 다운로드한 zip 파일을 추출합니다.\n",
    "    with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "        zip_ref.extractall('.')  # 현재 디렉토리에 압축 해제\n",
    "    os.remove(filename)  # zip 파일 삭제\n",
    "\n",
    "# 이제 'tiny-imagenet-200' 폴더 안에 데이터셋이 준비되어 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 데이터 증강을 위한 전처리\n",
    "# transform_train = transforms.Compose([\n",
    "#     transforms.RandomResizedCrop(224),\n",
    "#     transforms.RandomHorizontalFlip(),  # 50% 확률로 수평 뒤집기\n",
    "#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # 색상 변경\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "# ])\n",
    "\n",
    "# transform_test = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "# ])\n",
    "\n",
    "# trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
    "# trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "# testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
    "# testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 경로 설정\n",
    "data_dir = './data/tiny-imagenet-200'  # Tiny ImageNet 데이터셋이 저장된 경로\n",
    "# WordNet ID와 클래스 이름을 매핑하는 사전을 생성합니다.\n",
    "id_to_class = {}\n",
    "with open(os.path.join(data_dir, 'wnids.txt'), 'r') as f:\n",
    "    for idx, line in enumerate(f):\n",
    "        wordnet_id = line.strip()\n",
    "        id_to_class[idx] = wordnet_id\n",
    "        \n",
    "id_to_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms 정의하기\n",
    "train_transform = transforms.Compose([\n",
    "    # transforms.Resize((112, 112), interpolation=transforms.InterpolationMode.LANCZOS),\n",
    "    # transforms.AutoAugment(AutoAugmentPolicy.IMAGENET),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클래스 이름 목록을 만듭니다.\n",
    "class_names = list(id_to_class.values())\n",
    "\n",
    "# 클래스 이름을 정수 인덱스로 매핑하는 사전 생성\n",
    "class_to_idx = {class_name: idx for idx, class_name in enumerate(class_names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용자 정의 데이터셋 클래스\n",
    "class TinyImageNetDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, is_train=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): 데이터셋의 디렉토리 경로.\n",
    "            transform (callable, optional): 적용할 transform.\n",
    "            is_train (bool, optional): 학습 데이터셋인지 테스트 데이터셋인지 구분.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.is_train = is_train\n",
    "        self.images = []  # 이미지 파일 경로를 저장할 리스트\n",
    "        self.labels = []  # 레이블을 저장할 리스트\n",
    "\n",
    "        # 이미지와 레이블을 로드하는 로직\n",
    "        if is_train:\n",
    "            for class_dir in os.listdir(os.path.join(root_dir, 'train')):\n",
    "                class_dir_path = os.path.join(root_dir, 'train', class_dir, 'images')\n",
    "                for img_file in os.listdir(class_dir_path):\n",
    "                    self.images.append(os.path.join(class_dir_path, img_file))\n",
    "                    self.labels.append(class_dir)\n",
    "        else:\n",
    "            with open(os.path.join(root_dir, 'val', 'val_annotations.txt'), 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    img_file, wordnet_id = parts[0], parts[1]\n",
    "                    self.images.append(os.path.join(root_dir, 'val', 'images', img_file))\n",
    "                    class_name = id_to_class.get(wordnet_id, \"\")\n",
    "                    self.labels.append(class_name)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        class_name = self.labels[idx]\n",
    "        if class_name in class_to_idx:\n",
    "            label_idx = class_to_idx[class_name]  # 클래스 이름을 정수 인덱스로 변환\n",
    "        else:\n",
    "            print(f\"Class name '{class_name}' not found in class_to_idx\")\n",
    "            label_idx = -1\n",
    "        return image, label_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 인스턴스 생성\n",
    "trainset = TinyImageNetDataset(data_dir, transform=train_transform, is_train=True)\n",
    "testset = TinyImageNetDataset(data_dir, transform=test_transform, is_train=False)\n",
    "\n",
    "# DataLoader 설정\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=False)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 첫번째 배치를 가져와서 확인합니다.\n",
    "images, labels = next(iter(trainloader))\n",
    "print(images.size())  # 결과: torch.Size([512, 3, 224, 224])\n",
    "print(labels[0])      # 첫 번째 레이블 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_to_words = {}\n",
    "with open(os.path.join(data_dir, 'words.txt'), 'r') as f:\n",
    "    for idx, line in enumerate(f):\n",
    "        classes, words = line.strip().split('\\t')\n",
    "        class_to_words[classes] = words\n",
    "        \n",
    "class_to_words['n02836392']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import AutoAugmentPolicy\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# 이미지와 레이블을 시각화하는 함수 정의\n",
    "def visualize_samples(dataloader, class_names, n=5):\n",
    "    \"\"\"\n",
    "    DataLoader에서 n개의 샘플을 시각화합니다.\n",
    "\n",
    "    Args:\n",
    "    - dataloader: PyTorch DataLoader 객체.\n",
    "    - class_names: 클래스 이름 목록.\n",
    "    - n (int): 출력할 이미지 수.\n",
    "    \"\"\"\n",
    "    # DataLoader에서 n개의 배치를 가져옵니다.\n",
    "    images, labels = next(iter(dataloader))\n",
    "\n",
    "    # 이미지를 표시합니다.\n",
    "    plt.figure(figsize=(25, 5))\n",
    "    for i in range(n):\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.imshow(torch.permute(images[i], (1, 2, 0)))  # 채널 순서 변경\n",
    "        plt.title(class_to_words[class_names[labels[i]]])\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# 함수 실행을 위한 매개변수 설정\n",
    "batch_size = 5  # 배치 크기 설정\n",
    "\n",
    "# 함수 실행\n",
    "visualize_samples(trainloader, class_names, n=5)  # n은 시각화할 이미지 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-12-12 14:56:43.270884: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-12 14:56:43.270960: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-12 14:56:43.271827: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-12 14:56:43.277218: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-12 14:56:44.031277: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-12-12 14:56:44.706589: W external/local_tsl/tsl/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata.google.internal\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: 155.84 GiB, total: 155.84 GiB) to /root/tensorflow_datasets/imagenet2012/5.1.0...\u001b[0m\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Manual directory /root/tensorflow_datasets/downloads/manual does not exist or is empty. Create it and download/extract dataset artifacts in there using instructions:\nmanual_dir should contain two files: ILSVRC2012_img_train.tar and\nILSVRC2012_img_val.tar.\nYou need to register on https://image-net.org/download-images in order\nto get the link to download the dataset.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfds\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# ImageNet 데이터셋 다운로드\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m imagenet_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtfds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimagenet2012\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# 데이터셋의 일부를 확인\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m imagenet_dataset\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m5\u001b[39m):  \u001b[38;5;66;03m# 처음 5개의 이미지를 불러옵니다.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_datasets/core/logging/__init__.py:166\u001b[0m, in \u001b[0;36m_FunctionDecorator.__call__\u001b[0;34m(self, function, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_call()\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    168\u001b[0m   metadata\u001b[38;5;241m.\u001b[39mmark_error()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_datasets/core/load.py:639\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Loads the named dataset into a `tf.data.Dataset`.\u001b[39;00m\n\u001b[1;32m    521\u001b[0m \n\u001b[1;32m    522\u001b[0m \u001b[38;5;124;03m`tfds.load` is a convenience method that:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;124;03m    Split-specific information is available in `ds_info.splits`.\u001b[39;00m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    633\u001b[0m dbuilder \u001b[38;5;241m=\u001b[39m _fetch_builder(\n\u001b[1;32m    634\u001b[0m     name,\n\u001b[1;32m    635\u001b[0m     data_dir,\n\u001b[1;32m    636\u001b[0m     builder_kwargs,\n\u001b[1;32m    637\u001b[0m     try_gcs,\n\u001b[1;32m    638\u001b[0m )\n\u001b[0;32m--> 639\u001b[0m \u001b[43m_download_and_prepare_builder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdbuilder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m as_dataset_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    642\u001b[0m   as_dataset_kwargs \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_datasets/core/load.py:498\u001b[0m, in \u001b[0;36m_download_and_prepare_builder\u001b[0;34m(dbuilder, download, download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[1;32m    497\u001b[0m   download_and_prepare_kwargs \u001b[38;5;241m=\u001b[39m download_and_prepare_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m--> 498\u001b[0m   \u001b[43mdbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_datasets/core/logging/__init__.py:166\u001b[0m, in \u001b[0;36m_FunctionDecorator.__call__\u001b[0;34m(self, function, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_call()\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    168\u001b[0m   metadata\u001b[38;5;241m.\u001b[39mmark_error()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_datasets/core/dataset_builder.py:691\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, download_dir, download_config, file_format)\u001b[0m\n\u001b[1;32m    689\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mread_from_directory(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dir)\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 691\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    696\u001b[0m   \u001b[38;5;66;03m# NOTE: If modifying the lines below to put additional information in\u001b[39;00m\n\u001b[1;32m    697\u001b[0m   \u001b[38;5;66;03m# DatasetInfo, you'll likely also want to update\u001b[39;00m\n\u001b[1;32m    698\u001b[0m   \u001b[38;5;66;03m# DatasetInfo.read_from_directory to possibly restore these attributes\u001b[39;00m\n\u001b[1;32m    699\u001b[0m   \u001b[38;5;66;03m# when reading from package data.\u001b[39;00m\n\u001b[1;32m    700\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdownload_size \u001b[38;5;241m=\u001b[39m dl_manager\u001b[38;5;241m.\u001b[39mdownloaded_size\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_datasets/core/dataset_builder.py:1547\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, download_config)\u001b[0m\n\u001b[1;32m   1545\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1546\u001b[0m   optional_pipeline_kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1547\u001b[0m split_generators \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_split_generators\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=unexpected-keyword-arg\u001b[39;49;00m\n\u001b[1;32m   1548\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptional_pipeline_kwargs\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1550\u001b[0m \u001b[38;5;66;03m# TODO(tfds): Could be removed once all datasets are migrated.\u001b[39;00m\n\u001b[1;32m   1551\u001b[0m \u001b[38;5;66;03m# https://github.com/tensorflow/datasets/issues/2537\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;66;03m# Legacy mode (eventually convert list[SplitGeneratorLegacy] -> dict)\u001b[39;00m\n\u001b[1;32m   1553\u001b[0m split_generators \u001b[38;5;241m=\u001b[39m split_builder\u001b[38;5;241m.\u001b[39mnormalize_legacy_split_generators(\n\u001b[1;32m   1554\u001b[0m     split_generators\u001b[38;5;241m=\u001b[39msplit_generators,\n\u001b[1;32m   1555\u001b[0m     generator_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_examples,\n\u001b[1;32m   1556\u001b[0m     is_beam\u001b[38;5;241m=\u001b[39m\u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, BeamBasedBuilder),\n\u001b[1;32m   1557\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_datasets/datasets/imagenet2012/imagenet2012_dataset_builder.py:98\u001b[0m, in \u001b[0;36mBuilder._split_generators\u001b[0;34m(self, dl_manager)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_split_generators\u001b[39m(\u001b[38;5;28mself\u001b[39m, dl_manager):\n\u001b[0;32m---> 98\u001b[0m   train_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_dir\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mILSVRC2012_img_train.tar\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     99\u001b[0m   val_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dl_manager\u001b[38;5;241m.\u001b[39mmanual_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mILSVRC2012_img_val.tar\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    100\u001b[0m   test_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dl_manager\u001b[38;5;241m.\u001b[39mmanual_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mILSVRC2012_img_test.tar\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/functools.py:981\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, instance, owner)\u001b[0m\n\u001b[1;32m    979\u001b[0m val \u001b[38;5;241m=\u001b[39m cache\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname, _NOT_FOUND)\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[0;32m--> 981\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    982\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    983\u001b[0m         cache[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname] \u001b[38;5;241m=\u001b[39m val\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_datasets/core/download/download_manager.py:705\u001b[0m, in \u001b[0;36mDownloadManager.manual_dir\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    700\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    701\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTo access `dl_manager.manual_dir`, please set \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    702\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`MANUAL_DOWNLOAD_INSTRUCTIONS` in your dataset.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    703\u001b[0m   )\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_manual_dir\u001b[38;5;241m.\u001b[39mexists() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_manual_dir\u001b[38;5;241m.\u001b[39miterdir()):\n\u001b[0;32m--> 705\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    706\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mManual directory \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_manual_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist or is empty. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    707\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCreate it and download/extract dataset artifacts in there using \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    708\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstructions:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_manual_dir_instructions\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    709\u001b[0m   )\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_manual_dir\n",
      "\u001b[0;31mAssertionError\u001b[0m: Manual directory /root/tensorflow_datasets/downloads/manual does not exist or is empty. Create it and download/extract dataset artifacts in there using instructions:\nmanual_dir should contain two files: ILSVRC2012_img_train.tar and\nILSVRC2012_img_val.tar.\nYou need to register on https://image-net.org/download-images in order\nto get the link to download the dataset."
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# ImageNet 데이터셋 다운로드\n",
    "imagenet_dataset = tfds.load('imagenet2012', split='train')\n",
    "\n",
    "# 데이터셋의 일부를 확인\n",
    "for example in imagenet_dataset.take(5):  # 처음 5개의 이미지를 불러옵니다.\n",
    "    image = example[\"image\"]\n",
    "    label = example[\"label\"]\n",
    "    print(image.shape, label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3)Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 및 옵티마이저 설정\n",
    "model = torch.nn.Linear(10, 2)  # 예시 모델\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 총 스텝 수\n",
    "total_steps = 100\n",
    "\n",
    "# 코사인 학습률 감소 스케줄러 설정\n",
    "scheduler_cosine = CosineAnnealingLR(optimizer, T_max=total_steps//4, eta_min=0.0005)\n",
    "\n",
    "# 학습률 감소 추적을 위한 리스트\n",
    "lrs_cosine = []\n",
    "\n",
    "# 코사인 스케줄러에 대해 학습률 기록\n",
    "for step in range(total_steps):\n",
    "    optimizer.step()\n",
    "    lrs_cosine.append(scheduler_cosine.get_last_lr()[0])\n",
    "    scheduler_cosine.step()\n",
    "\n",
    "# 그래프 그리기\n",
    "steps = np.arange(total_steps)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(steps, lrs_cosine, label='Cosine Decay')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Cosine Learning Rate Decay Schedule')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 및 옵티마이저 설정\n",
    "model = torch.nn.Linear(10, 2)  # 예시 모델\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 총 스텝 수\n",
    "total_steps = 100\n",
    "\n",
    "# 코사인 학습률 감소 스케줄러 설정 (주기를 전체 스텝 수로 변경)\n",
    "scheduler_cosine = CosineAnnealingLR(optimizer, T_max=total_steps)\n",
    "\n",
    "# 코사인 및 선형 학습률 감소 추적을 위한 리스트\n",
    "lrs_cosine = []\n",
    "lrs_linear = []\n",
    "\n",
    "# 초기 학습률\n",
    "initial_lr = 0.001\n",
    "\n",
    "# 선형 감소 함수 정의\n",
    "def linear_decay(step, total_steps, initial_lr):\n",
    "    return initial_lr * (1 - step / total_steps)\n",
    "\n",
    "# 스케줄러에 대해 학습률 기록\n",
    "for step in range(total_steps):\n",
    "    optimizer.step()\n",
    "    lrs_cosine.append(scheduler_cosine.get_last_lr()[0])\n",
    "    scheduler_cosine.step()\n",
    "\n",
    "    linear_lr = linear_decay(step, total_steps, initial_lr)\n",
    "    lrs_linear.append(linear_lr)\n",
    "\n",
    "# 그래프 그리기\n",
    "steps = np.arange(total_steps)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(steps, lrs_cosine, label='Cosine Decay')\n",
    "plt.plot(steps, lrs_linear, label='Linear Decay', linestyle='--')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Learning Rate Decay Schedules')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarmupCosineAnnealingLR(_LRScheduler):\n",
    "    def __init__(self, optimizer, warmup_steps, total_steps, last_epoch=-1):\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.total_steps = total_steps\n",
    "        super(WarmupCosineAnnealingLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch < self.warmup_steps: # during warmup\n",
    "            return [base_lr * self.last_epoch / self.warmup_steps for base_lr in self.base_lrs]\n",
    "        else: # post warmup\n",
    "            current_gap = self.last_epoch - self.warmup_steps\n",
    "            total_gap = self.total_steps - self.warmup_steps\n",
    "            cosine_decay = 0.5 * (1 + math.cos(math.pi * current_gap / total_gap))\n",
    "            return [base_lr * cosine_decay for base_lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 및 옵티마이저 설정\n",
    "model = torch.nn.Linear(10, 2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "# 스케줄러 초기화\n",
    "total_steps = 100\n",
    "warmup_steps = 10  # 예시로 10 스텝의 warmup 설정\n",
    "scheduler = WarmupCosineAnnealingLR(optimizer, warmup_steps, total_steps)\n",
    "\n",
    "# 학습률 기록 및 업데이트\n",
    "lrs = []\n",
    "for epoch in range(total_steps):\n",
    "    optimizer.step()\n",
    "    lrs.append(scheduler.get_last_lr())\n",
    "    scheduler.step()\n",
    "\n",
    "steps = np.arange(total_steps)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(steps, lrs, label='Warmup with Cosine Decay')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Learning Rate Schedule: Warmup with Cosine Decay')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Optimizer & Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_steps = len(trainloader) * epochs\n",
    "warmup_steps = total_steps * 0.1\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
    "optimizer = optim.Adam(vit.parameters(), lr=learning_rate, betas=[0.9,0.999], weight_decay=0.3)\n",
    "scheduler = WarmupCosineAnnealingLR(optimizer, warmup_steps, total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5)EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss == None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "training_time = 0\n",
    "early_stopping = EarlyStopping(patience=10)\n",
    "losses = []\n",
    "lrs = []\n",
    "best_loss = float('inf')\n",
    "\n",
    "vit_save = False\n",
    "vit.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    vit.train()\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(enumerate(trainloader), total=len(trainloader), desc=f\"Epoch {epoch + 1}\")\n",
    "    for i, data in pbar:\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = vit(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        running_loss += loss.item()\n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        lrs.append(lr)\n",
    "    epoch_loss = running_loss / len(trainloader)\n",
    "    losses.append(epoch_loss)\n",
    "    \n",
    "    # 모델 저장\n",
    "    if epoch_loss < best_loss:\n",
    "        best_loss = loss\n",
    "        vit_save = True\n",
    "        torch.save(vit.state_dict(), model_path)\n",
    "\n",
    "    epoch_duration = time.time() - start_time\n",
    "    training_time += epoch_duration\n",
    "    if vit_save:\n",
    "        print(f'\\tLoss: {epoch_loss}, LR: {lr}, Duration: {epoch_duration:.2f} sec - model saved!')\n",
    "        vit_save = False\n",
    "    else :\n",
    "        print(f'\\tLoss: {epoch_loss}, LR: {lr}, Duration: {epoch_duration:.2f} sec')\n",
    "\n",
    "    # Early Stopping 체크\n",
    "    early_stopping(loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "    \n",
    "torch.save(vit.state_dict(), './last_tiny_imagenet.pth')\n",
    "\n",
    "# 학습 및 검증 손실 시각화\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(losses, label='Training Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(lrs, label='Learning Rate')\n",
    "plt.title('Learning Rate')\n",
    "plt.xlabel('Batch number')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 수행 및 레이블 저장\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in trainloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = vit(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# 혼동 행렬 생성\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# 혼동 행렬 시각화\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# 예측과 실제 레이블\n",
    "y_true = all_labels  # 실제 레이블\n",
    "y_pred = all_preds  # 모델에 의해 예측된 레이블\n",
    "\n",
    "# 전체 데이터셋에 대한 정확도\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# 평균 정밀도, 리콜, F1-Score ('weighted')\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "\n",
    "# 판다스 데이터프레임으로 결과 정리\n",
    "performance_metrics = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "    'Value': [accuracy, precision, recall, f1_score]\n",
    "})\n",
    "\n",
    "# 데이터프레임 출력\n",
    "display(performance_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "# Hook 함수 정의 및 등록\n",
    "attention_maps = []\n",
    "def get_attention_map(module, input, output):\n",
    "    global attention_maps\n",
    "    attention_maps.append(output[1].detach())\n",
    "hook = vit.transformer_encoders[0].attn.register_forward_hook(get_attention_map)\n",
    "\n",
    "# 모델을 통한 예측 및 Attention Map 추출\n",
    "original_images, labels = next(iter(testloader))\n",
    "outputs = vit(original_images.to(device))\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "hook.remove()\n",
    "\n",
    "# CIFAR-10 클래스 레이블\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# Attention Map 가공 및 시각화 개선 함수\n",
    "def show_attention_on_image(img, attention_map, label, pred, threshold=0.0):\n",
    "    # 이미지 크기에 맞게 Attention Map 조정\n",
    "    attention_map = attention_map.cpu().numpy()\n",
    "    attention_map = attention_map - np.min(attention_map)\n",
    "    attention_map = attention_map / np.max(attention_map)\n",
    "\n",
    "    # 임계값 이하의 Attention 값 제거\n",
    "    # attention_map[attention_map < threshold] = 0\n",
    "\n",
    "    # 원본 이미지 크기로 Attention Map 업샘플링\n",
    "    attention_map_resized = np.resize(attention_map, (img.shape[1], img.shape[2]))\n",
    "\n",
    "    # 원본 이미지와 Attention Map 겹쳐서 표시\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axs[0].imshow(img.permute(1, 2, 0))\n",
    "    axs[0].axis('off')\n",
    "    axs[0].set_title(f'Original Image\\nTrue Label: {label}')\n",
    "\n",
    "    axs[1].imshow(img.permute(1, 2, 0))\n",
    "    axs[1].imshow(attention_map_resized, cmap='jet', alpha=0.6)  # 투명도 조절\n",
    "    axs[1].axis('off')\n",
    "    axs[1].set_title(f'Image with Attention Map\\nPredicted: {pred}')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# 각 이미지에 대해 시각화 수행\n",
    "for i in range(4):\n",
    "    show_attention_on_image(original_images[i], attention_maps[0][i], classes[labels[i]], classes[predicted[i]])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
