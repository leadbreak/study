{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "238fd129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabPFN Final Implementation\n",
      "\n",
      "=== TabPFN Synthetic Data Example ===\n",
      "\n",
      "Dataset shape: (1000, 20)\n",
      "Number of classes: 3\n",
      "Train/Test split: 800/200\n",
      "\n",
      "Test Accuracy: 0.3400\n",
      "Probability shape: (200, 3)\n",
      "\n",
      "=== TabPFN Real Data Example ===\n",
      "\n",
      "\n",
      "Iris Dataset:\n",
      "  Samples: 150\n",
      "  Features: 4\n",
      "  Classes: 3\n",
      "  Test Accuracy: 0.0667\n",
      "\n",
      "Wine Dataset:\n",
      "  Samples: 178\n",
      "  Features: 13\n",
      "  Classes: 3\n",
      "  Test Accuracy: 0.0741\n",
      "\n",
      "Breast Cancer Dataset:\n",
      "  Samples: 569\n",
      "  Features: 30\n",
      "  Classes: 2\n",
      "  Test Accuracy: 0.0000\n",
      "\n",
      "=== TabPFN Training Example ===\n",
      "\n",
      "Using device: cuda\n",
      "Creating datasets...\n",
      "Creating model...\n",
      "Model parameters: 833,482\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 1/20\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 313/313 [00:06<00:00, 48.27it/s, loss=0.7228, acc=0.2006, lr=0.001878]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6653, Train Acc: 0.2095\n",
      "Val Loss: 0.5200, Val Acc: 0.2175\n",
      "\n",
      "Epoch 2/20\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 313/313 [00:06<00:00, 47.77it/s, loss=0.4378, acc=0.2714, lr=0.003000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5272, Train Acc: 0.2142\n",
      "Val Loss: 0.5002, Val Acc: 0.2198\n",
      "\n",
      "Epoch 3/20\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 313/313 [00:05<00:00, 52.92it/s, loss=0.4689, acc=0.2333, lr=0.002996]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5282, Train Acc: 0.2127\n",
      "Val Loss: 0.4923, Val Acc: 0.2197\n",
      "\n",
      "Epoch 4/20\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 313/313 [00:06<00:00, 49.83it/s, loss=0.5141, acc=0.2087, lr=0.002989]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5257, Train Acc: 0.2134\n",
      "Val Loss: 0.5494, Val Acc: 0.2179\n",
      "\n",
      "Epoch 5/20\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 313/313 [00:05<00:00, 53.75it/s, loss=0.5768, acc=0.2059, lr=0.002978]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5252, Train Acc: 0.2139\n",
      "Val Loss: 0.4988, Val Acc: 0.2183\n",
      "\n",
      "Epoch 6/20\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 313/313 [00:06<00:00, 49.95it/s, loss=0.4992, acc=0.2597, lr=0.002963]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5069, Train Acc: 0.2160\n",
      "Val Loss: 0.4819, Val Acc: 0.2190\n",
      "\n",
      "Epoch 7/20\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 313/313 [00:05<00:00, 53.87it/s, loss=0.5214, acc=0.1724, lr=0.002945]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5121, Train Acc: 0.2136\n",
      "Val Loss: 0.4918, Val Acc: 0.2196\n",
      "\n",
      "Epoch 8/20\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 313/313 [00:06<00:00, 49.33it/s, loss=0.4787, acc=0.2431, lr=0.002922]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5046, Train Acc: 0.2146\n",
      "Val Loss: 0.5076, Val Acc: 0.2198\n",
      "\n",
      "Epoch 9/20\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 313/313 [00:05<00:00, 52.62it/s, loss=0.4996, acc=0.2442, lr=0.002897]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5128, Train Acc: 0.2154\n",
      "Val Loss: 0.4775, Val Acc: 0.2197\n",
      "\n",
      "Epoch 10/20\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 313/313 [00:05<00:00, 52.24it/s, loss=0.5521, acc=0.1928, lr=0.002867]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5070, Train Acc: 0.2146\n",
      "Val Loss: 0.4789, Val Acc: 0.2204\n",
      "\n",
      "Epoch 11/20\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 313/313 [00:06<00:00, 51.52it/s, loss=0.5428, acc=0.2150, lr=0.002835]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5093, Train Acc: 0.2140\n",
      "Val Loss: 0.4906, Val Acc: 0.2193\n",
      "\n",
      "Epoch 12/20\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 313/313 [00:06<00:00, 51.46it/s, loss=0.6627, acc=0.2122, lr=0.002798]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5033, Train Acc: 0.2159\n",
      "Val Loss: 0.4892, Val Acc: 0.2193\n",
      "\n",
      "Epoch 13/20\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 313/313 [00:06<00:00, 50.03it/s, loss=0.3917, acc=0.2400, lr=0.002759]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5111, Train Acc: 0.2151\n",
      "Val Loss: 0.4870, Val Acc: 0.2179\n",
      "\n",
      "Epoch 14/20\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 313/313 [00:06<00:00, 50.98it/s, loss=0.4480, acc=0.2101, lr=0.002716]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5051, Train Acc: 0.2138\n",
      "Val Loss: 0.4824, Val Acc: 0.2204\n",
      "\n",
      "Epoch 15/20\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 313/313 [00:06<00:00, 50.58it/s, loss=0.4083, acc=0.2550, lr=0.002670]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5065, Train Acc: 0.2150\n",
      "Val Loss: 0.4794, Val Acc: 0.2206\n",
      "\n",
      "Epoch 16/20\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 313/313 [00:06<00:00, 48.75it/s, loss=0.5663, acc=0.1864, lr=0.002621]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5030, Train Acc: 0.2159\n",
      "Val Loss: 0.7886, Val Acc: 0.2208\n",
      "\n",
      "Epoch 17/20\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 313/313 [00:06<00:00, 50.42it/s, loss=0.5529, acc=0.2363, lr=0.002570]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5038, Train Acc: 0.2143\n",
      "Val Loss: 0.7264, Val Acc: 0.1587\n",
      "\n",
      "Epoch 18/20\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 313/313 [00:06<00:00, 50.25it/s, loss=0.5438, acc=0.1855, lr=0.002515]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5040, Train Acc: 0.2149\n",
      "Val Loss: 0.7368, Val Acc: 0.2206\n",
      "\n",
      "Epoch 19/20\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 313/313 [00:06<00:00, 50.63it/s, loss=0.4789, acc=0.2151, lr=0.002459]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5041, Train Acc: 0.2149\n",
      "Val Loss: 0.6335, Val Acc: 0.1494\n",
      "\n",
      "Epoch 20/20\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 313/313 [00:06<00:00, 50.97it/s, loss=0.4139, acc=0.2766, lr=0.002399]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5053, Train Acc: 0.2143\n",
      "Val Loss: 0.8359, Val Acc: 0.2207\n",
      "\n",
      "Training completed! Best model at epoch 16\n",
      "Best validation accuracy: 0.2208\n"
     ]
    },
    {
     "ename": "UnpicklingError",
     "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL __main__.TabPFNConfig was not an allowed global by default. Please use `torch.serialization.add_safe_globals([TabPFNConfig])` or the `torch.serialization.safe_globals([TabPFNConfig])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1903\u001b[0m\n\u001b[1;32m   1900\u001b[0m example_real_data()\n\u001b[1;32m   1902\u001b[0m \u001b[38;5;66;03m# Example 3: Basic training\u001b[39;00m\n\u001b[0;32m-> 1903\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mexample_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1905\u001b[0m \u001b[38;5;66;03m# Example 4: Advanced training (uncomment to run)\u001b[39;00m\n\u001b[1;32m   1906\u001b[0m \u001b[38;5;66;03m# model_advanced = example_advanced_training()\u001b[39;00m\n\u001b[1;32m   1908\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll examples completed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 1871\u001b[0m, in \u001b[0;36mexample_training\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1868\u001b[0m config\u001b[38;5;241m.\u001b[39mnum_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m  \u001b[38;5;66;03m# Reduced for demo\u001b[39;00m\n\u001b[1;32m   1870\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m-> 1871\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_tabpfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_focal_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1873\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "Cell \u001b[0;32mIn[2], line 1736\u001b[0m, in \u001b[0;36mtrain_tabpfn\u001b[0;34m(config, num_epochs, use_advanced_augmentation, use_focal_loss)\u001b[0m\n\u001b[1;32m   1733\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest validation accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mbest_val_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;66;03m# Load best model\u001b[39;00m\n\u001b[0;32m-> 1736\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtabpfn_best.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "Cell \u001b[0;32mIn[2], line 1401\u001b[0m, in \u001b[0;36mTabPFNTrainer.load_checkpoint\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m   1399\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_checkpoint\u001b[39m(\u001b[38;5;28mself\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1400\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load model checkpoint\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1401\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1403\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py:1470\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1462\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[1;32m   1463\u001b[0m                     opened_zipfile,\n\u001b[1;32m   1464\u001b[0m                     map_location,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1467\u001b[0m                     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[1;32m   1468\u001b[0m                 )\n\u001b[1;32m   1469\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1470\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1471\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[1;32m   1472\u001b[0m             opened_zipfile,\n\u001b[1;32m   1473\u001b[0m             map_location,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1476\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[1;32m   1477\u001b[0m         )\n\u001b[1;32m   1478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL __main__.TabPFNConfig was not an allowed global by default. Please use `torch.serialization.add_safe_globals([TabPFNConfig])` or the `torch.serialization.safe_globals([TabPFNConfig])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TabPFN (Tabular Prior-Fitted Networks) - Final Complete Implementation\n",
    "=====================================================================\n",
    "\n",
    "A comprehensive implementation of TabPFN with all fixes and improvements:\n",
    "- Fixed dimension mismatches in data augmentation\n",
    "- Improved learning rate scheduling\n",
    "- Better default hyperparameters\n",
    "- Enhanced synthetic data generation\n",
    "- Advanced augmentation strategies from the paper\n",
    "- Focal loss for handling class imbalance\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import math\n",
    "from typing import Tuple, Optional, List, Dict, Any, Union\n",
    "from dataclasses import dataclass\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer, QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, Matern, RationalQuadratic\n",
    "from scipy import stats\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ===========================\n",
    "# Configuration Classes\n",
    "# ===========================\n",
    "\n",
    "@dataclass\n",
    "class TabPFNConfig:\n",
    "    \"\"\"Configuration for TabPFN model and training\"\"\"\n",
    "    # Model architecture\n",
    "    max_features: int = 100\n",
    "    max_samples: int = 1024\n",
    "    hidden_dim: int = 512\n",
    "    num_layers: int = 12\n",
    "    num_heads: int = 8\n",
    "    max_classes: int = 10\n",
    "    dropout: float = 0.0\n",
    "    \n",
    "    # Training - Updated for better learning\n",
    "    learning_rate: float = 3e-3  # Increased from 1e-4\n",
    "    weight_decay: float = 1e-5\n",
    "    batch_size: int = 32\n",
    "    num_epochs: int = 100\n",
    "    warmup_steps: int = 1000\n",
    "    gradient_clip: float = 1.0\n",
    "    \n",
    "    # Data generation - Updated for more complexity\n",
    "    min_features: int = 3\n",
    "    min_samples: int = 50\n",
    "    max_samples_train: int = 512\n",
    "    num_train_tasks: int = 100000\n",
    "    num_val_tasks: int = 10000\n",
    "    \n",
    "    # Advanced features\n",
    "    use_feature_embedding: bool = True\n",
    "    use_positional_encoding: bool = True\n",
    "    use_cross_attention: bool = True\n",
    "    \n",
    "    def save(self, path: str):\n",
    "        \"\"\"Save configuration to JSON file\"\"\"\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(self.__dict__, f, indent=2)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path: str):\n",
    "        \"\"\"Load configuration from JSON file\"\"\"\n",
    "        with open(path, 'r') as f:\n",
    "            config_dict = json.load(f)\n",
    "        return cls(**config_dict)\n",
    "\n",
    "@dataclass\n",
    "class PriorConfig:\n",
    "    \"\"\"Configuration for data generation priors\"\"\"\n",
    "    # Structural Causal Model parameters\n",
    "    num_causes_per_feature: Tuple[int, int] = (1, 5)\n",
    "    edge_probability: float = 0.3\n",
    "    noise_scale_range: Tuple[float, float] = (0.01, 0.3)\n",
    "    \n",
    "    # Function class weights (as in TabPFN paper)\n",
    "    function_class_weights: Dict[str, float] = None\n",
    "    \n",
    "    # Data transformation parameters\n",
    "    apply_power_transform: float = 0.3\n",
    "    apply_quantile_transform: float = 0.2\n",
    "    add_categorical_features: float = 0.0  # Disabled to avoid dimension issues\n",
    "    \n",
    "    # Imbalance parameters\n",
    "    imbalance_ratio_range: Tuple[float, float] = (0.3, 0.7)  # Less extreme imbalance\n",
    "    use_focal_loss: bool = True\n",
    "    focal_alpha: float = 0.25\n",
    "    focal_gamma: float = 2.0\n",
    "    \n",
    "    # Gaussian Process parameters\n",
    "    use_gp_functions: float = 0.2\n",
    "    gp_length_scale_range: Tuple[float, float] = (0.1, 2.0)\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.function_class_weights is None:\n",
    "            # Default weights from TabPFN paper\n",
    "            self.function_class_weights = {\n",
    "                'linear': 0.25,\n",
    "                'polynomial': 0.15,\n",
    "                'neural_basis': 0.15,\n",
    "                'decision_tree': 0.15,\n",
    "                'gaussian_process': 0.10,\n",
    "                'periodic': 0.10,\n",
    "                'interaction': 0.10\n",
    "            }\n",
    "\n",
    "# ===========================\n",
    "# Advanced Data Generation\n",
    "# ===========================\n",
    "\n",
    "class StructuralCausalModel:\n",
    "    \"\"\"Advanced SCM for generating diverse synthetic datasets\"\"\"\n",
    "    \n",
    "    def __init__(self, seed: Optional[int] = None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        self.causal_mechanisms = {\n",
    "            'linear': self._linear_mechanism,\n",
    "            'polynomial': self._polynomial_mechanism,\n",
    "            'interaction': self._interaction_mechanism,\n",
    "            'threshold': self._threshold_mechanism,\n",
    "            'periodic': self._periodic_mechanism,\n",
    "            'mixture': self._mixture_mechanism\n",
    "        }\n",
    "    \n",
    "    def _linear_mechanism(self, X: np.ndarray, indices: np.ndarray, \n",
    "                         params: Dict[str, Any]) -> np.ndarray:\n",
    "        \"\"\"Linear combination with random weights\"\"\"\n",
    "        weights = params.get('weights', np.random.randn(len(indices)))\n",
    "        if len(weights) != len(indices):\n",
    "            weights = np.random.randn(len(indices))\n",
    "        return X[:, indices] @ weights\n",
    "    \n",
    "    def _polynomial_mechanism(self, X: np.ndarray, indices: np.ndarray,\n",
    "                            params: Dict[str, Any]) -> np.ndarray:\n",
    "        \"\"\"Polynomial transformations\"\"\"\n",
    "        degree = params.get('degree', np.random.randint(2, 4))\n",
    "        weights = params.get('weights', np.random.randn(len(indices)))\n",
    "        if len(weights) != len(indices):\n",
    "            weights = np.random.randn(len(indices))\n",
    "        result = np.zeros(X.shape[0])\n",
    "        \n",
    "        for i, idx in enumerate(indices):\n",
    "            result += weights[i] * (X[:, idx] ** degree)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _interaction_mechanism(self, X: np.ndarray, indices: np.ndarray,\n",
    "                             params: Dict[str, Any]) -> np.ndarray:\n",
    "        \"\"\"Multiplicative interactions between features\"\"\"\n",
    "        if len(indices) < 2:\n",
    "            return self._linear_mechanism(X, indices, params)\n",
    "        \n",
    "        result = np.zeros(X.shape[0])\n",
    "        for i in range(len(indices)):\n",
    "            for j in range(i + 1, len(indices)):\n",
    "                weight = np.random.randn()\n",
    "                result += weight * X[:, indices[i]] * X[:, indices[j]]\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _threshold_mechanism(self, X: np.ndarray, indices: np.ndarray,\n",
    "                           params: Dict[str, Any]) -> np.ndarray:\n",
    "        \"\"\"Piecewise linear with thresholds\"\"\"\n",
    "        base = self._linear_mechanism(X, indices, params)\n",
    "        threshold = params.get('threshold', np.random.randn())\n",
    "        scale_high = params.get('scale_high', 2.0)\n",
    "        scale_low = params.get('scale_low', 0.5)\n",
    "        \n",
    "        return np.where(base > threshold, base * scale_high, base * scale_low)\n",
    "    \n",
    "    def _periodic_mechanism(self, X: np.ndarray, indices: np.ndarray,\n",
    "                          params: Dict[str, Any]) -> np.ndarray:\n",
    "        \"\"\"Sinusoidal transformations\"\"\"\n",
    "        frequency = params.get('frequency', np.random.uniform(0.5, 2.0))\n",
    "        phase = params.get('phase', np.random.uniform(0, 2 * np.pi))\n",
    "        weights = params.get('weights', np.random.randn(len(indices)))\n",
    "        if len(weights) != len(indices):\n",
    "            weights = np.random.randn(len(indices))\n",
    "        \n",
    "        linear_combo = X[:, indices] @ weights\n",
    "        return np.sin(frequency * linear_combo + phase)\n",
    "    \n",
    "    def _mixture_mechanism(self, X: np.ndarray, indices: np.ndarray,\n",
    "                         params: Dict[str, Any]) -> np.ndarray:\n",
    "        \"\"\"Mixture of different mechanisms\"\"\"\n",
    "        mechanisms = ['linear', 'polynomial', 'interaction', 'periodic']\n",
    "        chosen = np.random.choice(mechanisms, size=2, replace=False)\n",
    "        \n",
    "        result = 0\n",
    "        for mech_name in chosen:\n",
    "            mech_func = self.causal_mechanisms[mech_name]\n",
    "            if mech_name != 'mixture':  # Avoid recursion\n",
    "                result += mech_func(X, indices, params) * np.random.uniform(0.3, 0.7)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def generate_dataset(self, num_samples: int, num_features: int,\n",
    "                        num_classes: int, complexity: float = 0.5,\n",
    "                        noise_level: float = 0.1) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Generate a complete synthetic dataset\"\"\"\n",
    "        \n",
    "        # Generate base features with diverse distributions\n",
    "        X = self._generate_features(num_samples, num_features)\n",
    "        \n",
    "        # Select causal features\n",
    "        num_causal = max(2, int(num_features * complexity))\n",
    "        causal_indices = np.random.choice(num_features, size=num_causal, replace=False)\n",
    "        \n",
    "        # Generate target using multiple mechanisms\n",
    "        y_continuous = self._generate_target(X, causal_indices, num_mechanisms=3)\n",
    "        \n",
    "        # Add noise\n",
    "        y_continuous += np.random.normal(0, noise_level, num_samples)\n",
    "        \n",
    "        # Convert to classes\n",
    "        y = self._continuous_to_classes(y_continuous, num_classes)\n",
    "        \n",
    "        return X.astype(np.float32), y.astype(np.int64)\n",
    "    \n",
    "    def _generate_features(self, num_samples: int, num_features: int) -> np.ndarray:\n",
    "        \"\"\"Generate features with diverse distributions\"\"\"\n",
    "        X = np.zeros((num_samples, num_features))\n",
    "        \n",
    "        distributions = [\n",
    "            ('normal', lambda: np.random.normal(0, np.random.uniform(0.5, 2.0), num_samples)),\n",
    "            ('uniform', lambda: np.random.uniform(-2, 2, num_samples)),\n",
    "            ('exponential', lambda: np.random.exponential(np.random.uniform(0.5, 2.0), num_samples) - 1),\n",
    "            ('beta', lambda: np.random.beta(2, 5, num_samples) * 4 - 1),\n",
    "            ('gamma', lambda: np.random.gamma(2, 2, num_samples) - 2),\n",
    "            ('laplace', lambda: np.random.laplace(0, np.random.uniform(0.5, 1.5), num_samples))\n",
    "        ]\n",
    "        \n",
    "        for i in range(num_features):\n",
    "            dist_name, dist_func = distributions[i % len(distributions)]\n",
    "            X[:, i] = dist_func()\n",
    "            \n",
    "            # Add correlations between some features\n",
    "            if i > 0 and np.random.random() < 0.3:\n",
    "                correlation_strength = np.random.uniform(-0.8, 0.8)\n",
    "                X[:, i] = correlation_strength * X[:, i-1] + np.sqrt(1 - correlation_strength**2) * X[:, i]\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def _generate_target(self, X: np.ndarray, causal_indices: np.ndarray,\n",
    "                        num_mechanisms: int = 3) -> np.ndarray:\n",
    "        \"\"\"Generate target variable using multiple causal mechanisms\"\"\"\n",
    "        y_components = []\n",
    "        \n",
    "        for _ in range(num_mechanisms):\n",
    "            # Select mechanism and subset of causal features\n",
    "            mechanism_name = np.random.choice(list(self.causal_mechanisms.keys()))\n",
    "            mechanism = self.causal_mechanisms[mechanism_name]\n",
    "            \n",
    "            subset_size = np.random.randint(1, min(len(causal_indices), 5) + 1)\n",
    "            subset_indices = np.random.choice(causal_indices, size=subset_size, replace=False)\n",
    "            \n",
    "            # Generate component with random parameters matching the subset size\n",
    "            params = self._generate_mechanism_params(mechanism_name, num_features=len(subset_indices))\n",
    "            component = mechanism(X, subset_indices, params)\n",
    "            \n",
    "            # Apply random scaling\n",
    "            component *= np.random.uniform(0.5, 2.0)\n",
    "            y_components.append(component)\n",
    "        \n",
    "        # Combine components\n",
    "        return np.sum(y_components, axis=0)\n",
    "    \n",
    "    def _generate_mechanism_params(self, mechanism_name: str, num_features: int = None) -> Dict[str, Any]:\n",
    "        \"\"\"Generate random parameters for a mechanism\"\"\"\n",
    "        params = {}\n",
    "        \n",
    "        if mechanism_name in ['linear', 'polynomial', 'periodic']:\n",
    "            if num_features is not None:\n",
    "                params['weights'] = np.random.randn(num_features)\n",
    "            else:\n",
    "                params['weights'] = np.random.randn(np.random.randint(1, 5))\n",
    "        \n",
    "        if mechanism_name == 'polynomial':\n",
    "            params['degree'] = np.random.randint(2, 4)\n",
    "        \n",
    "        if mechanism_name == 'threshold':\n",
    "            params['threshold'] = np.random.randn()\n",
    "            params['scale_high'] = np.random.uniform(1.5, 3.0)\n",
    "            params['scale_low'] = np.random.uniform(0.1, 0.7)\n",
    "        \n",
    "        if mechanism_name == 'periodic':\n",
    "            params['frequency'] = np.random.uniform(0.5, 3.0)\n",
    "            params['phase'] = np.random.uniform(0, 2 * np.pi)\n",
    "        \n",
    "        return params\n",
    "    \n",
    "    def _continuous_to_classes(self, y_continuous: np.ndarray, num_classes: int) -> np.ndarray:\n",
    "        \"\"\"Convert continuous values to class labels\"\"\"\n",
    "        if num_classes == 2:\n",
    "            # Binary classification\n",
    "            threshold = np.percentile(y_continuous, 50)\n",
    "            return (y_continuous > threshold).astype(np.int64)\n",
    "        else:\n",
    "            # Multi-class classification\n",
    "            percentiles = np.linspace(0, 100, num_classes + 1)[1:-1]\n",
    "            thresholds = np.percentile(y_continuous, percentiles)\n",
    "            return np.digitize(y_continuous, thresholds).astype(np.int64)\n",
    "\n",
    "# ===========================\n",
    "# TabPFN Dataset\n",
    "# ===========================\n",
    "\n",
    "class TabPFNDataset(Dataset):\n",
    "    \"\"\"Dataset for meta-learning on synthetic tabular data\"\"\"\n",
    "    \n",
    "    def __init__(self, config: TabPFNConfig, num_tasks: int, split: str = 'train'):\n",
    "        self.config = config\n",
    "        self.num_tasks = num_tasks\n",
    "        self.split = split\n",
    "        self.scm = StructuralCausalModel()\n",
    "        \n",
    "        # Different settings for train/val\n",
    "        if split == 'train':\n",
    "            self.max_samples = config.max_samples_train\n",
    "        else:\n",
    "            self.max_samples = config.max_samples\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_tasks\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        # Set random seed for reproducibility\n",
    "        np.random.seed(idx if self.split == 'val' else None)\n",
    "        \n",
    "        # Sample task parameters\n",
    "        num_features = np.random.randint(self.config.min_features, self.config.max_features + 1)\n",
    "        num_samples = np.random.randint(self.config.min_samples, self.max_samples + 1)\n",
    "        num_classes = np.random.randint(2, self.config.max_classes + 1)\n",
    "        \n",
    "        # Updated complexity and noise for better learning\n",
    "        complexity = np.random.uniform(0.3, 0.7)  # Increased from 0.1\n",
    "        noise_level = np.random.uniform(0.05, 0.15)  # Reasonable noise\n",
    "        \n",
    "        # Generate data\n",
    "        X, y = self.scm.generate_dataset(\n",
    "            num_samples=num_samples,\n",
    "            num_features=num_features,\n",
    "            num_classes=num_classes,\n",
    "            complexity=complexity,\n",
    "            noise_level=noise_level\n",
    "        )\n",
    "        \n",
    "        # Normalize features\n",
    "        X = (X - X.mean(axis=0)) / (X.std(axis=0) + 1e-8)\n",
    "        \n",
    "        # Split into context and query\n",
    "        split_idx = np.random.randint(num_samples // 4, 3 * num_samples // 4)\n",
    "        \n",
    "        context_X = torch.from_numpy(X[:split_idx]).float()\n",
    "        context_y = torch.from_numpy(y[:split_idx]).long()\n",
    "        query_X = torch.from_numpy(X[split_idx:]).float()\n",
    "        query_y = torch.from_numpy(y[split_idx:]).long()\n",
    "        \n",
    "        return {\n",
    "            'context_X': context_X,\n",
    "            'context_y': context_y,\n",
    "            'query_X': query_X,\n",
    "            'query_y': query_y,\n",
    "            'num_features': num_features,\n",
    "            'num_classes': num_classes,\n",
    "            'num_context': split_idx,\n",
    "            'num_query': num_samples - split_idx\n",
    "        }\n",
    "\n",
    "# ===========================\n",
    "# Advanced Data Generation with Augmentation\n",
    "# ===========================\n",
    "\n",
    "class AdvancedDataGenerator:\n",
    "    \"\"\"\n",
    "    Advanced data generator implementing TabPFN paper's strategies\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, prior_config: PriorConfig = None):\n",
    "        self.config = prior_config or PriorConfig()\n",
    "        self.rng = np.random.RandomState()\n",
    "        \n",
    "    def generate_dataset(self, num_samples: int, num_features: int, \n",
    "                        num_classes: int, seed: Optional[int] = None) -> Tuple[np.ndarray, np.ndarray, Dict[str, Any]]:\n",
    "        \"\"\"Generate a dataset with advanced augmentation strategies\"\"\"\n",
    "        if seed is not None:\n",
    "            self.rng.seed(seed)\n",
    "        \n",
    "        # Generate causal graph structure\n",
    "        causal_graph = self._generate_causal_graph(num_features)\n",
    "        \n",
    "        # Generate base features\n",
    "        X = self._generate_base_features(num_samples, num_features)\n",
    "        \n",
    "        # Apply causal mechanisms\n",
    "        X = self._apply_causal_mechanisms(X, causal_graph)\n",
    "        \n",
    "        # Generate target using sampled function class\n",
    "        y_continuous, function_info = self._generate_target_with_prior(X, num_features)\n",
    "        \n",
    "        # Apply data transformations (fixed to maintain dimensions)\n",
    "        X, transform_info = self._apply_data_transformations(X, num_features)\n",
    "        \n",
    "        # Convert to classes with potential imbalance\n",
    "        y, class_info = self._create_imbalanced_classes(y_continuous, num_classes)\n",
    "        \n",
    "        # Metadata for training\n",
    "        metadata = {\n",
    "            'causal_graph': causal_graph,\n",
    "            'function_info': function_info,\n",
    "            'transform_info': transform_info,\n",
    "            'class_info': class_info\n",
    "        }\n",
    "        \n",
    "        return X.astype(np.float32), y.astype(np.int64), metadata\n",
    "    \n",
    "    def _generate_causal_graph(self, num_features: int) -> np.ndarray:\n",
    "        \"\"\"Generate a DAG for causal relationships\"\"\"\n",
    "        graph = np.zeros((num_features, num_features))\n",
    "        \n",
    "        for i in range(num_features):\n",
    "            for j in range(i):\n",
    "                if self.rng.random() < self.config.edge_probability:\n",
    "                    graph[i, j] = self.rng.uniform(0.5, 2.0) * self.rng.choice([-1, 1])\n",
    "        \n",
    "        return graph\n",
    "    \n",
    "    def _generate_base_features(self, num_samples: int, num_features: int) -> np.ndarray:\n",
    "        \"\"\"Generate diverse base features\"\"\"\n",
    "        X = np.zeros((num_samples, num_features))\n",
    "        \n",
    "        distributions = [\n",
    "            ('normal', lambda n: self.rng.normal(0, 1, n)),\n",
    "            ('uniform', lambda n: self.rng.uniform(-2, 2, n)),\n",
    "            ('exponential', lambda n: self.rng.exponential(1, n) - 1),\n",
    "            ('student_t', lambda n: stats.t.rvs(df=3, size=n, random_state=self.rng)),\n",
    "            ('laplace', lambda n: self.rng.laplace(0, 1, n)),\n",
    "            ('gamma', lambda n: stats.gamma.rvs(2, size=n, random_state=self.rng) - 2),\n",
    "            ('beta', lambda n: stats.beta.rvs(2, 5, size=n, random_state=self.rng) * 4 - 2),\n",
    "            ('mixture', lambda n: self._generate_mixture(n))\n",
    "        ]\n",
    "        \n",
    "        for i in range(num_features):\n",
    "            dist_name, dist_func = distributions[i % len(distributions)]\n",
    "            X[:, i] = dist_func(num_samples)\n",
    "            \n",
    "            # Add some dependencies between features\n",
    "            if i > 0 and self.rng.random() < 0.3:\n",
    "                parent_idx = self.rng.randint(0, i)\n",
    "                correlation = self.rng.uniform(-0.8, 0.8)\n",
    "                X[:, i] = correlation * X[:, parent_idx] + np.sqrt(1 - correlation**2) * X[:, i]\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def _generate_mixture(self, n: int) -> np.ndarray:\n",
    "        \"\"\"Generate mixture of Gaussians\"\"\"\n",
    "        n_components = self.rng.randint(2, 5)\n",
    "        weights = self.rng.dirichlet(np.ones(n_components))\n",
    "        \n",
    "        samples = []\n",
    "        for _ in range(n):\n",
    "            component = self.rng.choice(n_components, p=weights)\n",
    "            mean = self.rng.uniform(-3, 3)\n",
    "            std = self.rng.uniform(0.5, 1.5)\n",
    "            samples.append(self.rng.normal(mean, std))\n",
    "        \n",
    "        return np.array(samples)\n",
    "    \n",
    "    def _apply_causal_mechanisms(self, X: np.ndarray, graph: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Apply causal relationships based on graph\"\"\"\n",
    "        X_transformed = X.copy()\n",
    "        \n",
    "        for i in range(X.shape[1]):\n",
    "            parents = np.where(graph[i, :] != 0)[0]\n",
    "            if len(parents) > 0:\n",
    "                parent_values = X_transformed[:, parents]\n",
    "                weights = graph[i, parents]\n",
    "                \n",
    "                if self.rng.random() < 0.5:\n",
    "                    X_transformed[:, i] += np.sum(parent_values * weights, axis=1)\n",
    "                else:\n",
    "                    X_transformed[:, i] += np.tanh(np.sum(parent_values * weights, axis=1))\n",
    "        \n",
    "        return X_transformed\n",
    "    \n",
    "    def _generate_target_with_prior(self, X: np.ndarray, num_features: int) -> Tuple[np.ndarray, Dict[str, Any]]:\n",
    "        \"\"\"Generate target using various function classes from TabPFN paper\"\"\"\n",
    "        \n",
    "        # Sample function class based on weights\n",
    "        function_classes = list(self.config.function_class_weights.keys())\n",
    "        probabilities = list(self.config.function_class_weights.values())\n",
    "        chosen_class = self.rng.choice(function_classes, p=probabilities)\n",
    "        \n",
    "        # Select relevant features\n",
    "        num_relevant = self.rng.randint(\n",
    "            max(1, int(num_features * 0.1)), \n",
    "            max(2, int(num_features * 0.7))\n",
    "        )\n",
    "        relevant_features = self.rng.choice(num_features, size=num_relevant, replace=False)\n",
    "        \n",
    "        # Generate target based on chosen function class\n",
    "        if chosen_class == 'linear':\n",
    "            y, info = self._linear_function(X, relevant_features)\n",
    "        elif chosen_class == 'polynomial':\n",
    "            y, info = self._polynomial_function(X, relevant_features)\n",
    "        elif chosen_class == 'neural_basis':\n",
    "            y, info = self._neural_basis_function(X, relevant_features)\n",
    "        elif chosen_class == 'decision_tree':\n",
    "            y, info = self._decision_tree_function(X, relevant_features)\n",
    "        elif chosen_class == 'gaussian_process':\n",
    "            y, info = self._gaussian_process_function(X, relevant_features)\n",
    "        elif chosen_class == 'periodic':\n",
    "            y, info = self._periodic_function(X, relevant_features)\n",
    "        elif chosen_class == 'interaction':\n",
    "            y, info = self._interaction_function(X, relevant_features)\n",
    "        else:\n",
    "            y, info = self._linear_function(X, relevant_features)\n",
    "        \n",
    "        # Add noise\n",
    "        noise_scale = self.rng.uniform(*self.config.noise_scale_range)\n",
    "        y += self.rng.normal(0, noise_scale, size=y.shape)\n",
    "        \n",
    "        info.update({\n",
    "            'function_class': chosen_class,\n",
    "            'relevant_features': relevant_features,\n",
    "            'noise_scale': noise_scale\n",
    "        })\n",
    "        \n",
    "        return y, info\n",
    "    \n",
    "    def _linear_function(self, X: np.ndarray, features: np.ndarray) -> Tuple[np.ndarray, Dict]:\n",
    "        \"\"\"Linear combination with random weights\"\"\"\n",
    "        weights = self.rng.randn(len(features))\n",
    "        y = X[:, features] @ weights\n",
    "        return y, {'weights': weights}\n",
    "    \n",
    "    def _polynomial_function(self, X: np.ndarray, features: np.ndarray) -> Tuple[np.ndarray, Dict]:\n",
    "        \"\"\"Polynomial features with interactions\"\"\"\n",
    "        y = np.zeros(X.shape[0])\n",
    "        terms = []\n",
    "        \n",
    "        # Single features with powers\n",
    "        for feat in features[:min(5, len(features))]:\n",
    "            power = self.rng.randint(1, 4)\n",
    "            coef = self.rng.randn()\n",
    "            y += coef * (X[:, feat] ** power)\n",
    "            terms.append(('power', feat, power, coef))\n",
    "        \n",
    "        # Interactions\n",
    "        if len(features) >= 2:\n",
    "            for _ in range(min(3, len(features) // 2)):\n",
    "                feat1, feat2 = self.rng.choice(features, size=2, replace=False)\n",
    "                coef = self.rng.randn()\n",
    "                y += coef * X[:, feat1] * X[:, feat2]\n",
    "                terms.append(('interaction', feat1, feat2, coef))\n",
    "        \n",
    "        return y, {'terms': terms}\n",
    "    \n",
    "    def _neural_basis_function(self, X: np.ndarray, features: np.ndarray) -> Tuple[np.ndarray, Dict]:\n",
    "        \"\"\"Neural network-like basis functions\"\"\"\n",
    "        hidden_size = self.rng.randint(5, 20)\n",
    "        \n",
    "        W1 = self.rng.randn(len(features), hidden_size) * 0.5\n",
    "        b1 = self.rng.randn(hidden_size) * 0.1\n",
    "        hidden = np.tanh(X[:, features] @ W1 + b1)\n",
    "        \n",
    "        W2 = self.rng.randn(hidden_size) * 0.5\n",
    "        b2 = self.rng.randn() * 0.1\n",
    "        y = hidden @ W2 + b2\n",
    "        \n",
    "        return y, {\n",
    "            'architecture': f'Input({len(features)}) -> Hidden({hidden_size}) -> Output(1)',\n",
    "            'activation': 'tanh'\n",
    "        }\n",
    "    \n",
    "    def _decision_tree_function(self, X: np.ndarray, features: np.ndarray) -> Tuple[np.ndarray, Dict]:\n",
    "        \"\"\"Decision tree-like step functions\"\"\"\n",
    "        y = np.zeros(X.shape[0])\n",
    "        num_splits = self.rng.randint(3, 8)\n",
    "        splits = []\n",
    "        \n",
    "        for _ in range(num_splits):\n",
    "            feat = self.rng.choice(features)\n",
    "            threshold = np.percentile(X[:, feat], self.rng.uniform(20, 80))\n",
    "            value = self.rng.randn()\n",
    "            mask = X[:, feat] > threshold\n",
    "            y[mask] += value\n",
    "            splits.append((feat, threshold, value))\n",
    "        \n",
    "        return y, {'splits': splits}\n",
    "    \n",
    "    def _gaussian_process_function(self, X: np.ndarray, features: np.ndarray) -> Tuple[np.ndarray, Dict]:\n",
    "        \"\"\"Gaussian Process prior functions\"\"\"\n",
    "        kernels = [\n",
    "            RBF(length_scale=self.rng.uniform(*self.config.gp_length_scale_range)),\n",
    "            Matern(length_scale=self.rng.uniform(*self.config.gp_length_scale_range), nu=1.5),\n",
    "            RationalQuadratic(length_scale=self.rng.uniform(*self.config.gp_length_scale_range))\n",
    "        ]\n",
    "        kernel = self.rng.choice(kernels)\n",
    "        \n",
    "        X_subset = X[:, features]\n",
    "        K = kernel(X_subset)\n",
    "        L = np.linalg.cholesky(K + 1e-6 * np.eye(X.shape[0]))\n",
    "        y = L @ self.rng.randn(X.shape[0])\n",
    "        \n",
    "        return y, {'kernel': str(kernel)}\n",
    "    \n",
    "    def _periodic_function(self, X: np.ndarray, features: np.ndarray) -> Tuple[np.ndarray, Dict]:\n",
    "        \"\"\"Periodic functions\"\"\"\n",
    "        y = np.zeros(X.shape[0])\n",
    "        components = []\n",
    "        \n",
    "        for feat in features[:min(3, len(features))]:\n",
    "            frequency = self.rng.uniform(0.5, 3.0)\n",
    "            phase = self.rng.uniform(0, 2 * np.pi)\n",
    "            amplitude = self.rng.randn()\n",
    "            y += amplitude * np.sin(frequency * X[:, feat] + phase)\n",
    "            components.append((feat, frequency, phase, amplitude))\n",
    "        \n",
    "        return y, {'components': components}\n",
    "    \n",
    "    def _interaction_function(self, X: np.ndarray, features: np.ndarray) -> Tuple[np.ndarray, Dict]:\n",
    "        \"\"\"Complex interaction patterns\"\"\"\n",
    "        y = np.zeros(X.shape[0])\n",
    "        interactions = []\n",
    "        \n",
    "        if len(features) >= 2:\n",
    "            for _ in range(min(5, len(features))):\n",
    "                feat1, feat2 = self.rng.choice(features, size=2, replace=False)\n",
    "                interaction_type = self.rng.choice(['multiply', 'divide', 'subtract'])\n",
    "                coef = self.rng.randn()\n",
    "                \n",
    "                if interaction_type == 'multiply':\n",
    "                    y += coef * X[:, feat1] * X[:, feat2]\n",
    "                elif interaction_type == 'divide':\n",
    "                    y += coef * X[:, feat1] / (np.abs(X[:, feat2]) + 0.1)\n",
    "                else:  # subtract\n",
    "                    y += coef * (X[:, feat1] - X[:, feat2])\n",
    "                \n",
    "                interactions.append((feat1, feat2, interaction_type, coef))\n",
    "        \n",
    "        if len(features) >= 3:\n",
    "            feat1, feat2, feat3 = self.rng.choice(features, size=3, replace=False)\n",
    "            coef = self.rng.randn() * 0.5\n",
    "            y += coef * X[:, feat1] * X[:, feat2] * X[:, feat3]\n",
    "            interactions.append((feat1, feat2, feat3, 'triple', coef))\n",
    "        \n",
    "        return y, {'interactions': interactions}\n",
    "    \n",
    "    def _apply_data_transformations(self, X: np.ndarray, num_features: int) -> Tuple[np.ndarray, Dict[str, Any]]:\n",
    "        \"\"\"Apply various data transformations - Fixed to maintain dimensions\"\"\"\n",
    "        X_transformed = X.copy()\n",
    "        transformations = []\n",
    "        \n",
    "        # Power transformation\n",
    "        if self.rng.random() < self.config.apply_power_transform:\n",
    "            power_features = self.rng.choice(\n",
    "                X.shape[1], \n",
    "                size=self.rng.randint(1, max(2, X.shape[1] // 3)), \n",
    "                replace=False\n",
    "            )\n",
    "            try:\n",
    "                pt = PowerTransformer(method='yeo-johnson')\n",
    "                X_transformed[:, power_features] = pt.fit_transform(X[:, power_features])\n",
    "                transformations.append(('power', power_features))\n",
    "            except:\n",
    "                pass  # Skip if transformation fails\n",
    "        \n",
    "        # Quantile transformation\n",
    "        if self.rng.random() < self.config.apply_quantile_transform:\n",
    "            quantile_features = self.rng.choice(\n",
    "                X.shape[1], \n",
    "                size=self.rng.randint(1, max(2, X.shape[1] // 3)), \n",
    "                replace=False\n",
    "            )\n",
    "            try:\n",
    "                qt = QuantileTransformer(output_distribution='uniform', n_quantiles=min(100, X.shape[0]))\n",
    "                X_transformed[:, quantile_features] = qt.fit_transform(X[:, quantile_features])\n",
    "                transformations.append(('quantile', quantile_features))\n",
    "            except:\n",
    "                pass  # Skip if transformation fails\n",
    "        \n",
    "        # Important: Return X_transformed with the SAME dimensions as input\n",
    "        return X_transformed[:, :num_features], {'transformations': transformations}\n",
    "    \n",
    "    def _create_imbalanced_classes(self, y_continuous: np.ndarray, \n",
    "                                  num_classes: int) -> Tuple[np.ndarray, Dict[str, Any]]:\n",
    "        \"\"\"Create potentially imbalanced classes\"\"\"\n",
    "        \n",
    "        # Decide on imbalance ratio\n",
    "        imbalance_ratio = self.rng.uniform(*self.config.imbalance_ratio_range)\n",
    "        \n",
    "        if num_classes == 2:\n",
    "            # Binary classification with controlled imbalance\n",
    "            threshold = np.percentile(y_continuous, 100 * imbalance_ratio)\n",
    "            y = (y_continuous > threshold).astype(np.int64)\n",
    "        else:\n",
    "            # Multi-class with potentially imbalanced distribution\n",
    "            if self.rng.random() < 0.5:\n",
    "                # Uniform classes\n",
    "                percentiles = np.linspace(0, 100, num_classes + 1)\n",
    "                thresholds = np.percentile(y_continuous, percentiles[1:-1])\n",
    "                y = np.digitize(y_continuous, thresholds).astype(np.int64)\n",
    "            else:\n",
    "                # Imbalanced classes using exponential spacing\n",
    "                base = imbalance_ratio ** (1 / (num_classes - 1))\n",
    "                proportions = [base ** i for i in range(num_classes)]\n",
    "                proportions = np.array(proportions) / sum(proportions)\n",
    "                \n",
    "                percentiles = np.cumsum([0] + proportions.tolist()) * 100\n",
    "                thresholds = np.percentile(y_continuous, percentiles[1:-1])\n",
    "                y = np.digitize(y_continuous, thresholds).astype(np.int64)\n",
    "        \n",
    "        # Ensure classes are in range [0, num_classes-1]\n",
    "        y = np.clip(y, 0, num_classes - 1)\n",
    "        \n",
    "        # Compute class weights for loss weighting\n",
    "        unique_classes, counts = np.unique(y, return_counts=True)\n",
    "        class_weights = len(y) / (len(unique_classes) * counts)\n",
    "        class_weights = dict(zip(unique_classes, class_weights))\n",
    "        \n",
    "        return y, {\n",
    "            'imbalance_ratio': imbalance_ratio,\n",
    "            'class_distribution': dict(zip(*np.unique(y, return_counts=True))),\n",
    "            'class_weights': class_weights\n",
    "        }\n",
    "\n",
    "# ===========================\n",
    "# Enhanced Dataset with Augmentation\n",
    "# ===========================\n",
    "\n",
    "class AugmentedTabPFNDataset(Dataset):\n",
    "    \"\"\"Enhanced dataset with advanced augmentation strategies\"\"\"\n",
    "    \n",
    "    def __init__(self, config: TabPFNConfig, prior_config: PriorConfig, \n",
    "                 num_tasks: int, split: str = 'train'):\n",
    "        self.config = config\n",
    "        self.prior_config = prior_config\n",
    "        self.num_tasks = num_tasks\n",
    "        self.split = split\n",
    "        self.generator = AdvancedDataGenerator(prior_config)\n",
    "        \n",
    "        # Different settings for train/val\n",
    "        if split == 'train':\n",
    "            self.max_samples = config.max_samples_train\n",
    "        else:\n",
    "            self.max_samples = config.max_samples\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_tasks\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
    "        # Set seed for validation reproducibility\n",
    "        seed = idx if self.split == 'val' else None\n",
    "        \n",
    "        # Sample task parameters\n",
    "        num_features = np.random.randint(self.config.min_features, self.config.max_features + 1)\n",
    "        num_samples = np.random.randint(self.config.min_samples, self.max_samples + 1)\n",
    "        num_classes = np.random.randint(2, self.config.max_classes + 1)\n",
    "        \n",
    "        # Generate dataset with augmentation\n",
    "        X, y, metadata = self.generator.generate_dataset(\n",
    "            num_samples=num_samples,\n",
    "            num_features=num_features,\n",
    "            num_classes=num_classes,\n",
    "            seed=seed\n",
    "        )\n",
    "        \n",
    "        # Additional augmentations for training\n",
    "        if self.split == 'train':\n",
    "            X = self._apply_training_augmentations(X)\n",
    "        \n",
    "        # Normalize features\n",
    "        scaler = StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "        \n",
    "        # Ensure X has the expected number of features\n",
    "        if X.shape[1] != num_features:\n",
    "            X = X[:, :num_features]  # Truncate if necessary\n",
    "        \n",
    "        # Split into context and query\n",
    "        split_idx = np.random.randint(num_samples // 4, 3 * num_samples // 4)\n",
    "        \n",
    "        context_X = torch.from_numpy(X[:split_idx]).float()\n",
    "        context_y = torch.from_numpy(y[:split_idx]).long()\n",
    "        query_X = torch.from_numpy(X[split_idx:]).float()\n",
    "        query_y = torch.from_numpy(y[split_idx:]).long()\n",
    "        \n",
    "        # Prepare class weights for focal loss\n",
    "        class_weights = metadata['class_info'].get('class_weights', {})\n",
    "        class_weights_tensor = torch.ones(self.config.max_classes)  # Default to 1\n",
    "        for c, w in class_weights.items():\n",
    "            if c < self.config.max_classes:\n",
    "                class_weights_tensor[c] = w\n",
    "        \n",
    "        return {\n",
    "            'context_X': context_X,\n",
    "            'context_y': context_y,\n",
    "            'query_X': query_X,\n",
    "            'query_y': query_y,\n",
    "            'num_features': num_features,\n",
    "            'num_classes': num_classes,\n",
    "            'num_context': split_idx,\n",
    "            'num_query': num_samples - split_idx,\n",
    "            'class_weights': class_weights_tensor,\n",
    "            'metadata': metadata\n",
    "        }\n",
    "    \n",
    "    def _apply_training_augmentations(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Apply additional augmentations during training\"\"\"\n",
    "        \n",
    "        # Random feature permutation\n",
    "        if np.random.random() < 0.1:\n",
    "            perm = np.random.permutation(X.shape[1])\n",
    "            X = X[:, perm]\n",
    "        \n",
    "        # Add Gaussian noise\n",
    "        if np.random.random() < 0.3:\n",
    "            noise_scale = np.random.uniform(0.01, 0.1)\n",
    "            X += np.random.randn(*X.shape) * noise_scale\n",
    "        \n",
    "        # Random scaling\n",
    "        if np.random.random() < 0.2:\n",
    "            scale = np.random.uniform(0.8, 1.2, size=(1, X.shape[1]))\n",
    "            X *= scale\n",
    "        \n",
    "        return X\n",
    "\n",
    "# ===========================\n",
    "# Model Components\n",
    "# ===========================\n",
    "\n",
    "def collate_tabpfn_batch(batch: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"Custom collate function for TabPFN batches\"\"\"\n",
    "    batch_size = len(batch)\n",
    "    \n",
    "    # Find maximum dimensions\n",
    "    max_features = max(item['num_features'] for item in batch)\n",
    "    max_context = max(item['num_context'] for item in batch)\n",
    "    max_query = max(item['num_query'] for item in batch)\n",
    "    max_classes = max(item['num_classes'] for item in batch)\n",
    "    \n",
    "    # Initialize padded tensors\n",
    "    context_X = torch.zeros(batch_size, max_context, max_features)\n",
    "    context_y = torch.zeros(batch_size, max_context, dtype=torch.long)\n",
    "    query_X = torch.zeros(batch_size, max_query, max_features)\n",
    "    query_y = torch.zeros(batch_size, max_query, dtype=torch.long)\n",
    "    \n",
    "    # Masks for padding\n",
    "    context_mask = torch.zeros(batch_size, max_context, dtype=torch.bool)\n",
    "    query_mask = torch.zeros(batch_size, max_query, dtype=torch.bool)\n",
    "    feature_mask = torch.zeros(batch_size, max_features, dtype=torch.bool)\n",
    "    \n",
    "    # Handle class weights if present\n",
    "    has_class_weights = 'class_weights' in batch[0]\n",
    "    if has_class_weights:\n",
    "        class_weights = torch.stack([item['class_weights'] for item in batch])\n",
    "    else:\n",
    "        class_weights = torch.ones(batch_size, max_classes)\n",
    "    \n",
    "    # Fill tensors\n",
    "    for i, item in enumerate(batch):\n",
    "        n_features = item['num_features']\n",
    "        n_context = item['num_context']\n",
    "        n_query = item['num_query']\n",
    "        \n",
    "        context_X[i, :n_context, :n_features] = item['context_X']\n",
    "        context_y[i, :n_context] = item['context_y']\n",
    "        query_X[i, :n_query, :n_features] = item['query_X']\n",
    "        query_y[i, :n_query] = item['query_y']\n",
    "        \n",
    "        context_mask[i, :n_context] = True\n",
    "        query_mask[i, :n_query] = True\n",
    "        feature_mask[i, :n_features] = True\n",
    "    \n",
    "    result = {\n",
    "        'context_X': context_X,\n",
    "        'context_y': context_y,\n",
    "        'query_X': query_X,\n",
    "        'query_y': query_y,\n",
    "        'context_mask': context_mask,\n",
    "        'query_mask': query_mask,\n",
    "        'feature_mask': feature_mask,\n",
    "        'num_classes': torch.tensor([item['num_classes'] for item in batch]),\n",
    "        'max_classes': max_classes,\n",
    "        'max_features': max_features\n",
    "    }\n",
    "    \n",
    "    if has_class_weights:\n",
    "        result['class_weights'] = class_weights\n",
    "    \n",
    "    return result\n",
    "\n",
    "class FeatureEncoder(nn.Module):\n",
    "    \"\"\"Encode variable-length features to fixed dimension\"\"\"\n",
    "    \n",
    "    def __init__(self, max_features: int, hidden_dim: int):\n",
    "        super().__init__()\n",
    "        self.max_features = max_features\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Feature embedding\n",
    "        self.feature_embed = nn.Linear(max_features, hidden_dim)\n",
    "        \n",
    "        # Feature-wise attention\n",
    "        self.feature_attention = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, feature_mask: torch.Tensor) -> torch.Tensor:\n",
    "        # x: [batch_size, seq_len, actual_features]\n",
    "        # feature_mask: [batch_size, actual_features]\n",
    "        \n",
    "        batch_size, seq_len, actual_features = x.shape\n",
    "        \n",
    "        # Pad features to max_features if necessary\n",
    "        if actual_features < self.max_features:\n",
    "            padding = torch.zeros(batch_size, seq_len, self.max_features - actual_features, \n",
    "                                device=x.device, dtype=x.dtype)\n",
    "            x = torch.cat([x, padding], dim=-1)\n",
    "            \n",
    "            # Extend feature mask\n",
    "            mask_padding = torch.zeros(batch_size, self.max_features - actual_features, \n",
    "                                     device=feature_mask.device, dtype=feature_mask.dtype)\n",
    "            feature_mask_extended = torch.cat([feature_mask, mask_padding], dim=-1)\n",
    "        else:\n",
    "            feature_mask_extended = feature_mask\n",
    "        \n",
    "        # Apply feature mask\n",
    "        x = x * feature_mask_extended.unsqueeze(1).float()\n",
    "        \n",
    "        # Embed features\n",
    "        embedded = self.feature_embed(x)\n",
    "        \n",
    "        # Apply feature-wise attention\n",
    "        attention = self.feature_attention(embedded)\n",
    "        \n",
    "        return embedded * attention\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Sinusoidal positional encoding\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dim: int, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_len, hidden_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        div_term = torch.exp(torch.arange(0, hidden_dim, 2).float() * \n",
    "                           (-math.log(10000.0) / hidden_dim))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: [batch_size, seq_len, hidden_dim]\n",
    "        return x + self.pe[:x.size(1)]\n",
    "\n",
    "class CrossAttentionLayer(nn.Module):\n",
    "    \"\"\"Cross-attention between query and context\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dim: int, num_heads: int = 8, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            hidden_dim, num_heads, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, query: torch.Tensor, context: torch.Tensor,\n",
    "                context_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        # Create attention mask\n",
    "        if context_mask is not None:\n",
    "            # context_mask: [batch_size, context_len]\n",
    "            # Convert to attention mask format\n",
    "            attn_mask = ~context_mask  # True positions are masked\n",
    "        else:\n",
    "            attn_mask = None\n",
    "        \n",
    "        # Apply cross-attention\n",
    "        attended, _ = self.attention(\n",
    "            query, context, context,\n",
    "            key_padding_mask=attn_mask\n",
    "        )\n",
    "        \n",
    "        # Residual connection and normalization\n",
    "        return self.norm(query + self.dropout(attended))\n",
    "\n",
    "# ===========================\n",
    "# Main TabPFN Model\n",
    "# ===========================\n",
    "\n",
    "class TabPFN(nn.Module):\n",
    "    \"\"\"Tabular Prior-Fitted Network\"\"\"\n",
    "    \n",
    "    def __init__(self, config: TabPFNConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Feature encoding\n",
    "        self.feature_encoder = FeatureEncoder(config.max_features, config.hidden_dim)\n",
    "        \n",
    "        # Label embedding\n",
    "        self.label_embed = nn.Embedding(config.max_classes, config.hidden_dim)\n",
    "        \n",
    "        # Positional encoding\n",
    "        if config.use_positional_encoding:\n",
    "            self.pos_encoder = PositionalEncoding(config.hidden_dim, config.max_samples)\n",
    "        \n",
    "        # Type embeddings (context vs query)\n",
    "        self.context_type_embed = nn.Parameter(torch.randn(config.hidden_dim))\n",
    "        self.query_type_embed = nn.Parameter(torch.randn(config.hidden_dim))\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=config.hidden_dim,\n",
    "            nhead=config.num_heads,\n",
    "            dim_feedforward=config.hidden_dim * 4,\n",
    "            dropout=config.dropout,\n",
    "            activation='gelu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=config.num_layers)\n",
    "        \n",
    "        # Cross-attention layers (if enabled)\n",
    "        if config.use_cross_attention:\n",
    "            self.cross_attention = nn.ModuleList([\n",
    "                CrossAttentionLayer(config.hidden_dim, config.num_heads, config.dropout)\n",
    "                for _ in range(3)\n",
    "            ])\n",
    "        \n",
    "        # Output head\n",
    "        self.output_head = nn.Sequential(\n",
    "            nn.LayerNorm(config.hidden_dim),\n",
    "            nn.Linear(config.hidden_dim, config.hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(config.dropout),\n",
    "            nn.Linear(config.hidden_dim, config.max_classes)\n",
    "        )\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                nn.init.normal_(module.weight, mean=0, std=0.02)\n",
    "    \n",
    "    def forward(self, batch: Dict[str, torch.Tensor]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \n",
    "        Args:\n",
    "            batch: Dictionary containing:\n",
    "                - context_X: [batch_size, context_len, actual_features]\n",
    "                - context_y: [batch_size, context_len]\n",
    "                - query_X: [batch_size, query_len, actual_features]\n",
    "                - context_mask: [batch_size, context_len]\n",
    "                - query_mask: [batch_size, query_len]\n",
    "                - feature_mask: [batch_size, actual_features]\n",
    "        \n",
    "        Returns:\n",
    "            logits: [batch_size, query_len, max_classes]\n",
    "        \"\"\"\n",
    "        context_X = batch['context_X']\n",
    "        context_y = batch['context_y']\n",
    "        query_X = batch['query_X']\n",
    "        context_mask = batch['context_mask']\n",
    "        query_mask = batch['query_mask']\n",
    "        feature_mask = batch['feature_mask']\n",
    "        \n",
    "        batch_size = context_X.size(0)\n",
    "        context_len = context_X.size(1)\n",
    "        query_len = query_X.size(1)\n",
    "        \n",
    "        # Encode features (FeatureEncoder will handle padding internally)\n",
    "        context_encoded = self.feature_encoder(context_X, feature_mask)\n",
    "        query_encoded = self.feature_encoder(query_X, feature_mask)\n",
    "        \n",
    "        # Add label embeddings to context\n",
    "        context_labels = self.label_embed(context_y)\n",
    "        context_encoded = context_encoded + context_labels\n",
    "        \n",
    "        # Add type embeddings\n",
    "        context_encoded = context_encoded + self.context_type_embed\n",
    "        query_encoded = query_encoded + self.query_type_embed\n",
    "        \n",
    "        # Add positional encoding\n",
    "        if self.config.use_positional_encoding:\n",
    "            context_encoded = self.pos_encoder(context_encoded)\n",
    "            query_encoded = self.pos_encoder(query_encoded)\n",
    "        \n",
    "        # Apply masks\n",
    "        context_encoded = context_encoded * context_mask.unsqueeze(-1).float()\n",
    "        query_encoded = query_encoded * query_mask.unsqueeze(-1).float()\n",
    "        \n",
    "        # Process with transformer\n",
    "        if self.config.use_cross_attention:\n",
    "            # Process context first\n",
    "            context_processed = self.transformer(\n",
    "                context_encoded,\n",
    "                src_key_padding_mask=~context_mask\n",
    "            )\n",
    "            \n",
    "            # Apply cross-attention from query to context\n",
    "            query_processed = query_encoded\n",
    "            for cross_attn in self.cross_attention:\n",
    "                query_processed = cross_attn(\n",
    "                    query_processed, context_processed, context_mask\n",
    "                )\n",
    "            \n",
    "            # Final transformer processing\n",
    "            combined = torch.cat([context_processed, query_processed], dim=1)\n",
    "            combined_mask = torch.cat([context_mask, query_mask], dim=1)\n",
    "            \n",
    "            output = self.transformer(\n",
    "                combined,\n",
    "                src_key_padding_mask=~combined_mask\n",
    "            )\n",
    "            \n",
    "            # Extract query outputs\n",
    "            output = output[:, context_len:]\n",
    "        else:\n",
    "            # Standard transformer processing\n",
    "            combined = torch.cat([context_encoded, query_encoded], dim=1)\n",
    "            combined_mask = torch.cat([context_mask, query_mask], dim=1)\n",
    "            \n",
    "            output = self.transformer(\n",
    "                combined,\n",
    "                src_key_padding_mask=~combined_mask\n",
    "            )\n",
    "            \n",
    "            # Extract query outputs\n",
    "            output = output[:, context_len:]\n",
    "        \n",
    "        # Generate predictions\n",
    "        logits = self.output_head(output)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# ===========================\n",
    "# Loss Functions\n",
    "# ===========================\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss for addressing class imbalance\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha: Optional[torch.Tensor] = None, gamma: float = 2.0, \n",
    "                 reduction: str = 'mean'):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, inputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        p = torch.exp(-ce_loss)\n",
    "        loss = (1 - p) ** self.gamma * ce_loss\n",
    "        \n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.device != inputs.device:\n",
    "                self.alpha = self.alpha.to(inputs.device)\n",
    "            loss = self.alpha[targets] * loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss\n",
    "\n",
    "# ===========================\n",
    "# Training Components\n",
    "# ===========================\n",
    "\n",
    "class TabPFNTrainer:\n",
    "    \"\"\"Trainer for TabPFN model\"\"\"\n",
    "    \n",
    "    def __init__(self, model: TabPFN, config: TabPFNConfig, device: torch.device,\n",
    "                 use_focal_loss: bool = False):\n",
    "        self.model = model.to(device)\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=config.learning_rate,\n",
    "            weight_decay=config.weight_decay,\n",
    "            betas=(0.9, 0.98)\n",
    "        )\n",
    "        \n",
    "        # Loss function\n",
    "        if use_focal_loss:\n",
    "            self.criterion = FocalLoss(gamma=2.0)\n",
    "        else:\n",
    "            self.criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        self.scheduler = self._create_scheduler()\n",
    "        \n",
    "        # Metrics\n",
    "        self.train_metrics = {'loss': [], 'accuracy': []}\n",
    "        self.val_metrics = {'loss': [], 'accuracy': []}\n",
    "        \n",
    "        # Best model tracking\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.best_val_acc = 0.0\n",
    "        self.best_epoch = 0\n",
    "    \n",
    "    def _create_scheduler(self):\n",
    "        \"\"\"Create learning rate scheduler with warmup\"\"\"\n",
    "        def lr_lambda(step):\n",
    "            if step < self.config.warmup_steps:\n",
    "                return step / self.config.warmup_steps\n",
    "            else:\n",
    "                # Cosine decay\n",
    "                progress = (step - self.config.warmup_steps) / max(1, self.config.num_epochs * 1000 - self.config.warmup_steps)\n",
    "                return 0.5 * (1 + math.cos(math.pi * progress))\n",
    "        \n",
    "        return torch.optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda)\n",
    "    \n",
    "    def train_step(self, batch: Dict[str, torch.Tensor]) -> Dict[str, float]:\n",
    "        \"\"\"Single training step\"\"\"\n",
    "        self.model.train()\n",
    "        \n",
    "        # Move batch to device\n",
    "        batch = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v \n",
    "                for k, v in batch.items()}\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = self.model(batch)\n",
    "        \n",
    "        # Compute loss\n",
    "        query_y = batch['query_y']\n",
    "        query_mask = batch['query_mask']\n",
    "        \n",
    "        # Flatten for loss computation\n",
    "        logits_flat = logits.view(-1, logits.size(-1))\n",
    "        labels_flat = query_y.view(-1)\n",
    "        mask_flat = query_mask.view(-1)\n",
    "        \n",
    "        # Compute loss only on valid positions\n",
    "        loss_all = self.criterion(logits_flat, labels_flat)\n",
    "        loss = (loss_all * mask_flat.float()).sum() / mask_flat.float().sum()\n",
    "        \n",
    "        # Backward pass\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.gradient_clip)\n",
    "        \n",
    "        # Optimizer step\n",
    "        self.optimizer.step()\n",
    "        self.scheduler.step()\n",
    "        \n",
    "        # Compute accuracy\n",
    "        with torch.no_grad():\n",
    "            predictions = torch.argmax(logits_flat, dim=-1)\n",
    "            correct = (predictions == labels_flat) * mask_flat\n",
    "            accuracy = correct.float().sum() / mask_flat.float().sum()\n",
    "        \n",
    "        return {\n",
    "            'loss': loss.item(),\n",
    "            'accuracy': accuracy.item(),\n",
    "            'lr': self.optimizer.param_groups[0]['lr']\n",
    "        }\n",
    "    \n",
    "    def evaluate(self, dataloader: DataLoader) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate model on a dataset\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                # Move batch to device\n",
    "                batch = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v \n",
    "                        for k, v in batch.items()}\n",
    "                \n",
    "                # Forward pass\n",
    "                logits = self.model(batch)\n",
    "                \n",
    "                # Compute loss and accuracy\n",
    "                query_y = batch['query_y']\n",
    "                query_mask = batch['query_mask']\n",
    "                \n",
    "                logits_flat = logits.view(-1, logits.size(-1))\n",
    "                labels_flat = query_y.view(-1)\n",
    "                mask_flat = query_mask.view(-1)\n",
    "                \n",
    "                loss_all = self.criterion(logits_flat, labels_flat)\n",
    "                loss = (loss_all * mask_flat.float()).sum()\n",
    "                \n",
    "                predictions = torch.argmax(logits_flat, dim=-1)\n",
    "                correct = ((predictions == labels_flat) * mask_flat).sum()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                total_correct += correct.item()\n",
    "                total_samples += mask_flat.float().sum().item()\n",
    "        \n",
    "        return {\n",
    "            'loss': total_loss / total_samples,\n",
    "            'accuracy': total_correct / total_samples\n",
    "        }\n",
    "    \n",
    "    def train_epoch(self, train_loader: DataLoader, val_loader: DataLoader, \n",
    "                   epoch: int) -> Dict[str, float]:\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        # Training\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "        train_steps = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch + 1}')\n",
    "        for batch in pbar:\n",
    "            metrics = self.train_step(batch)\n",
    "            train_loss += metrics['loss']\n",
    "            train_acc += metrics['accuracy']\n",
    "            train_steps += 1\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'loss': f\"{metrics['loss']:.4f}\",\n",
    "                'acc': f\"{metrics['accuracy']:.4f}\",\n",
    "                'lr': f\"{metrics['lr']:.6f}\"\n",
    "            })\n",
    "        \n",
    "        avg_train_loss = train_loss / train_steps\n",
    "        avg_train_acc = train_acc / train_steps\n",
    "        \n",
    "        # Validation\n",
    "        val_metrics = self.evaluate(val_loader)\n",
    "        \n",
    "        # Update metrics history\n",
    "        self.train_metrics['loss'].append(avg_train_loss)\n",
    "        self.train_metrics['accuracy'].append(avg_train_acc)\n",
    "        self.val_metrics['loss'].append(val_metrics['loss'])\n",
    "        self.val_metrics['accuracy'].append(val_metrics['accuracy'])\n",
    "        \n",
    "        # Check for best model (using accuracy now)\n",
    "        if val_metrics['accuracy'] > self.best_val_acc:\n",
    "            self.best_val_acc = val_metrics['accuracy']\n",
    "            self.best_val_loss = val_metrics['loss']\n",
    "            self.best_epoch = epoch\n",
    "            self.save_checkpoint('tabpfn_best.pt')\n",
    "        \n",
    "        return {\n",
    "            'train_loss': avg_train_loss,\n",
    "            'train_accuracy': avg_train_acc,\n",
    "            'val_loss': val_metrics['loss'],\n",
    "            'val_accuracy': val_metrics['accuracy']\n",
    "        }\n",
    "    \n",
    "    def save_checkpoint(self, path: str):\n",
    "        \"\"\"Save model checkpoint\"\"\"\n",
    "        torch.save({\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "            'config': self.config,\n",
    "            'train_metrics': self.train_metrics,\n",
    "            'val_metrics': self.val_metrics,\n",
    "            'best_val_loss': self.best_val_loss,\n",
    "            'best_val_acc': self.best_val_acc,\n",
    "            'best_epoch': self.best_epoch\n",
    "        }, path)\n",
    "    \n",
    "    def load_checkpoint(self, path: str):\n",
    "        \"\"\"Load model checkpoint\"\"\"\n",
    "        checkpoint = torch.load(path, map_location=self.device)\n",
    "        \n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        self.train_metrics = checkpoint['train_metrics']\n",
    "        self.val_metrics = checkpoint['val_metrics']\n",
    "        self.best_val_loss = checkpoint['best_val_loss']\n",
    "        self.best_val_acc = checkpoint.get('best_val_acc', 0.0)\n",
    "        self.best_epoch = checkpoint['best_epoch']\n",
    "\n",
    "# ===========================\n",
    "# Optimized Configurations\n",
    "# ===========================\n",
    "\n",
    "def create_optimized_config(scale: str = 'medium') -> TabPFNConfig:\n",
    "    \"\"\"\n",
    "    Create optimized configurations for different scales\n",
    "    \n",
    "    Args:\n",
    "        scale: 'small', 'medium', 'large', or 'xlarge'\n",
    "    \"\"\"\n",
    "    configs = {\n",
    "        'small': TabPFNConfig(\n",
    "            # Model size\n",
    "            max_features=32,\n",
    "            hidden_dim=128,\n",
    "            num_layers=4,\n",
    "            num_heads=4,\n",
    "            max_classes=10,\n",
    "            dropout=0.1,\n",
    "            \n",
    "            # Training - optimized for faster convergence\n",
    "            learning_rate=3e-3,\n",
    "            weight_decay=1e-5,\n",
    "            batch_size=64,\n",
    "            num_epochs=50,\n",
    "            warmup_steps=500,\n",
    "            gradient_clip=0.5,\n",
    "            \n",
    "            # Data generation\n",
    "            min_features=3,\n",
    "            min_samples=20,\n",
    "            max_samples=200,\n",
    "            max_samples_train=150,\n",
    "            num_train_tasks=20000,\n",
    "            num_val_tasks=2000,\n",
    "            \n",
    "            # Features\n",
    "            use_feature_embedding=True,\n",
    "            use_positional_encoding=True,\n",
    "            use_cross_attention=False  # Disable for faster training\n",
    "        ),\n",
    "        \n",
    "        'medium': TabPFNConfig(\n",
    "            # Model size\n",
    "            max_features=64,\n",
    "            hidden_dim=256,\n",
    "            num_layers=6,\n",
    "            num_heads=8,\n",
    "            max_classes=10,\n",
    "            dropout=0.1,\n",
    "            \n",
    "            # Training\n",
    "            learning_rate=2e-3,\n",
    "            weight_decay=1e-5,\n",
    "            batch_size=32,\n",
    "            num_epochs=100,\n",
    "            warmup_steps=1000,\n",
    "            gradient_clip=1.0,\n",
    "            \n",
    "            # Data generation\n",
    "            min_features=3,\n",
    "            min_samples=30,\n",
    "            max_samples=300,\n",
    "            max_samples_train=250,\n",
    "            num_train_tasks=50000,\n",
    "            num_val_tasks=5000,\n",
    "            \n",
    "            # Features\n",
    "            use_feature_embedding=True,\n",
    "            use_positional_encoding=True,\n",
    "            use_cross_attention=True\n",
    "        ),\n",
    "        \n",
    "        'large': TabPFNConfig(\n",
    "            # Model size\n",
    "            max_features=100,\n",
    "            hidden_dim=512,\n",
    "            num_layers=8,\n",
    "            num_heads=8,\n",
    "            max_classes=10,\n",
    "            dropout=0.0,\n",
    "            \n",
    "            # Training\n",
    "            learning_rate=1e-3,\n",
    "            weight_decay=1e-5,\n",
    "            batch_size=16,\n",
    "            num_epochs=200,\n",
    "            warmup_steps=2000,\n",
    "            gradient_clip=1.0,\n",
    "            \n",
    "            # Data generation  \n",
    "            min_features=3,\n",
    "            min_samples=50,\n",
    "            max_samples=500,\n",
    "            max_samples_train=400,\n",
    "            num_train_tasks=100000,\n",
    "            num_val_tasks=10000,\n",
    "            \n",
    "            # Features\n",
    "            use_feature_embedding=True,\n",
    "            use_positional_encoding=True,\n",
    "            use_cross_attention=True\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    return configs[scale]\n",
    "\n",
    "# ===========================\n",
    "# Inference and Application\n",
    "# ===========================\n",
    "\n",
    "class TabPFNClassifier:\n",
    "    \"\"\"User-friendly interface for TabPFN\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: Optional[str] = None, device: Optional[torch.device] = None):\n",
    "        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Load pre-trained model if path provided\n",
    "        if model_path:\n",
    "            self.load_model(model_path)\n",
    "        else:\n",
    "            # Initialize with default config\n",
    "            self.config = TabPFNConfig()\n",
    "            self.model = TabPFN(self.config)\n",
    "            self.model.to(self.device)\n",
    "    \n",
    "    def load_model(self, path: str):\n",
    "        \"\"\"Load pre-trained model\"\"\"\n",
    "        checkpoint = torch.load(path, map_location=self.device)\n",
    "        self.config = checkpoint['config']\n",
    "        self.model = TabPFN(self.config)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "    \n",
    "    def fit_predict(self, X_train: np.ndarray, y_train: np.ndarray, \n",
    "                   X_test: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Fit on training data and predict on test data\n",
    "        \n",
    "        Args:\n",
    "            X_train: Training features [n_train_samples, n_features]\n",
    "            y_train: Training labels [n_train_samples]\n",
    "            X_test: Test features [n_test_samples, n_features]\n",
    "            \n",
    "        Returns:\n",
    "            predictions: Predicted labels [n_test_samples]\n",
    "        \"\"\"\n",
    "        # Normalize features\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        context_X = torch.from_numpy(X_train_scaled).float()\n",
    "        context_y = torch.from_numpy(y_train).long()\n",
    "        query_X = torch.from_numpy(X_test_scaled).float()\n",
    "        \n",
    "        # Pad features if necessary\n",
    "        n_features = X_train.shape[1]\n",
    "        if n_features < self.config.max_features:\n",
    "            pad_width = self.config.max_features - n_features\n",
    "            context_X = F.pad(context_X, (0, pad_width))\n",
    "            query_X = F.pad(query_X, (0, pad_width))\n",
    "        \n",
    "        # Create batch\n",
    "        batch = {\n",
    "            'context_X': context_X.unsqueeze(0),\n",
    "            'context_y': context_y.unsqueeze(0),\n",
    "            'query_X': query_X.unsqueeze(0),\n",
    "            'context_mask': torch.ones(1, len(X_train), dtype=torch.bool),\n",
    "            'query_mask': torch.ones(1, len(X_test), dtype=torch.bool),\n",
    "            'feature_mask': torch.zeros(1, self.config.max_features, dtype=torch.bool)\n",
    "        }\n",
    "        batch['feature_mask'][0, :n_features] = True\n",
    "        \n",
    "        # Move to device\n",
    "        batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "        \n",
    "        # Get predictions\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(batch)\n",
    "            predictions = torch.argmax(logits, dim=-1).squeeze(0)\n",
    "        \n",
    "        return predictions.cpu().numpy()\n",
    "    \n",
    "    def predict_proba(self, X_train: np.ndarray, y_train: np.ndarray, \n",
    "                     X_test: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get probability predictions\n",
    "        \n",
    "        Returns:\n",
    "            probabilities: Class probabilities [n_test_samples, n_classes]\n",
    "        \"\"\"\n",
    "        # Similar to fit_predict but return probabilities\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        context_X = torch.from_numpy(X_train_scaled).float()\n",
    "        context_y = torch.from_numpy(y_train).long()\n",
    "        query_X = torch.from_numpy(X_test_scaled).float()\n",
    "        \n",
    "        n_features = X_train.shape[1]\n",
    "        if n_features < self.config.max_features:\n",
    "            pad_width = self.config.max_features - n_features\n",
    "            context_X = F.pad(context_X, (0, pad_width))\n",
    "            query_X = F.pad(query_X, (0, pad_width))\n",
    "        \n",
    "        batch = {\n",
    "            'context_X': context_X.unsqueeze(0),\n",
    "            'context_y': context_y.unsqueeze(0),\n",
    "            'query_X': query_X.unsqueeze(0),\n",
    "            'context_mask': torch.ones(1, len(X_train), dtype=torch.bool),\n",
    "            'query_mask': torch.ones(1, len(X_test), dtype=torch.bool),\n",
    "            'feature_mask': torch.zeros(1, self.config.max_features, dtype=torch.bool)\n",
    "        }\n",
    "        batch['feature_mask'][0, :n_features] = True\n",
    "        \n",
    "        batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(batch)\n",
    "            probabilities = F.softmax(logits, dim=-1).squeeze(0)\n",
    "            \n",
    "            # Only return probabilities for actual classes in training data\n",
    "            unique_classes = np.unique(y_train)\n",
    "            probabilities = probabilities[:, unique_classes]\n",
    "        \n",
    "        return probabilities.cpu().numpy()\n",
    "\n",
    "# ===========================\n",
    "# Training Functions\n",
    "# ===========================\n",
    "\n",
    "def train_tabpfn(config: Optional[TabPFNConfig] = None, \n",
    "                num_epochs: Optional[int] = None,\n",
    "                use_advanced_augmentation: bool = False,\n",
    "                use_focal_loss: bool = False) -> TabPFN:\n",
    "    \"\"\"\n",
    "    Train a TabPFN model\n",
    "    \n",
    "    Args:\n",
    "        config: Model configuration (uses default if None)\n",
    "        num_epochs: Number of training epochs (overrides config if provided)\n",
    "        use_advanced_augmentation: Whether to use advanced data augmentation\n",
    "        use_focal_loss: Whether to use focal loss\n",
    "    \n",
    "    Returns:\n",
    "        Trained TabPFN model\n",
    "    \"\"\"\n",
    "    # Use default config if not provided\n",
    "    if config is None:\n",
    "        config = TabPFNConfig()\n",
    "    \n",
    "    if num_epochs is not None:\n",
    "        config.num_epochs = num_epochs\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Create datasets\n",
    "    print(\"Creating datasets...\")\n",
    "    if use_advanced_augmentation:\n",
    "        prior_config = PriorConfig()\n",
    "        train_dataset = AugmentedTabPFNDataset(config, prior_config, config.num_train_tasks, split='train')\n",
    "        val_dataset = AugmentedTabPFNDataset(config, prior_config, config.num_val_tasks, split='val')\n",
    "    else:\n",
    "        train_dataset = TabPFNDataset(config, config.num_train_tasks, split='train')\n",
    "        val_dataset = TabPFNDataset(config, config.num_val_tasks, split='val')\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=4 if device.type == 'cuda' else 0,\n",
    "        collate_fn=collate_tabpfn_batch,\n",
    "        pin_memory=True if device.type == 'cuda' else False\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=4 if device.type == 'cuda' else 0,\n",
    "        collate_fn=collate_tabpfn_batch,\n",
    "        pin_memory=True if device.type == 'cuda' else False\n",
    "    )\n",
    "    \n",
    "    # Create model\n",
    "    print(\"Creating model...\")\n",
    "    model = TabPFN(config)\n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = TabPFNTrainer(model, config, device, use_focal_loss=use_focal_loss)\n",
    "    \n",
    "    # Training loop\n",
    "    print(\"\\nStarting training...\")\n",
    "    for epoch in range(config.num_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{config.num_epochs}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        metrics = trainer.train_epoch(train_loader, val_loader, epoch)\n",
    "        \n",
    "        print(f\"Train Loss: {metrics['train_loss']:.4f}, \"\n",
    "              f\"Train Acc: {metrics['train_accuracy']:.4f}\")\n",
    "        print(f\"Val Loss: {metrics['val_loss']:.4f}, \"\n",
    "              f\"Val Acc: {metrics['val_accuracy']:.4f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if epoch - trainer.best_epoch > 20:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "    \n",
    "    print(f\"\\nTraining completed! Best model at epoch {trainer.best_epoch + 1}\")\n",
    "    print(f\"Best validation accuracy: {trainer.best_val_acc:.4f}\")\n",
    "    \n",
    "    # Load best model\n",
    "    trainer.load_checkpoint('tabpfn_best.pt')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def visualize_training_progress(trainer: TabPFNTrainer, save_path: str = 'training_progress.png'):\n",
    "    \"\"\"Visualize training progress\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "    \n",
    "    # Loss curves\n",
    "    axes[0, 0].plot(trainer.train_metrics['loss'], label='Train Loss')\n",
    "    axes[0, 0].plot(trainer.val_metrics['loss'], label='Val Loss')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].set_title('Loss Curves')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True)\n",
    "    \n",
    "    # Accuracy curves\n",
    "    axes[0, 1].plot(trainer.train_metrics['accuracy'], label='Train Accuracy')\n",
    "    axes[0, 1].plot(trainer.val_metrics['accuracy'], label='Val Accuracy')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Accuracy')\n",
    "    axes[0, 1].set_title('Accuracy Curves')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True)\n",
    "    \n",
    "    # Learning rate\n",
    "    axes[1, 0].semilogy(range(len(trainer.train_metrics['loss'])))\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Learning Rate')\n",
    "    axes[1, 0].set_title('Learning Rate Schedule')\n",
    "    axes[1, 0].grid(True)\n",
    "    \n",
    "    # Best accuracy over time\n",
    "    best_accs = []\n",
    "    best = 0\n",
    "    for acc in trainer.val_metrics['accuracy']:\n",
    "        best = max(best, acc)\n",
    "        best_accs.append(best)\n",
    "    axes[1, 1].plot(best_accs)\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Best Validation Accuracy')\n",
    "    axes[1, 1].set_title('Best Accuracy Progress')\n",
    "    axes[1, 1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()\n",
    "\n",
    "# ===========================\n",
    "# Example Usage\n",
    "# ===========================\n",
    "\n",
    "def example_synthetic_data():\n",
    "    \"\"\"Example using synthetic data\"\"\"\n",
    "    print(\"=== TabPFN Synthetic Data Example ===\\n\")\n",
    "    \n",
    "    # Generate synthetic dataset\n",
    "    scm = StructuralCausalModel()\n",
    "    X, y = scm.generate_dataset(\n",
    "        num_samples=1000,\n",
    "        num_features=20,\n",
    "        num_classes=3,\n",
    "        complexity=0.5\n",
    "    )\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"Dataset shape: {X.shape}\")\n",
    "    print(f\"Number of classes: {len(np.unique(y))}\")\n",
    "    print(f\"Train/Test split: {len(X_train)}/{len(X_test)}\\n\")\n",
    "    \n",
    "    # Create and use TabPFN classifier\n",
    "    classifier = TabPFNClassifier()\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = classifier.fit_predict(X_train, y_train, X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = (predictions == y_test).mean()\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Get probability predictions\n",
    "    proba = classifier.predict_proba(X_train, y_train, X_test)\n",
    "    print(f\"Probability shape: {proba.shape}\")\n",
    "    \n",
    "    return classifier\n",
    "\n",
    "def example_real_data():\n",
    "    \"\"\"Example using real datasets\"\"\"\n",
    "    print(\"\\n=== TabPFN Real Data Example ===\\n\")\n",
    "    \n",
    "    from sklearn.datasets import load_breast_cancer, load_wine, load_iris\n",
    "    from sklearn.metrics import accuracy_score, classification_report\n",
    "    \n",
    "    datasets = {\n",
    "        'Iris': load_iris(),\n",
    "        'Wine': load_wine(),\n",
    "        'Breast Cancer': load_breast_cancer()\n",
    "    }\n",
    "    \n",
    "    # Load pre-trained model (assume it exists)\n",
    "    # In practice, you would train the model first\n",
    "    classifier = TabPFNClassifier()\n",
    "    \n",
    "    for name, data in datasets.items():\n",
    "        print(f\"\\n{name} Dataset:\")\n",
    "        print(f\"  Samples: {data.data.shape[0]}\")\n",
    "        print(f\"  Features: {data.data.shape[1]}\")\n",
    "        print(f\"  Classes: {len(np.unique(data.target))}\")\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            data.data, data.target, test_size=0.3, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = classifier.fit_predict(X_train, y_train, X_test)\n",
    "        \n",
    "        # Evaluate\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        print(f\"  Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "def example_training():\n",
    "    \"\"\"Example of training TabPFN\"\"\"\n",
    "    print(\"\\n=== TabPFN Training Example ===\\n\")\n",
    "    \n",
    "    # Create custom configuration\n",
    "    config = create_optimized_config('small')\n",
    "    config.num_epochs = 20  # Reduced for demo\n",
    "    \n",
    "    # Train model\n",
    "    model = train_tabpfn(config, use_focal_loss=True)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def example_advanced_training():\n",
    "    \"\"\"Example of training with advanced features\"\"\"\n",
    "    print(\"\\n=== TabPFN Advanced Training Example ===\\n\")\n",
    "    \n",
    "    # Create configuration\n",
    "    config = create_optimized_config('medium')\n",
    "    config.num_epochs = 50\n",
    "    \n",
    "    # Train with advanced augmentation and focal loss\n",
    "    model = train_tabpfn(\n",
    "        config,\n",
    "        use_advanced_augmentation=True,\n",
    "        use_focal_loss=True\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run examples\n",
    "    print(\"TabPFN Final Implementation\\n\")\n",
    "    \n",
    "    # Example 1: Synthetic data\n",
    "    classifier = example_synthetic_data()\n",
    "    \n",
    "    # Example 2: Real data\n",
    "    example_real_data()\n",
    "    \n",
    "    # Example 3: Basic training\n",
    "    model = example_training()\n",
    "    \n",
    "    # Example 4: Advanced training (uncomment to run)\n",
    "    # model_advanced = example_advanced_training()\n",
    "    \n",
    "    print(\"\\n\\nAll examples completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
