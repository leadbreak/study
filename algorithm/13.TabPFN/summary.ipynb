{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabPFN Complete Implementation\n",
      "\n",
      "=== TabPFN Synthetic Data Example ===\n",
      "\n",
      "Dataset shape: (1000, 20)\n",
      "Number of classes: 3\n",
      "Train/Test split: 800/200\n",
      "\n",
      "Test Accuracy: 0.3550\n",
      "Probability shape: (200, 3)\n",
      "\n",
      "=== TabPFN Real Data Example ===\n",
      "\n",
      "\n",
      "Iris Dataset:\n",
      "  Samples: 150\n",
      "  Features: 4\n",
      "  Classes: 3\n",
      "  Test Accuracy: 0.4222\n",
      "\n",
      "Wine Dataset:\n",
      "  Samples: 178\n",
      "  Features: 13\n",
      "  Classes: 3\n",
      "  Test Accuracy: 0.3889\n",
      "\n",
      "Breast Cancer Dataset:\n",
      "  Samples: 569\n",
      "  Features: 30\n",
      "  Classes: 2\n",
      "  Test Accuracy: 0.6316\n",
      "\n",
      "=== TabPFN Training Example ===\n",
      "\n",
      "Using device: cuda\n",
      "Creating datasets...\n",
      "Creating model...\n",
      "Model parameters: 5,680,522\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 1/10\n",
      "--------------------------------------------------\n",
      "  Step 100: Loss=1.6992, Acc=0.2040, LR=0.000010\n",
      "  Step 200: Loss=1.7419, Acc=0.2077, LR=0.000020\n",
      "  Step 300: Loss=1.7836, Acc=0.1967, LR=0.000030\n",
      "Train Loss: 1.6961, Train Acc: 0.2141\n",
      "Val Loss: 1.6620, Val Acc: 0.2190\n",
      "\n",
      "Epoch 2/10\n",
      "--------------------------------------------------\n",
      "  Step 100: Loss=1.5432, Acc=0.2463, LR=0.000041\n",
      "  Step 200: Loss=1.7884, Acc=0.2054, LR=0.000051\n",
      "  Step 300: Loss=1.4489, Acc=0.2843, LR=0.000061\n",
      "Train Loss: 1.6799, Train Acc: 0.2140\n",
      "Val Loss: 1.6604, Val Acc: 0.2194\n",
      "\n",
      "Epoch 3/10\n",
      "--------------------------------------------------\n",
      "  Step 100: Loss=1.8086, Acc=0.1752, LR=0.000073\n",
      "  Step 200: Loss=1.6602, Acc=0.2269, LR=0.000083\n",
      "  Step 300: Loss=1.7423, Acc=0.1961, LR=0.000093\n",
      "Train Loss: 1.6810, Train Acc: 0.2148\n",
      "Val Loss: 1.6598, Val Acc: 0.2203\n",
      "\n",
      "Epoch 4/10\n",
      "--------------------------------------------------\n",
      "  Step 100: Loss=1.7849, Acc=0.1940, LR=0.000100\n",
      "  Step 200: Loss=1.7081, Acc=0.2059, LR=0.000100\n",
      "  Step 300: Loss=1.6598, Acc=0.2136, LR=0.000100\n",
      "Train Loss: 1.6710, Train Acc: 0.2207\n",
      "Val Loss: 1.6596, Val Acc: 0.2227\n",
      "\n",
      "Epoch 5/10\n",
      "--------------------------------------------------\n",
      "  Step 100: Loss=1.7280, Acc=0.2070, LR=0.000100\n",
      "  Step 200: Loss=1.5640, Acc=0.2484, LR=0.000099\n",
      "  Step 300: Loss=1.5806, Acc=0.2440, LR=0.000099\n",
      "Train Loss: 1.6758, Train Acc: 0.2224\n",
      "Val Loss: 1.6572, Val Acc: 0.2295\n",
      "\n",
      "Epoch 6/10\n",
      "--------------------------------------------------\n",
      "  Step 100: Loss=1.6729, Acc=0.2324, LR=0.000099\n",
      "  Step 200: Loss=1.9051, Acc=0.1796, LR=0.000099\n",
      "  Step 300: Loss=1.6940, Acc=0.2220, LR=0.000098\n",
      "Train Loss: 1.6807, Train Acc: 0.2225\n",
      "Val Loss: 1.6567, Val Acc: 0.2311\n",
      "\n",
      "Epoch 7/10\n",
      "--------------------------------------------------\n",
      "  Step 100: Loss=1.6504, Acc=0.2340, LR=0.000098\n",
      "  Step 200: Loss=1.7738, Acc=0.2214, LR=0.000097\n",
      "  Step 300: Loss=1.4633, Acc=0.2825, LR=0.000097\n",
      "Train Loss: 1.6613, Train Acc: 0.2287\n",
      "Val Loss: 1.6569, Val Acc: 0.2296\n",
      "\n",
      "Epoch 8/10\n",
      "--------------------------------------------------\n",
      "  Step 100: Loss=1.6894, Acc=0.2224, LR=0.000096\n",
      "  Step 200: Loss=1.6455, Acc=0.2498, LR=0.000095\n",
      "  Step 300: Loss=1.7461, Acc=0.1960, LR=0.000095\n",
      "Train Loss: 1.6791, Train Acc: 0.2258\n",
      "Val Loss: 1.6563, Val Acc: 0.2306\n",
      "\n",
      "Epoch 9/10\n",
      "--------------------------------------------------\n",
      "  Step 100: Loss=1.8011, Acc=0.1883, LR=0.000094\n",
      "  Step 200: Loss=1.6057, Acc=0.2316, LR=0.000093\n",
      "  Step 300: Loss=1.7019, Acc=0.2133, LR=0.000092\n",
      "Train Loss: 1.6782, Train Acc: 0.2265\n",
      "Val Loss: 1.6561, Val Acc: 0.2341\n",
      "\n",
      "Epoch 10/10\n",
      "--------------------------------------------------\n",
      "  Step 100: Loss=1.5482, Acc=0.2533, LR=0.000091\n",
      "  Step 200: Loss=1.5649, Acc=0.2470, LR=0.000090\n",
      "  Step 300: Loss=1.6737, Acc=0.2352, LR=0.000089\n",
      "Train Loss: 1.6775, Train Acc: 0.2277\n",
      "Val Loss: 1.6555, Val Acc: 0.2335\n",
      "\n",
      "Training completed! Best model at epoch 10\n"
     ]
    },
    {
     "ename": "UnpicklingError",
     "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL __main__.TabPFNConfig was not an allowed global by default. Please use `torch.serialization.add_safe_globals([TabPFNConfig])` or the `torch.serialization.safe_globals([TabPFNConfig])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1187\u001b[0m\n\u001b[1;32m   1184\u001b[0m example_real_data()\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# Example 3: Training (uncomment to run)\u001b[39;00m\n\u001b[0;32m-> 1187\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mexample_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll examples completed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 1172\u001b[0m, in \u001b[0;36mexample_training\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1159\u001b[0m config \u001b[38;5;241m=\u001b[39m TabPFNConfig(\n\u001b[1;32m   1160\u001b[0m     max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[1;32m   1161\u001b[0m     hidden_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1168\u001b[0m     num_val_tasks\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m   1169\u001b[0m )\n\u001b[1;32m   1171\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m-> 1172\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_tabpfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "Cell \u001b[0;32mIn[5], line 1073\u001b[0m, in \u001b[0;36mtrain_tabpfn\u001b[0;34m(config, num_epochs)\u001b[0m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining completed! Best model at epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mbest_epoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1072\u001b[0m \u001b[38;5;66;03m# Load best model\u001b[39;00m\n\u001b[0;32m-> 1073\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtabpfn_best.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "Cell \u001b[0;32mIn[5], line 855\u001b[0m, in \u001b[0;36mTabPFNTrainer.load_checkpoint\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    853\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_checkpoint\u001b[39m(\u001b[38;5;28mself\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    854\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load model checkpoint\"\"\"\u001b[39;00m\n\u001b[0;32m--> 855\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    858\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py:1470\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1462\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[1;32m   1463\u001b[0m                     opened_zipfile,\n\u001b[1;32m   1464\u001b[0m                     map_location,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1467\u001b[0m                     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[1;32m   1468\u001b[0m                 )\n\u001b[1;32m   1469\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1470\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1471\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[1;32m   1472\u001b[0m             opened_zipfile,\n\u001b[1;32m   1473\u001b[0m             map_location,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1476\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[1;32m   1477\u001b[0m         )\n\u001b[1;32m   1478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL __main__.TabPFNConfig was not an allowed global by default. Please use `torch.serialization.add_safe_globals([TabPFNConfig])` or the `torch.serialization.safe_globals([TabPFNConfig])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TabPFN (Tabular Prior-Fitted Networks) - Complete Implementation\n",
    "================================================================\n",
    "\n",
    "A comprehensive implementation of TabPFN with advanced features including:\n",
    "- Sophisticated synthetic data generation with structural causal models\n",
    "- State-of-the-art transformer architecture with cross-attention\n",
    "- Efficient training and inference pipelines\n",
    "- Real-world dataset integration capabilities\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import math\n",
    "from typing import Tuple, Optional, List, Dict, Any, Union\n",
    "from dataclasses import dataclass\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ===========================\n",
    "# Configuration Classes\n",
    "# ===========================\n",
    "\n",
    "@dataclass\n",
    "class TabPFNConfig:\n",
    "    \"\"\"Configuration for TabPFN model and training\"\"\"\n",
    "    # Model architecture\n",
    "    max_features: int = 100\n",
    "    max_samples: int = 1024\n",
    "    hidden_dim: int = 512\n",
    "    num_layers: int = 12\n",
    "    num_heads: int = 8\n",
    "    max_classes: int = 10\n",
    "    dropout: float = 0.0\n",
    "    \n",
    "    # Training\n",
    "    learning_rate: float = 1e-3\n",
    "    weight_decay: float = 1e-4\n",
    "    batch_size: int = 512\n",
    "    num_epochs: int = 100\n",
    "    warmup_steps: int = 1000\n",
    "    gradient_clip: float = 1.0\n",
    "    \n",
    "    # Data generation\n",
    "    min_features: int = 3\n",
    "    min_samples: int = 50\n",
    "    max_samples_train: int = 512\n",
    "    num_train_tasks: int = 100000\n",
    "    num_val_tasks: int = 10000\n",
    "    \n",
    "    # Advanced features\n",
    "    use_feature_embedding: bool = True\n",
    "    use_positional_encoding: bool = True\n",
    "    use_cross_attention: bool = True\n",
    "    \n",
    "    def save(self, path: str):\n",
    "        \"\"\"Save configuration to JSON file\"\"\"\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(self.__dict__, f, indent=2)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path: str):\n",
    "        \"\"\"Load configuration from JSON file\"\"\"\n",
    "        with open(path, 'r') as f:\n",
    "            config_dict = json.load(f)\n",
    "        return cls(**config_dict)\n",
    "\n",
    "# ===========================\n",
    "# Advanced Data Generation\n",
    "# ===========================\n",
    "\n",
    "class StructuralCausalModel:\n",
    "    \"\"\"Advanced SCM for generating diverse synthetic datasets\"\"\"\n",
    "    \n",
    "    def __init__(self, seed: Optional[int] = None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        self.causal_mechanisms = {\n",
    "            'linear': self._linear_mechanism,\n",
    "            'polynomial': self._polynomial_mechanism,\n",
    "            'interaction': self._interaction_mechanism,\n",
    "            'threshold': self._threshold_mechanism,\n",
    "            'periodic': self._periodic_mechanism,\n",
    "            'mixture': self._mixture_mechanism\n",
    "        }\n",
    "    \n",
    "    def _linear_mechanism(self, X: np.ndarray, indices: np.ndarray, \n",
    "                         params: Dict[str, Any]) -> np.ndarray:\n",
    "        \"\"\"Linear combination with random weights\"\"\"\n",
    "        weights = params.get('weights', np.random.randn(len(indices)))\n",
    "        # Ensure weights match the number of indices\n",
    "        if len(weights) != len(indices):\n",
    "            weights = np.random.randn(len(indices))\n",
    "        return X[:, indices] @ weights\n",
    "    \n",
    "    def _polynomial_mechanism(self, X: np.ndarray, indices: np.ndarray,\n",
    "                            params: Dict[str, Any]) -> np.ndarray:\n",
    "        \"\"\"Polynomial transformations\"\"\"\n",
    "        degree = params.get('degree', np.random.randint(2, 4))\n",
    "        weights = params.get('weights', np.random.randn(len(indices)))\n",
    "        # Ensure weights match the number of indices\n",
    "        if len(weights) != len(indices):\n",
    "            weights = np.random.randn(len(indices))\n",
    "        result = np.zeros(X.shape[0])\n",
    "        \n",
    "        for i, idx in enumerate(indices):\n",
    "            result += weights[i] * (X[:, idx] ** degree)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _interaction_mechanism(self, X: np.ndarray, indices: np.ndarray,\n",
    "                             params: Dict[str, Any]) -> np.ndarray:\n",
    "        \"\"\"Multiplicative interactions between features\"\"\"\n",
    "        if len(indices) < 2:\n",
    "            return self._linear_mechanism(X, indices, params)\n",
    "        \n",
    "        result = np.zeros(X.shape[0])\n",
    "        for i in range(len(indices)):\n",
    "            for j in range(i + 1, len(indices)):\n",
    "                weight = np.random.randn()\n",
    "                result += weight * X[:, indices[i]] * X[:, indices[j]]\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _threshold_mechanism(self, X: np.ndarray, indices: np.ndarray,\n",
    "                           params: Dict[str, Any]) -> np.ndarray:\n",
    "        \"\"\"Piecewise linear with thresholds\"\"\"\n",
    "        base = self._linear_mechanism(X, indices, params)\n",
    "        threshold = params.get('threshold', np.random.randn())\n",
    "        scale_high = params.get('scale_high', 2.0)\n",
    "        scale_low = params.get('scale_low', 0.5)\n",
    "        \n",
    "        return np.where(base > threshold, base * scale_high, base * scale_low)\n",
    "    \n",
    "    def _periodic_mechanism(self, X: np.ndarray, indices: np.ndarray,\n",
    "                          params: Dict[str, Any]) -> np.ndarray:\n",
    "        \"\"\"Sinusoidal transformations\"\"\"\n",
    "        frequency = params.get('frequency', np.random.uniform(0.5, 2.0))\n",
    "        phase = params.get('phase', np.random.uniform(0, 2 * np.pi))\n",
    "        weights = params.get('weights', np.random.randn(len(indices)))\n",
    "        # Ensure weights match the number of indices\n",
    "        if len(weights) != len(indices):\n",
    "            weights = np.random.randn(len(indices))\n",
    "        \n",
    "        linear_combo = X[:, indices] @ weights\n",
    "        return np.sin(frequency * linear_combo + phase)\n",
    "    \n",
    "    def _mixture_mechanism(self, X: np.ndarray, indices: np.ndarray,\n",
    "                         params: Dict[str, Any]) -> np.ndarray:\n",
    "        \"\"\"Mixture of different mechanisms\"\"\"\n",
    "        mechanisms = ['linear', 'polynomial', 'interaction', 'periodic']\n",
    "        chosen = np.random.choice(mechanisms, size=2, replace=False)\n",
    "        \n",
    "        result = 0\n",
    "        for mech_name in chosen:\n",
    "            mech_func = self.causal_mechanisms[mech_name]\n",
    "            if mech_name != 'mixture':  # Avoid recursion\n",
    "                result += mech_func(X, indices, params) * np.random.uniform(0.3, 0.7)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def generate_dataset(self, num_samples: int, num_features: int,\n",
    "                        num_classes: int, complexity: float = 0.1,\n",
    "                        noise_level: float = 0.1) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Generate a complete synthetic dataset\"\"\"\n",
    "        \n",
    "        # Generate base features with diverse distributions\n",
    "        X = self._generate_features(num_samples, num_features)\n",
    "        \n",
    "        # Select causal features\n",
    "        num_causal = max(2, int(num_features * complexity))\n",
    "        causal_indices = np.random.choice(num_features, size=num_causal, replace=False)\n",
    "        \n",
    "        # Generate target using multiple mechanisms\n",
    "        y_continuous = self._generate_target(X, causal_indices, num_mechanisms=3)\n",
    "        \n",
    "        # Add noise\n",
    "        y_continuous += np.random.normal(0, noise_level, num_samples)\n",
    "        \n",
    "        # Convert to classes\n",
    "        y = self._continuous_to_classes(y_continuous, num_classes)\n",
    "        \n",
    "        return X.astype(np.float32), y.astype(np.int64)\n",
    "    \n",
    "    def _generate_features(self, num_samples: int, num_features: int) -> np.ndarray:\n",
    "        \"\"\"Generate features with diverse distributions\"\"\"\n",
    "        X = np.zeros((num_samples, num_features))\n",
    "        \n",
    "        distributions = [\n",
    "            ('normal', lambda: np.random.normal(0, np.random.uniform(0.5, 2.0), num_samples)),\n",
    "            ('uniform', lambda: np.random.uniform(-2, 2, num_samples)),\n",
    "            ('exponential', lambda: np.random.exponential(np.random.uniform(0.5, 2.0), num_samples) - 1),\n",
    "            ('beta', lambda: np.random.beta(2, 5, num_samples) * 4 - 1),\n",
    "            ('gamma', lambda: np.random.gamma(2, 2, num_samples) - 2),\n",
    "            ('laplace', lambda: np.random.laplace(0, np.random.uniform(0.5, 1.5), num_samples))\n",
    "        ]\n",
    "        \n",
    "        for i in range(num_features):\n",
    "            dist_name, dist_func = distributions[i % len(distributions)]\n",
    "            X[:, i] = dist_func()\n",
    "            \n",
    "            # Add correlations between some features\n",
    "            if i > 0 and np.random.random() < 0.3:\n",
    "                correlation_strength = np.random.uniform(-0.8, 0.8)\n",
    "                X[:, i] = correlation_strength * X[:, i-1] + np.sqrt(1 - correlation_strength**2) * X[:, i]\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def _generate_target(self, X: np.ndarray, causal_indices: np.ndarray,\n",
    "                        num_mechanisms: int = 3) -> np.ndarray:\n",
    "        \"\"\"Generate target variable using multiple causal mechanisms\"\"\"\n",
    "        y_components = []\n",
    "        \n",
    "        for _ in range(num_mechanisms):\n",
    "            # Select mechanism and subset of causal features\n",
    "            mechanism_name = np.random.choice(list(self.causal_mechanisms.keys()))\n",
    "            mechanism = self.causal_mechanisms[mechanism_name]\n",
    "            \n",
    "            subset_size = np.random.randint(1, min(len(causal_indices), 5) + 1)\n",
    "            subset_indices = np.random.choice(causal_indices, size=subset_size, replace=False)\n",
    "            \n",
    "            # Generate component with random parameters matching the subset size\n",
    "            params = self._generate_mechanism_params(mechanism_name, num_features=len(subset_indices))\n",
    "            component = mechanism(X, subset_indices, params)\n",
    "            \n",
    "            # Apply random scaling\n",
    "            component *= np.random.uniform(0.5, 2.0)\n",
    "            y_components.append(component)\n",
    "        \n",
    "        # Combine components\n",
    "        return np.sum(y_components, axis=0)\n",
    "    \n",
    "    def _generate_mechanism_params(self, mechanism_name: str, num_features: int = None) -> Dict[str, Any]:\n",
    "        \"\"\"Generate random parameters for a mechanism\"\"\"\n",
    "        params = {}\n",
    "        \n",
    "        if mechanism_name in ['linear', 'polynomial', 'periodic']:\n",
    "            # Make sure weights match the number of features that will be used\n",
    "            if num_features is not None:\n",
    "                params['weights'] = np.random.randn(num_features)\n",
    "            else:\n",
    "                params['weights'] = np.random.randn(np.random.randint(1, 5))\n",
    "        \n",
    "        if mechanism_name == 'polynomial':\n",
    "            params['degree'] = np.random.randint(2, 4)\n",
    "        \n",
    "        if mechanism_name == 'threshold':\n",
    "            params['threshold'] = np.random.randn()\n",
    "            params['scale_high'] = np.random.uniform(1.5, 3.0)\n",
    "            params['scale_low'] = np.random.uniform(0.1, 0.7)\n",
    "        \n",
    "        if mechanism_name == 'periodic':\n",
    "            params['frequency'] = np.random.uniform(0.5, 3.0)\n",
    "            params['phase'] = np.random.uniform(0, 2 * np.pi)\n",
    "        \n",
    "        return params\n",
    "    \n",
    "    def _continuous_to_classes(self, y_continuous: np.ndarray, num_classes: int) -> np.ndarray:\n",
    "        \"\"\"Convert continuous values to class labels\"\"\"\n",
    "        if num_classes == 2:\n",
    "            # Binary classification\n",
    "            threshold = np.percentile(y_continuous, 50)\n",
    "            return (y_continuous > threshold).astype(np.int64)\n",
    "        else:\n",
    "            # Multi-class classification\n",
    "            percentiles = np.linspace(0, 100, num_classes + 1)[1:-1]\n",
    "            thresholds = np.percentile(y_continuous, percentiles)\n",
    "            return np.digitize(y_continuous, thresholds).astype(np.int64)\n",
    "\n",
    "# ===========================\n",
    "# TabPFN Dataset\n",
    "# ===========================\n",
    "\n",
    "class TabPFNDataset(Dataset):\n",
    "    \"\"\"Dataset for meta-learning on synthetic tabular data\"\"\"\n",
    "    \n",
    "    def __init__(self, config: TabPFNConfig, num_tasks: int, split: str = 'train'):\n",
    "        self.config = config\n",
    "        self.num_tasks = num_tasks\n",
    "        self.split = split\n",
    "        self.scm = StructuralCausalModel()\n",
    "        \n",
    "        # Different settings for train/val\n",
    "        if split == 'train':\n",
    "            self.max_samples = config.max_samples_train\n",
    "        else:\n",
    "            self.max_samples = config.max_samples\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_tasks\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        # Set random seed for reproducibility\n",
    "        np.random.seed(idx if self.split == 'val' else None)\n",
    "        \n",
    "        # Sample task parameters\n",
    "        num_features = np.random.randint(self.config.min_features, self.config.max_features + 1)\n",
    "        num_samples = np.random.randint(self.config.min_samples, self.max_samples + 1)\n",
    "        num_classes = np.random.randint(2, self.config.max_classes + 1)\n",
    "        # complexity = np.random.uniform(0.2, 0.8)\n",
    "        # noise_level = np.random.uniform(0.05, 0.2)\n",
    "        \n",
    "        complexity = 0.1\n",
    "        noise_level = 0.0\n",
    "        \n",
    "        # Generate data\n",
    "        X, y = self.scm.generate_dataset(\n",
    "            num_samples=num_samples,\n",
    "            num_features=num_features,\n",
    "            num_classes=num_classes,\n",
    "            complexity=complexity,\n",
    "            noise_level=noise_level\n",
    "        )\n",
    "        \n",
    "        # Normalize features\n",
    "        X = (X - X.mean(axis=0)) / (X.std(axis=0) + 1e-8)\n",
    "        \n",
    "        # Split into context and query\n",
    "        split_idx = np.random.randint(num_samples // 4, 3 * num_samples // 4)\n",
    "        \n",
    "        context_X = torch.from_numpy(X[:split_idx]).float()\n",
    "        context_y = torch.from_numpy(y[:split_idx]).long()\n",
    "        query_X = torch.from_numpy(X[split_idx:]).float()\n",
    "        query_y = torch.from_numpy(y[split_idx:]).long()\n",
    "        \n",
    "        return {\n",
    "            'context_X': context_X,\n",
    "            'context_y': context_y,\n",
    "            'query_X': query_X,\n",
    "            'query_y': query_y,\n",
    "            'num_features': num_features,\n",
    "            'num_classes': num_classes,\n",
    "            'num_context': split_idx,\n",
    "            'num_query': num_samples - split_idx\n",
    "        }\n",
    "\n",
    "def collate_tabpfn_batch(batch: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"Custom collate function for TabPFN batches\"\"\"\n",
    "    batch_size = len(batch)\n",
    "    \n",
    "    # Find maximum dimensions\n",
    "    max_features = max(item['num_features'] for item in batch)\n",
    "    max_context = max(item['num_context'] for item in batch)\n",
    "    max_query = max(item['num_query'] for item in batch)\n",
    "    max_classes = max(item['num_classes'] for item in batch)\n",
    "    \n",
    "    # Get the configured max features from any item (they should all have the same config)\n",
    "    config_max_features = batch[0]['context_X'].size(-1) if hasattr(batch[0], 'config_max_features') else max_features\n",
    "    \n",
    "    # Initialize padded tensors\n",
    "    context_X = torch.zeros(batch_size, max_context, max_features)\n",
    "    context_y = torch.zeros(batch_size, max_context, dtype=torch.long)\n",
    "    query_X = torch.zeros(batch_size, max_query, max_features)\n",
    "    query_y = torch.zeros(batch_size, max_query, dtype=torch.long)\n",
    "    \n",
    "    # Masks for padding\n",
    "    context_mask = torch.zeros(batch_size, max_context, dtype=torch.bool)\n",
    "    query_mask = torch.zeros(batch_size, max_query, dtype=torch.bool)\n",
    "    feature_mask = torch.zeros(batch_size, max_features, dtype=torch.bool)\n",
    "    \n",
    "    # Fill tensors\n",
    "    for i, item in enumerate(batch):\n",
    "        n_features = item['num_features']\n",
    "        n_context = item['num_context']\n",
    "        n_query = item['num_query']\n",
    "        \n",
    "        context_X[i, :n_context, :n_features] = item['context_X']\n",
    "        context_y[i, :n_context] = item['context_y']\n",
    "        query_X[i, :n_query, :n_features] = item['query_X']\n",
    "        query_y[i, :n_query] = item['query_y']\n",
    "        \n",
    "        context_mask[i, :n_context] = True\n",
    "        query_mask[i, :n_query] = True\n",
    "        feature_mask[i, :n_features] = True\n",
    "    \n",
    "    return {\n",
    "        'context_X': context_X,\n",
    "        'context_y': context_y,\n",
    "        'query_X': query_X,\n",
    "        'query_y': query_y,\n",
    "        'context_mask': context_mask,\n",
    "        'query_mask': query_mask,\n",
    "        'feature_mask': feature_mask,\n",
    "        'num_classes': torch.tensor([item['num_classes'] for item in batch]),\n",
    "        'max_classes': max_classes,\n",
    "        'max_features': max_features\n",
    "    }\n",
    "\n",
    "# ===========================\n",
    "# Model Components\n",
    "# ===========================\n",
    "\n",
    "class FeatureEncoder(nn.Module):\n",
    "    \"\"\"Encode variable-length features to fixed dimension\"\"\"\n",
    "    \n",
    "    def __init__(self, max_features: int, hidden_dim: int):\n",
    "        super().__init__()\n",
    "        self.max_features = max_features\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Feature embedding\n",
    "        self.feature_embed = nn.Linear(max_features, hidden_dim)\n",
    "        \n",
    "        # Feature-wise attention\n",
    "        self.feature_attention = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, feature_mask: torch.Tensor) -> torch.Tensor:\n",
    "        # x: [batch_size, seq_len, actual_features]\n",
    "        # feature_mask: [batch_size, actual_features]\n",
    "        \n",
    "        batch_size, seq_len, actual_features = x.shape\n",
    "        \n",
    "        # Pad features to max_features if necessary\n",
    "        if actual_features < self.max_features:\n",
    "            padding = torch.zeros(batch_size, seq_len, self.max_features - actual_features, \n",
    "                                device=x.device, dtype=x.dtype)\n",
    "            x = torch.cat([x, padding], dim=-1)\n",
    "            \n",
    "            # Extend feature mask\n",
    "            mask_padding = torch.zeros(batch_size, self.max_features - actual_features, \n",
    "                                     device=feature_mask.device, dtype=feature_mask.dtype)\n",
    "            feature_mask_extended = torch.cat([feature_mask, mask_padding], dim=-1)\n",
    "        else:\n",
    "            feature_mask_extended = feature_mask\n",
    "        \n",
    "        # Apply feature mask\n",
    "        x = x * feature_mask_extended.unsqueeze(1).float()\n",
    "        \n",
    "        # Embed features\n",
    "        embedded = self.feature_embed(x)\n",
    "        \n",
    "        # Apply feature-wise attention\n",
    "        attention = self.feature_attention(embedded)\n",
    "        \n",
    "        return embedded * attention\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Sinusoidal positional encoding\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dim: int, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_len, hidden_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        div_term = torch.exp(torch.arange(0, hidden_dim, 2).float() * \n",
    "                           (-math.log(10000.0) / hidden_dim))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: [batch_size, seq_len, hidden_dim]\n",
    "        return x + self.pe[:x.size(1)]\n",
    "\n",
    "class CrossAttentionLayer(nn.Module):\n",
    "    \"\"\"Cross-attention between query and context\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dim: int, num_heads: int = 8, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            hidden_dim, num_heads, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, query: torch.Tensor, context: torch.Tensor,\n",
    "                context_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        # Create attention mask\n",
    "        if context_mask is not None:\n",
    "            # context_mask: [batch_size, context_len]\n",
    "            # Convert to attention mask format\n",
    "            attn_mask = ~context_mask  # True positions are masked\n",
    "        else:\n",
    "            attn_mask = None\n",
    "        \n",
    "        # Apply cross-attention\n",
    "        attended, _ = self.attention(\n",
    "            query, context, context,\n",
    "            key_padding_mask=attn_mask\n",
    "        )\n",
    "        \n",
    "        # Residual connection and normalization\n",
    "        return self.norm(query + self.dropout(attended))\n",
    "\n",
    "# ===========================\n",
    "# Main TabPFN Model\n",
    "# ===========================\n",
    "\n",
    "class TabPFN(nn.Module):\n",
    "    \"\"\"Tabular Prior-Fitted Network\"\"\"\n",
    "    \n",
    "    def __init__(self, config: TabPFNConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Feature encoding\n",
    "        self.feature_encoder = FeatureEncoder(config.max_features, config.hidden_dim)\n",
    "        \n",
    "        # Label embedding\n",
    "        self.label_embed = nn.Embedding(config.max_classes, config.hidden_dim)\n",
    "        \n",
    "        # Positional encoding\n",
    "        if config.use_positional_encoding:\n",
    "            self.pos_encoder = PositionalEncoding(config.hidden_dim, config.max_samples)\n",
    "        \n",
    "        # Type embeddings (context vs query)\n",
    "        self.context_type_embed = nn.Parameter(torch.randn(config.hidden_dim))\n",
    "        self.query_type_embed = nn.Parameter(torch.randn(config.hidden_dim))\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=config.hidden_dim,\n",
    "            nhead=config.num_heads,\n",
    "            dim_feedforward=config.hidden_dim * 4,\n",
    "            dropout=config.dropout,\n",
    "            activation='gelu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=config.num_layers)\n",
    "        \n",
    "        # Cross-attention layers (if enabled)\n",
    "        if config.use_cross_attention:\n",
    "            self.cross_attention = nn.ModuleList([\n",
    "                CrossAttentionLayer(config.hidden_dim, config.num_heads, config.dropout)\n",
    "                for _ in range(3)\n",
    "            ])\n",
    "        \n",
    "        # Output head\n",
    "        self.output_head = nn.Sequential(\n",
    "            nn.LayerNorm(config.hidden_dim),\n",
    "            nn.Linear(config.hidden_dim, config.hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(config.dropout),\n",
    "            nn.Linear(config.hidden_dim, config.max_classes)\n",
    "        )\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                nn.init.normal_(module.weight, mean=0, std=0.02)\n",
    "    \n",
    "    def forward(self, batch: Dict[str, torch.Tensor]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \n",
    "        Args:\n",
    "            batch: Dictionary containing:\n",
    "                - context_X: [batch_size, context_len, actual_features]\n",
    "                - context_y: [batch_size, context_len]\n",
    "                - query_X: [batch_size, query_len, actual_features]\n",
    "                - context_mask: [batch_size, context_len]\n",
    "                - query_mask: [batch_size, query_len]\n",
    "                - feature_mask: [batch_size, actual_features]\n",
    "        \n",
    "        Returns:\n",
    "            logits: [batch_size, query_len, max_classes]\n",
    "        \"\"\"\n",
    "        context_X = batch['context_X']\n",
    "        context_y = batch['context_y']\n",
    "        query_X = batch['query_X']\n",
    "        context_mask = batch['context_mask']\n",
    "        query_mask = batch['query_mask']\n",
    "        feature_mask = batch['feature_mask']\n",
    "        \n",
    "        batch_size = context_X.size(0)\n",
    "        context_len = context_X.size(1)\n",
    "        query_len = query_X.size(1)\n",
    "        \n",
    "        # Encode features (FeatureEncoder will handle padding internally)\n",
    "        context_encoded = self.feature_encoder(context_X, feature_mask)\n",
    "        query_encoded = self.feature_encoder(query_X, feature_mask)\n",
    "        \n",
    "        # Add label embeddings to context\n",
    "        context_labels = self.label_embed(context_y)\n",
    "        context_encoded = context_encoded + context_labels\n",
    "        \n",
    "        # Add type embeddings\n",
    "        context_encoded = context_encoded + self.context_type_embed\n",
    "        query_encoded = query_encoded + self.query_type_embed\n",
    "        \n",
    "        # Add positional encoding\n",
    "        if self.config.use_positional_encoding:\n",
    "            context_encoded = self.pos_encoder(context_encoded)\n",
    "            query_encoded = self.pos_encoder(query_encoded)\n",
    "        \n",
    "        # Apply masks\n",
    "        context_encoded = context_encoded * context_mask.unsqueeze(-1).float()\n",
    "        query_encoded = query_encoded * query_mask.unsqueeze(-1).float()\n",
    "        \n",
    "        # Process with transformer\n",
    "        if self.config.use_cross_attention:\n",
    "            # Process context first\n",
    "            context_processed = self.transformer(\n",
    "                context_encoded,\n",
    "                src_key_padding_mask=~context_mask\n",
    "            )\n",
    "            \n",
    "            # Apply cross-attention from query to context\n",
    "            query_processed = query_encoded\n",
    "            for cross_attn in self.cross_attention:\n",
    "                query_processed = cross_attn(\n",
    "                    query_processed, context_processed, context_mask\n",
    "                )\n",
    "            \n",
    "            # Final transformer processing\n",
    "            combined = torch.cat([context_processed, query_processed], dim=1)\n",
    "            combined_mask = torch.cat([context_mask, query_mask], dim=1)\n",
    "            \n",
    "            output = self.transformer(\n",
    "                combined,\n",
    "                src_key_padding_mask=~combined_mask\n",
    "            )\n",
    "            \n",
    "            # Extract query outputs\n",
    "            output = output[:, context_len:]\n",
    "        else:\n",
    "            # Standard transformer processing\n",
    "            combined = torch.cat([context_encoded, query_encoded], dim=1)\n",
    "            combined_mask = torch.cat([context_mask, query_mask], dim=1)\n",
    "            \n",
    "            output = self.transformer(\n",
    "                combined,\n",
    "                src_key_padding_mask=~combined_mask\n",
    "            )\n",
    "            \n",
    "            # Extract query outputs\n",
    "            output = output[:, context_len:]\n",
    "        \n",
    "        # Generate predictions\n",
    "        logits = self.output_head(output)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# ===========================\n",
    "# Training Components\n",
    "# ===========================\n",
    "\n",
    "class TabPFNTrainer:\n",
    "    \"\"\"Trainer for TabPFN model\"\"\"\n",
    "    \n",
    "    def __init__(self, model: TabPFN, config: TabPFNConfig, device: torch.device):\n",
    "        self.model = model.to(device)\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=config.learning_rate,\n",
    "            weight_decay=config.weight_decay,\n",
    "            betas=(0.9, 0.98)\n",
    "        )\n",
    "        \n",
    "        # Loss function\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        self.scheduler = self._create_scheduler()\n",
    "        \n",
    "        # Metrics\n",
    "        self.train_metrics = {'loss': [], 'accuracy': []}\n",
    "        self.val_metrics = {'loss': [], 'accuracy': []}\n",
    "        \n",
    "        # Best model tracking\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.best_epoch = 0\n",
    "    \n",
    "    def _create_scheduler(self):\n",
    "        \"\"\"Create learning rate scheduler with warmup\"\"\"\n",
    "        def lr_lambda(step):\n",
    "            if step < self.config.warmup_steps:\n",
    "                return step / self.config.warmup_steps\n",
    "            else:\n",
    "                return 0.5 * (1 + math.cos(math.pi * (step - self.config.warmup_steps) / \n",
    "                                          (self.config.num_epochs * 1000)))\n",
    "        \n",
    "        return torch.optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda)\n",
    "    \n",
    "    def train_step(self, batch: Dict[str, torch.Tensor]) -> Dict[str, float]:\n",
    "        \"\"\"Single training step\"\"\"\n",
    "        self.model.train()\n",
    "        \n",
    "        # Move batch to device\n",
    "        batch = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v \n",
    "                for k, v in batch.items()}\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = self.model(batch)\n",
    "        \n",
    "        # Compute loss\n",
    "        query_y = batch['query_y']\n",
    "        query_mask = batch['query_mask']\n",
    "        \n",
    "        # Flatten for loss computation\n",
    "        logits_flat = logits.view(-1, logits.size(-1))\n",
    "        labels_flat = query_y.view(-1)\n",
    "        mask_flat = query_mask.view(-1)\n",
    "        \n",
    "        # Compute loss only on valid positions\n",
    "        loss_all = self.criterion(logits_flat, labels_flat)\n",
    "        loss = (loss_all * mask_flat.float()).sum() / mask_flat.float().sum()\n",
    "        \n",
    "        # Backward pass\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.gradient_clip)\n",
    "        \n",
    "        # Optimizer step\n",
    "        self.optimizer.step()\n",
    "        self.scheduler.step()\n",
    "        \n",
    "        # Compute accuracy\n",
    "        with torch.no_grad():\n",
    "            predictions = torch.argmax(logits_flat, dim=-1)\n",
    "            correct = (predictions == labels_flat) * mask_flat\n",
    "            accuracy = correct.float().sum() / mask_flat.float().sum()\n",
    "        \n",
    "        return {\n",
    "            'loss': loss.item(),\n",
    "            'accuracy': accuracy.item(),\n",
    "            'lr': self.optimizer.param_groups[0]['lr']\n",
    "        }\n",
    "    \n",
    "    def evaluate(self, dataloader: DataLoader) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate model on a dataset\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                # Move batch to device\n",
    "                batch = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v \n",
    "                        for k, v in batch.items()}\n",
    "                \n",
    "                # Forward pass\n",
    "                logits = self.model(batch)\n",
    "                \n",
    "                # Compute loss and accuracy\n",
    "                query_y = batch['query_y']\n",
    "                query_mask = batch['query_mask']\n",
    "                \n",
    "                logits_flat = logits.view(-1, logits.size(-1))\n",
    "                labels_flat = query_y.view(-1)\n",
    "                mask_flat = query_mask.view(-1)\n",
    "                \n",
    "                loss_all = self.criterion(logits_flat, labels_flat)\n",
    "                loss = (loss_all * mask_flat.float()).sum()\n",
    "                \n",
    "                predictions = torch.argmax(logits_flat, dim=-1)\n",
    "                correct = ((predictions == labels_flat) * mask_flat).sum()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                total_correct += correct.item()\n",
    "                total_samples += mask_flat.float().sum().item()\n",
    "        \n",
    "        return {\n",
    "            'loss': total_loss / total_samples,\n",
    "            'accuracy': total_correct / total_samples\n",
    "        }\n",
    "    \n",
    "    def train_epoch(self, train_loader: DataLoader, val_loader: DataLoader, \n",
    "                   epoch: int) -> Dict[str, float]:\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        # Training\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "        train_steps = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            metrics = self.train_step(batch)\n",
    "            train_loss += metrics['loss']\n",
    "            train_acc += metrics['accuracy']\n",
    "            train_steps += 1\n",
    "            \n",
    "            if train_steps % 100 == 0:\n",
    "                print(f\"  Step {train_steps}: Loss={metrics['loss']:.4f}, \"\n",
    "                      f\"Acc={metrics['accuracy']:.4f}, LR={metrics['lr']:.6f}\")\n",
    "        \n",
    "        avg_train_loss = train_loss / train_steps\n",
    "        avg_train_acc = train_acc / train_steps\n",
    "        \n",
    "        # Validation\n",
    "        val_metrics = self.evaluate(val_loader)\n",
    "        \n",
    "        # Update metrics history\n",
    "        self.train_metrics['loss'].append(avg_train_loss)\n",
    "        self.train_metrics['accuracy'].append(avg_train_acc)\n",
    "        self.val_metrics['loss'].append(val_metrics['loss'])\n",
    "        self.val_metrics['accuracy'].append(val_metrics['accuracy'])\n",
    "        \n",
    "        # Check for best model\n",
    "        if val_metrics['loss'] < self.best_val_loss:\n",
    "            self.best_val_loss = val_metrics['loss']\n",
    "            self.best_epoch = epoch\n",
    "            self.save_checkpoint(f'tabpfn_best.pt')\n",
    "        \n",
    "        return {\n",
    "            'train_loss': avg_train_loss,\n",
    "            'train_accuracy': avg_train_acc,\n",
    "            'val_loss': val_metrics['loss'],\n",
    "            'val_accuracy': val_metrics['accuracy']\n",
    "        }\n",
    "    \n",
    "    def save_checkpoint(self, path: str):\n",
    "        \"\"\"Save model checkpoint\"\"\"\n",
    "        torch.save({\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "            'config': self.config,\n",
    "            'train_metrics': self.train_metrics,\n",
    "            'val_metrics': self.val_metrics,\n",
    "            'best_val_loss': self.best_val_loss,\n",
    "            'best_epoch': self.best_epoch\n",
    "        }, path)\n",
    "    \n",
    "    def load_checkpoint(self, path: str):\n",
    "        \"\"\"Load model checkpoint\"\"\"\n",
    "        checkpoint = torch.load(path, map_location=self.device)\n",
    "        \n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        self.train_metrics = checkpoint['train_metrics']\n",
    "        self.val_metrics = checkpoint['val_metrics']\n",
    "        self.best_val_loss = checkpoint['best_val_loss']\n",
    "        self.best_epoch = checkpoint['best_epoch']\n",
    "\n",
    "# ===========================\n",
    "# Inference and Application\n",
    "# ===========================\n",
    "\n",
    "class TabPFNClassifier:\n",
    "    \"\"\"User-friendly interface for TabPFN\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: Optional[str] = None, device: Optional[torch.device] = None):\n",
    "        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Load pre-trained model if path provided\n",
    "        if model_path:\n",
    "            self.load_model(model_path)\n",
    "        else:\n",
    "            # Initialize with default config\n",
    "            self.config = TabPFNConfig()\n",
    "            self.model = TabPFN(self.config)\n",
    "            self.model.to(self.device)\n",
    "    \n",
    "    def load_model(self, path: str):\n",
    "        \"\"\"Load pre-trained model\"\"\"\n",
    "        checkpoint = torch.load(path, map_location=self.device)\n",
    "        self.config = checkpoint['config']\n",
    "        self.model = TabPFN(self.config)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "    \n",
    "    def fit_predict(self, X_train: np.ndarray, y_train: np.ndarray, \n",
    "                   X_test: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Fit on training data and predict on test data\n",
    "        \n",
    "        Args:\n",
    "            X_train: Training features [n_train_samples, n_features]\n",
    "            y_train: Training labels [n_train_samples]\n",
    "            X_test: Test features [n_test_samples, n_features]\n",
    "            \n",
    "        Returns:\n",
    "            predictions: Predicted labels [n_test_samples]\n",
    "        \"\"\"\n",
    "        # Normalize features\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        context_X = torch.from_numpy(X_train_scaled).float()\n",
    "        context_y = torch.from_numpy(y_train).long()\n",
    "        query_X = torch.from_numpy(X_test_scaled).float()\n",
    "        \n",
    "        # Pad features if necessary\n",
    "        n_features = X_train.shape[1]\n",
    "        if n_features < self.config.max_features:\n",
    "            pad_width = self.config.max_features - n_features\n",
    "            context_X = F.pad(context_X, (0, pad_width))\n",
    "            query_X = F.pad(query_X, (0, pad_width))\n",
    "        \n",
    "        # Create batch\n",
    "        batch = {\n",
    "            'context_X': context_X.unsqueeze(0),\n",
    "            'context_y': context_y.unsqueeze(0),\n",
    "            'query_X': query_X.unsqueeze(0),\n",
    "            'context_mask': torch.ones(1, len(X_train), dtype=torch.bool),\n",
    "            'query_mask': torch.ones(1, len(X_test), dtype=torch.bool),\n",
    "            'feature_mask': torch.zeros(1, self.config.max_features, dtype=torch.bool)\n",
    "        }\n",
    "        batch['feature_mask'][0, :n_features] = True\n",
    "        \n",
    "        # Move to device\n",
    "        batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "        \n",
    "        # Get predictions\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(batch)\n",
    "            predictions = torch.argmax(logits, dim=-1).squeeze(0)\n",
    "        \n",
    "        return predictions.cpu().numpy()\n",
    "    \n",
    "    def predict_proba(self, X_train: np.ndarray, y_train: np.ndarray, \n",
    "                     X_test: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get probability predictions\n",
    "        \n",
    "        Returns:\n",
    "            probabilities: Class probabilities [n_test_samples, n_classes]\n",
    "        \"\"\"\n",
    "        # Similar to fit_predict but return probabilities\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        context_X = torch.from_numpy(X_train_scaled).float()\n",
    "        context_y = torch.from_numpy(y_train).long()\n",
    "        query_X = torch.from_numpy(X_test_scaled).float()\n",
    "        \n",
    "        n_features = X_train.shape[1]\n",
    "        if n_features < self.config.max_features:\n",
    "            pad_width = self.config.max_features - n_features\n",
    "            context_X = F.pad(context_X, (0, pad_width))\n",
    "            query_X = F.pad(query_X, (0, pad_width))\n",
    "        \n",
    "        batch = {\n",
    "            'context_X': context_X.unsqueeze(0),\n",
    "            'context_y': context_y.unsqueeze(0),\n",
    "            'query_X': query_X.unsqueeze(0),\n",
    "            'context_mask': torch.ones(1, len(X_train), dtype=torch.bool),\n",
    "            'query_mask': torch.ones(1, len(X_test), dtype=torch.bool),\n",
    "            'feature_mask': torch.zeros(1, self.config.max_features, dtype=torch.bool)\n",
    "        }\n",
    "        batch['feature_mask'][0, :n_features] = True\n",
    "        \n",
    "        batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(batch)\n",
    "            probabilities = F.softmax(logits, dim=-1).squeeze(0)\n",
    "            \n",
    "            # Only return probabilities for actual classes in training data\n",
    "            unique_classes = np.unique(y_train)\n",
    "            probabilities = probabilities[:, unique_classes]\n",
    "        \n",
    "        return probabilities.cpu().numpy()\n",
    "\n",
    "# ===========================\n",
    "# Training Script\n",
    "# ===========================\n",
    "\n",
    "def train_tabpfn(config: Optional[TabPFNConfig] = None, \n",
    "                num_epochs: Optional[int] = None) -> TabPFN:\n",
    "    \"\"\"\n",
    "    Train a TabPFN model\n",
    "    \n",
    "    Args:\n",
    "        config: Model configuration (uses default if None)\n",
    "        num_epochs: Number of training epochs (overrides config if provided)\n",
    "    \n",
    "    Returns:\n",
    "        Trained TabPFN model\n",
    "    \"\"\"\n",
    "    # Use default config if not provided\n",
    "    if config is None:\n",
    "        config = TabPFNConfig()\n",
    "    \n",
    "    if num_epochs is not None:\n",
    "        config.num_epochs = num_epochs\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Create datasets\n",
    "    print(\"Creating datasets...\")\n",
    "    train_dataset = TabPFNDataset(config, config.num_train_tasks, split='train')\n",
    "    val_dataset = TabPFNDataset(config, config.num_val_tasks, split='val')\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        collate_fn=collate_tabpfn_batch,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        collate_fn=collate_tabpfn_batch,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Create model\n",
    "    print(\"Creating model...\")\n",
    "    model = TabPFN(config)\n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = TabPFNTrainer(model, config, device)\n",
    "    \n",
    "    # Training loop\n",
    "    print(\"\\nStarting training...\")\n",
    "    for epoch in range(config.num_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{config.num_epochs}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        metrics = trainer.train_epoch(train_loader, val_loader, epoch)\n",
    "        \n",
    "        print(f\"Train Loss: {metrics['train_loss']:.4f}, \"\n",
    "              f\"Train Acc: {metrics['train_accuracy']:.4f}\")\n",
    "        print(f\"Val Loss: {metrics['val_loss']:.4f}, \"\n",
    "              f\"Val Acc: {metrics['val_accuracy']:.4f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if epoch - trainer.best_epoch > 10:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "    \n",
    "    print(f\"\\nTraining completed! Best model at epoch {trainer.best_epoch + 1}\")\n",
    "    \n",
    "    # Load best model\n",
    "    trainer.load_checkpoint('tabpfn_best.pt')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# ===========================\n",
    "# Example Usage\n",
    "# ===========================\n",
    "\n",
    "def example_synthetic_data():\n",
    "    \"\"\"Example using synthetic data\"\"\"\n",
    "    print(\"=== TabPFN Synthetic Data Example ===\\n\")\n",
    "    \n",
    "    # Generate synthetic dataset\n",
    "    scm = StructuralCausalModel()\n",
    "    X, y = scm.generate_dataset(\n",
    "        num_samples=1000,\n",
    "        num_features=20,\n",
    "        num_classes=3,\n",
    "        complexity=0.1\n",
    "    )\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"Dataset shape: {X.shape}\")\n",
    "    print(f\"Number of classes: {len(np.unique(y))}\")\n",
    "    print(f\"Train/Test split: {len(X_train)}/{len(X_test)}\\n\")\n",
    "    \n",
    "    # Create and use TabPFN classifier\n",
    "    classifier = TabPFNClassifier()\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = classifier.fit_predict(X_train, y_train, X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = (predictions == y_test).mean()\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Get probability predictions\n",
    "    proba = classifier.predict_proba(X_train, y_train, X_test)\n",
    "    print(f\"Probability shape: {proba.shape}\")\n",
    "    \n",
    "    return classifier\n",
    "\n",
    "def example_real_data():\n",
    "    \"\"\"Example using real datasets\"\"\"\n",
    "    print(\"\\n=== TabPFN Real Data Example ===\\n\")\n",
    "    \n",
    "    from sklearn.datasets import load_breast_cancer, load_wine, load_iris\n",
    "    from sklearn.metrics import accuracy_score, classification_report\n",
    "    \n",
    "    datasets = {\n",
    "        'Iris': load_iris(),\n",
    "        'Wine': load_wine(),\n",
    "        'Breast Cancer': load_breast_cancer()\n",
    "    }\n",
    "    \n",
    "    # Load pre-trained model (assume it exists)\n",
    "    # In practice, you would train the model first\n",
    "    classifier = TabPFNClassifier()\n",
    "    \n",
    "    for name, data in datasets.items():\n",
    "        print(f\"\\n{name} Dataset:\")\n",
    "        print(f\"  Samples: {data.data.shape[0]}\")\n",
    "        print(f\"  Features: {data.data.shape[1]}\")\n",
    "        print(f\"  Classes: {len(np.unique(data.target))}\")\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            data.data, data.target, test_size=0.3, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = classifier.fit_predict(X_train, y_train, X_test)\n",
    "        \n",
    "        # Evaluate\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        print(f\"  Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "def example_training():\n",
    "    \"\"\"Example of training TabPFN\"\"\"\n",
    "    print(\"\\n=== TabPFN Training Example ===\\n\")\n",
    "    \n",
    "    # Create custom configuration\n",
    "    config = TabPFNConfig(\n",
    "        max_features=50,\n",
    "        hidden_dim=256,\n",
    "        num_layers=6,\n",
    "        num_heads=8,\n",
    "        learning_rate=1e-4,\n",
    "        batch_size=32,\n",
    "        num_epochs=10,  # Small for demo\n",
    "        num_train_tasks=10000,  # Reduced for demo\n",
    "        num_val_tasks=1000\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    model = train_tabpfn(config)\n",
    "    \n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run examples\n",
    "    print(\"TabPFN Complete Implementation\\n\")\n",
    "    \n",
    "    # Example 1: Synthetic data\n",
    "    classifier = example_synthetic_data()\n",
    "    \n",
    "    # Example 2: Real data\n",
    "    example_real_data()\n",
    "    \n",
    "    # Example 3: Training (uncomment to run)\n",
    "    model = example_training()\n",
    "    \n",
    "    print(\"\\n\\nAll examples completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f732f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TabPFN Improved Training Example ===\n",
      "\n",
      "Using device: cuda\n",
      "Training small model with 20000 training tasks\n",
      "Creating datasets...\n",
      "\n",
      "Creating small model...\n",
      "Model parameters: 833,482\n",
      "Trainable parameters: 833,482\n",
      "\n",
      "Starting training from epoch 1...\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 313/313 [00:08<00:00, 36.18it/s, loss_ma=1.6816, acc_ma=0.2152, lr=0.000626]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/50 - 9.4s\n",
      "Train Loss: 1.6816, Train Acc: 0.2152\n",
      "Val Loss: 1.6657, Val Acc: 0.2192\n",
      "Best Val Acc: 0.2192 (Epoch 1)\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 313/313 [00:09<00:00, 34.74it/s, loss_ma=1.6791, acc_ma=0.2164, lr=0.001000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/50 - 9.5s\n",
      "Train Loss: 1.6791, Train Acc: 0.2164\n",
      "Val Loss: 1.6646, Val Acc: 0.2200\n",
      "Best Val Acc: 0.2200 (Epoch 2)\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 313/313 [00:08<00:00, 36.31it/s, loss_ma=1.6764, acc_ma=0.2182, lr=0.000998]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/50 - 9.1s\n",
      "Train Loss: 1.6764, Train Acc: 0.2182\n",
      "Val Loss: 1.6646, Val Acc: 0.2182\n",
      "Best Val Acc: 0.2200 (Epoch 2)\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 313/313 [00:08<00:00, 35.03it/s, loss_ma=1.6928, acc_ma=0.2146, lr=0.000994]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/50 - 9.4s\n",
      "Train Loss: 1.6928, Train Acc: 0.2146\n",
      "Val Loss: 1.6647, Val Acc: 0.2199\n",
      "Best Val Acc: 0.2200 (Epoch 2)\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 313/313 [00:08<00:00, 34.86it/s, loss_ma=1.6876, acc_ma=0.2151, lr=0.000988]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/50 - 9.5s\n",
      "Train Loss: 1.6876, Train Acc: 0.2151\n",
      "Val Loss: 1.6646, Val Acc: 0.2200\n",
      "Best Val Acc: 0.2200 (Epoch 2)\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 313/313 [00:08<00:00, 34.89it/s, loss_ma=1.6722, acc_ma=0.2190, lr=0.000980]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/50 - 9.4s\n",
      "Train Loss: 1.6722, Train Acc: 0.2190\n",
      "Val Loss: 1.6646, Val Acc: 0.2200\n",
      "Best Val Acc: 0.2200 (Epoch 2)\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 313/313 [00:08<00:00, 34.91it/s, loss_ma=1.6765, acc_ma=0.2192, lr=0.000969]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/50 - 9.4s\n",
      "Train Loss: 1.6765, Train Acc: 0.2192\n",
      "Val Loss: 1.6646, Val Acc: 0.2195\n",
      "Best Val Acc: 0.2200 (Epoch 2)\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 313/313 [00:08<00:00, 35.15it/s, loss_ma=1.6896, acc_ma=0.2156, lr=0.000957]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/50 - 9.4s\n",
      "Train Loss: 1.6896, Train Acc: 0.2156\n",
      "Val Loss: 1.6646, Val Acc: 0.2187\n",
      "Best Val Acc: 0.2200 (Epoch 2)\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 313/313 [00:08<00:00, 35.26it/s, loss_ma=1.6892, acc_ma=0.2163, lr=0.000943]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/50 - 9.4s\n",
      "Train Loss: 1.6892, Train Acc: 0.2163\n",
      "Val Loss: 1.6646, Val Acc: 0.2200\n",
      "Best Val Acc: 0.2200 (Epoch 2)\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 313/313 [00:08<00:00, 35.54it/s, loss_ma=1.6767, acc_ma=0.2179, lr=0.000927]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/50 - 9.3s\n",
      "Train Loss: 1.6767, Train Acc: 0.2179\n",
      "Val Loss: 1.6646, Val Acc: 0.2200\n",
      "Best Val Acc: 0.2200 (Epoch 2)\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 313/313 [00:08<00:00, 35.86it/s, loss_ma=1.6827, acc_ma=0.2163, lr=0.000909]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11/50 - 9.2s\n",
      "Train Loss: 1.6827, Train Acc: 0.2163\n",
      "Val Loss: 1.6646, Val Acc: 0.2200\n",
      "Best Val Acc: 0.2200 (Epoch 2)\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 313/313 [00:08<00:00, 36.29it/s, loss_ma=1.6778, acc_ma=0.2188, lr=0.000890]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12/50 - 9.1s\n",
      "Train Loss: 1.6778, Train Acc: 0.2188\n",
      "Val Loss: 1.6646, Val Acc: 0.2200\n",
      "Best Val Acc: 0.2200 (Epoch 2)\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 313/313 [00:08<00:00, 36.74it/s, loss_ma=1.6847, acc_ma=0.2162, lr=0.000868]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13/50 - 9.0s\n",
      "Train Loss: 1.6847, Train Acc: 0.2162\n",
      "Val Loss: 1.6646, Val Acc: 0.2200\n",
      "Best Val Acc: 0.2200 (Epoch 2)\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 313/313 [00:08<00:00, 35.79it/s, loss_ma=1.6781, acc_ma=0.2186, lr=0.000846]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14/50 - 9.2s\n",
      "Train Loss: 1.6781, Train Acc: 0.2186\n",
      "Val Loss: 1.6646, Val Acc: 0.2195\n",
      "Best Val Acc: 0.2200 (Epoch 2)\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 313/313 [00:08<00:00, 34.95it/s, loss_ma=1.6755, acc_ma=0.2174, lr=0.000821]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15/50 - 9.4s\n",
      "Train Loss: 1.6755, Train Acc: 0.2174\n",
      "Val Loss: 1.6646, Val Acc: 0.2200\n",
      "Best Val Acc: 0.2200 (Epoch 2)\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 313/313 [00:08<00:00, 36.03it/s, loss_ma=1.6764, acc_ma=0.2184, lr=0.000796]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16/50 - 9.2s\n",
      "Train Loss: 1.6764, Train Acc: 0.2184\n",
      "Val Loss: 1.6646, Val Acc: 0.2199\n",
      "Best Val Acc: 0.2200 (Epoch 2)\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 313/313 [00:07<00:00, 39.88it/s, loss_ma=1.6896, acc_ma=0.2159, lr=0.000769]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17/50 - 8.3s\n",
      "Train Loss: 1.6896, Train Acc: 0.2159\n",
      "Val Loss: 1.6646, Val Acc: 0.2200\n",
      "Best Val Acc: 0.2200 (Epoch 2)\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 313/313 [00:07<00:00, 40.05it/s, loss_ma=1.6702, acc_ma=0.2182, lr=0.000741]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18/50 - 8.3s\n",
      "Train Loss: 1.6702, Train Acc: 0.2182\n",
      "Val Loss: 1.6646, Val Acc: 0.2195\n",
      "Best Val Acc: 0.2200 (Epoch 2)\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 313/313 [00:07<00:00, 39.62it/s, loss_ma=1.6803, acc_ma=0.2163, lr=0.000712]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19/50 - 8.4s\n",
      "Train Loss: 1.6803, Train Acc: 0.2163\n",
      "Val Loss: 1.6646, Val Acc: 0.2200\n",
      "Best Val Acc: 0.2200 (Epoch 2)\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 313/313 [00:07<00:00, 40.95it/s, loss_ma=1.6808, acc_ma=0.2172, lr=0.000682]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20/50 - 8.1s\n",
      "Train Loss: 1.6808, Train Acc: 0.2172\n",
      "Val Loss: 1.6646, Val Acc: 0.2200\n",
      "Best Val Acc: 0.2200 (Epoch 2)\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 313/313 [00:07<00:00, 42.02it/s, loss_ma=1.6779, acc_ma=0.2170, lr=0.000651]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21/50 - 7.9s\n",
      "Train Loss: 1.6779, Train Acc: 0.2170\n",
      "Val Loss: 1.6646, Val Acc: 0.2200\n",
      "Best Val Acc: 0.2200 (Epoch 2)\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 313/313 [00:08<00:00, 35.09it/s, loss_ma=1.6667, acc_ma=0.2203, lr=0.000620]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22/50 - 9.7s\n",
      "Train Loss: 1.6667, Train Acc: 0.2203\n",
      "Val Loss: 1.6646, Val Acc: 0.2200\n",
      "Best Val Acc: 0.2200 (Epoch 2)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Early stopping triggered after 22 epochs\n",
      "\n",
      "Training completed! Best model achieved 0.2200 validation accuracy\n"
     ]
    },
    {
     "ename": "UnpicklingError",
     "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([scalar])` or the `torch.serialization.safe_globals([scalar])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 505\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;66;03m# Run improved training\u001b[39;00m\n\u001b[0;32m--> 505\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mexample_improved_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;66;03m# Show different scales\u001b[39;00m\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;66;03m# scales = example_different_scales()\u001b[39;00m\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mImproved training completed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 465\u001b[0m, in \u001b[0;36mexample_improved_training\u001b[0;34m()\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== TabPFN Improved Training Example ===\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    464\u001b[0m \u001b[38;5;66;03m# Train a medium-scale model\u001b[39;00m\n\u001b[0;32m--> 465\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_tabpfn_improved\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msmall\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Override for faster demo\u001b[39;49;00m\n\u001b[1;32m    468\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "Cell \u001b[0;32mIn[6], line 418\u001b[0m, in \u001b[0;36mtrain_tabpfn_improved\u001b[0;34m(scale, num_epochs, checkpoint_path)\u001b[0m\n\u001b[1;32m    415\u001b[0m     visualize_training_progress(trainer)\n\u001b[1;32m    417\u001b[0m \u001b[38;5;66;03m# Load best model\u001b[39;00m\n\u001b[0;32m--> 418\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtabpfn_best.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "Cell \u001b[0;32mIn[5], line 855\u001b[0m, in \u001b[0;36mTabPFNTrainer.load_checkpoint\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    853\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_checkpoint\u001b[39m(\u001b[38;5;28mself\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    854\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load model checkpoint\"\"\"\u001b[39;00m\n\u001b[0;32m--> 855\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    858\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py:1470\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1462\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[1;32m   1463\u001b[0m                     opened_zipfile,\n\u001b[1;32m   1464\u001b[0m                     map_location,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1467\u001b[0m                     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[1;32m   1468\u001b[0m                 )\n\u001b[1;32m   1469\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1470\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1471\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[1;32m   1472\u001b[0m             opened_zipfile,\n\u001b[1;32m   1473\u001b[0m             map_location,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1476\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[1;32m   1477\u001b[0m         )\n\u001b[1;32m   1478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([scalar])` or the `torch.serialization.safe_globals([scalar])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Improved TabPFN Training with Better Configurations and Monitoring\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Optional\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import the TabPFN components from the main implementation\n",
    "# Assuming the main TabPFN code is already loaded\n",
    "\n",
    "def create_optimized_config(scale: str = 'medium') -> TabPFNConfig:\n",
    "    \"\"\"\n",
    "    Create optimized configurations for different scales\n",
    "    \n",
    "    Args:\n",
    "        scale: 'small', 'medium', 'large', or 'xlarge'\n",
    "    \"\"\"\n",
    "    configs = {\n",
    "        'small': TabPFNConfig(\n",
    "            # Model size\n",
    "            max_features=32,\n",
    "            hidden_dim=128,\n",
    "            num_layers=4,\n",
    "            num_heads=4,\n",
    "            max_classes=10,\n",
    "            dropout=0.1,\n",
    "            \n",
    "            # Training\n",
    "            learning_rate=1e-3,\n",
    "            weight_decay=1e-5,\n",
    "            batch_size=64,\n",
    "            num_epochs=50,\n",
    "            warmup_steps=500,\n",
    "            gradient_clip=0.5,\n",
    "            \n",
    "            # Data generation\n",
    "            min_features=3,\n",
    "            min_samples=20,\n",
    "            max_samples=200,\n",
    "            max_samples_train=150,\n",
    "            num_train_tasks=20000,\n",
    "            num_val_tasks=2000,\n",
    "            \n",
    "            # Features\n",
    "            use_feature_embedding=True,\n",
    "            use_positional_encoding=False,\n",
    "            use_cross_attention=False  # Disable for faster training\n",
    "        ),\n",
    "        \n",
    "        'medium': TabPFNConfig(\n",
    "            # Model size\n",
    "            max_features=64,\n",
    "            hidden_dim=256,\n",
    "            num_layers=6,\n",
    "            num_heads=8,\n",
    "            max_classes=10,\n",
    "            dropout=0.1,\n",
    "            \n",
    "            # Training\n",
    "            learning_rate=2e-3,\n",
    "            weight_decay=1e-5,\n",
    "            batch_size=32,\n",
    "            num_epochs=100,\n",
    "            warmup_steps=1000,\n",
    "            gradient_clip=1.0,\n",
    "            \n",
    "            # Data generation\n",
    "            min_features=3,\n",
    "            min_samples=30,\n",
    "            max_samples=300,\n",
    "            max_samples_train=250,\n",
    "            num_train_tasks=50000,\n",
    "            num_val_tasks=5000,\n",
    "            \n",
    "            # Features\n",
    "            use_feature_embedding=True,\n",
    "            use_positional_encoding=True,\n",
    "            use_cross_attention=True\n",
    "        ),\n",
    "        \n",
    "        'large': TabPFNConfig(\n",
    "            # Model size\n",
    "            max_features=100,\n",
    "            hidden_dim=512,\n",
    "            num_layers=8,\n",
    "            num_heads=8,\n",
    "            max_classes=10,\n",
    "            dropout=0.0,\n",
    "            \n",
    "            # Training\n",
    "            learning_rate=1e-3,\n",
    "            weight_decay=1e-5,\n",
    "            batch_size=16,\n",
    "            num_epochs=200,\n",
    "            warmup_steps=2000,\n",
    "            gradient_clip=1.0,\n",
    "            \n",
    "            # Data generation  \n",
    "            min_features=3,\n",
    "            min_samples=50,\n",
    "            max_samples=500,\n",
    "            max_samples_train=400,\n",
    "            num_train_tasks=100000,\n",
    "            num_val_tasks=10000,\n",
    "            \n",
    "            # Features\n",
    "            use_feature_embedding=True,\n",
    "            use_positional_encoding=True,\n",
    "            use_cross_attention=True\n",
    "        ),\n",
    "        \n",
    "        'xlarge': TabPFNConfig(\n",
    "            # Model size\n",
    "            max_features=100,\n",
    "            hidden_dim=768,\n",
    "            num_layers=12,\n",
    "            num_heads=12,\n",
    "            max_classes=10,\n",
    "            dropout=0.0,\n",
    "            \n",
    "            # Training\n",
    "            learning_rate=5e-4,\n",
    "            weight_decay=1e-5,\n",
    "            batch_size=8,\n",
    "            num_epochs=300,\n",
    "            warmup_steps=4000,\n",
    "            gradient_clip=1.0,\n",
    "            \n",
    "            # Data generation\n",
    "            min_features=3,\n",
    "            min_samples=50,\n",
    "            max_samples=1000,\n",
    "            max_samples_train=800,\n",
    "            num_train_tasks=200000,\n",
    "            num_val_tasks=20000,\n",
    "            \n",
    "            # Features\n",
    "            use_feature_embedding=True,\n",
    "            use_positional_encoding=True,\n",
    "            use_cross_attention=True\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    return configs[scale]\n",
    "\n",
    "class ImprovedTabPFNTrainer(TabPFNTrainer):\n",
    "    \"\"\"Enhanced trainer with moving-average monitoring and history tracking\"\"\"\n",
    "\n",
    "    def __init__(self, model: TabPFN, config: TabPFNConfig, device: torch.device):\n",
    "        super().__init__(model, config, device)\n",
    "        self.step_count = 0\n",
    "        self.best_accuracy = 0.0\n",
    "        self.train_loss_ma: List[float] = []\n",
    "        self.train_acc_ma: List[float] = []\n",
    "        # 에폭별 기록용 히스토리\n",
    "        self.history: Dict[str, List[float]] = {\n",
    "            'train_loss': [], 'train_accuracy': [],\n",
    "            'val_loss': [],   'val_accuracy': [],\n",
    "            'lr': []\n",
    "        }\n",
    "        self.scheduler = self._create_cosine_scheduler_with_warmup()\n",
    "\n",
    "    def _create_cosine_scheduler_with_warmup(self):\n",
    "        def lr_lambda(step):\n",
    "            if step < self.config.warmup_steps:\n",
    "                return step / self.config.warmup_steps\n",
    "            total_steps = self.config.num_epochs * (self.config.num_train_tasks // self.config.batch_size)\n",
    "            progress = (step - self.config.warmup_steps) / max(1, total_steps - self.config.warmup_steps)\n",
    "            return 0.5 * (1 + np.cos(np.pi * progress))\n",
    "        return torch.optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda)\n",
    "\n",
    "    def train_step(self, batch: Dict[str, torch.Tensor]) -> Dict[str, float]:\n",
    "        self.model.train()\n",
    "        self.step_count += 1\n",
    "\n",
    "        batch = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v\n",
    "                 for k, v in batch.items()}\n",
    "\n",
    "        logits = self.model(batch)\n",
    "        query_y, query_mask = batch['query_y'], batch['query_mask']\n",
    "        logits_flat = logits.view(-1, logits.size(-1))\n",
    "        labels_flat = query_y.view(-1)\n",
    "        mask_flat = query_mask.view(-1).float()\n",
    "\n",
    "        loss_all = self.criterion(logits_flat, labels_flat)\n",
    "        loss = (loss_all * mask_flat).sum() / mask_flat.sum()\n",
    "\n",
    "        if hasattr(self.model, 'cross_attention') and self.config.use_cross_attention:\n",
    "            l2_reg = sum(torch.norm(p, 2) for layer in self.model.cross_attention for p in layer.parameters())\n",
    "            loss = loss + 1e-5 * l2_reg\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.gradient_clip)\n",
    "        self.optimizer.step()\n",
    "        self.scheduler.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds = torch.argmax(logits_flat, dim=-1)\n",
    "            correct = (preds == labels_flat).float() * mask_flat\n",
    "            accuracy = correct.sum() / mask_flat.sum()\n",
    "\n",
    "        # 이동 평균 업데이트\n",
    "        self.train_loss_ma.append(loss.item())\n",
    "        self.train_acc_ma.append(accuracy.item())\n",
    "        if len(self.train_loss_ma) > 100:\n",
    "            self.train_loss_ma.pop(0)\n",
    "            self.train_acc_ma.pop(0)\n",
    "\n",
    "        return {\n",
    "            'loss': loss.item(),\n",
    "            'accuracy': accuracy.item(),\n",
    "            'lr': self.optimizer.param_groups[0]['lr'],\n",
    "            'grad_norm': grad_norm,\n",
    "            'loss_ma': np.mean(self.train_loss_ma),\n",
    "            'acc_ma': np.mean(self.train_acc_ma)\n",
    "        }\n",
    "\n",
    "    def train_epoch_with_progress(self, train_loader, val_loader, epoch: int):\n",
    "        # 이동 평균 리스트 초기화\n",
    "        self.train_loss_ma.clear()\n",
    "        self.train_acc_ma.clear()\n",
    "\n",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}')\n",
    "        last_metrics = None\n",
    "        for batch in pbar:\n",
    "            last_metrics = self.train_step(batch)\n",
    "            pbar.set_postfix({\n",
    "                'loss_ma': f\"{last_metrics['loss_ma']:.4f}\",\n",
    "                'acc_ma':  f\"{last_metrics['acc_ma']:.4f}\",\n",
    "                'lr':      f\"{last_metrics['lr']:.6f}\"\n",
    "            })\n",
    "\n",
    "        # 검증\n",
    "        val_metrics = self.evaluate(val_loader)\n",
    "\n",
    "        # 베스트 모델 저장\n",
    "        if val_metrics['accuracy'] > self.best_accuracy:\n",
    "            self.best_accuracy = val_metrics['accuracy']\n",
    "            self.best_epoch = epoch\n",
    "            self.save_checkpoint('tabpfn_best.pt')\n",
    "\n",
    "        # 히스토리 기록\n",
    "        self.history['train_loss'].append(np.mean(self.train_loss_ma))\n",
    "        self.history['train_accuracy'].append(np.mean(self.train_acc_ma))\n",
    "        self.history['val_loss'].append(val_metrics['loss'])\n",
    "        self.history['val_accuracy'].append(val_metrics['accuracy'])\n",
    "        self.history['lr'].append(last_metrics['lr'] if last_metrics else self.scheduler.get_last_lr()[0])\n",
    "\n",
    "        return {\n",
    "            'train_loss': self.history['train_loss'][-1],\n",
    "            'train_accuracy': self.history['train_accuracy'][-1],\n",
    "            'val_loss': val_metrics['loss'],\n",
    "            'val_accuracy': val_metrics['accuracy']\n",
    "        }\n",
    "\n",
    "def visualize_training_progress(trainer: ImprovedTabPFNTrainer,\n",
    "                                save_path: str = 'training_progress.png'):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "    # Loss curves\n",
    "    axes[0, 0].plot(trainer.history['train_loss'], label='Train Loss')\n",
    "    axes[0, 0].plot(trainer.history['val_loss'],   label='Val Loss')\n",
    "    axes[0, 0].set(title='Loss Curves', xlabel='Epoch', ylabel='Loss')\n",
    "    axes[0, 0].legend(); axes[0, 0].grid(True)\n",
    "\n",
    "    # Accuracy curves\n",
    "    axes[0, 1].plot(trainer.history['train_accuracy'], label='Train Acc')\n",
    "    axes[0, 1].plot(trainer.history['val_accuracy'],   label='Val Acc')\n",
    "    axes[0, 1].set(title='Accuracy Curves', xlabel='Epoch', ylabel='Accuracy')\n",
    "    axes[0, 1].legend(); axes[0, 1].grid(True)\n",
    "\n",
    "    # Learning rate schedule\n",
    "    axes[1, 0].plot(trainer.history['lr'])\n",
    "    axes[1, 0].set(title='Learning Rate Schedule', xlabel='Epoch', ylabel='LR')\n",
    "    axes[1, 0].grid(True)\n",
    "\n",
    "    # Best validation accuracy over time\n",
    "    best_accs = []\n",
    "    current_best = 0.0\n",
    "    for acc in trainer.history['val_accuracy']:\n",
    "        current_best = max(current_best, acc)\n",
    "        best_accs.append(current_best)\n",
    "    axes[1, 1].plot(best_accs)\n",
    "    axes[1, 1].set(title='Best Val Accuracy', xlabel='Epoch', ylabel='Accuracy')\n",
    "    axes[1, 1].grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()\n",
    "\n",
    "def train_tabpfn_improved(scale: str = 'small', \n",
    "                         num_epochs: Optional[int] = None,\n",
    "                         checkpoint_path: Optional[str] = None) -> TabPFN:\n",
    "    \"\"\"\n",
    "    Improved training function with better defaults and monitoring\n",
    "    \n",
    "    Args:\n",
    "        scale: Model scale ('small', 'medium', 'large', 'xlarge')\n",
    "        num_epochs: Override number of epochs\n",
    "        checkpoint_path: Path to resume from checkpoint\n",
    "    \"\"\"\n",
    "    # Get configuration\n",
    "    config = create_optimized_config(scale)\n",
    "    if num_epochs is not None:\n",
    "        config.num_epochs = num_epochs\n",
    "    \n",
    "    # Device setup\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    print(f\"Training {scale} model with {config.num_train_tasks} training tasks\")\n",
    "    \n",
    "    # Create datasets with proper workers\n",
    "    num_workers = 4 if device.type == 'cuda' else 0\n",
    "    \n",
    "    print(\"Creating datasets...\")\n",
    "    train_dataset = TabPFNDataset(config, config.num_train_tasks, split='train')\n",
    "    val_dataset = TabPFNDataset(config, config.num_val_tasks, split='val')\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=collate_tabpfn_batch,\n",
    "        pin_memory=True if device.type == 'cuda' else False,\n",
    "        persistent_workers=True if num_workers > 0 else False\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config.batch_size * 2,  # Larger batch for validation\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=collate_tabpfn_batch,\n",
    "        pin_memory=True if device.type == 'cuda' else False,\n",
    "        persistent_workers=True if num_workers > 0 else False\n",
    "    )\n",
    "    \n",
    "    # Create model\n",
    "    print(f\"\\nCreating {scale} model...\")\n",
    "    model = TabPFN(config)\n",
    "    \n",
    "    # Initialize weights with better strategy\n",
    "    def init_weights(module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_uniform_(module.weight, gain=1.0)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            nn.init.ones_(module.weight)\n",
    "            nn.init.zeros_(module.bias)\n",
    "    \n",
    "    model.apply(init_weights)\n",
    "    \n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = ImprovedTabPFNTrainer(model, config, device)\n",
    "    \n",
    "    # Load checkpoint if provided\n",
    "    start_epoch = 0\n",
    "    if checkpoint_path and os.path.exists(checkpoint_path):\n",
    "        print(f\"Loading checkpoint from {checkpoint_path}\")\n",
    "        trainer.load_checkpoint(checkpoint_path)\n",
    "        start_epoch = trainer.best_epoch + 1\n",
    "    \n",
    "    # Training loop\n",
    "    print(f\"\\nStarting training from epoch {start_epoch + 1}...\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    early_stopping_patience = 20\n",
    "    epochs_without_improvement = 0\n",
    "    \n",
    "    for epoch in range(start_epoch, config.num_epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Train epoch\n",
    "        metrics = trainer.train_epoch_with_progress(train_loader, val_loader, epoch)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        epoch_time = time.time() - start_time\n",
    "        print(f\"\\nEpoch {epoch + 1}/{config.num_epochs} - {epoch_time:.1f}s\")\n",
    "        print(f\"Train Loss: {metrics['train_loss']:.4f}, Train Acc: {metrics['train_accuracy']:.4f}\")\n",
    "        print(f\"Val Loss: {metrics['val_loss']:.4f}, Val Acc: {metrics['val_accuracy']:.4f}\")\n",
    "        print(f\"Best Val Acc: {trainer.best_accuracy:.4f} (Epoch {trainer.best_epoch + 1})\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        # Early stopping\n",
    "        if epoch == trainer.best_epoch:\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "        \n",
    "        if epochs_without_improvement >= early_stopping_patience:\n",
    "            print(f\"\\nEarly stopping triggered after {epoch + 1} epochs\")\n",
    "            break\n",
    "        \n",
    "        # Save periodic checkpoints\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            trainer.save_checkpoint(f'tabpfn_epoch_{epoch+1}.pt')\n",
    "    \n",
    "    print(f\"\\nTraining completed! Best model achieved {trainer.best_accuracy:.4f} validation accuracy\")\n",
    "    \n",
    "    # Visualize progress\n",
    "    if len(trainer.train_metrics['loss']) > 0:\n",
    "        visualize_training_progress(trainer)\n",
    "    \n",
    "    # Load best model\n",
    "    trainer.load_checkpoint('tabpfn_best.pt')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def diagnose_training_issues(model: TabPFN, train_loader: DataLoader, device: torch.device):\n",
    "    \"\"\"Diagnose potential training issues\"\"\"\n",
    "    print(\"\\n=== Training Diagnostics ===\")\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Check a few batches\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            if i >= 5:\n",
    "                break\n",
    "                \n",
    "            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v \n",
    "                    for k, v in batch.items()}\n",
    "            \n",
    "            # Get model outputs\n",
    "            logits = model(batch)\n",
    "            \n",
    "            # Analyze outputs\n",
    "            print(f\"\\nBatch {i+1}:\")\n",
    "            print(f\"  Logits range: [{logits.min().item():.3f}, {logits.max().item():.3f}]\")\n",
    "            print(f\"  Logits mean: {logits.mean().item():.3f}\")\n",
    "            print(f\"  Logits std: {logits.std().item():.3f}\")\n",
    "            \n",
    "            # Check predictions distribution\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            unique_preds = torch.unique(predictions)\n",
    "            print(f\"  Unique predictions: {unique_preds.tolist()}\")\n",
    "            \n",
    "            # Check if model is collapsed\n",
    "            if len(unique_preds) == 1:\n",
    "                print(\"  WARNING: Model is predicting only one class!\")\n",
    "            \n",
    "            # Check attention weights if available\n",
    "            if hasattr(model, 'transformer') and hasattr(model.transformer.layers[0].self_attn, 'attention_weights'):\n",
    "                attn = model.transformer.layers[0].self_attn.attention_weights\n",
    "                print(f\"  Attention weights range: [{attn.min().item():.3f}, {attn.max().item():.3f}]\")\n",
    "\n",
    "# Example usage functions\n",
    "def example_improved_training():\n",
    "    \"\"\"Example of improved training\"\"\"\n",
    "    print(\"=== TabPFN Improved Training Example ===\\n\")\n",
    "    \n",
    "    # Train a medium-scale model\n",
    "    model = train_tabpfn_improved(\n",
    "        scale='small',\n",
    "        num_epochs=50  # Override for faster demo\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def example_resume_training():\n",
    "    \"\"\"Example of resuming training from checkpoint\"\"\"\n",
    "    print(\"=== TabPFN Resume Training Example ===\\n\")\n",
    "    \n",
    "    # Resume training from checkpoint\n",
    "    model = train_tabpfn_improved(\n",
    "        scale='small',\n",
    "        checkpoint_path='tabpfn_best.pt'\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def example_different_scales():\n",
    "    \"\"\"Compare different model scales\"\"\"\n",
    "    print(\"=== TabPFN Scale Comparison ===\\n\")\n",
    "    \n",
    "    scales = ['small', 'medium', 'large']\n",
    "    results = {}\n",
    "    \n",
    "    for scale in scales:\n",
    "        print(f\"\\nTraining {scale} model...\")\n",
    "        config = create_optimized_config(scale)\n",
    "        print(f\"  Hidden dim: {config.hidden_dim}\")\n",
    "        print(f\"  Num layers: {config.num_layers}\")\n",
    "        print(f\"  Train tasks: {config.num_train_tasks}\")\n",
    "        \n",
    "        # Just print config, don't actually train (for demo)\n",
    "        results[scale] = config\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run improved training\n",
    "    model = example_improved_training()\n",
    "    \n",
    "    # Show different scales\n",
    "    # scales = example_different_scales()\n",
    "    \n",
    "    print(\"\\nImproved training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac00c83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TabPFN with Advanced Augmentation ===\n",
      "\n",
      "Using device: cuda\n",
      "Training small model with advanced augmentation\n",
      "Focal Loss: True, Advanced Augmentation: True\n",
      "\n",
      "Creating augmented datasets...\n",
      "\n",
      "Creating small model...\n",
      "Model parameters: 833,482\n",
      "\n",
      "Starting training...\n",
      "================================================================================\n",
      "\n",
      "Epoch 1/20\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/313 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (8448x36 and 32x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1121\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Acc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mm[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1116\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrec=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mm[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Rec=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mm[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1117\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF1=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mm[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (n=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mm[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;66;03m# Run advanced training\u001b[39;00m\n\u001b[0;32m-> 1121\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mexample_advanced_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1123\u001b[0m     \u001b[38;5;66;03m# Visualize augmentation\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m     example_visualize_augmentation()\n",
      "Cell \u001b[0;32mIn[7], line 1059\u001b[0m, in \u001b[0;36mexample_advanced_training\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== TabPFN with Advanced Augmentation ===\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;66;03m# Train with all advanced features\u001b[39;00m\n\u001b[0;32m-> 1059\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_tabpfn_with_augmentation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1060\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msmall\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Start small for testing\u001b[39;49;00m\n\u001b[1;32m   1061\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1062\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_advanced_augmentation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1063\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_focal_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m   1064\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "Cell \u001b[0;32mIn[7], line 840\u001b[0m, in \u001b[0;36mtrain_tabpfn_with_augmentation\u001b[0;34m(scale, num_epochs, use_advanced_augmentation, use_focal_loss, checkpoint_path)\u001b[0m\n\u001b[1;32m    838\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    839\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[0;32m--> 840\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    841\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    842\u001b[0m     train_acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[0;32mIn[7], line 646\u001b[0m, in \u001b[0;36mImbalanceAwareTrainer.train_step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    642\u001b[0m batch \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m v \n\u001b[1;32m    643\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    645\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m--> 646\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;66;03m# Get class weights if using focal loss\u001b[39;00m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprior_config\u001b[38;5;241m.\u001b[39muse_focal_loss \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass_weights\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[5], line 594\u001b[0m, in \u001b[0;36mTabPFN.forward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    591\u001b[0m query_len \u001b[38;5;241m=\u001b[39m query_X\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    593\u001b[0m \u001b[38;5;66;03m# Encode features (FeatureEncoder will handle padding internally)\u001b[39;00m\n\u001b[0;32m--> 594\u001b[0m context_encoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    595\u001b[0m query_encoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_encoder(query_X, feature_mask)\n\u001b[1;32m    597\u001b[0m \u001b[38;5;66;03m# Add label embeddings to context\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[5], line 444\u001b[0m, in \u001b[0;36mFeatureEncoder.forward\u001b[0;34m(self, x, feature_mask)\u001b[0m\n\u001b[1;32m    441\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m*\u001b[39m feature_mask_extended\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m    443\u001b[0m \u001b[38;5;66;03m# Embed features\u001b[39;00m\n\u001b[0;32m--> 444\u001b[0m embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;66;03m# Apply feature-wise attention\u001b[39;00m\n\u001b[1;32m    447\u001b[0m attention \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_attention(embedded)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (8448x36 and 32x128)"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TabPFN with Advanced Data Augmentation and Imbalance Handling\n",
    "Based on the original TabPFN paper's data generation strategies\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "from dataclasses import dataclass\n",
    "import math\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer, QuantileTransformer\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, Matern, RationalQuadratic\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ===========================\n",
    "# Advanced Prior Specifications (from TabPFN paper)\n",
    "# ===========================\n",
    "\n",
    "@dataclass\n",
    "class PriorConfig:\n",
    "    \"\"\"Configuration for data generation priors\"\"\"\n",
    "    # Structural Causal Model parameters\n",
    "    num_causes_per_feature: Tuple[int, int] = (1, 5)\n",
    "    edge_probability: float = 0.3\n",
    "    noise_scale_range: Tuple[float, float] = (0.01, 0.3)\n",
    "    \n",
    "    # Function class weights (as in TabPFN paper)\n",
    "    function_class_weights: Dict[str, float] = None\n",
    "    \n",
    "    # Data transformation parameters\n",
    "    apply_power_transform: float = 0.3\n",
    "    apply_quantile_transform: float = 0.2\n",
    "    add_categorical_features: float = 0.3\n",
    "    \n",
    "    # Imbalance parameters\n",
    "    imbalance_ratio_range: Tuple[float, float] = (0.1, 1.0)\n",
    "    use_focal_loss: bool = True\n",
    "    focal_alpha: float = 0.25\n",
    "    focal_gamma: float = 2.0\n",
    "    \n",
    "    # Gaussian Process parameters\n",
    "    use_gp_functions: float = 0.2\n",
    "    gp_length_scale_range: Tuple[float, float] = (0.1, 2.0)\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.function_class_weights is None:\n",
    "            # Default weights from TabPFN paper\n",
    "            self.function_class_weights = {\n",
    "                'linear': 0.25,\n",
    "                'polynomial': 0.15,\n",
    "                'neural_basis': 0.15,\n",
    "                'decision_tree': 0.15,\n",
    "                'gaussian_process': 0.10,\n",
    "                'periodic': 0.10,\n",
    "                'interaction': 0.10\n",
    "            }\n",
    "\n",
    "class AdvancedDataGenerator:\n",
    "    \"\"\"\n",
    "    Advanced data generator implementing TabPFN paper's strategies:\n",
    "    1. Structural Causal Models with various function classes\n",
    "    2. Gaussian Process priors\n",
    "    3. Neural network basis functions\n",
    "    4. Decision tree-like functions\n",
    "    5. Data transformations and augmentations\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, prior_config: PriorConfig = None):\n",
    "        self.config = prior_config or PriorConfig()\n",
    "        self.rng = np.random.RandomState()\n",
    "        \n",
    "    def generate_dataset(self, num_samples: int, num_features: int, \n",
    "                        num_classes: int, seed: Optional[int] = None) -> Tuple[np.ndarray, np.ndarray, Dict[str, Any]]:\n",
    "        \"\"\"Generate a dataset with advanced augmentation strategies\"\"\"\n",
    "        if seed is not None:\n",
    "            self.rng.seed(seed)\n",
    "        \n",
    "        # Generate causal graph structure\n",
    "        causal_graph = self._generate_causal_graph(num_features)\n",
    "        \n",
    "        # Generate base features\n",
    "        X = self._generate_base_features(num_samples, num_features)\n",
    "        \n",
    "        # Apply causal mechanisms\n",
    "        X = self._apply_causal_mechanisms(X, causal_graph)\n",
    "        \n",
    "        # Generate target using sampled function class\n",
    "        y_continuous, function_info = self._generate_target_with_prior(X, num_features)\n",
    "        \n",
    "        # Apply data transformations\n",
    "        X, transform_info = self._apply_data_transformations(X)\n",
    "        \n",
    "        # Convert to classes with potential imbalance\n",
    "        y, class_info = self._create_imbalanced_classes(y_continuous, num_classes)\n",
    "        \n",
    "        # Metadata for training\n",
    "        metadata = {\n",
    "            'causal_graph': causal_graph,\n",
    "            'function_info': function_info,\n",
    "            'transform_info': transform_info,\n",
    "            'class_info': class_info\n",
    "        }\n",
    "        \n",
    "        return X.astype(np.float32), y.astype(np.int64), metadata\n",
    "    \n",
    "    def _generate_causal_graph(self, num_features: int) -> np.ndarray:\n",
    "        \"\"\"Generate a DAG for causal relationships\"\"\"\n",
    "        # Lower triangular matrix ensures DAG\n",
    "        graph = np.zeros((num_features, num_features))\n",
    "        \n",
    "        for i in range(num_features):\n",
    "            for j in range(i):\n",
    "                if self.rng.random() < self.config.edge_probability:\n",
    "                    graph[i, j] = self.rng.uniform(0.5, 2.0) * self.rng.choice([-1, 1])\n",
    "        \n",
    "        return graph\n",
    "    \n",
    "    def _generate_base_features(self, num_samples: int, num_features: int) -> np.ndarray:\n",
    "        \"\"\"Generate diverse base features\"\"\"\n",
    "        X = np.zeros((num_samples, num_features))\n",
    "        \n",
    "        # Mix of different distributions (as in TabPFN paper)\n",
    "        distributions = [\n",
    "            ('normal', lambda n: self.rng.normal(0, 1, n)),\n",
    "            ('uniform', lambda n: self.rng.uniform(-2, 2, n)),\n",
    "            ('exponential', lambda n: self.rng.exponential(1, n) - 1),\n",
    "            ('student_t', lambda n: stats.t.rvs(df=3, size=n, random_state=self.rng)),\n",
    "            ('laplace', lambda n: self.rng.laplace(0, 1, n)),\n",
    "            ('gamma', lambda n: stats.gamma.rvs(2, size=n, random_state=self.rng) - 2),\n",
    "            ('beta', lambda n: stats.beta.rvs(2, 5, size=n, random_state=self.rng) * 4 - 2),\n",
    "            ('mixture', lambda n: self._generate_mixture(n))\n",
    "        ]\n",
    "        \n",
    "        for i in range(num_features):\n",
    "            dist_name, dist_func = distributions[i % len(distributions)]\n",
    "            X[:, i] = dist_func(num_samples)\n",
    "            \n",
    "            # Add some dependencies between features\n",
    "            if i > 0 and self.rng.random() < 0.3:\n",
    "                parent_idx = self.rng.randint(0, i)\n",
    "                correlation = self.rng.uniform(-0.8, 0.8)\n",
    "                X[:, i] = correlation * X[:, parent_idx] + np.sqrt(1 - correlation**2) * X[:, i]\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def _generate_mixture(self, n: int) -> np.ndarray:\n",
    "        \"\"\"Generate mixture of Gaussians\"\"\"\n",
    "        n_components = self.rng.randint(2, 5)\n",
    "        weights = self.rng.dirichlet(np.ones(n_components))\n",
    "        \n",
    "        samples = []\n",
    "        for _ in range(n):\n",
    "            component = self.rng.choice(n_components, p=weights)\n",
    "            mean = self.rng.uniform(-3, 3)\n",
    "            std = self.rng.uniform(0.5, 1.5)\n",
    "            samples.append(self.rng.normal(mean, std))\n",
    "        \n",
    "        return np.array(samples)\n",
    "    \n",
    "    def _apply_causal_mechanisms(self, X: np.ndarray, graph: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Apply causal relationships based on graph\"\"\"\n",
    "        X_transformed = X.copy()\n",
    "        \n",
    "        for i in range(X.shape[1]):\n",
    "            parents = np.where(graph[i, :] != 0)[0]\n",
    "            if len(parents) > 0:\n",
    "                # Apply causal mechanism\n",
    "                parent_values = X_transformed[:, parents]\n",
    "                weights = graph[i, parents]\n",
    "                \n",
    "                # Add non-linear transformation\n",
    "                if self.rng.random() < 0.5:\n",
    "                    X_transformed[:, i] += np.sum(parent_values * weights, axis=1)\n",
    "                else:\n",
    "                    # Non-linear interaction\n",
    "                    X_transformed[:, i] += np.tanh(np.sum(parent_values * weights, axis=1))\n",
    "        \n",
    "        return X_transformed\n",
    "    \n",
    "    def _generate_target_with_prior(self, X: np.ndarray, num_features: int) -> Tuple[np.ndarray, Dict[str, Any]]:\n",
    "        \"\"\"Generate target using various function classes from TabPFN paper\"\"\"\n",
    "        \n",
    "        # Sample function class based on weights\n",
    "        function_classes = list(self.config.function_class_weights.keys())\n",
    "        probabilities = list(self.config.function_class_weights.values())\n",
    "        chosen_class = self.rng.choice(function_classes, p=probabilities)\n",
    "        \n",
    "        # Select relevant features\n",
    "        num_relevant = self.rng.randint(\n",
    "            max(1, int(num_features * 0.1)), \n",
    "            max(2, int(num_features * 0.7))\n",
    "        )\n",
    "        relevant_features = self.rng.choice(num_features, size=num_relevant, replace=False)\n",
    "        \n",
    "        # Generate target based on chosen function class\n",
    "        if chosen_class == 'linear':\n",
    "            y, info = self._linear_function(X, relevant_features)\n",
    "        elif chosen_class == 'polynomial':\n",
    "            y, info = self._polynomial_function(X, relevant_features)\n",
    "        elif chosen_class == 'neural_basis':\n",
    "            y, info = self._neural_basis_function(X, relevant_features)\n",
    "        elif chosen_class == 'decision_tree':\n",
    "            y, info = self._decision_tree_function(X, relevant_features)\n",
    "        elif chosen_class == 'gaussian_process':\n",
    "            y, info = self._gaussian_process_function(X, relevant_features)\n",
    "        elif chosen_class == 'periodic':\n",
    "            y, info = self._periodic_function(X, relevant_features)\n",
    "        elif chosen_class == 'interaction':\n",
    "            y, info = self._interaction_function(X, relevant_features)\n",
    "        else:\n",
    "            y, info = self._linear_function(X, relevant_features)\n",
    "        \n",
    "        # Add noise\n",
    "        noise_scale = self.rng.uniform(*self.config.noise_scale_range)\n",
    "        y += self.rng.normal(0, noise_scale, size=y.shape)\n",
    "        \n",
    "        info.update({\n",
    "            'function_class': chosen_class,\n",
    "            'relevant_features': relevant_features,\n",
    "            'noise_scale': noise_scale\n",
    "        })\n",
    "        \n",
    "        return y, info\n",
    "    \n",
    "    def _linear_function(self, X: np.ndarray, features: np.ndarray) -> Tuple[np.ndarray, Dict]:\n",
    "        \"\"\"Linear combination with random weights\"\"\"\n",
    "        weights = self.rng.randn(len(features))\n",
    "        y = X[:, features] @ weights\n",
    "        return y, {'weights': weights}\n",
    "    \n",
    "    def _polynomial_function(self, X: np.ndarray, features: np.ndarray) -> Tuple[np.ndarray, Dict]:\n",
    "        \"\"\"Polynomial features with interactions\"\"\"\n",
    "        y = np.zeros(X.shape[0])\n",
    "        terms = []\n",
    "        \n",
    "        # Single features with powers\n",
    "        for feat in features[:min(5, len(features))]:\n",
    "            power = self.rng.randint(1, 4)\n",
    "            coef = self.rng.randn()\n",
    "            y += coef * (X[:, feat] ** power)\n",
    "            terms.append(('power', feat, power, coef))\n",
    "        \n",
    "        # Interactions\n",
    "        if len(features) >= 2:\n",
    "            for _ in range(min(3, len(features) // 2)):\n",
    "                feat1, feat2 = self.rng.choice(features, size=2, replace=False)\n",
    "                coef = self.rng.randn()\n",
    "                y += coef * X[:, feat1] * X[:, feat2]\n",
    "                terms.append(('interaction', feat1, feat2, coef))\n",
    "        \n",
    "        return y, {'terms': terms}\n",
    "    \n",
    "    def _neural_basis_function(self, X: np.ndarray, features: np.ndarray) -> Tuple[np.ndarray, Dict]:\n",
    "        \"\"\"Neural network-like basis functions\"\"\"\n",
    "        hidden_size = self.rng.randint(5, 20)\n",
    "        \n",
    "        # First layer\n",
    "        W1 = self.rng.randn(len(features), hidden_size) * 0.5\n",
    "        b1 = self.rng.randn(hidden_size) * 0.1\n",
    "        \n",
    "        # Hidden activation\n",
    "        hidden = np.tanh(X[:, features] @ W1 + b1)\n",
    "        \n",
    "        # Output layer\n",
    "        W2 = self.rng.randn(hidden_size) * 0.5\n",
    "        b2 = self.rng.randn() * 0.1\n",
    "        \n",
    "        y = hidden @ W2 + b2\n",
    "        \n",
    "        return y, {\n",
    "            'architecture': f'Input({len(features)}) -> Hidden({hidden_size}) -> Output(1)',\n",
    "            'activation': 'tanh'\n",
    "        }\n",
    "    \n",
    "    def _decision_tree_function(self, X: np.ndarray, features: np.ndarray) -> Tuple[np.ndarray, Dict]:\n",
    "        \"\"\"Decision tree-like step functions\"\"\"\n",
    "        y = np.zeros(X.shape[0])\n",
    "        \n",
    "        # Create tree-like splits\n",
    "        num_splits = self.rng.randint(3, 8)\n",
    "        splits = []\n",
    "        \n",
    "        for _ in range(num_splits):\n",
    "            feat = self.rng.choice(features)\n",
    "            threshold = np.percentile(X[:, feat], self.rng.uniform(20, 80))\n",
    "            value = self.rng.randn()\n",
    "            \n",
    "            # Apply split\n",
    "            mask = X[:, feat] > threshold\n",
    "            y[mask] += value\n",
    "            \n",
    "            splits.append((feat, threshold, value))\n",
    "        \n",
    "        return y, {'splits': splits}\n",
    "    \n",
    "    def _gaussian_process_function(self, X: np.ndarray, features: np.ndarray) -> Tuple[np.ndarray, Dict]:\n",
    "        \"\"\"Gaussian Process prior functions\"\"\"\n",
    "        # Sample kernel\n",
    "        kernels = [\n",
    "            RBF(length_scale=self.rng.uniform(*self.config.gp_length_scale_range)),\n",
    "            Matern(length_scale=self.rng.uniform(*self.config.gp_length_scale_range), nu=1.5),\n",
    "            RationalQuadratic(length_scale=self.rng.uniform(*self.config.gp_length_scale_range))\n",
    "        ]\n",
    "        kernel = self.rng.choice(kernels)\n",
    "        \n",
    "        # Sample from GP prior\n",
    "        X_subset = X[:, features]\n",
    "        K = kernel(X_subset)\n",
    "        L = np.linalg.cholesky(K + 1e-6 * np.eye(X.shape[0]))\n",
    "        y = L @ self.rng.randn(X.shape[0])\n",
    "        \n",
    "        return y, {'kernel': str(kernel)}\n",
    "    \n",
    "    def _periodic_function(self, X: np.ndarray, features: np.ndarray) -> Tuple[np.ndarray, Dict]:\n",
    "        \"\"\"Periodic functions\"\"\"\n",
    "        y = np.zeros(X.shape[0])\n",
    "        components = []\n",
    "        \n",
    "        for feat in features[:min(3, len(features))]:\n",
    "            frequency = self.rng.uniform(0.5, 3.0)\n",
    "            phase = self.rng.uniform(0, 2 * np.pi)\n",
    "            amplitude = self.rng.randn()\n",
    "            \n",
    "            y += amplitude * np.sin(frequency * X[:, feat] + phase)\n",
    "            components.append((feat, frequency, phase, amplitude))\n",
    "        \n",
    "        return y, {'components': components}\n",
    "    \n",
    "    def _interaction_function(self, X: np.ndarray, features: np.ndarray) -> Tuple[np.ndarray, Dict]:\n",
    "        \"\"\"Complex interaction patterns\"\"\"\n",
    "        y = np.zeros(X.shape[0])\n",
    "        interactions = []\n",
    "        \n",
    "        # Pairwise interactions\n",
    "        if len(features) >= 2:\n",
    "            for _ in range(min(5, len(features))):\n",
    "                feat1, feat2 = self.rng.choice(features, size=2, replace=False)\n",
    "                \n",
    "                interaction_type = self.rng.choice(['multiply', 'divide', 'subtract'])\n",
    "                coef = self.rng.randn()\n",
    "                \n",
    "                if interaction_type == 'multiply':\n",
    "                    y += coef * X[:, feat1] * X[:, feat2]\n",
    "                elif interaction_type == 'divide':\n",
    "                    y += coef * X[:, feat1] / (np.abs(X[:, feat2]) + 0.1)\n",
    "                else:  # subtract\n",
    "                    y += coef * (X[:, feat1] - X[:, feat2])\n",
    "                \n",
    "                interactions.append((feat1, feat2, interaction_type, coef))\n",
    "        \n",
    "        # Three-way interactions\n",
    "        if len(features) >= 3:\n",
    "            feat1, feat2, feat3 = self.rng.choice(features, size=3, replace=False)\n",
    "            coef = self.rng.randn() * 0.5\n",
    "            y += coef * X[:, feat1] * X[:, feat2] * X[:, feat3]\n",
    "            interactions.append((feat1, feat2, feat3, 'triple', coef))\n",
    "        \n",
    "        return y, {'interactions': interactions}\n",
    "    \n",
    "    def _apply_data_transformations(self, X: np.ndarray) -> Tuple[np.ndarray, Dict[str, Any]]:\n",
    "        \"\"\"Apply various data transformations as in TabPFN paper\"\"\"\n",
    "        X_transformed = X.copy()\n",
    "        transformations = []\n",
    "        \n",
    "        # Power transformation\n",
    "        if self.rng.random() < self.config.apply_power_transform:\n",
    "            power_features = self.rng.choice(\n",
    "                X.shape[1], \n",
    "                size=self.rng.randint(1, max(2, X.shape[1] // 3)), \n",
    "                replace=False\n",
    "            )\n",
    "            pt = PowerTransformer(method='yeo-johnson')\n",
    "            X_transformed[:, power_features] = pt.fit_transform(X[:, power_features])\n",
    "            transformations.append(('power', power_features))\n",
    "        \n",
    "        # Quantile transformation\n",
    "        if self.rng.random() < self.config.apply_quantile_transform:\n",
    "            quantile_features = self.rng.choice(\n",
    "                X.shape[1], \n",
    "                size=self.rng.randint(1, max(2, X.shape[1] // 3)), \n",
    "                replace=False\n",
    "            )\n",
    "            qt = QuantileTransformer(output_distribution='uniform', n_quantiles=min(100, X.shape[0]))\n",
    "            X_transformed[:, quantile_features] = qt.fit_transform(X[:, quantile_features])\n",
    "            transformations.append(('quantile', quantile_features))\n",
    "        \n",
    "        # Add categorical features through binning\n",
    "        if self.rng.random() < self.config.add_categorical_features:\n",
    "            num_cat_features = self.rng.randint(1, max(2, X.shape[1] // 4))\n",
    "            cat_features = []\n",
    "            \n",
    "            for _ in range(num_cat_features):\n",
    "                source_feat = self.rng.randint(0, X.shape[1])\n",
    "                num_bins = self.rng.randint(3, 10)\n",
    "                \n",
    "                # Create bins\n",
    "                bins = np.percentile(X[:, source_feat], np.linspace(0, 100, num_bins + 1))\n",
    "                bins[0] = -np.inf\n",
    "                bins[-1] = np.inf\n",
    "                \n",
    "                # Digitize\n",
    "                cat_feat = np.digitize(X[:, source_feat], bins) - 1\n",
    "                cat_features.append(cat_feat)\n",
    "            \n",
    "            # One-hot encode and append\n",
    "            if cat_features:\n",
    "                cat_features = np.column_stack(cat_features)\n",
    "                # Simple encoding: just use the categorical values scaled\n",
    "                X_transformed = np.hstack([X_transformed, cat_features / num_bins])\n",
    "                transformations.append(('categorical', num_cat_features))\n",
    "        \n",
    "        return X_transformed, {'transformations': transformations}\n",
    "    \n",
    "    def _create_imbalanced_classes(self, y_continuous: np.ndarray, \n",
    "                                  num_classes: int) -> Tuple[np.ndarray, Dict[str, Any]]:\n",
    "        \"\"\"Create potentially imbalanced classes\"\"\"\n",
    "        \n",
    "        # Decide on imbalance ratio\n",
    "        imbalance_ratio = self.rng.uniform(*self.config.imbalance_ratio_range)\n",
    "        \n",
    "        if num_classes == 2:\n",
    "            # Binary classification with controlled imbalance\n",
    "            threshold = np.percentile(y_continuous, 100 * imbalance_ratio)\n",
    "            y = (y_continuous > threshold).astype(np.int64)\n",
    "        else:\n",
    "            # Multi-class with potentially imbalanced distribution\n",
    "            if self.rng.random() < 0.5:\n",
    "                # Uniform classes\n",
    "                percentiles = np.linspace(0, 100, num_classes + 1)\n",
    "                thresholds = np.percentile(y_continuous, percentiles[1:-1])\n",
    "                y = np.digitize(y_continuous, thresholds).astype(np.int64)\n",
    "            else:\n",
    "                # Imbalanced classes using exponential spacing\n",
    "                base = imbalance_ratio ** (1 / (num_classes - 1))\n",
    "                proportions = [base ** i for i in range(num_classes)]\n",
    "                proportions = np.array(proportions) / sum(proportions)\n",
    "                \n",
    "                percentiles = np.cumsum([0] + proportions.tolist()) * 100\n",
    "                thresholds = np.percentile(y_continuous, percentiles[1:-1])\n",
    "                y = np.digitize(y_continuous, thresholds).astype(np.int64)\n",
    "        \n",
    "        # Compute class weights for loss weighting\n",
    "        unique_classes, counts = np.unique(y, return_counts=True)\n",
    "        class_weights = len(y) / (len(unique_classes) * counts)\n",
    "        class_weights = dict(zip(unique_classes, class_weights))\n",
    "        \n",
    "        return y, {\n",
    "            'imbalance_ratio': imbalance_ratio,\n",
    "            'class_distribution': dict(zip(*np.unique(y, return_counts=True))),\n",
    "            'class_weights': class_weights\n",
    "        }\n",
    "\n",
    "# ===========================\n",
    "# Enhanced Loss Functions\n",
    "# ===========================\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss for addressing class imbalance\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha: Optional[torch.Tensor] = None, gamma: float = 2.0, \n",
    "                 reduction: str = 'mean'):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, inputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        p = torch.exp(-ce_loss)\n",
    "        loss = (1 - p) ** self.gamma * ce_loss\n",
    "        \n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.device != inputs.device:\n",
    "                self.alpha = self.alpha.to(inputs.device)\n",
    "            loss = self.alpha[targets] * loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss\n",
    "\n",
    "class BalancedCrossEntropyLoss(nn.Module):\n",
    "    \"\"\"Balanced Cross Entropy Loss with automatic class weight computation\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.class_counts = torch.zeros(num_classes)\n",
    "        self.total_samples = 0\n",
    "    \n",
    "    def update_class_weights(self, targets: torch.Tensor):\n",
    "        \"\"\"Update class weights based on observed distribution\"\"\"\n",
    "        for c in range(self.num_classes):\n",
    "            self.class_counts[c] += (targets == c).sum().item()\n",
    "        self.total_samples += len(targets)\n",
    "    \n",
    "    def get_weights(self) -> torch.Tensor:\n",
    "        \"\"\"Compute balanced class weights\"\"\"\n",
    "        weights = self.total_samples / (self.num_classes * self.class_counts + 1e-6)\n",
    "        return weights / weights.sum() * self.num_classes\n",
    "    \n",
    "    def forward(self, inputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        self.update_class_weights(targets)\n",
    "        weights = self.get_weights().to(inputs.device)\n",
    "        return F.cross_entropy(inputs, targets, weight=weights)\n",
    "\n",
    "# ===========================\n",
    "# Enhanced TabPFN Dataset with Augmentation\n",
    "# ===========================\n",
    "\n",
    "class AugmentedTabPFNDataset(Dataset):\n",
    "    \"\"\"Enhanced dataset with advanced augmentation strategies\"\"\"\n",
    "    \n",
    "    def __init__(self, config: TabPFNConfig, prior_config: PriorConfig, \n",
    "                 num_tasks: int, split: str = 'train'):\n",
    "        self.config = config\n",
    "        self.prior_config = prior_config\n",
    "        self.num_tasks = num_tasks\n",
    "        self.split = split\n",
    "        self.generator = AdvancedDataGenerator(prior_config)\n",
    "        \n",
    "        # Cache for class weights\n",
    "        self.class_weights_cache = {}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_tasks\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
    "        # Set seed for validation reproducibility\n",
    "        seed = idx if self.split == 'val' else None\n",
    "        \n",
    "        # Sample task parameters\n",
    "        num_features = np.random.randint(self.config.min_features, self.config.max_features + 1)\n",
    "        num_samples = np.random.randint(self.config.min_samples, self.config.max_samples + 1)\n",
    "        num_classes = np.random.randint(2, self.config.max_classes + 1)\n",
    "        \n",
    "        # Generate dataset with augmentation\n",
    "        X, y, metadata = self.generator.generate_dataset(\n",
    "            num_samples=num_samples,\n",
    "            num_features=num_features,\n",
    "            num_classes=num_classes,\n",
    "            seed=seed\n",
    "        )\n",
    "        \n",
    "        # Additional augmentations for training\n",
    "        if self.split == 'train':\n",
    "            X = self._apply_training_augmentations(X)\n",
    "        \n",
    "        # Normalize features\n",
    "        scaler = StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "        \n",
    "        # Handle variable feature dimensions\n",
    "        if X.shape[1] > num_features:\n",
    "            # If transformations added features, keep track\n",
    "            actual_features = X.shape[1]\n",
    "        else:\n",
    "            actual_features = num_features\n",
    "        \n",
    "        # Split into context and query\n",
    "        split_idx = np.random.randint(num_samples // 4, 3 * num_samples // 4)\n",
    "        \n",
    "        context_X = torch.from_numpy(X[:split_idx]).float()\n",
    "        context_y = torch.from_numpy(y[:split_idx]).long()\n",
    "        query_X = torch.from_numpy(X[split_idx:]).float()\n",
    "        query_y = torch.from_numpy(y[split_idx:]).long()\n",
    "        \n",
    "        # Prepare class weights for focal loss\n",
    "        class_weights = metadata['class_info'].get('class_weights', {})\n",
    "        class_weights_tensor = torch.zeros(self.config.max_classes)\n",
    "        for c, w in class_weights.items():\n",
    "            if c < self.config.max_classes:\n",
    "                class_weights_tensor[c] = w\n",
    "        \n",
    "        return {\n",
    "            'context_X': context_X,\n",
    "            'context_y': context_y,\n",
    "            'query_X': query_X,\n",
    "            'query_y': query_y,\n",
    "            'num_features': actual_features,\n",
    "            'num_classes': num_classes,\n",
    "            'num_context': split_idx,\n",
    "            'num_query': num_samples - split_idx,\n",
    "            'class_weights': class_weights_tensor,\n",
    "            'metadata': metadata\n",
    "        }\n",
    "    \n",
    "    def _apply_training_augmentations(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Apply additional augmentations during training\"\"\"\n",
    "        \n",
    "        # Random feature permutation\n",
    "        if np.random.random() < 0.1:\n",
    "            perm = np.random.permutation(X.shape[1])\n",
    "            X = X[:, perm]\n",
    "        \n",
    "        # Add Gaussian noise\n",
    "        if np.random.random() < 0.3:\n",
    "            noise_scale = np.random.uniform(0.01, 0.1)\n",
    "            X += np.random.randn(*X.shape) * noise_scale\n",
    "        \n",
    "        # Random scaling\n",
    "        if np.random.random() < 0.2:\n",
    "            scale = np.random.uniform(0.8, 1.2, size=(1, X.shape[1]))\n",
    "            X *= scale\n",
    "        \n",
    "        return X\n",
    "\n",
    "# ===========================\n",
    "# Training with Imbalance Handling\n",
    "# ===========================\n",
    "\n",
    "class ImbalanceAwareTrainer(TabPFNTrainer):\n",
    "    \"\"\"Trainer with focal loss and imbalance handling\"\"\"\n",
    "    \n",
    "    def __init__(self, model: TabPFN, config: TabPFNConfig, \n",
    "                 prior_config: PriorConfig, device: torch.device):\n",
    "        super().__init__(model, config, device)\n",
    "        self.prior_config = prior_config\n",
    "        \n",
    "        # Loss functions\n",
    "        if prior_config.use_focal_loss:\n",
    "            self.criterion = FocalLoss(gamma=prior_config.focal_gamma)\n",
    "        else:\n",
    "            self.criterion = BalancedCrossEntropyLoss(config.max_classes)\n",
    "        \n",
    "        # Track class distribution\n",
    "        self.class_distribution = torch.zeros(config.max_classes)\n",
    "        self.total_samples = 0\n",
    "    \n",
    "    def train_step(self, batch: Dict[str, torch.Tensor]) -> Dict[str, float]:\n",
    "        \"\"\"Training step with imbalance awareness\"\"\"\n",
    "        self.model.train()\n",
    "        \n",
    "        # Move batch to device\n",
    "        batch = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v \n",
    "                for k, v in batch.items()}\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = self.model(batch)\n",
    "        \n",
    "        # Get class weights if using focal loss\n",
    "        if self.prior_config.use_focal_loss and 'class_weights' in batch:\n",
    "            class_weights = batch['class_weights'].mean(dim=0)  # Average over batch\n",
    "            if self.criterion.alpha is None:\n",
    "                self.criterion.alpha = class_weights\n",
    "        \n",
    "        # Compute loss\n",
    "        query_y = batch['query_y']\n",
    "        query_mask = batch['query_mask']\n",
    "        \n",
    "        # Flatten for loss computation\n",
    "        logits_flat = logits.view(-1, logits.size(-1))\n",
    "        labels_flat = query_y.view(-1)\n",
    "        mask_flat = query_mask.view(-1)\n",
    "        \n",
    "        # Filter valid positions\n",
    "        if mask_flat.sum() > 0:\n",
    "            valid_logits = logits_flat[mask_flat]\n",
    "            valid_labels = labels_flat[mask_flat]\n",
    "            \n",
    "            loss = self.criterion(valid_logits, valid_labels)\n",
    "            \n",
    "            # Update class distribution tracking\n",
    "            self._update_class_distribution(valid_labels)\n",
    "        else:\n",
    "            loss = torch.tensor(0.0, device=self.device)\n",
    "        \n",
    "        # Backward pass\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.gradient_clip)\n",
    "        \n",
    "        # Optimizer step\n",
    "        self.optimizer.step()\n",
    "        self.scheduler.step()\n",
    "        \n",
    "        # Compute accuracy with class-wise metrics\n",
    "        with torch.no_grad():\n",
    "            if mask_flat.sum() > 0:\n",
    "                predictions = torch.argmax(valid_logits, dim=-1)\n",
    "                accuracy = (predictions == valid_labels).float().mean().item()\n",
    "                \n",
    "                # Per-class accuracy\n",
    "                class_accuracies = {}\n",
    "                for c in range(self.config.max_classes):\n",
    "                    mask_c = valid_labels == c\n",
    "                    if mask_c.sum() > 0:\n",
    "                        acc_c = (predictions[mask_c] == c).float().mean().item()\n",
    "                        class_accuracies[c] = acc_c\n",
    "            else:\n",
    "                accuracy = 0.0\n",
    "                class_accuracies = {}\n",
    "        \n",
    "        return {\n",
    "            'loss': loss.item(),\n",
    "            'accuracy': accuracy,\n",
    "            'lr': self.optimizer.param_groups[0]['lr'],\n",
    "            'class_accuracies': class_accuracies\n",
    "        }\n",
    "    \n",
    "    def _update_class_distribution(self, labels: torch.Tensor):\n",
    "        \"\"\"Update tracked class distribution\"\"\"\n",
    "        for c in range(self.config.max_classes):\n",
    "            self.class_distribution[c] += (labels == c).sum().item()\n",
    "        self.total_samples += len(labels)\n",
    "    \n",
    "    def get_class_distribution(self) -> Dict[int, float]:\n",
    "        \"\"\"Get normalized class distribution\"\"\"\n",
    "        if self.total_samples == 0:\n",
    "            return {}\n",
    "        \n",
    "        distribution = self.class_distribution / self.total_samples\n",
    "        return {i: dist.item() for i, dist in enumerate(distribution) if dist > 0}\n",
    "\n",
    "# ===========================\n",
    "# Advanced Training Pipeline\n",
    "# ===========================\n",
    "\n",
    "def train_tabpfn_with_augmentation(\n",
    "    scale: str = 'medium',\n",
    "    num_epochs: Optional[int] = None,\n",
    "    use_advanced_augmentation: bool = True,\n",
    "    use_focal_loss: bool = True,\n",
    "    checkpoint_path: Optional[str] = None\n",
    ") -> TabPFN:\n",
    "    \"\"\"\n",
    "    Train TabPFN with advanced augmentation and imbalance handling\n",
    "    \n",
    "    Args:\n",
    "        scale: Model scale ('small', 'medium', 'large')\n",
    "        num_epochs: Number of training epochs\n",
    "        use_advanced_augmentation: Whether to use advanced data generation\n",
    "        use_focal_loss: Whether to use focal loss for imbalance\n",
    "        checkpoint_path: Path to resume from checkpoint\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create configurations\n",
    "    config = create_optimized_config(scale)\n",
    "    if num_epochs is not None:\n",
    "        config.num_epochs = num_epochs\n",
    "    \n",
    "    prior_config = PriorConfig(\n",
    "        use_focal_loss=use_focal_loss,\n",
    "        focal_gamma=2.0,\n",
    "        focal_alpha=0.25,\n",
    "        imbalance_ratio_range=(0.1, 0.9),\n",
    "        use_gp_functions=0.2,\n",
    "        apply_power_transform=0.3,\n",
    "        apply_quantile_transform=0.2\n",
    "    )\n",
    "    \n",
    "    # Device setup\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    print(f\"Training {scale} model with advanced augmentation\")\n",
    "    print(f\"Focal Loss: {use_focal_loss}, Advanced Augmentation: {use_advanced_augmentation}\")\n",
    "    \n",
    "    # Create datasets\n",
    "    print(\"\\nCreating augmented datasets...\")\n",
    "    if use_advanced_augmentation:\n",
    "        train_dataset = AugmentedTabPFNDataset(\n",
    "            config, prior_config, config.num_train_tasks, split='train'\n",
    "        )\n",
    "        val_dataset = AugmentedTabPFNDataset(\n",
    "            config, prior_config, config.num_val_tasks, split='val'\n",
    "        )\n",
    "    else:\n",
    "        # Fall back to original dataset\n",
    "        train_dataset = TabPFNDataset(config, config.num_train_tasks, split='train')\n",
    "        val_dataset = TabPFNDataset(config, config.num_val_tasks, split='val')\n",
    "    \n",
    "    # Create dataloaders\n",
    "    num_workers = 4 if device.type == 'cuda' else 0\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=collate_tabpfn_batch,\n",
    "        pin_memory=True if device.type == 'cuda' else False\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config.batch_size * 2,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=collate_tabpfn_batch,\n",
    "        pin_memory=True if device.type == 'cuda' else False\n",
    "    )\n",
    "    \n",
    "    # Create model\n",
    "    print(f\"\\nCreating {scale} model...\")\n",
    "    model = TabPFN(config)\n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # Create trainer\n",
    "    if use_advanced_augmentation:\n",
    "        trainer = ImbalanceAwareTrainer(model, config, prior_config, device)\n",
    "    else:\n",
    "        trainer = TabPFNTrainer(model, config, device)\n",
    "    \n",
    "    # Load checkpoint if provided\n",
    "    start_epoch = 0\n",
    "    if checkpoint_path and os.path.exists(checkpoint_path):\n",
    "        print(f\"\\nLoading checkpoint from {checkpoint_path}\")\n",
    "        trainer.load_checkpoint(checkpoint_path)\n",
    "        start_epoch = trainer.best_epoch + 1\n",
    "    \n",
    "    # Training loop with monitoring\n",
    "    print(f\"\\nStarting training...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    best_balanced_accuracy = 0.0\n",
    "    epochs_without_improvement = 0\n",
    "    early_stopping_patience = 20\n",
    "    \n",
    "    for epoch in range(start_epoch, config.num_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{config.num_epochs}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Training phase\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "        train_steps = 0\n",
    "        class_acc_accumulator = {}\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc='Training')\n",
    "        for batch in pbar:\n",
    "            metrics = trainer.train_step(batch)\n",
    "            train_loss += metrics['loss']\n",
    "            train_acc += metrics['accuracy']\n",
    "            train_steps += 1\n",
    "            \n",
    "            # Accumulate class accuracies\n",
    "            for c, acc in metrics.get('class_accuracies', {}).items():\n",
    "                if c not in class_acc_accumulator:\n",
    "                    class_acc_accumulator[c] = []\n",
    "                class_acc_accumulator[c].append(acc)\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'loss': f\"{metrics['loss']:.4f}\",\n",
    "                'acc': f\"{metrics['accuracy']:.4f}\",\n",
    "                'lr': f\"{metrics['lr']:.6f}\"\n",
    "            })\n",
    "        \n",
    "        # Calculate average metrics\n",
    "        avg_train_loss = train_loss / train_steps\n",
    "        avg_train_acc = train_acc / train_steps\n",
    "        \n",
    "        # Calculate per-class average accuracies\n",
    "        avg_class_accuracies = {}\n",
    "        for c, accs in class_acc_accumulator.items():\n",
    "            avg_class_accuracies[c] = np.mean(accs)\n",
    "        \n",
    "        # Validation phase\n",
    "        val_metrics = trainer.evaluate(val_loader)\n",
    "        \n",
    "        # Calculate balanced accuracy\n",
    "        if hasattr(trainer, 'get_class_distribution'):\n",
    "            class_dist = trainer.get_class_distribution()\n",
    "            if avg_class_accuracies:\n",
    "                balanced_acc = np.mean(list(avg_class_accuracies.values()))\n",
    "            else:\n",
    "                balanced_acc = avg_train_acc\n",
    "        else:\n",
    "            balanced_acc = avg_train_acc\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f\"\\nTrain Loss: {avg_train_loss:.4f}, Train Acc: {avg_train_acc:.4f}\")\n",
    "        print(f\"Val Loss: {val_metrics['loss']:.4f}, Val Acc: {val_metrics['accuracy']:.4f}\")\n",
    "        print(f\"Balanced Accuracy: {balanced_acc:.4f}\")\n",
    "        \n",
    "        # Print class distribution if available\n",
    "        if hasattr(trainer, 'get_class_distribution'):\n",
    "            class_dist = trainer.get_class_distribution()\n",
    "            if class_dist:\n",
    "                print(\"\\nClass Distribution:\")\n",
    "                for c, prop in sorted(class_dist.items()):\n",
    "                    acc = avg_class_accuracies.get(c, 0.0)\n",
    "                    print(f\"  Class {c}: {prop:.3f} (Acc: {acc:.3f})\")\n",
    "        \n",
    "        # Save best model based on balanced accuracy\n",
    "        if balanced_acc > best_balanced_accuracy:\n",
    "            best_balanced_accuracy = balanced_acc\n",
    "            trainer.best_epoch = epoch\n",
    "            trainer.save_checkpoint('tabpfn_best_balanced.pt')\n",
    "            print(f\"\\nNew best balanced accuracy: {best_balanced_accuracy:.4f}\")\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "        \n",
    "        # Early stopping\n",
    "        if epochs_without_improvement >= early_stopping_patience:\n",
    "            print(f\"\\nEarly stopping triggered after {epoch + 1} epochs\")\n",
    "            break\n",
    "        \n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    print(f\"\\nTraining completed!\")\n",
    "    print(f\"Best balanced accuracy: {best_balanced_accuracy:.4f}\")\n",
    "    \n",
    "    # Load best model\n",
    "    trainer.load_checkpoint('tabpfn_best_balanced.pt')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# ===========================\n",
    "# Evaluation Utilities\n",
    "# ===========================\n",
    "\n",
    "def evaluate_with_class_metrics(model: TabPFN, dataloader: DataLoader, \n",
    "                               device: torch.device) -> Dict[str, Any]:\n",
    "    \"\"\"Evaluate model with detailed class-wise metrics\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_logits = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc='Evaluating'):\n",
    "            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v \n",
    "                    for k, v in batch.items()}\n",
    "            \n",
    "            logits = model(batch)\n",
    "            query_y = batch['query_y']\n",
    "            query_mask = batch['query_mask']\n",
    "            \n",
    "            # Collect valid predictions\n",
    "            for b in range(logits.size(0)):\n",
    "                mask = query_mask[b]\n",
    "                if mask.any():\n",
    "                    batch_logits = logits[b][mask]\n",
    "                    batch_labels = query_y[b][mask]\n",
    "                    batch_preds = torch.argmax(batch_logits, dim=-1)\n",
    "                    \n",
    "                    all_logits.append(batch_logits.cpu())\n",
    "                    all_labels.append(batch_labels.cpu())\n",
    "                    all_predictions.append(batch_preds.cpu())\n",
    "    \n",
    "    # Concatenate all results\n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "    all_predictions = torch.cat(all_predictions, dim=0)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    overall_accuracy = (all_predictions == all_labels).float().mean().item()\n",
    "    \n",
    "    # Per-class metrics\n",
    "    unique_classes = torch.unique(all_labels)\n",
    "    class_metrics = {}\n",
    "    \n",
    "    for c in unique_classes:\n",
    "        mask = all_labels == c\n",
    "        if mask.sum() > 0:\n",
    "            true_positives = ((all_predictions == c) & mask).sum().item()\n",
    "            false_positives = ((all_predictions == c) & ~mask).sum().item()\n",
    "            false_negatives = ((all_predictions != c) & mask).sum().item()\n",
    "            \n",
    "            precision = true_positives / (true_positives + false_positives + 1e-8)\n",
    "            recall = true_positives / (true_positives + false_negatives + 1e-8)\n",
    "            f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "            \n",
    "            class_metrics[c.item()] = {\n",
    "                'count': mask.sum().item(),\n",
    "                'accuracy': (all_predictions[mask] == c).float().mean().item(),\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1': f1\n",
    "            }\n",
    "    \n",
    "    # Calculate balanced accuracy\n",
    "    class_accuracies = [m['accuracy'] for m in class_metrics.values()]\n",
    "    balanced_accuracy = np.mean(class_accuracies)\n",
    "    \n",
    "    # Calculate macro-averaged F1\n",
    "    macro_f1 = np.mean([m['f1'] for m in class_metrics.values()])\n",
    "    \n",
    "    return {\n",
    "        'overall_accuracy': overall_accuracy,\n",
    "        'balanced_accuracy': balanced_accuracy,\n",
    "        'macro_f1': macro_f1,\n",
    "        'class_metrics': class_metrics,\n",
    "        'predictions': all_predictions,\n",
    "        'labels': all_labels,\n",
    "        'logits': all_logits\n",
    "    }\n",
    "\n",
    "# ===========================\n",
    "# Visualization Tools\n",
    "# ===========================\n",
    "\n",
    "def visualize_augmentation_effects(prior_config: PriorConfig, num_examples: int = 5):\n",
    "    \"\"\"Visualize the effects of data augmentation\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    generator = AdvancedDataGenerator(prior_config)\n",
    "    \n",
    "    fig, axes = plt.subplots(num_examples, 4, figsize=(16, 4 * num_examples))\n",
    "    \n",
    "    for i in range(num_examples):\n",
    "        # Generate dataset\n",
    "        X, y, metadata = generator.generate_dataset(\n",
    "            num_samples=200,\n",
    "            num_features=10,\n",
    "            num_classes=3\n",
    "        )\n",
    "        \n",
    "        # Plot original features\n",
    "        axes[i, 0].scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', alpha=0.6)\n",
    "        axes[i, 0].set_title(f\"Original (Function: {metadata['function_info']['function_class']})\")\n",
    "        axes[i, 0].set_xlabel(\"Feature 1\")\n",
    "        axes[i, 0].set_ylabel(\"Feature 2\")\n",
    "        \n",
    "        # Plot class distribution\n",
    "        unique, counts = np.unique(y, return_counts=True)\n",
    "        axes[i, 1].bar(unique, counts)\n",
    "        axes[i, 1].set_title(\"Class Distribution\")\n",
    "        axes[i, 1].set_xlabel(\"Class\")\n",
    "        axes[i, 1].set_ylabel(\"Count\")\n",
    "        \n",
    "        # Plot feature distributions\n",
    "        axes[i, 2].boxplot([X[y == c, 0] for c in unique])\n",
    "        axes[i, 2].set_title(\"Feature 1 by Class\")\n",
    "        axes[i, 2].set_xlabel(\"Class\")\n",
    "        axes[i, 2].set_ylabel(\"Feature 1 Value\")\n",
    "        \n",
    "        # Plot correlation matrix\n",
    "        corr = np.corrcoef(X.T)\n",
    "        im = axes[i, 3].imshow(corr, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "        axes[i, 3].set_title(\"Feature Correlations\")\n",
    "        plt.colorbar(im, ax=axes[i, 3])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('augmentation_effects.png', dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "# ===========================\n",
    "# Example Usage\n",
    "# ===========================\n",
    "\n",
    "def example_advanced_training():\n",
    "    \"\"\"Example of training with advanced augmentation\"\"\"\n",
    "    print(\"=== TabPFN with Advanced Augmentation ===\\n\")\n",
    "    \n",
    "    # Train with all advanced features\n",
    "    model = train_tabpfn_with_augmentation(\n",
    "        scale='small',  # Start small for testing\n",
    "        num_epochs=20,\n",
    "        use_advanced_augmentation=True,\n",
    "        use_focal_loss=True\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def example_visualize_augmentation():\n",
    "    \"\"\"Example of visualizing augmentation effects\"\"\"\n",
    "    print(\"=== Visualizing Data Augmentation ===\\n\")\n",
    "    \n",
    "    prior_config = PriorConfig(\n",
    "        use_focal_loss=True,\n",
    "        imbalance_ratio_range=(0.1, 0.9),\n",
    "        apply_power_transform=0.5,\n",
    "        use_gp_functions=0.3\n",
    "    )\n",
    "    \n",
    "    visualize_augmentation_effects(prior_config, num_examples=3)\n",
    "\n",
    "def example_evaluate_with_metrics():\n",
    "    \"\"\"Example of detailed evaluation\"\"\"\n",
    "    print(\"=== Detailed Model Evaluation ===\\n\")\n",
    "    \n",
    "    # Assume we have a trained model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Create test dataset\n",
    "    config = create_optimized_config('small')\n",
    "    prior_config = PriorConfig()\n",
    "    \n",
    "    test_dataset = AugmentedTabPFNDataset(\n",
    "        config, prior_config, num_tasks=1000, split='val'\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_tabpfn_batch\n",
    "    )\n",
    "    \n",
    "    # Load model (assuming it exists)\n",
    "    model = TabPFN(config).to(device)\n",
    "    \n",
    "    # Evaluate\n",
    "    metrics = evaluate_with_class_metrics(model, test_loader, device)\n",
    "    \n",
    "    print(f\"Overall Accuracy: {metrics['overall_accuracy']:.4f}\")\n",
    "    print(f\"Balanced Accuracy: {metrics['balanced_accuracy']:.4f}\")\n",
    "    print(f\"Macro F1-Score: {metrics['macro_f1']:.4f}\")\n",
    "    \n",
    "    print(\"\\nPer-Class Metrics:\")\n",
    "    for c, m in sorted(metrics['class_metrics'].items()):\n",
    "        print(f\"  Class {c}: Acc={m['accuracy']:.3f}, \"\n",
    "              f\"Prec={m['precision']:.3f}, Rec={m['recall']:.3f}, \"\n",
    "              f\"F1={m['f1']:.3f} (n={m['count']})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run advanced training\n",
    "    model = example_advanced_training()\n",
    "    \n",
    "    # Visualize augmentation\n",
    "    example_visualize_augmentation()\n",
    "    \n",
    "    # Evaluate with detailed metrics\n",
    "    example_evaluate_with_metrics()\n",
    "    \n",
    "    print(\"\\nAdvanced training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238fd129",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
