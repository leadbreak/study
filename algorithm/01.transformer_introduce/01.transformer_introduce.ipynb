{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Scaled dot Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "# 임의의 입력 벡터(임베딩)를 가정\n",
    "input_vectors = np.array([[1, 0, 1], [0, 2, 0], [1, 1, 1]])\n",
    "\n",
    "# 가중치 행렬 (임의로 설정)\n",
    "W_q = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])  # Q에 대한 가중치\n",
    "W_k = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])  # K에 대한 가중치\n",
    "W_v = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])  # V에 대한 가중치\n",
    "\n",
    "# Q, K, V 계산\n",
    "Q = np.dot(input_vectors, W_q)  # 쿼리\n",
    "K = np.dot(input_vectors, W_k)  # 키\n",
    "V = np.dot(input_vectors, W_v)  # 밸류\n",
    "\n",
    "# 스케일드 닷-프로덕트 어텐션 계산\n",
    "dk = K.shape[1]  # 키 벡터의 차원\n",
    "attention_scores = np.matmul(Q, K.T) / np.sqrt(dk) # Scaled Dot-Product\n",
    "attention_weights = softmax(attention_scores, axis=1) # softmax -> attention weight\n",
    "output = np.matmul(attention_weights, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1, 0, 1],\n",
       "        [0, 2, 0],\n",
       "        [1, 1, 1]]),\n",
       " array([[1, 0, 1],\n",
       "        [0, 2, 0],\n",
       "        [1, 1, 1]]),\n",
       " array([[1, 0, 1],\n",
       "        [0, 2, 0],\n",
       "        [1, 1, 1]]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 결과 확인\n",
    "Q, K, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4319371 , 0.1361258 , 0.4319371 ],\n",
       "       [0.07021749, 0.70697728, 0.22280523],\n",
       "       [0.26445846, 0.26445846, 0.47108308]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8638742 , 0.7041887 , 0.8638742 ],\n",
       "       [0.29302272, 1.63675979, 0.29302272],\n",
       "       [0.73554154, 1.        , 0.73554154]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8638742 , 0.7041887 , 0.8638742 ],\n",
       "       [0.29302272, 1.63675979, 0.29302272],\n",
       "       [0.73554154, 1.        , 0.73554154]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights@V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4319371 , 0.1361258 , 0.4319371 ],\n",
       "       [0.07021749, 0.70697728, 0.22280523],\n",
       "       [0.26445846, 0.26445846, 0.47108308]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(attention_scores) / np.sum(np.exp(attention_scores), axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadAttentionWithMask(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttentionWithMask, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.depth = d_model // num_heads\n",
    "\n",
    "        self.wq = nn.Linear(d_model, d_model)\n",
    "        self.wk = nn.Linear(d_model, d_model)\n",
    "        self.wv = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    # 마스크 영역 생성을 위한 함수\n",
    "    def create_look_ahead_mask(self, size):\n",
    "        mask = torch.triu(torch.ones(size, size), diagonal=1)\n",
    "        return mask  \n",
    "    \n",
    "    # Multi-Head Attention을 위한 함수\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.depth)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "    \n",
    "    def forward(self, q, k, v):\n",
    "        batch_size = q.size(0)\n",
    "\n",
    "        q = self.split_heads(self.wq(q), batch_size)\n",
    "        k = self.split_heads(self.wk(k), batch_size)\n",
    "        v = self.split_heads(self.wv(v), batch_size)\n",
    "\n",
    "        matmul_qk = torch.matmul(q, k.transpose(-2, -1))\n",
    "\n",
    "        # 마스크 적용 전 스코어\n",
    "        dk = torch.tensor(self.depth, dtype=torch.float32)\n",
    "        attention_before_mask = matmul_qk / torch.sqrt(dk)\n",
    "\n",
    "        # 마스크 영역\n",
    "        mask_area = self.create_look_ahead_mask(q.size(2))\n",
    "        matmul_qk += (mask_area * -1e9)\n",
    "\n",
    "        # 마스크 적용 후 스코어\n",
    "        attention_after_mask = matmul_qk / torch.sqrt(dk)\n",
    "\n",
    "        attention_weights = nn.functional.softmax(attention_after_mask, dim=-1)\n",
    "\n",
    "        output = torch.matmul(attention_weights, v)\n",
    "\n",
    "        output = output.permute(0, 2, 1, 3).contiguous()\n",
    "        output = output.view(batch_size, -1, self.d_model)\n",
    "\n",
    "        return attention_before_mask, attention_after_mask, mask_area, output\n",
    "\n",
    "# 모델 초기화\n",
    "d_model = 64\n",
    "num_heads = 4\n",
    "\n",
    "# 더미 입력 데이터 생성 (q, k, v)\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "q = torch.rand(batch_size, seq_len, d_model)\n",
    "k = torch.rand(batch_size, seq_len, d_model)\n",
    "v = torch.rand(batch_size, seq_len, d_model)\n",
    "\n",
    "# 멀티헤드 어텐션 모델 초기화 (마스크 포함)\n",
    "multi_head_attn_with_mask = MultiHeadAttentionWithMask(d_model, num_heads)\n",
    "\n",
    "# 멀티헤드 어텐션에 마스크 적용하여 수행\n",
    "scores_before_mask, scores_after_mask, mask_area, output = multi_head_attn_with_mask(q, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 출력 길이 설정\n",
    "torch.set_printoptions(linewidth=140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5456e-02,  4.1445e-02,  3.5639e-04, -4.4191e-02, -3.3841e-03,  7.5077e-02,  6.1469e-02, -5.1129e-02,  2.8336e-02, -2.6438e-02],\n",
       "        [-7.7577e-02, -1.1581e-02, -6.0852e-02, -8.6383e-02, -6.6699e-02,  1.4562e-02, -2.7794e-03, -1.0405e-01, -2.9481e-02, -7.7760e-02],\n",
       "        [-1.8181e-02,  3.8231e-02, -5.2480e-04,  1.3417e-02, -1.0679e-05,  3.1950e-02,  4.4274e-02, -3.6149e-02,  4.6171e-02,  5.4952e-03],\n",
       "        [-1.9081e-01, -1.4831e-01, -1.5368e-01, -2.3596e-01, -1.9699e-01, -5.2835e-02, -7.0669e-02, -2.3505e-01, -1.1223e-01, -1.6724e-01],\n",
       "        [-8.6314e-03,  3.8139e-02, -2.5298e-02, -4.7316e-04, -8.1381e-02,  4.9788e-02,  4.4812e-02, -4.4572e-02,  9.3385e-03, -6.0012e-02],\n",
       "        [-1.4960e-01, -8.4267e-02, -1.3960e-01, -1.3799e-01, -1.1551e-01, -9.7467e-03, -3.0405e-02, -1.5251e-01, -6.4841e-02, -1.2215e-01],\n",
       "        [-1.7652e-01, -7.3223e-02, -1.5140e-01, -1.8178e-01, -1.2315e-01, -3.0111e-02, -1.1649e-01, -1.8518e-01, -1.1341e-01, -1.4087e-01],\n",
       "        [-2.1848e-02,  4.1641e-02, -4.1888e-02, -4.6538e-04, -4.6424e-02,  1.1684e-01,  3.5187e-02, -1.1062e-01,  6.3995e-02, -2.8281e-02],\n",
       "        [-5.9225e-02, -4.4329e-02, -5.9218e-02, -9.5778e-02, -1.0607e-01, -1.7878e-02, -3.0971e-02, -1.0823e-01, -4.1539e-02, -1.1044e-01],\n",
       "        [-9.3825e-02, -8.2601e-02, -8.0836e-02, -1.0074e-01, -1.6536e-01,  3.3626e-02,  2.8884e-02, -1.0452e-01, -2.8437e-02, -9.0287e-02]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_before_mask[0][0] # mask가 적용되지 않은 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5456e-02, -2.5000e+08, -2.5000e+08, -2.5000e+08, -2.5000e+08, -2.5000e+08, -2.5000e+08, -2.5000e+08, -2.5000e+08, -2.5000e+08],\n",
       "        [-7.7577e-02, -1.1581e-02, -2.5000e+08, -2.5000e+08, -2.5000e+08, -2.5000e+08, -2.5000e+08, -2.5000e+08, -2.5000e+08, -2.5000e+08],\n",
       "        [-1.8181e-02,  3.8231e-02, -5.2480e-04, -2.5000e+08, -2.5000e+08, -2.5000e+08, -2.5000e+08, -2.5000e+08, -2.5000e+08, -2.5000e+08],\n",
       "        [-1.9081e-01, -1.4831e-01, -1.5368e-01, -2.3596e-01, -2.5000e+08, -2.5000e+08, -2.5000e+08, -2.5000e+08, -2.5000e+08, -2.5000e+08],\n",
       "        [-8.6314e-03,  3.8139e-02, -2.5298e-02, -4.7316e-04, -8.1381e-02, -2.5000e+08, -2.5000e+08, -2.5000e+08, -2.5000e+08, -2.5000e+08],\n",
       "        [-1.4960e-01, -8.4267e-02, -1.3960e-01, -1.3799e-01, -1.1551e-01, -9.7467e-03, -2.5000e+08, -2.5000e+08, -2.5000e+08, -2.5000e+08],\n",
       "        [-1.7652e-01, -7.3223e-02, -1.5140e-01, -1.8178e-01, -1.2315e-01, -3.0111e-02, -1.1649e-01, -2.5000e+08, -2.5000e+08, -2.5000e+08],\n",
       "        [-2.1848e-02,  4.1641e-02, -4.1888e-02, -4.6538e-04, -4.6424e-02,  1.1684e-01,  3.5187e-02, -1.1062e-01, -2.5000e+08, -2.5000e+08],\n",
       "        [-5.9225e-02, -4.4329e-02, -5.9218e-02, -9.5778e-02, -1.0607e-01, -1.7878e-02, -3.0971e-02, -1.0823e-01, -4.1539e-02, -2.5000e+08],\n",
       "        [-9.3825e-02, -8.2601e-02, -8.0836e-02, -1.0074e-01, -1.6536e-01,  3.3626e-02,  2.8884e-02, -1.0452e-01, -2.8437e-02, -9.0287e-02]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_after_mask[0][0]  # mask가 적용된 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(linewidth=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [4],\n",
       "        [5],\n",
       "        [6],\n",
       "        [7],\n",
       "        [8],\n",
       "        [9]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.arange(10).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [2],\n",
       "       [3],\n",
       "       [4],\n",
       "       [5],\n",
       "       [6],\n",
       "       [7],\n",
       "       [8],\n",
       "       [9]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(10).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 7.4989e-01, 5.6234e-01, 4.2170e-01, 3.1623e-01, 2.3714e-01,\n",
       "        1.7783e-01, 1.3335e-01, 1.0000e-01, 7.4989e-02, 5.6234e-02, 4.2170e-02,\n",
       "        3.1623e-02, 2.3714e-02, 1.7783e-02, 1.3335e-02, 1.0000e-02, 7.4989e-03,\n",
       "        5.6234e-03, 4.2170e-03, 3.1623e-03, 2.3714e-03, 1.7783e-03, 1.3335e-03,\n",
       "        1.0000e-03, 7.4989e-04, 5.6234e-04, 4.2170e-04, 3.1623e-04, 2.3714e-04,\n",
       "        1.7783e-04, 1.3335e-04])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "max_len = 10\n",
    "d_model = 64\n",
    "torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0)/d_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "position = np.exp(np.arange(0, d_model, 2) * (-np.log(10000)/d_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "div_term = np.arange(max_len).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 8.41470985e-01,  6.81561350e-01,  5.33168440e-01,  4.09308924e-01,  3.10983593e-01,  2.34921076e-01,  1.76892186e-01,  1.32957266e-01,  9.98334166e-02,  7.49191579e-02,  5.62044992e-02,\n",
       "         4.21571532e-02,  3.16175064e-02,  2.37115146e-02,  1.77818569e-02,  1.33348191e-02,  9.99983333e-03,  7.49887181e-03,  5.62338361e-03,  4.21695254e-03,  3.16227239e-03,  2.37137148e-03,\n",
       "         1.77827847e-03,  1.33352104e-03,  9.99999833e-04,  7.49894139e-04,  5.62341296e-04,  4.21696491e-04,  3.16227761e-04,  2.37137368e-04,  1.77827940e-04,  1.33352143e-04],\n",
       "       [ 9.09297427e-01,  9.97479998e-01,  9.02130715e-01,  7.46903535e-01,  5.91127117e-01,  4.56693360e-01,  3.48205276e-01,  2.63553681e-01,  1.98669331e-01,  1.49417212e-01,  1.12231311e-01,\n",
       "         8.42393503e-02,  6.32033979e-02,  4.74096958e-02,  3.55580908e-02,  2.66672669e-02,  1.99986667e-02,  1.49973219e-02,  1.12465894e-02,  8.43383008e-03,  6.32451316e-03,  4.74272963e-03,\n",
       "         3.55655132e-03,  2.66703970e-03,  1.99999867e-03,  1.49978786e-03,  1.12468241e-03,  8.43392907e-04,  6.32455490e-04,  4.74274723e-04,  3.55655875e-04,  2.66704283e-04],\n",
       "       [ 1.41120008e-01,  7.78272522e-01,  9.93253167e-01,  9.53634462e-01,  8.12648897e-01,  6.52904012e-01,  5.08536134e-01,  3.89470317e-01,  2.95520207e-01,  2.23075425e-01,  1.67903310e-01,\n",
       "         1.26171769e-01,  9.47260913e-02,  7.10812179e-02,  5.33230805e-02,  3.99949726e-02,  2.99955002e-02,  2.24949287e-02,  1.68694395e-02,  1.26505577e-02,  9.48669068e-03,  7.11406111e-03,\n",
       "         5.33481292e-03,  4.00055363e-03,  2.99999550e-03,  2.24968073e-03,  1.68702318e-03,  1.26508917e-03,  9.48683156e-04,  7.11412052e-04,  5.33483798e-04,  4.00056419e-04],\n",
       "       [-7.56802495e-01,  1.41538923e-01,  7.78471741e-01,  9.93280735e-01,  9.53580740e-01,  8.12570908e-01,  6.52828002e-01,  5.08471340e-01,  3.89418342e-01,  2.95479780e-01,  2.23044492e-01,\n",
       "         1.67879851e-01,  1.26154067e-01,  9.47127699e-02,  7.10712085e-02,  5.33155662e-02,  3.99893342e-02,  2.99912705e-02,  2.24917562e-02,  1.68670603e-02,  1.26487733e-02,  9.48535258e-03,\n",
       "         7.11305766e-03,  5.33406043e-03,  3.99998933e-03,  2.99957234e-03,  2.24936340e-03,  1.68678521e-03,  1.26491073e-03,  9.48549340e-04,  7.11311704e-04,  5.33408548e-04],\n",
       "       [-9.58924275e-01, -5.71127201e-01,  3.23935204e-01,  8.58895997e-01,  9.99946517e-01,  9.26757313e-01,  7.76529980e-01,  6.18443713e-01,  4.79425539e-01,  3.66223309e-01,  2.77480531e-01,\n",
       "         2.09289441e-01,  1.57455898e-01,  1.18291064e-01,  8.87968624e-02,  6.66266790e-02,  4.99791693e-02,  3.74859257e-02,  2.81133617e-02,  2.10832629e-02,  1.58107295e-02,  1.18565907e-02,\n",
       "         8.89127990e-03,  6.66755776e-03,  4.99997917e-03,  3.74946226e-03,  2.81170292e-03,  2.10848095e-03,  1.58113817e-03,  1.18568658e-03,  8.89139588e-04,  6.66760667e-04],\n",
       "       [-2.79415498e-01, -9.77396119e-01, -2.30367475e-01,  5.74025569e-01,  9.47148158e-01,  9.89072090e-01,  8.75740567e-01,  7.17434716e-01,  5.64642473e-01,  4.34908378e-01,  3.31039330e-01,\n",
       "         2.50326911e-01,  1.88600287e-01,  1.41802840e-01,  1.06494437e-01,  7.99259438e-02,  5.99640065e-02,  4.49784730e-02,  3.37340781e-02,  2.52990907e-02,  1.89725276e-02,  1.42277622e-02,\n",
       "         1.06694740e-02,  8.00104322e-03,  5.99996400e-03,  4.49935007e-03,  3.37404155e-03,  2.53017632e-03,  1.89736546e-03,  1.42282374e-03,  1.06696744e-03,  8.00112774e-04],\n",
       "       [ 6.56986599e-01, -8.59313475e-01, -7.13721168e-01,  1.88581107e-01,  8.00421646e-01,  9.96027411e-01,  9.47330707e-01,  8.03686621e-01,  6.44217687e-01,  5.01148923e-01,  3.83551568e-01,\n",
       "         2.90919296e-01,  2.19556091e-01,  1.65234879e-01,  1.24158336e-01,  9.32109958e-02,  6.99428473e-02,  5.24684910e-02,  3.93537277e-02,  2.95144685e-02,  2.21341359e-02,  1.65988536e-02,\n",
       "         1.24476344e-02,  9.33451446e-03,  6.99994283e-03,  5.24923536e-03,  3.93637911e-03,  2.95187124e-03,  2.21359255e-03,  1.65996083e-03,  1.24479527e-03,  9.33464867e-04],\n",
       "       [ 9.89358247e-01, -2.80228014e-01, -9.77261746e-01, -2.29904325e-01,  5.74317769e-01,  9.47233979e-01,  9.89042481e-01,  8.75667898e-01,  7.17356091e-01,  5.64572621e-01,  4.34851228e-01,\n",
       "         3.30994421e-01,  2.50292358e-01,  1.88574004e-01,  1.41782974e-01,  1.06479472e-01,  7.99146940e-02,  5.99555585e-02,  4.49721329e-02,  3.37293215e-02,  2.52955229e-02,  1.89698517e-02,\n",
       "         1.42257554e-02,  1.06679691e-02,  7.99991467e-03,  5.99911769e-03,  4.49871543e-03,  3.37356563e-03,  2.52981943e-03,  1.89709783e-03,  1.42262305e-03,  1.06681694e-03],\n",
       "       [ 4.12118485e-01,  4.49193624e-01, -9.39823513e-01, -6.08108617e-01,  2.91259121e-01,  8.45422817e-01,  9.99560318e-01,  9.32100416e-01,  7.83326910e-01,  6.24822982e-01,  4.84776130e-01,\n",
       "         3.70481033e-01,  2.80778353e-01,  2.11807091e-01,  1.59362777e-01,  1.19729014e-01,  8.98785492e-02,  6.74392544e-02,  5.05891159e-02,  3.79435748e-02,  2.84566569e-02,  2.13407432e-02,\n",
       "         1.60038315e-02,  1.20014048e-02,  8.99987850e-03,  6.74899665e-03,  5.06105032e-03,  3.79525942e-03,  2.84604605e-03,  2.13423471e-03,  1.60045079e-03,  1.20016900e-03]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sin(position*div_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  1.        ,  0.        ,  1.        ,  0.        ,  1.        ,  0.        ,  1.        ],\n",
       "       [ 0.84147098,  0.54030231,  0.68156135,  0.73176098,  0.53316844,  0.84600911,  0.40930892,  0.91239586],\n",
       "       [ 0.90929743, -0.41614684,  0.99748   ,  0.07094825,  0.90213071,  0.43146283,  0.74690354,  0.66493241],\n",
       "       [ 0.14112001, -0.9899925 ,  0.77827252, -0.62792665,  0.99325317, -0.11596614,  0.95363446,  0.30096729],\n",
       "       [-0.7568025 , -0.65364362,  0.14153892, -0.98993269,  0.77847174, -0.62767965,  0.99328073, -0.11572978],\n",
       "       [-0.95892427,  0.28366219, -0.5711272 , -0.82086157,  0.3239352 , -0.94607927,  0.858896  , -0.51215004],\n",
       "       [-0.2794155 ,  0.96017029, -0.97739612, -0.21141624, -0.23036747, -0.97310371,  0.57402557, -0.81883737],\n",
       "       [ 0.6569866 ,  0.75390225, -0.85931347,  0.51144927, -0.71372117, -0.70042994,  0.18858111, -0.98205762],\n",
       "       [ 0.98935825, -0.14550003, -0.28022801,  0.95993347, -0.97726175, -0.2120365 , -0.22990433, -0.97321324],\n",
       "       [ 0.41211849, -0.91113026,  0.44919362,  0.89343443, -0.93982351,  0.34166031, -0.60810862, -0.79385383]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def positional_encoding(max_len, d_model):\n",
    "    \"\"\"\n",
    "    주어진 최대 길이(max_len)와 모델 차원(d_model)에 대한 위치 인코딩을 생성합니다.\n",
    "    \n",
    "    :param max_len: 시퀀스의 최대 길이.\n",
    "    :param d_model: 모델의 차원.\n",
    "    :return: (max_len, d_model) 형태의 numpy 배열로, 위치 인코딩을 포함합니다.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 사전에 시퀀스의 위치와 주기 특징(주파수)에 대한 변수 작성\n",
    "    position = np.arange(max_len).reshape(-1, 1) # 시퀀스의 각 위치\n",
    "    div_term = np.exp(np.arange(0, d_model, 2) * (-np.log(10000.0)/d_model)) # 주기\n",
    "    \n",
    "    pos_enc = np.zeros((max_len, d_model)) # max_len x d_model 사이즈의 영행렬 생성\n",
    "    \n",
    "    # 짝수 인덱스에는 사인 값을, 홀수 인덱스에는 코사인 값을 저장\n",
    "    pos_enc[:, 0::2] = np.sin(position*div_term)\n",
    "    pos_enc[:, 1::2] = np.cos(position*div_term)\n",
    "    \n",
    "    return pos_enc\n",
    "\n",
    "# 최대 길이 10과 모델 차원 64를 가진 시퀀스에 대한 위치 인코딩\n",
    "pos_encoding = positional_encoding(10, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 64),\n",
       " array([[ 0.        ,  1.        ,  0.        ,  1.        ,  0.        ,  1.        ,  0.        ,  1.        ],\n",
       "        [ 0.84147098,  0.54030231,  0.68156135,  0.73176098,  0.53316844,  0.84600911,  0.40930892,  0.91239586],\n",
       "        [ 0.90929743, -0.41614684,  0.99748   ,  0.07094825,  0.90213071,  0.43146283,  0.74690354,  0.66493241],\n",
       "        [ 0.14112001, -0.9899925 ,  0.77827252, -0.62792665,  0.99325317, -0.11596614,  0.95363446,  0.30096729],\n",
       "        [-0.7568025 , -0.65364362,  0.14153892, -0.98993269,  0.77847174, -0.62767965,  0.99328073, -0.11572978],\n",
       "        [-0.95892427,  0.28366219, -0.5711272 , -0.82086157,  0.3239352 , -0.94607927,  0.858896  , -0.51215004],\n",
       "        [-0.2794155 ,  0.96017029, -0.97739612, -0.21141624, -0.23036747, -0.97310371,  0.57402557, -0.81883737],\n",
       "        [ 0.6569866 ,  0.75390225, -0.85931347,  0.51144927, -0.71372117, -0.70042994,  0.18858111, -0.98205762],\n",
       "        [ 0.98935825, -0.14550003, -0.28022801,  0.95993347, -0.97726175, -0.2120365 , -0.22990433, -0.97321324],\n",
       "        [ 0.41211849, -0.91113026,  0.44919362,  0.89343443, -0.93982351,  0.34166031, -0.60810862, -0.79385383]]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# positional encoding 출력결과\n",
    "pos_encoding.shape, pos_encoding[:, :8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  1.00000000e+00],\n",
       "       [ 8.41470985e-01,  5.40302306e-01,  6.81561350e-01,  7.31760976e-01,  5.33168440e-01,  8.46009110e-01,  4.09308924e-01,  9.12395860e-01,  3.10983593e-01,  9.50415280e-01,  2.34921076e-01,\n",
       "         9.72014449e-01,  1.76892186e-01,  9.84230234e-01,  1.32957266e-01,  9.91121771e-01,  9.98334166e-02,  9.95004165e-01,  7.49191579e-02,  9.97189611e-01,  5.62044992e-02,  9.98419278e-01,\n",
       "         4.21571532e-02,  9.99110992e-01,  3.16175064e-02,  9.99500042e-01,  2.37115146e-02,  9.99718843e-01,  1.77818569e-02,  9.99841890e-01,  1.33348191e-02,  9.99911087e-01,  9.99983333e-03,\n",
       "         9.99950000e-01,  7.49887181e-03,  9.99971883e-01,  5.62338361e-03,  9.99984189e-01,  4.21695254e-03,  9.99991109e-01,  3.16227239e-03,  9.99995000e-01,  2.37137148e-03,  9.99997188e-01,\n",
       "         1.77827847e-03,  9.99998419e-01,  1.33352104e-03,  9.99999111e-01,  9.99999833e-04,  9.99999500e-01,  7.49894139e-04,  9.99999719e-01,  5.62341296e-04,  9.99999842e-01,  4.21696491e-04,\n",
       "         9.99999911e-01,  3.16227761e-04,  9.99999950e-01,  2.37137368e-04,  9.99999972e-01,  1.77827940e-04,  9.99999984e-01,  1.33352143e-04,  9.99999991e-01],\n",
       "       [ 9.09297427e-01, -4.16146837e-01,  9.97479998e-01,  7.09482514e-02,  9.02130715e-01,  4.31462829e-01,  7.46903535e-01,  6.64932409e-01,  5.91127117e-01,  8.06578410e-01,  4.56693360e-01,\n",
       "         8.89624176e-01,  3.48205276e-01,  9.37418309e-01,  2.63553681e-01,  9.64644731e-01,  1.98669331e-01,  9.80066578e-01,  1.49417212e-01,  9.88774240e-01,  1.12231311e-01,  9.93682109e-01,\n",
       "         8.42393503e-02,  9.96445549e-01,  6.32033979e-02,  9.98000667e-01,  4.74096958e-02,  9.98875528e-01,  3.55580908e-02,  9.99367611e-01,  2.66672669e-02,  9.99644365e-01,  1.99986667e-02,\n",
       "         9.99800007e-01,  1.49973219e-02,  9.99887534e-01,  1.12465894e-02,  9.99936755e-01,  8.43383008e-03,  9.99964435e-01,  6.32451316e-03,  9.99980000e-01,  4.74272963e-03,  9.99988753e-01,\n",
       "         3.55655132e-03,  9.99993675e-01,  2.66703970e-03,  9.99996443e-01,  1.99999867e-03,  9.99998000e-01,  1.49978786e-03,  9.99998875e-01,  1.12468241e-03,  9.99999368e-01,  8.43392907e-04,\n",
       "         9.99999644e-01,  6.32455490e-04,  9.99999800e-01,  4.74274723e-04,  9.99999888e-01,  3.55655875e-04,  9.99999937e-01,  2.66704283e-04,  9.99999964e-01],\n",
       "       [ 1.41120008e-01, -9.89992497e-01,  7.78272522e-01, -6.27926652e-01,  9.93253167e-01, -1.15966142e-01,  9.53634462e-01,  3.00967295e-01,  8.12648897e-01,  5.82753611e-01,  6.52904012e-01,\n",
       "         7.57440658e-01,  5.08536134e-01,  8.61040649e-01,  3.89470317e-01,  9.21039018e-01,  2.95520207e-01,  9.55336489e-01,  2.23075425e-01,  9.74801187e-01,  1.67903310e-01,  9.85803469e-01,\n",
       "         1.26171769e-01,  9.92008410e-01,  9.47260913e-02,  9.95503374e-01,  7.10812179e-02,  9.97470531e-01,  5.33230805e-02,  9.98577313e-01,  3.99949726e-02,  9.99199881e-01,  2.99955002e-02,\n",
       "         9.99550034e-01,  2.24949287e-02,  9.99746957e-01,  1.68694395e-02,  9.99857701e-01,  1.26505577e-02,  9.99919978e-01,  9.48669068e-03,  9.99955000e-01,  7.11406111e-03,  9.99974695e-01,\n",
       "         5.33481292e-03,  9.99985770e-01,  4.00055363e-03,  9.99991998e-01,  2.99999550e-03,  9.99995500e-01,  2.24968073e-03,  9.99997469e-01,  1.68702318e-03,  9.99998577e-01,  1.26508917e-03,\n",
       "         9.99999200e-01,  9.48683156e-04,  9.99999550e-01,  7.11412052e-04,  9.99999747e-01,  5.33483798e-04,  9.99999858e-01,  4.00056419e-04,  9.99999920e-01],\n",
       "       [-7.56802495e-01, -6.53643621e-01,  1.41538923e-01, -9.89932691e-01,  7.78471741e-01, -6.27679654e-01,  9.93280735e-01, -1.15729782e-01,  9.53580740e-01,  3.01137463e-01,  8.12570908e-01,\n",
       "         5.82862351e-01,  6.52828002e-01,  7.57506172e-01,  5.08471340e-01,  8.61078914e-01,  3.89418342e-01,  9.21060994e-01,  2.95479780e-01,  9.55348994e-01,  2.23044492e-01,  9.74808266e-01,\n",
       "         1.67879851e-01,  9.85807464e-01,  1.26154067e-01,  9.92010661e-01,  9.47127699e-02,  9.95504641e-01,  7.10712085e-02,  9.97471244e-01,  5.33155662e-02,  9.98577714e-01,  3.99893342e-02,\n",
       "         9.99200107e-01,  2.99912705e-02,  9.99550161e-01,  2.24917562e-02,  9.99747028e-01,  1.68670603e-02,  9.99857741e-01,  1.26487733e-02,  9.99920001e-01,  9.48535258e-03,  9.99955013e-01,\n",
       "         7.11305766e-03,  9.99974702e-01,  5.33406043e-03,  9.99985774e-01,  3.99998933e-03,  9.99992000e-01,  2.99957234e-03,  9.99995501e-01,  2.24936340e-03,  9.99997470e-01,  1.68678521e-03,\n",
       "         9.99998577e-01,  1.26491073e-03,  9.99999200e-01,  9.48549340e-04,  9.99999550e-01,  7.11311704e-04,  9.99999747e-01,  5.33408548e-04,  9.99999858e-01],\n",
       "       [-9.58924275e-01,  2.83662185e-01, -5.71127201e-01, -8.20861572e-01,  3.23935204e-01, -9.46079269e-01,  8.58895997e-01, -5.12150043e-01,  9.99946517e-01, -1.03423189e-02,  9.26757313e-01,\n",
       "         3.75660595e-01,  7.76529980e-01,  6.30080304e-01,  6.18443713e-01,  7.85829100e-01,  4.79425539e-01,  8.77582562e-01,  3.66223309e-01,  9.30526995e-01,  2.77480531e-01,  9.60731261e-01,\n",
       "         2.09289441e-01,  9.77853736e-01,  1.57455898e-01,  9.87526020e-01,  1.18291064e-01,  9.92978965e-01,  8.87968624e-02,  9.96049756e-01,  6.66266790e-02,  9.97777974e-01,  4.99791693e-02,\n",
       "         9.98750260e-01,  3.74859257e-02,  9.99297156e-01,  2.81133617e-02,  9.99604741e-01,  2.10832629e-02,  9.99777723e-01,  1.58107295e-02,  9.99875003e-01,  1.18565907e-02,  9.99929708e-01,\n",
       "         8.89127990e-03,  9.99960472e-01,  6.66755776e-03,  9.99977772e-01,  4.99997917e-03,  9.99987500e-01,  3.74946226e-03,  9.99992971e-01,  2.81170292e-03,  9.99996047e-01,  2.10848095e-03,\n",
       "         9.99997777e-01,  1.58113817e-03,  9.99998750e-01,  1.18568658e-03,  9.99999297e-01,  8.89139588e-04,  9.99999605e-01,  6.66760667e-04,  9.99999778e-01],\n",
       "       [-2.79415498e-01,  9.60170287e-01, -9.77396119e-01, -2.11416238e-01, -2.30367475e-01, -9.73103708e-01,  5.74025569e-01, -8.18837375e-01,  9.47148158e-01, -3.20796458e-01,  9.89072090e-01,\n",
       "         1.47432701e-01,  8.75740567e-01,  4.82782000e-01,  7.17434716e-01,  6.96625745e-01,  5.64642473e-01,  8.25335615e-01,  4.34908378e-01,  9.00474710e-01,  3.31039330e-01,  9.43616957e-01,\n",
       "         2.50326911e-01,  9.68161370e-01,  1.88600287e-01,  9.82053935e-01,  1.41802840e-01,  9.89894921e-01,  1.06494437e-01,  9.94313298e-01,  7.99259438e-02,  9.96800804e-01,  5.99640065e-02,\n",
       "         9.98200540e-01,  4.49784730e-02,  9.98987956e-01,  3.37340781e-02,  9.99430844e-01,  2.52990907e-02,  9.99679927e-01,  1.89725276e-02,  9.99820005e-01,  1.42277622e-02,  9.99898780e-01,\n",
       "         1.06694740e-02,  9.99943080e-01,  8.00104322e-03,  9.99967991e-01,  5.99996400e-03,  9.99982000e-01,  4.49935007e-03,  9.99989878e-01,  3.37404155e-03,  9.99994308e-01,  2.53017632e-03,\n",
       "         9.99996799e-01,  1.89736546e-03,  9.99998200e-01,  1.42282374e-03,  9.99998988e-01,  1.06696744e-03,  9.99999431e-01,  8.00112774e-04,  9.99999680e-01],\n",
       "       [ 6.56986599e-01,  7.53902254e-01, -8.59313475e-01,  5.11449266e-01, -7.13721168e-01, -7.00429935e-01,  1.88581107e-01, -9.82057618e-01,  8.00421646e-01, -5.99437393e-01,  9.96027411e-01,\n",
       "        -8.90471635e-02,  9.47330707e-01,  3.20256978e-01,  8.03686621e-01,  5.95052784e-01,  6.44217687e-01,  7.64842187e-01,  5.01148923e-01,  8.65361056e-01,  3.83551568e-01,  9.23519461e-01,\n",
       "         2.90919296e-01,  9.56747597e-01,  2.19556091e-01,  9.75599878e-01,  1.65234879e-01,  9.86254244e-01,  1.24158336e-01,  9.92262419e-01,  9.32109958e-02,  9.95646378e-01,  6.99428473e-02,\n",
       "         9.97551000e-01,  5.24684910e-02,  9.98622580e-01,  3.93537277e-02,  9.99225342e-01,  2.95144685e-02,  9.99564353e-01,  2.21341359e-02,  9.99755010e-01,  1.65988536e-02,  9.99862230e-01,\n",
       "         1.24476344e-02,  9.99922525e-01,  9.33451446e-03,  9.99956432e-01,  6.99994283e-03,  9.99975500e-01,  5.24923536e-03,  9.99986223e-01,  3.93637911e-03,  9.99992252e-01,  2.95187124e-03,\n",
       "         9.99995643e-01,  2.21359255e-03,  9.99997550e-01,  1.65996083e-03,  9.99998622e-01,  1.24479527e-03,  9.99999225e-01,  9.33464867e-04,  9.99999564e-01],\n",
       "       [ 9.89358247e-01, -1.45500034e-01, -2.80228014e-01,  9.59933466e-01, -9.77261746e-01, -2.12036505e-01, -2.29904325e-01, -9.73213235e-01,  5.74317769e-01, -8.18632457e-01,  9.47233979e-01,\n",
       "        -3.20542960e-01,  9.89042481e-01,  1.47631200e-01,  8.75667898e-01,  4.82913794e-01,  7.17356091e-01,  6.96706709e-01,  5.64572621e-01,  8.25383399e-01,  4.34851228e-01,  9.00502310e-01,\n",
       "         3.30994421e-01,  9.43632711e-01,  2.50292358e-01,  9.68170303e-01,  1.88574004e-01,  9.82058982e-01,  1.41782974e-01,  9.89897767e-01,  1.06479472e-01,  9.94314901e-01,  7.99146940e-02,\n",
       "         9.96801706e-01,  5.99555585e-02,  9.98201047e-01,  4.49721329e-02,  9.98988242e-01,  3.37293215e-02,  9.99431005e-01,  2.52955229e-02,  9.99680017e-01,  1.89698517e-02,  9.99820056e-01,\n",
       "         1.42257554e-02,  9.99898809e-01,  1.06679691e-02,  9.99943096e-01,  7.99991467e-03,  9.99968000e-01,  5.99911769e-03,  9.99982005e-01,  4.49871543e-03,  9.99989881e-01,  3.37356563e-03,\n",
       "         9.99994310e-01,  2.52981943e-03,  9.99996800e-01,  1.89709783e-03,  9.99998201e-01,  1.42262305e-03,  9.99998988e-01,  1.06681694e-03,  9.99999431e-01],\n",
       "       [ 4.12118485e-01, -9.11130262e-01,  4.49193624e-01,  8.93434434e-01, -9.39823513e-01,  3.41660306e-01, -6.08108617e-01, -7.93853834e-01,  2.91259121e-01, -9.56644200e-01,  8.45422817e-01,\n",
       "        -5.34097614e-01,  9.99560318e-01, -2.96507958e-02,  9.32100416e-01,  3.62199965e-01,  7.83326910e-01,  6.21609968e-01,  6.24822982e-01,  7.80766445e-01,  4.84776130e-01,  8.74638270e-01,\n",
       "         3.70481033e-01,  9.28840031e-01,  2.80778353e-01,  9.59772638e-01,  2.11807091e-01,  9.77311494e-01,  1.59362777e-01,  9.87220090e-01,  1.19729014e-01,  9.92806609e-01,  8.98785492e-02,\n",
       "         9.95952733e-01,  6.74392544e-02,  9.97723382e-01,  5.05891159e-02,  9.98719551e-01,  3.79435748e-02,  9.99279883e-01,  2.84566569e-02,  9.99595027e-01,  2.13407432e-02,  9.99772260e-01,\n",
       "         1.60038315e-02,  9.99871930e-01,  1.20014048e-02,  9.99927981e-01,  8.99987850e-03,  9.99959500e-01,  6.74899665e-03,  9.99977225e-01,  5.06105032e-03,  9.99987193e-01,  3.79525942e-03,\n",
       "         9.99992798e-01,  2.84604605e-03,  9.99995950e-01,  2.13423471e-03,  9.99997723e-01,  1.60045079e-03,  9.99998719e-01,  1.20016900e-03,  9.99999280e-01]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Cross Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def cross_attention(decoder_output, encoder_output, mask=None):\n",
    "    \"\"\"\n",
    "    Cross Attention을 수행하는 함수.\n",
    "\n",
    "    :param decoder_output: Decoder에서 나온 Query 행렬 (batch_size, target_seq_len, d_model)\n",
    "    :param encoder_output: Encoder에서 나온 Key, Value 행렬 (batch_size, source_seq_len, d_model)\n",
    "    :param mask: 선택적 Mask 행렬 (batch_size, 1, target_seq_len, source_seq_len)\n",
    "    :return: Attention을 적용한 결과와 attention weights\n",
    "    \"\"\"\n",
    "    d_model = decoder_output.size(-1)\n",
    "\n",
    "    # Decoder 출력을 Query로, Encoder 출력을 Key와 Value로 사용\n",
    "    query = decoder_output\n",
    "    key = value = encoder_output\n",
    "\n",
    "    # Scaled Dot-Product Attention\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_model, dtype=torch.float32))\n",
    "\n",
    "    # Mask가 제공된 경우 적용\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "    # Softmax를 적용하여 확률 분포 얻기\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "    # Attention weights를 Value에 적용\n",
    "    output = torch.matmul(attention_weights, value)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
