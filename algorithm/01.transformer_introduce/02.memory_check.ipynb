{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ". Estimate the memory usage of L-layers Transformer **Encoder** with the dot  \n",
    "product attention scoring, H-head self-attentions, position-wise networks  \n",
    "with D-dim output, and maximal sequence length T.  \n",
    ". Estimate the memory usage of L-layers Transformer **Decoder** with the  \n",
    "dot product attention scoring, H-head self-attentions, position-wise  \n",
    "networks with D-dim output, and maximal sequence length T .  \n",
    ". To increase the expressive power, which parameter should be modified?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Transformer Encoder]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-Head Self-Attention:\n",
    "\n",
    "Query/Key/Value를 위한 프로젝션: 각각 D×D 파라미터를 가지는 세 개의 가중치 행렬 (W_Q, W_K, W_V)이 있으며, 헤드 수 H에 따라 병렬적 수행하나 파라미터 수는 여전히 O(D²) 규모.\n",
    "결과를 다시 D 차원으로 매핑하는 O(D²) 규모의 출력 프로젝션.\n",
    "=> 한 층의 MHA 파라미터 수는 대략 4D²에 해당하지만, 여기서는 파라미터보다 메모리 사용량 추정이 핵심.\n",
    "메모리 관점에서 T 길이의 입력에 대해 Self-Attention은 O(T²) 크기의 어텐션 가중치(각 헤드마다 T×T 크기, H개 헤드 합쳐도 O(H×T²)) 및 각 스텝별로 D 차원 상태를 저장(T×D)한다.\n",
    "따라서 한 Encoder Layer에서의 메모리 사용량은 대략 O(H×T² + T×D).\n",
    "\n",
    "Position-wise Feed-Forward Networks(FFN):\n",
    "일반적으로 2개의 선형 변환(W_1: D→αD, W_2: αD→D; α는 확장 비율, 보통 4 정도)로 이루어져 있으며, 이는 추가로 O(T×D) 정도의 중간 표현을 저장한다.\n",
    "\n",
    "통합하면, 한 층당 메모리 사용량은 대략 Attention을 통한 O(H×T²) 부분과 FFN을 통한 O(T×D) 부분이 합쳐진다. 파라미터 고정 시, 시퀀스 길이 T가 커지면 T² 항목이 주도적이므로, Encoder L개 층에 대해 총 메모리 사용량은 대략:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O(L×(H×T²+T×D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Memory Estimate: 1179648\n",
      "Decoder Memory Estimate: 1966080\n",
      "Encoder Memory with Increased D: 1572864\n",
      "Decoder Memory with Increased D: 2359296\n"
     ]
    }
   ],
   "source": [
    "def transformer_encoder_memory(L, H, D, T):\n",
    "    # Encoder memory: O(L * (H*T^2 + T*D))\n",
    "    return L * (H * (T**2) + T * D)\n",
    "\n",
    "def transformer_decoder_memory(L, H, D, T):\n",
    "    # Decoder memory: O(L * (2*H*T^2 + T*D))\n",
    "    return L * ((2 * H * (T**2)) + T * D)\n",
    "\n",
    "# 예시 파라미터\n",
    "L = 6   # 레이어 수\n",
    "H = 8   # 헤드 수\n",
    "D = 512 # 차원\n",
    "T = 128 # 시퀀스 길이\n",
    "\n",
    "enc_mem = transformer_encoder_memory(L, H, D, T)\n",
    "dec_mem = transformer_decoder_memory(L, H, D, T)\n",
    "\n",
    "print(\"Encoder Memory Estimate:\", enc_mem)\n",
    "print(\"Decoder Memory Estimate:\", dec_mem)\n",
    "\n",
    "# 표현력 증가: D를 늘려봄\n",
    "D_new = 1024\n",
    "enc_mem_expanded = transformer_encoder_memory(L, H, D_new, T)\n",
    "dec_mem_expanded = transformer_decoder_memory(L, H, D_new, T)\n",
    "\n",
    "print(\"Encoder Memory with Increased D:\", enc_mem_expanded)\n",
    "print(\"Decoder Memory with Increased D:\", dec_mem_expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Encoder-only 모델 메모리 사용량 ===\n",
      "Total Parameters: 18874368.0000\n",
      "Parameters Memory (GB): 0.0703\n",
      "Activations Memory (GB): 0.9375\n",
      "Total Memory (GB): 1.0078\n",
      "\n",
      "=== Decoder-only 모델 메모리 사용량 ===\n",
      "Total Parameters: 18874368.0000\n",
      "Parameters Memory (GB): 0.0703\n",
      "Activations Memory (GB): 0.9375\n",
      "Total Memory (GB): 1.0078\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Transformer 구성 요소 정의\n",
    "class SimpleMultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, D, H):\n",
    "        super().__init__()\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.d_k = D // H\n",
    "        self.W_q = nn.Linear(D, D)\n",
    "        self.W_k = nn.Linear(D, D)\n",
    "        self.W_v = nn.Linear(D, D)\n",
    "        self.W_o = nn.Linear(D, D)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, D = x.size()\n",
    "        Q = self.W_q(x).view(B, T, self.H, self.d_k)\n",
    "        K = self.W_k(x).view(B, T, self.H, self.d_k)\n",
    "        V = self.W_v(x).view(B, T, self.H, self.d_k)\n",
    "\n",
    "        attn_weights = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
    "        attn = torch.matmul(torch.softmax(attn_weights, dim=-1), V)\n",
    "        attn = attn.view(B, T, D)\n",
    "        out = self.W_o(attn)\n",
    "        return out\n",
    "\n",
    "class PositionWiseFFN(nn.Module):\n",
    "    def __init__(self, D, expansion=4):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(D, D*expansion)\n",
    "        self.fc2 = nn.Linear(D*expansion, D)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc2(torch.relu(self.fc1(x)))\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, D, H, k=4):\n",
    "        super().__init__()\n",
    "        self.self_attn = SimpleMultiHeadSelfAttention(D, H)\n",
    "        self.ffn = PositionWiseFFN(D, expansion=k)\n",
    "        self.norm1 = nn.LayerNorm(D)\n",
    "        self.norm2 = nn.LayerNorm(D)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        attn_out = self.self_attn(x)\n",
    "        x = x + attn_out\n",
    "        x = self.norm1(x)\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = x + ffn_out\n",
    "        x = self.norm2(x)\n",
    "        return x\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, D, H, k=4):\n",
    "        super().__init__()\n",
    "        self.self_attn = SimpleMultiHeadSelfAttention(D, H)\n",
    "        self.ffn = PositionWiseFFN(D, expansion=k)\n",
    "        self.norm1 = nn.LayerNorm(D)\n",
    "        self.norm2 = nn.LayerNorm(D)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        attn_out = self.self_attn(x)\n",
    "        x = x + attn_out\n",
    "        x = self.norm1(x)\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = x + ffn_out\n",
    "        x = self.norm2(x)\n",
    "        return x\n",
    "\n",
    "# 메모리 계산 함수\n",
    "def calculate_memory(L, H, D, T, k=4):\n",
    "    # 파라미터 수 계산\n",
    "    params_per_layer = 4 * D**2 + 2 * k * D**2  # 4D^2 for attention, 2kD^2 for FFN\n",
    "    total_params = L * params_per_layer\n",
    "\n",
    "    # 활성화 메모리 계산 (float32 기준 4바이트)\n",
    "    activations_per_layer = (T**2 * D + T * D**2)  # T^2 D for attention scores, T D^2 for FFN\n",
    "    total_activations = L * activations_per_layer\n",
    "    total_activation_bytes = total_activations * 4  # float32\n",
    "\n",
    "    # 파라미터 메모리 (float32 기준 4바이트)\n",
    "    total_params_bytes = total_params * 4  # float32\n",
    "\n",
    "    # 총 메모리 (바이트 단위)\n",
    "    total_memory_bytes = total_params_bytes + total_activation_bytes\n",
    "\n",
    "    # 메모리를 GB 단위로 변환\n",
    "    total_memory_gb = total_memory_bytes / (1024 ** 3)\n",
    "\n",
    "    return {\n",
    "        'Total Parameters': total_params,\n",
    "        'Parameters Memory (GB)': total_params_bytes / (1024 ** 3),\n",
    "        'Activations Memory (GB)': total_activation_bytes / (1024 ** 3),\n",
    "        'Total Memory (GB)': total_memory_gb\n",
    "    }\n",
    "\n",
    "# 실험용 함수\n",
    "def test_model_memory(L, H, D, T, k=4):\n",
    "    encoder_memory = calculate_memory(L, H, D, T, k)\n",
    "    decoder_memory = calculate_memory(L, H, D, T, k)\n",
    "\n",
    "    print(\"=== Encoder-only 모델 메모리 사용량 ===\")\n",
    "    for key, value in encoder_memory.items():\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "    \n",
    "    print(\"\\n=== Decoder-only 모델 메모리 사용량 ===\")\n",
    "    for key, value in decoder_memory.items():\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "# 파라미터 설정\n",
    "L = 6       # 레이어 수\n",
    "H = 8       # 헤드 수\n",
    "D = 512     # 모델 차원\n",
    "T = 128     # 시퀀스 길이\n",
    "k = 4       # FFN 확장 비율\n",
    "\n",
    "test_model_memory(L, H, D, T, k)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
