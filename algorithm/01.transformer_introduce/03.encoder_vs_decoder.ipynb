{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class EncoderOnlyTransformer(nn.Module):\n",
    "    def __init__(self, T, D, H, L):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([EncoderLayer(T, D, H) for _ in range(L)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, T, D, H):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadSelfAttention(D, H)\n",
    "        self.positionwise_ff = nn.Sequential(\n",
    "            nn.Linear(D, D * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(D * 4, D),\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(D)\n",
    "        self.norm2 = nn.LayerNorm(D)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        attn_out = self.self_attention(x)\n",
    "        x = self.norm1(x + attn_out)\n",
    "        ff_out = self.positionwise_ff(x)\n",
    "        return self.norm2(x + ff_out)\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, D, H):\n",
    "        super().__init__()\n",
    "        self.H = H\n",
    "        self.D = D\n",
    "        self.query = nn.Linear(D, D)\n",
    "        self.key = nn.Linear(D, D)\n",
    "        self.value = nn.Linear(D, D)\n",
    "        self.proj = nn.Linear(D, D)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, D = x.size()\n",
    "        Q = self.query(x).view(B, T, self.H, D // self.H).transpose(1, 2)\n",
    "        K = self.key(x).view(B, T, self.H, D // self.H).transpose(1, 2)\n",
    "        V = self.value(x).view(B, T, self.H, D // self.H).transpose(1, 2)\n",
    "        scores = Q @ K.transpose(-2, -1) / (D // self.H) ** 0.5\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        context = (attn @ V).transpose(1, 2).contiguous().view(B, T, D)\n",
    "        return self.proj(context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderOnlyTransformer(nn.Module):\n",
    "    def __init__(self, T, D, H, L):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([DecoderLayer(T, D, H) for _ in range(L)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, T, D, H):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadSelfAttention(D, H)\n",
    "        self.positionwise_ff = nn.Sequential(\n",
    "            nn.Linear(D, D * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(D * 4, D),\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(D)\n",
    "        self.norm2 = nn.LayerNorm(D)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        attn_out = self.self_attention(x, mask=True)\n",
    "        x = self.norm1(x + attn_out)\n",
    "        ff_out = self.positionwise_ff(x)\n",
    "        return self.norm2(x + ff_out)\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, D, H):\n",
    "        super().__init__()\n",
    "        self.H = H\n",
    "        self.D = D\n",
    "        self.query = nn.Linear(D, D)\n",
    "        self.key = nn.Linear(D, D)\n",
    "        self.value = nn.Linear(D, D)\n",
    "        self.proj = nn.Linear(D, D)\n",
    "    \n",
    "    def forward(self, x, mask=False):\n",
    "        B, T, D = x.size()\n",
    "        Q = self.query(x).view(B, T, self.H, D // self.H).transpose(1, 2)\n",
    "        K = self.key(x).view(B, T, self.H, D // self.H).transpose(1, 2)\n",
    "        V = self.value(x).view(B, T, self.H, D // self.H).transpose(1, 2)\n",
    "        scores = Q @ K.transpose(-2, -1) / (D // self.H) ** 0.5\n",
    "        if mask:\n",
    "            mask = torch.triu(torch.ones(T, T), diagonal=1).bool().to(x.device)\n",
    "            scores = scores.masked_fill(mask, float('-inf'))\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        context = (attn @ V).transpose(1, 2).contiguous().view(B, T, D)\n",
    "        return self.proj(context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "def profile_model(model, input_shape):\n",
    "    print(summary(model, input_size=input_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 설정\n",
    "T, D, H, L = 128, 512, 8, 6  # 시퀀스 길이, 임베딩 차원, 헤드 수, 레이어 수\n",
    "\n",
    "# 입력 텐서의 배치 크기 설정\n",
    "batch_size = 16\n",
    "\n",
    "# Encoder-only Transformer\n",
    "encoder_model = EncoderOnlyTransformer(T, D, H, L)\n",
    "\n",
    "# Decoder-only Transformer\n",
    "decoder_model = DecoderOnlyTransformer(T, D, H, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder-only Transformer Profiling:\n",
      "===============================================================================================\n",
      "Layer (type:depth-idx)                        Output Shape              Param #\n",
      "===============================================================================================\n",
      "EncoderOnlyTransformer                        [16, 128, 512]            --\n",
      "├─ModuleList: 1-1                             --                        --\n",
      "│    └─EncoderLayer: 2-1                      [16, 128, 512]            --\n",
      "│    │    └─MultiHeadSelfAttention: 3-1       [16, 128, 512]            1,050,624\n",
      "│    │    └─LayerNorm: 3-2                    [16, 128, 512]            1,024\n",
      "│    │    └─Sequential: 3-3                   [16, 128, 512]            2,099,712\n",
      "│    │    └─LayerNorm: 3-4                    [16, 128, 512]            1,024\n",
      "│    └─EncoderLayer: 2-2                      [16, 128, 512]            --\n",
      "│    │    └─MultiHeadSelfAttention: 3-5       [16, 128, 512]            1,050,624\n",
      "│    │    └─LayerNorm: 3-6                    [16, 128, 512]            1,024\n",
      "│    │    └─Sequential: 3-7                   [16, 128, 512]            2,099,712\n",
      "│    │    └─LayerNorm: 3-8                    [16, 128, 512]            1,024\n",
      "│    └─EncoderLayer: 2-3                      [16, 128, 512]            --\n",
      "│    │    └─MultiHeadSelfAttention: 3-9       [16, 128, 512]            1,050,624\n",
      "│    │    └─LayerNorm: 3-10                   [16, 128, 512]            1,024\n",
      "│    │    └─Sequential: 3-11                  [16, 128, 512]            2,099,712\n",
      "│    │    └─LayerNorm: 3-12                   [16, 128, 512]            1,024\n",
      "│    └─EncoderLayer: 2-4                      [16, 128, 512]            --\n",
      "│    │    └─MultiHeadSelfAttention: 3-13      [16, 128, 512]            1,050,624\n",
      "│    │    └─LayerNorm: 3-14                   [16, 128, 512]            1,024\n",
      "│    │    └─Sequential: 3-15                  [16, 128, 512]            2,099,712\n",
      "│    │    └─LayerNorm: 3-16                   [16, 128, 512]            1,024\n",
      "│    └─EncoderLayer: 2-5                      [16, 128, 512]            --\n",
      "│    │    └─MultiHeadSelfAttention: 3-17      [16, 128, 512]            1,050,624\n",
      "│    │    └─LayerNorm: 3-18                   [16, 128, 512]            1,024\n",
      "│    │    └─Sequential: 3-19                  [16, 128, 512]            2,099,712\n",
      "│    │    └─LayerNorm: 3-20                   [16, 128, 512]            1,024\n",
      "│    └─EncoderLayer: 2-6                      [16, 128, 512]            --\n",
      "│    │    └─MultiHeadSelfAttention: 3-21      [16, 128, 512]            1,050,624\n",
      "│    │    └─LayerNorm: 3-22                   [16, 128, 512]            1,024\n",
      "│    │    └─Sequential: 3-23                  [16, 128, 512]            2,099,712\n",
      "│    │    └─LayerNorm: 3-24                   [16, 128, 512]            1,024\n",
      "===============================================================================================\n",
      "Total params: 18,914,304\n",
      "Trainable params: 18,914,304\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 302.63\n",
      "===============================================================================================\n",
      "Input size (MB): 4.19\n",
      "Forward/backward pass size (MB): 553.65\n",
      "Params size (MB): 75.66\n",
      "Estimated Total Size (MB): 633.50\n",
      "===============================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"Encoder-only Transformer Profiling:\")\n",
    "profile_model(encoder_model, (batch_size, T, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder-only Transformer Profiling:\n",
      "===============================================================================================\n",
      "Layer (type:depth-idx)                        Output Shape              Param #\n",
      "===============================================================================================\n",
      "DecoderOnlyTransformer                        [16, 128, 512]            --\n",
      "├─ModuleList: 1-1                             --                        --\n",
      "│    └─DecoderLayer: 2-1                      [16, 128, 512]            --\n",
      "│    │    └─MultiHeadSelfAttention: 3-1       [16, 128, 512]            1,050,624\n",
      "│    │    └─LayerNorm: 3-2                    [16, 128, 512]            1,024\n",
      "│    │    └─Sequential: 3-3                   [16, 128, 512]            2,099,712\n",
      "│    │    └─LayerNorm: 3-4                    [16, 128, 512]            1,024\n",
      "│    └─DecoderLayer: 2-2                      [16, 128, 512]            --\n",
      "│    │    └─MultiHeadSelfAttention: 3-5       [16, 128, 512]            1,050,624\n",
      "│    │    └─LayerNorm: 3-6                    [16, 128, 512]            1,024\n",
      "│    │    └─Sequential: 3-7                   [16, 128, 512]            2,099,712\n",
      "│    │    └─LayerNorm: 3-8                    [16, 128, 512]            1,024\n",
      "│    └─DecoderLayer: 2-3                      [16, 128, 512]            --\n",
      "│    │    └─MultiHeadSelfAttention: 3-9       [16, 128, 512]            1,050,624\n",
      "│    │    └─LayerNorm: 3-10                   [16, 128, 512]            1,024\n",
      "│    │    └─Sequential: 3-11                  [16, 128, 512]            2,099,712\n",
      "│    │    └─LayerNorm: 3-12                   [16, 128, 512]            1,024\n",
      "│    └─DecoderLayer: 2-4                      [16, 128, 512]            --\n",
      "│    │    └─MultiHeadSelfAttention: 3-13      [16, 128, 512]            1,050,624\n",
      "│    │    └─LayerNorm: 3-14                   [16, 128, 512]            1,024\n",
      "│    │    └─Sequential: 3-15                  [16, 128, 512]            2,099,712\n",
      "│    │    └─LayerNorm: 3-16                   [16, 128, 512]            1,024\n",
      "│    └─DecoderLayer: 2-5                      [16, 128, 512]            --\n",
      "│    │    └─MultiHeadSelfAttention: 3-17      [16, 128, 512]            1,050,624\n",
      "│    │    └─LayerNorm: 3-18                   [16, 128, 512]            1,024\n",
      "│    │    └─Sequential: 3-19                  [16, 128, 512]            2,099,712\n",
      "│    │    └─LayerNorm: 3-20                   [16, 128, 512]            1,024\n",
      "│    └─DecoderLayer: 2-6                      [16, 128, 512]            --\n",
      "│    │    └─MultiHeadSelfAttention: 3-21      [16, 128, 512]            1,050,624\n",
      "│    │    └─LayerNorm: 3-22                   [16, 128, 512]            1,024\n",
      "│    │    └─Sequential: 3-23                  [16, 128, 512]            2,099,712\n",
      "│    │    └─LayerNorm: 3-24                   [16, 128, 512]            1,024\n",
      "===============================================================================================\n",
      "Total params: 18,914,304\n",
      "Trainable params: 18,914,304\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 302.63\n",
      "===============================================================================================\n",
      "Input size (MB): 4.19\n",
      "Forward/backward pass size (MB): 553.65\n",
      "Params size (MB): 75.66\n",
      "Estimated Total Size (MB): 633.50\n",
      "===============================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"Decoder-only Transformer Profiling:\")\n",
    "profile_model(decoder_model, (batch_size, T, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self, T, D, H, L):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([EncoderLayer(T, D, H) for _ in range(L)])\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask=mask)  # 추가된 마스크 입력\n",
    "        return x\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, T, D, H):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadSelfAttention(D, H)\n",
    "        self.positionwise_ff = nn.Sequential(\n",
    "            nn.Linear(D, D * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(D * 4, D),\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(D)\n",
    "        self.norm2 = nn.LayerNorm(D)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        attn_out = self.self_attention(x, mask=mask)\n",
    "        x = self.norm1(x + attn_out)\n",
    "        ff_out = self.positionwise_ff(x)\n",
    "        return self.norm2(x + ff_out)\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, D, H):\n",
    "        super().__init__()\n",
    "        self.H = H\n",
    "        self.D = D\n",
    "        self.query = nn.Linear(D, D)\n",
    "        self.key = nn.Linear(D, D)\n",
    "        self.value = nn.Linear(D, D)\n",
    "        self.proj = nn.Linear(D, D)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        B, T, D = x.size()\n",
    "        Q = self.query(x).view(B, T, self.H, D // self.H).transpose(1, 2)\n",
    "        K = self.key(x).view(B, T, self.H, D // self.H).transpose(1, 2)\n",
    "        V = self.value(x).view(B, T, self.H, D // self.H).transpose(1, 2)\n",
    "        scores = Q @ K.transpose(-2, -1) / (D // self.H) ** 0.5\n",
    "        if mask is not None:  # 마스킹 추가\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        context = (attn @ V).transpose(1, 2).contiguous().view(B, T, D)\n",
    "        return self.proj(context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, T, D, H, L):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([DecoderLayer(T, D, H) for _ in range(L)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, T, D, H):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadSelfAttention(D, H)\n",
    "        self.positionwise_ff = nn.Sequential(\n",
    "            nn.Linear(D, D * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(D * 4, D),\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(D)\n",
    "        self.norm2 = nn.LayerNorm(D)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        attn_out = self.self_attention(x, mask=True)  # Causal Mask 적용\n",
    "        x = self.norm1(x + attn_out)\n",
    "        ff_out = self.positionwise_ff(x)\n",
    "        return self.norm2(x + ff_out)\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, D, H):\n",
    "        super().__init__()\n",
    "        self.H = H\n",
    "        self.D = D\n",
    "        self.query = nn.Linear(D, D)\n",
    "        self.key = nn.Linear(D, D)\n",
    "        self.value = nn.Linear(D, D)\n",
    "        self.proj = nn.Linear(D, D)\n",
    "    \n",
    "    def forward(self, x, mask=False):\n",
    "        B, T, D = x.size()\n",
    "        Q = self.query(x).view(B, T, self.H, D // self.H).transpose(1, 2)\n",
    "        K = self.key(x).view(B, T, self.H, D // self.H).transpose(1, 2)\n",
    "        V = self.value(x).view(B, T, self.H, D // self.H).transpose(1, 2)\n",
    "        scores = Q @ K.transpose(-2, -1) / (D // self.H) ** 0.5\n",
    "        if mask:  # Causal Mask 적용\n",
    "            causal_mask = torch.triu(torch.ones(T, T), diagonal=1).bool().to(x.device)\n",
    "            scores = scores.masked_fill(causal_mask, float('-inf'))\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        context = (attn @ V).transpose(1, 2).contiguous().view(B, T, D)\n",
    "        return self.proj(context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "def profile_model(model, input_shape, model_name=\"Model\"):\n",
    "    print(f\"\\n{model_name} Profiling:\")\n",
    "    print(summary(model, input_size=input_shape, depth=3, col_names=[\"output_size\", \"num_params\", \"trainable\"], verbose=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터 설정\n",
    "T, D, H, L = 128, 512, 8, 6  # 시퀀스 길이, 임베딩 차원, 헤드 수, 레이어 수\n",
    "batch_size = 16\n",
    "\n",
    "# BERT와 GPT 모델 생성\n",
    "bert_model = BERT(T, D, H, L)\n",
    "gpt_model = GPT(T, D, H, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BERT (Encoder-only) Profiling:\n",
      "========================================================================================================================\n",
      "Layer (type:depth-idx)                        Output Shape              Param #                   Trainable\n",
      "========================================================================================================================\n",
      "BERT                                          [16, 128, 512]            --                        True\n",
      "├─ModuleList: 1-1                             --                        --                        True\n",
      "│    └─EncoderLayer: 2-1                      [16, 128, 512]            --                        True\n",
      "│    │    └─MultiHeadSelfAttention: 3-1       [16, 128, 512]            1,050,624                 True\n",
      "│    │    └─LayerNorm: 3-2                    [16, 128, 512]            1,024                     True\n",
      "│    │    └─Sequential: 3-3                   [16, 128, 512]            2,099,712                 True\n",
      "│    │    └─LayerNorm: 3-4                    [16, 128, 512]            1,024                     True\n",
      "│    └─EncoderLayer: 2-2                      [16, 128, 512]            --                        True\n",
      "│    │    └─MultiHeadSelfAttention: 3-5       [16, 128, 512]            1,050,624                 True\n",
      "│    │    └─LayerNorm: 3-6                    [16, 128, 512]            1,024                     True\n",
      "│    │    └─Sequential: 3-7                   [16, 128, 512]            2,099,712                 True\n",
      "│    │    └─LayerNorm: 3-8                    [16, 128, 512]            1,024                     True\n",
      "│    └─EncoderLayer: 2-3                      [16, 128, 512]            --                        True\n",
      "│    │    └─MultiHeadSelfAttention: 3-9       [16, 128, 512]            1,050,624                 True\n",
      "│    │    └─LayerNorm: 3-10                   [16, 128, 512]            1,024                     True\n",
      "│    │    └─Sequential: 3-11                  [16, 128, 512]            2,099,712                 True\n",
      "│    │    └─LayerNorm: 3-12                   [16, 128, 512]            1,024                     True\n",
      "│    └─EncoderLayer: 2-4                      [16, 128, 512]            --                        True\n",
      "│    │    └─MultiHeadSelfAttention: 3-13      [16, 128, 512]            1,050,624                 True\n",
      "│    │    └─LayerNorm: 3-14                   [16, 128, 512]            1,024                     True\n",
      "│    │    └─Sequential: 3-15                  [16, 128, 512]            2,099,712                 True\n",
      "│    │    └─LayerNorm: 3-16                   [16, 128, 512]            1,024                     True\n",
      "│    └─EncoderLayer: 2-5                      [16, 128, 512]            --                        True\n",
      "│    │    └─MultiHeadSelfAttention: 3-17      [16, 128, 512]            1,050,624                 True\n",
      "│    │    └─LayerNorm: 3-18                   [16, 128, 512]            1,024                     True\n",
      "│    │    └─Sequential: 3-19                  [16, 128, 512]            2,099,712                 True\n",
      "│    │    └─LayerNorm: 3-20                   [16, 128, 512]            1,024                     True\n",
      "│    └─EncoderLayer: 2-6                      [16, 128, 512]            --                        True\n",
      "│    │    └─MultiHeadSelfAttention: 3-21      [16, 128, 512]            1,050,624                 True\n",
      "│    │    └─LayerNorm: 3-22                   [16, 128, 512]            1,024                     True\n",
      "│    │    └─Sequential: 3-23                  [16, 128, 512]            2,099,712                 True\n",
      "│    │    └─LayerNorm: 3-24                   [16, 128, 512]            1,024                     True\n",
      "========================================================================================================================\n",
      "Total params: 18,914,304\n",
      "Trainable params: 18,914,304\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 302.63\n",
      "========================================================================================================================\n",
      "Input size (MB): 4.19\n",
      "Forward/backward pass size (MB): 553.65\n",
      "Params size (MB): 75.66\n",
      "Estimated Total Size (MB): 633.50\n",
      "========================================================================================================================\n",
      "========================================================================================================================\n",
      "Layer (type:depth-idx)                        Output Shape              Param #                   Trainable\n",
      "========================================================================================================================\n",
      "BERT                                          [16, 128, 512]            --                        True\n",
      "├─ModuleList: 1-1                             --                        --                        True\n",
      "│    └─EncoderLayer: 2-1                      [16, 128, 512]            --                        True\n",
      "│    │    └─MultiHeadSelfAttention: 3-1       [16, 128, 512]            1,050,624                 True\n",
      "│    │    └─LayerNorm: 3-2                    [16, 128, 512]            1,024                     True\n",
      "│    │    └─Sequential: 3-3                   [16, 128, 512]            2,099,712                 True\n",
      "│    │    └─LayerNorm: 3-4                    [16, 128, 512]            1,024                     True\n",
      "│    └─EncoderLayer: 2-2                      [16, 128, 512]            --                        True\n",
      "│    │    └─MultiHeadSelfAttention: 3-5       [16, 128, 512]            1,050,624                 True\n",
      "│    │    └─LayerNorm: 3-6                    [16, 128, 512]            1,024                     True\n",
      "│    │    └─Sequential: 3-7                   [16, 128, 512]            2,099,712                 True\n",
      "│    │    └─LayerNorm: 3-8                    [16, 128, 512]            1,024                     True\n",
      "│    └─EncoderLayer: 2-3                      [16, 128, 512]            --                        True\n",
      "│    │    └─MultiHeadSelfAttention: 3-9       [16, 128, 512]            1,050,624                 True\n",
      "│    │    └─LayerNorm: 3-10                   [16, 128, 512]            1,024                     True\n",
      "│    │    └─Sequential: 3-11                  [16, 128, 512]            2,099,712                 True\n",
      "│    │    └─LayerNorm: 3-12                   [16, 128, 512]            1,024                     True\n",
      "│    └─EncoderLayer: 2-4                      [16, 128, 512]            --                        True\n",
      "│    │    └─MultiHeadSelfAttention: 3-13      [16, 128, 512]            1,050,624                 True\n",
      "│    │    └─LayerNorm: 3-14                   [16, 128, 512]            1,024                     True\n",
      "│    │    └─Sequential: 3-15                  [16, 128, 512]            2,099,712                 True\n",
      "│    │    └─LayerNorm: 3-16                   [16, 128, 512]            1,024                     True\n",
      "│    └─EncoderLayer: 2-5                      [16, 128, 512]            --                        True\n",
      "│    │    └─MultiHeadSelfAttention: 3-17      [16, 128, 512]            1,050,624                 True\n",
      "│    │    └─LayerNorm: 3-18                   [16, 128, 512]            1,024                     True\n",
      "│    │    └─Sequential: 3-19                  [16, 128, 512]            2,099,712                 True\n",
      "│    │    └─LayerNorm: 3-20                   [16, 128, 512]            1,024                     True\n",
      "│    └─EncoderLayer: 2-6                      [16, 128, 512]            --                        True\n",
      "│    │    └─MultiHeadSelfAttention: 3-21      [16, 128, 512]            1,050,624                 True\n",
      "│    │    └─LayerNorm: 3-22                   [16, 128, 512]            1,024                     True\n",
      "│    │    └─Sequential: 3-23                  [16, 128, 512]            2,099,712                 True\n",
      "│    │    └─LayerNorm: 3-24                   [16, 128, 512]            1,024                     True\n",
      "========================================================================================================================\n",
      "Total params: 18,914,304\n",
      "Trainable params: 18,914,304\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 302.63\n",
      "========================================================================================================================\n",
      "Input size (MB): 4.19\n",
      "Forward/backward pass size (MB): 553.65\n",
      "Params size (MB): 75.66\n",
      "Estimated Total Size (MB): 633.50\n",
      "========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# BERT 모델 프로파일링\n",
    "profile_model(bert_model, (batch_size, T, D), model_name=\"BERT (Encoder-only)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPT (Decoder-only) Profiling:\n",
      "========================================================================================================================\n",
      "Layer (type:depth-idx)                        Output Shape              Param #                   Trainable\n",
      "========================================================================================================================\n",
      "GPT                                           [16, 128, 512]            --                        True\n",
      "├─ModuleList: 1-1                             --                        --                        True\n",
      "│    └─DecoderLayer: 2-1                      [16, 128, 512]            --                        True\n",
      "│    │    └─MultiHeadSelfAttention: 3-1       [16, 128, 512]            1,050,624                 True\n",
      "│    │    └─LayerNorm: 3-2                    [16, 128, 512]            1,024                     True\n",
      "│    │    └─Sequential: 3-3                   [16, 128, 512]            2,099,712                 True\n",
      "│    │    └─LayerNorm: 3-4                    [16, 128, 512]            1,024                     True\n",
      "│    └─DecoderLayer: 2-2                      [16, 128, 512]            --                        True\n",
      "│    │    └─MultiHeadSelfAttention: 3-5       [16, 128, 512]            1,050,624                 True\n",
      "│    │    └─LayerNorm: 3-6                    [16, 128, 512]            1,024                     True\n",
      "│    │    └─Sequential: 3-7                   [16, 128, 512]            2,099,712                 True\n",
      "│    │    └─LayerNorm: 3-8                    [16, 128, 512]            1,024                     True\n",
      "│    └─DecoderLayer: 2-3                      [16, 128, 512]            --                        True\n",
      "│    │    └─MultiHeadSelfAttention: 3-9       [16, 128, 512]            1,050,624                 True\n",
      "│    │    └─LayerNorm: 3-10                   [16, 128, 512]            1,024                     True\n",
      "│    │    └─Sequential: 3-11                  [16, 128, 512]            2,099,712                 True\n",
      "│    │    └─LayerNorm: 3-12                   [16, 128, 512]            1,024                     True\n",
      "│    └─DecoderLayer: 2-4                      [16, 128, 512]            --                        True\n",
      "│    │    └─MultiHeadSelfAttention: 3-13      [16, 128, 512]            1,050,624                 True\n",
      "│    │    └─LayerNorm: 3-14                   [16, 128, 512]            1,024                     True\n",
      "│    │    └─Sequential: 3-15                  [16, 128, 512]            2,099,712                 True\n",
      "│    │    └─LayerNorm: 3-16                   [16, 128, 512]            1,024                     True\n",
      "│    └─DecoderLayer: 2-5                      [16, 128, 512]            --                        True\n",
      "│    │    └─MultiHeadSelfAttention: 3-17      [16, 128, 512]            1,050,624                 True\n",
      "│    │    └─LayerNorm: 3-18                   [16, 128, 512]            1,024                     True\n",
      "│    │    └─Sequential: 3-19                  [16, 128, 512]            2,099,712                 True\n",
      "│    │    └─LayerNorm: 3-20                   [16, 128, 512]            1,024                     True\n",
      "│    └─DecoderLayer: 2-6                      [16, 128, 512]            --                        True\n",
      "│    │    └─MultiHeadSelfAttention: 3-21      [16, 128, 512]            1,050,624                 True\n",
      "│    │    └─LayerNorm: 3-22                   [16, 128, 512]            1,024                     True\n",
      "│    │    └─Sequential: 3-23                  [16, 128, 512]            2,099,712                 True\n",
      "│    │    └─LayerNorm: 3-24                   [16, 128, 512]            1,024                     True\n",
      "========================================================================================================================\n",
      "Total params: 18,914,304\n",
      "Trainable params: 18,914,304\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 302.63\n",
      "========================================================================================================================\n",
      "Input size (MB): 4.19\n",
      "Forward/backward pass size (MB): 553.65\n",
      "Params size (MB): 75.66\n",
      "Estimated Total Size (MB): 633.50\n",
      "========================================================================================================================\n",
      "========================================================================================================================\n",
      "Layer (type:depth-idx)                        Output Shape              Param #                   Trainable\n",
      "========================================================================================================================\n",
      "GPT                                           [16, 128, 512]            --                        True\n",
      "├─ModuleList: 1-1                             --                        --                        True\n",
      "│    └─DecoderLayer: 2-1                      [16, 128, 512]            --                        True\n",
      "│    │    └─MultiHeadSelfAttention: 3-1       [16, 128, 512]            1,050,624                 True\n",
      "│    │    └─LayerNorm: 3-2                    [16, 128, 512]            1,024                     True\n",
      "│    │    └─Sequential: 3-3                   [16, 128, 512]            2,099,712                 True\n",
      "│    │    └─LayerNorm: 3-4                    [16, 128, 512]            1,024                     True\n",
      "│    └─DecoderLayer: 2-2                      [16, 128, 512]            --                        True\n",
      "│    │    └─MultiHeadSelfAttention: 3-5       [16, 128, 512]            1,050,624                 True\n",
      "│    │    └─LayerNorm: 3-6                    [16, 128, 512]            1,024                     True\n",
      "│    │    └─Sequential: 3-7                   [16, 128, 512]            2,099,712                 True\n",
      "│    │    └─LayerNorm: 3-8                    [16, 128, 512]            1,024                     True\n",
      "│    └─DecoderLayer: 2-3                      [16, 128, 512]            --                        True\n",
      "│    │    └─MultiHeadSelfAttention: 3-9       [16, 128, 512]            1,050,624                 True\n",
      "│    │    └─LayerNorm: 3-10                   [16, 128, 512]            1,024                     True\n",
      "│    │    └─Sequential: 3-11                  [16, 128, 512]            2,099,712                 True\n",
      "│    │    └─LayerNorm: 3-12                   [16, 128, 512]            1,024                     True\n",
      "│    └─DecoderLayer: 2-4                      [16, 128, 512]            --                        True\n",
      "│    │    └─MultiHeadSelfAttention: 3-13      [16, 128, 512]            1,050,624                 True\n",
      "│    │    └─LayerNorm: 3-14                   [16, 128, 512]            1,024                     True\n",
      "│    │    └─Sequential: 3-15                  [16, 128, 512]            2,099,712                 True\n",
      "│    │    └─LayerNorm: 3-16                   [16, 128, 512]            1,024                     True\n",
      "│    └─DecoderLayer: 2-5                      [16, 128, 512]            --                        True\n",
      "│    │    └─MultiHeadSelfAttention: 3-17      [16, 128, 512]            1,050,624                 True\n",
      "│    │    └─LayerNorm: 3-18                   [16, 128, 512]            1,024                     True\n",
      "│    │    └─Sequential: 3-19                  [16, 128, 512]            2,099,712                 True\n",
      "│    │    └─LayerNorm: 3-20                   [16, 128, 512]            1,024                     True\n",
      "│    └─DecoderLayer: 2-6                      [16, 128, 512]            --                        True\n",
      "│    │    └─MultiHeadSelfAttention: 3-21      [16, 128, 512]            1,050,624                 True\n",
      "│    │    └─LayerNorm: 3-22                   [16, 128, 512]            1,024                     True\n",
      "│    │    └─Sequential: 3-23                  [16, 128, 512]            2,099,712                 True\n",
      "│    │    └─LayerNorm: 3-24                   [16, 128, 512]            1,024                     True\n",
      "========================================================================================================================\n",
      "Total params: 18,914,304\n",
      "Trainable params: 18,914,304\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 302.63\n",
      "========================================================================================================================\n",
      "Input size (MB): 4.19\n",
      "Forward/backward pass size (MB): 553.65\n",
      "Params size (MB): 75.66\n",
      "Estimated Total Size (MB): 633.50\n",
      "========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# GPT 모델 프로파일링\n",
    "profile_model(gpt_model, (batch_size, T, D), model_name=\"GPT (Decoder-only)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
