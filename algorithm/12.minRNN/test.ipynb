{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd25c295",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "# --- 모델 정의 ---\n",
    "\n",
    "# 1. 표준 GRU (수식 기반 직접 구현)\n",
    "class StandardGRU(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # 입력 x_t와 이전 은닉 상태 h_{t-1}를 합친 크기\n",
    "        combined_dim = input_dim + hidden_dim\n",
    "\n",
    "        # W_r, W_z, W_n 에 해당하는 Linear 레이어들\n",
    "        self.linear_rz = nn.Linear(combined_dim, 2 * hidden_dim) # 리셋(r), 업데이트(z) 게이트 동시 계산\n",
    "        self.linear_n = nn.Linear(combined_dim, hidden_dim)    # 후보(n) 상태 계산용\n",
    "\n",
    "    def forward(self, x, h_prev=None):\n",
    "        # x: (batch_size, seq_len, input_dim)\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "\n",
    "        # 초기 은닉 상태 (없으면 0으로 초기화)\n",
    "        if h_prev is None:\n",
    "            h_prev = torch.zeros(batch_size, self.hidden_dim).to(x.device)\n",
    "\n",
    "        outputs = [] # 각 타임스텝의 은닉 상태를 저장할 리스트\n",
    "\n",
    "        # 시퀀스를 한 스텝씩 처리\n",
    "        for t in range(seq_len):\n",
    "            xt = x[:, t, :] # 현재 타임스텝 입력: (batch_size, input_dim)\n",
    "            combined_input = torch.cat([h_prev, xt], dim=1) # (batch_size, hidden_dim + input_dim)\n",
    "\n",
    "            # 리셋(r) 및 업데이트(z) 게이트 계산\n",
    "            rz = self.linear_rz(combined_input)\n",
    "            r_t, z_t = torch.chunk(rz, 2, dim=1) # 결과를 반으로 나눠 r, z 얻음\n",
    "            r_t = torch.sigmoid(r_t) # 리셋 게이트: (batch_size, hidden_dim)\n",
    "            z_t = torch.sigmoid(z_t) # 업데이트 게이트: (batch_size, hidden_dim)\n",
    "\n",
    "            # 후보(n) 은닉 상태 계산\n",
    "            # n_t = tanh(W_n * [r_t ⊙ h_{t-1}, x_t] + b_n)\n",
    "            combined_n = torch.cat([r_t * h_prev, xt], dim=1) # 리셋 게이트 적용된 h와 x 결합\n",
    "            n_t = torch.tanh(self.linear_n(combined_n)) # (batch_size, hidden_dim)\n",
    "\n",
    "            # 최종 은닉 상태(h) 계산\n",
    "            # h_t = (1 - z_t) ⊙ n_t + z_t ⊙ h_{t-1}\n",
    "            h_t = (1 - z_t) * n_t + z_t * h_prev # (batch_size, hidden_dim)\n",
    "\n",
    "            outputs.append(h_t.unsqueeze(1)) # (batch_size, 1, hidden_dim)\n",
    "            h_prev = h_t # 다음 스텝을 위해 현재 은닉 상태 저장\n",
    "\n",
    "        # 모든 타임스텝의 출력을 모음\n",
    "        outputs = torch.cat(outputs, dim=1) # (batch_size, seq_len, hidden_dim)\n",
    "        # 마지막 은닉 상태와 모든 시퀀스 출력을 반환\n",
    "        return outputs, h_prev # h_prev는 마지막 타임스텝의 h_t와 동일\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01563d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Minimal GRU (minGRU) (수식 기반 직접 구현)\n",
    "class MinimalGRU(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # 입력 x_t와 이전 은닉 상태 h_{t-1}를 합친 크기\n",
    "        combined_dim = input_dim + hidden_dim\n",
    "\n",
    "        # W_f, W_h 에 해당하는 Linear 레이어들\n",
    "        self.linear_f = nn.Linear(combined_dim, hidden_dim) # 망각(f) 게이트 계산용\n",
    "        self.linear_h = nn.Linear(combined_dim, hidden_dim) # 후보(h̃) 상태 계산용\n",
    "\n",
    "    def forward(self, x, h_prev=None):\n",
    "        # x: (batch_size, seq_len, input_dim)\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "\n",
    "        # 초기 은닉 상태 (없으면 0으로 초기화)\n",
    "        if h_prev is None:\n",
    "            h_prev = torch.zeros(batch_size, self.hidden_dim).to(x.device)\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            xt = x[:, t, :]\n",
    "            combined_input = torch.cat([h_prev, xt], dim=1)\n",
    "\n",
    "            # 망각(f) 게이트 계산\n",
    "            # f_t = σ(W_f * [h_{t-1}, x_t] + b_f)\n",
    "            f_t = torch.sigmoid(self.linear_f(combined_input)) # (batch_size, hidden_dim)\n",
    "\n",
    "            # 후보(h̃) 은닉 상태 계산\n",
    "            # h̃_t = tanh(W_h * [f_t ⊙ h_{t-1}, x_t] + b_h)\n",
    "            combined_h = torch.cat([f_t * h_prev, xt], dim=1) # 망각 게이트 적용된 h와 x 결합\n",
    "            h_tilde = torch.tanh(self.linear_h(combined_h)) # (batch_size, hidden_dim)\n",
    "\n",
    "            # 최종 은닉 상태(h) 계산 (구현 방식 2: h_t = f_t * h_{t-1} + (1 - f_t) * h̃_t)\n",
    "            h_t = f_t * h_prev + (1 - f_t) * h_tilde # (batch_size, hidden_dim)\n",
    "\n",
    "            outputs.append(h_t.unsqueeze(1))\n",
    "            h_prev = h_t\n",
    "\n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "        return outputs, h_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd45dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 모델 파라미터 수 비교 ---\n",
      "Standard GRU (Manual): 45,300\n",
      "Minimal GRU (minGRU): 30,200\n",
      "Standard GRU (Built-in): 45,600\n",
      "--------------------\n",
      "--- 모델 실행 시간 및 출력 형태 비교 ---\n",
      "Standard GRU (Manual) - 실행 시간: 0.0113s\n",
      "  Output shape: torch.Size([4, 30, 100])\n",
      "  Hidden shape: torch.Size([4, 100])\n",
      "Minimal GRU (minGRU) - 실행 시간: 0.0061s\n",
      "  Output shape: torch.Size([4, 30, 100])\n",
      "  Hidden shape: torch.Size([4, 100])\n",
      "Standard GRU (Built-in) - 실행 시간: 0.0052s\n",
      "  Output shape: torch.Size([4, 30, 100])\n",
      "  Hidden shape: torch.Size([1, 4, 100])\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 비교 실행 ---\n",
    "# 파라미터 설정\n",
    "input_dim = 50\n",
    "hidden_dim = 100\n",
    "seq_len = 30\n",
    "batch_size = 4\n",
    "\n",
    "# 모델 인스턴스화\n",
    "std_gru_manual = StandardGRU(input_dim, hidden_dim)\n",
    "min_gru = MinimalGRU(input_dim, hidden_dim)\n",
    "# 참고: PyTorch 내장 GRU (비교용)\n",
    "std_gru_builtin = nn.GRU(input_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "# 파라미터 수 계산 함수\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# 각 모델의 파라미터 수 출력\n",
    "params_std_manual = count_parameters(std_gru_manual)\n",
    "params_min = count_parameters(min_gru)\n",
    "params_std_builtin = count_parameters(std_gru_builtin)\n",
    "\n",
    "print(f\"--- 모델 파라미터 수 비교 ---\")\n",
    "print(f\"Standard GRU (Manual): {params_std_manual:,}\")\n",
    "print(f\"Minimal GRU (minGRU): {params_min:,}\")\n",
    "print(f\"Standard GRU (Built-in): {params_std_builtin:,}\")\n",
    "print(\"-\" * 20)\n",
    "# 참고: 직접 구현한 Standard GRU와 내장 GRU의 파라미터 수가 정확히 일치하는 것을 볼 수 있습니다.\n",
    "# minGRU는 게이트가 줄어 파라미터 수가 더 적습니다. (표준 GRU의 약 2/3)\n",
    "\n",
    "# 더미 입력 데이터 생성\n",
    "dummy_input = torch.randn(batch_size, seq_len, input_dim)\n",
    "\n",
    "# 모델 실행 및 출력 확인 (시간 측정 포함)\n",
    "print(f\"--- 모델 실행 시간 및 출력 형태 비교 ---\")\n",
    "start_time = time.time()\n",
    "outputs_std_manual, hn_std_manual = std_gru_manual(dummy_input)\n",
    "print(f\"Standard GRU (Manual) - 실행 시간: {time.time() - start_time:.4f}s\")\n",
    "print(f\"  Output shape: {outputs_std_manual.shape}\") # (batch_size, seq_len, hidden_dim)\n",
    "print(f\"  Hidden shape: {hn_std_manual.shape}\")     # (batch_size, hidden_dim)\n",
    "\n",
    "start_time = time.time()\n",
    "outputs_min, hn_min = min_gru(dummy_input)\n",
    "print(f\"Minimal GRU (minGRU) - 실행 시간: {time.time() - start_time:.4f}s\")\n",
    "print(f\"  Output shape: {outputs_min.shape}\")\n",
    "print(f\"  Hidden shape: {hn_min.shape}\")\n",
    "\n",
    "start_time = time.time()\n",
    "outputs_std_builtin, hn_std_builtin = std_gru_builtin(dummy_input)\n",
    "print(f\"Standard GRU (Built-in) - 실행 시간: {time.time() - start_time:.4f}s\")\n",
    "print(f\"  Output shape: {outputs_std_builtin.shape}\")\n",
    "print(f\"  Hidden shape: {hn_std_builtin.shape}\") # Built-in은 (num_layers, batch_size, hidden_dim) 형태\n",
    "print(\"-\" * 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
