{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83fe85c7",
   "metadata": {},
   "source": [
    "```Performance Optimization with torch compiler```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20ec8bf0",
   "metadata": {
    "id": "20ec8bf0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3390f4",
   "metadata": {
    "id": "5a3390f4"
   },
   "source": [
    "## Hyperparameters and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0c61f36",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1732703737281,
     "user": {
      "displayName": "Hugman Sangkeun Jung",
      "userId": "08689291704194029524"
     },
     "user_tz": -540
    },
    "id": "d0c61f36",
    "outputId": "45d9894c-a14d-4164-9f2d-699025751e72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Modified hyperparameters\n",
    "SEQUENCE_LENGTH = 1024\n",
    "\n",
    "EMBEDDING_DIM = 64\n",
    "HIDDEN_DIM = EMBEDDING_DIM*2\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 1e-3\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.1\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c9b962",
   "metadata": {
    "id": "89c9b962"
   },
   "source": [
    "## Data Preparation\n",
    "\n",
    "We are using the TinyShakespeare dataset, a small character-level text corpus consisting of a subset of Shakespeare's plays. It's often used for testing sequence models, as it includes a rich set of vocabulary and provides a challenging task for next-character prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "089830e2",
   "metadata": {
    "id": "089830e2"
   },
   "outputs": [],
   "source": [
    "## Utility Functions\n",
    "\n",
    "def load_data(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "def create_char_mappings(text):\n",
    "    chars = sorted(list(set(text))[:1024])\n",
    "    char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "    idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "    return chars, char_to_idx, idx_to_char"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fc6452",
   "metadata": {
    "id": "30fc6452"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f07670d9",
   "metadata": {
    "id": "f07670d9"
   },
   "outputs": [],
   "source": [
    "class CharDataset(Dataset):\n",
    "    def __init__(self, text, seq_length, char_to_idx):\n",
    "        self.text = text\n",
    "        self.seq_length = seq_length\n",
    "        self.char_to_idx = char_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text) - self.seq_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = [self.char_to_idx[ch] for ch in self.text[idx:idx+self.seq_length]]\n",
    "        y = [self.char_to_idx[ch] for ch in self.text[idx+1:idx+self.seq_length+1]]\n",
    "        return torch.tensor(x), torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9172da13",
   "metadata": {
    "id": "9172da13"
   },
   "outputs": [],
   "source": [
    "def prepare_data(text, seq_length, batch_size, val_split):\n",
    "    text = text[:1024*256]\n",
    "    chars, char_to_idx, idx_to_char = create_char_mappings(text)\n",
    "\n",
    "    # Split data into train and validation\n",
    "    val_size = int(len(text) * val_split)\n",
    "    train_text, val_text = text[:-val_size], text[-val_size:]\n",
    "\n",
    "    train_dataset = CharDataset(train_text, seq_length, char_to_idx)\n",
    "    val_dataset = CharDataset(val_text, seq_length, char_to_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=12, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=12, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader, chars, char_to_idx, idx_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "nEwKFB_8L6AG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3706,
     "status": "ok",
     "timestamp": 1732704246464,
     "user": {
      "displayName": "Hugman Sangkeun Jung",
      "userId": "08689291704194029524"
     },
     "user_tz": -540
    },
    "id": "nEwKFB_8L6AG",
    "outputId": "e75cdff9-3775-461e-d930-0ced534bf74d"
   },
   "outputs": [],
   "source": [
    "# !wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=19zosLuU0z4MxIMKbGVYEGlg52QyfbTIy' -O input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47d03398",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 668,
     "status": "ok",
     "timestamp": 1732704255324,
     "user": {
      "displayName": "Hugman Sangkeun Jung",
      "userId": "08689291704194029524"
     },
     "user_tz": -540
    },
    "id": "47d03398",
    "outputId": "f247f5b3-88ff-4ecb-e8a3-4b2e42a2820b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 1115394\n",
      "Vocabulary size: 62\n",
      "Train dataset size: 234906\n",
      "Validation dataset size: 25190\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "text = load_data('../data/input.txt')\n",
    "train_loader, val_loader, chars, char_to_idx, idx_to_char = prepare_data(text, SEQUENCE_LENGTH, BATCH_SIZE, VALIDATION_SPLIT)\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(f\"Total characters: {len(text)}\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Train dataset size: {len(train_loader.dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9442b104",
   "metadata": {
    "id": "9442b104"
   },
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1f0a6a1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 916,
     "status": "ok",
     "timestamp": 1732704257555,
     "user": {
      "displayName": "Hugman Sangkeun Jung",
      "userId": "08689291704194029524"
     },
     "user_tz": -540
    },
    "id": "f1f0a6a1",
    "outputId": "9ef6eb5a-0015-40ad-f143-236e3d38fb7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([256, 1024])\n",
      "Target shape: torch.Size([256, 1024])\n",
      "Sample 1: ------------------------------\n",
      "Input sequence : k, theoak not to be wind-shaken.CORIOLANUS:We will before the walls of Rome tomorrowSet down our host. My partner in this action,You must report to the Volscian lords, how plainlyI have borne this business.AUFIDIUS:Only their endsYou have respected; stopp'd your ears againstThe general suit of Rome; never admittedA private whisper, no, not with such friendsThat thought them sure of you.CORIOLANUS:This last old man,Whom with a crack'd heart I have sent to Rome,Loved me above the measure of a father;Nay, godded me, indeed. Their latest refugeWas to send him; for whose old love I have,Though I show'd sourly to him, once more offer'dThe first conditions, which they did refuseAnd cannot now accept; to grace him onlyThat thought he could do more, a very littleI have yielded to: fresh embassies and suits,Nor from the state nor private friends, hereafterWill I lend ear to. Ha! what shout is this?Shall I be tempted to infringe my vowIn the same time 'tis made? I will not.My wife comes fo\n",
      "Target sequence: , theoak not to be wind-shaken.CORIOLANUS:We will before the walls of Rome tomorrowSet down our host. My partner in this action,You must report to the Volscian lords, how plainlyI have borne this business.AUFIDIUS:Only their endsYou have respected; stopp'd your ears againstThe general suit of Rome; never admittedA private whisper, no, not with such friendsThat thought them sure of you.CORIOLANUS:This last old man,Whom with a crack'd heart I have sent to Rome,Loved me above the measure of a father;Nay, godded me, indeed. Their latest refugeWas to send him; for whose old love I have,Though I show'd sourly to him, once more offer'dThe first conditions, which they did refuseAnd cannot now accept; to grace him onlyThat thought he could do more, a very littleI have yielded to: fresh embassies and suits,Nor from the state nor private friends, hereafterWill I lend ear to. Ha! what shout is this?Shall I be tempted to infringe my vowIn the same time 'tis made? I will not.My wife comes for\n",
      "\n",
      "Sample 2: ------------------------------\n",
      "Input sequence : he envious slanders of her false accusers;Or, if she be accused in true report,Bear with her weakness, which, I think proceedsFrom wayward sickness, and no grounded malice.RIVERS:Saw you the king to-day, my Lord of Derby?DERBY:But now the Duke of Buckingham and IAre come from visiting his majesty.QUEEN ELIZABETH:What likelihood of his amendment, lords?BUCKINGHAM:Madam, good hope; his grace speaks cheerfully.QUEEN ELIZABETH:God grant him health! Did you confer with him?BUCKINGHAM:Madam, we did: he desires to make atonementBetwixt the Duke of Gloucester and your brothers,And betwixt them and my lord chamberlain;And sent to warn them to his royal presence.QUEEN ELIZABETH:Would all were well! but that will never beI fear our happiness is at the highest.GLOUCESTER:They do me wrong, and I will not endure it:Who are they that complain unto the king,That I, forsooth, am stern, and love them not?By holy Paul, they love his grace but lightlyThat fill his ears with such dissentious ru\n",
      "Target sequence: e envious slanders of her false accusers;Or, if she be accused in true report,Bear with her weakness, which, I think proceedsFrom wayward sickness, and no grounded malice.RIVERS:Saw you the king to-day, my Lord of Derby?DERBY:But now the Duke of Buckingham and IAre come from visiting his majesty.QUEEN ELIZABETH:What likelihood of his amendment, lords?BUCKINGHAM:Madam, good hope; his grace speaks cheerfully.QUEEN ELIZABETH:God grant him health! Did you confer with him?BUCKINGHAM:Madam, we did: he desires to make atonementBetwixt the Duke of Gloucester and your brothers,And betwixt them and my lord chamberlain;And sent to warn them to his royal presence.QUEEN ELIZABETH:Would all were well! but that will never beI fear our happiness is at the highest.GLOUCESTER:They do me wrong, and I will not endure it:Who are they that complain unto the king,That I, forsooth, am stern, and love them not?By holy Paul, they love his grace but lightlyThat fill his ears with such dissentious rum\n",
      "\n",
      "Sample 3: ------------------------------\n",
      "Input sequence : hat thou wilt war with God by murdering me?Ah, sirs, consider, he that set you onTo do this deed will hate you for the deed.Second Murderer:What shall we do?CLARENCE:Relent, and save your souls.First Murderer:Relent! 'tis cowardly and womanish.CLARENCE:Not to relent is beastly, savage, devilish.Which of you, if you were a prince's son,Being pent from liberty, as I am now,if two such murderers as yourselves came to you,Would not entreat for life?My friend, I spy some pity in thy looks:O, if thine eye be not a flatterer,Come thou on my side, and entreat for me,As you would beg, were you in my distressA begging prince what beggar pities not?Second Murderer:Look behind you, my lord.First Murderer:Take that, and that: if all this will not do,I'll drown you in the malmsey-butt within.Second Murderer:A bloody deed, and desperately dispatch'd!How fain, like Pilate, would I wash my handsOf this most grievous guilty murder done!First Murderer:How now! what mean'st thou, that thou h\n",
      "Target sequence: at thou wilt war with God by murdering me?Ah, sirs, consider, he that set you onTo do this deed will hate you for the deed.Second Murderer:What shall we do?CLARENCE:Relent, and save your souls.First Murderer:Relent! 'tis cowardly and womanish.CLARENCE:Not to relent is beastly, savage, devilish.Which of you, if you were a prince's son,Being pent from liberty, as I am now,if two such murderers as yourselves came to you,Would not entreat for life?My friend, I spy some pity in thy looks:O, if thine eye be not a flatterer,Come thou on my side, and entreat for me,As you would beg, were you in my distressA begging prince what beggar pities not?Second Murderer:Look behind you, my lord.First Murderer:Take that, and that: if all this will not do,I'll drown you in the malmsey-butt within.Second Murderer:A bloody deed, and desperately dispatch'd!How fain, like Pilate, would I wash my handsOf this most grievous guilty murder done!First Murderer:How now! what mean'st thou, that thou he\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to convert index sequence to character sequence\n",
    "def indices_to_text(indices, idx_to_char):\n",
    "    return ''.join([idx_to_char[idx.item()] for idx in indices])\n",
    "\n",
    "# Get a batch of data\n",
    "dataiter = iter(train_loader)\n",
    "batch_x, batch_y = next(dataiter)\n",
    "\n",
    "print(f\"Input shape: {batch_x.shape}\")\n",
    "print(f\"Target shape: {batch_y.shape}\")\n",
    "\n",
    "# Print a few samples from the batch\n",
    "num_samples = 3\n",
    "for i in range(num_samples):\n",
    "    print(f\"Sample {i+1}: ------------------------------\" )\n",
    "    print(\"Input sequence :\", indices_to_text(batch_x[i], idx_to_char).replace('\\n',''))\n",
    "    print(\"Target sequence:\", indices_to_text(batch_y[i], idx_to_char).replace('\\n',''))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5486f73f",
   "metadata": {
    "id": "5486f73f"
   },
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61a52f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_vram_usage(device=\"cuda\"):\n",
    "    allocated = torch.cuda.memory_allocated(device) / (1024**2)  # in MB\n",
    "    reserved = torch.cuda.memory_reserved(device) / (1024**2)    # in MB\n",
    "    max_allocated = torch.cuda.max_memory_allocated(device) / (1024**2)  # in MB\n",
    "    print(f\"Allocated: {allocated:.2f} MB, Reserved: {reserved:.2f} MB, Max Allocated: {max_allocated:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59178a11",
   "metadata": {
    "id": "59178a11"
   },
   "outputs": [],
   "source": [
    "def validate(model, dataloader, criterion, device, epoch, step):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            output, _ = model(x)\n",
    "            loss = criterion(output.view(-1, output.size(-1)), y.view(-1))\n",
    "            losses.append((step, epoch, loss.item()))\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea5e61f5",
   "metadata": {
    "id": "ea5e61f5"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, dataloader, criterion, optimizer, device, epoch, step):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    vram_usage = []\n",
    "    scaler = torch.amp.GradScaler('cuda')\n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch}\", leave=False)\n",
    "    for batch, (x, y) in enumerate(pbar):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "            output, _ = model(x)\n",
    "            loss = criterion(output.view(-1, output.size(-1)), y.view(-1))\n",
    "            \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        step += 1\n",
    "        losses.append((step, epoch, loss.item()))\n",
    "        \n",
    "        # VRAM 사용량을 progress bar의 postfix로 업데이트\n",
    "        allocated = torch.cuda.memory_allocated(device) / (1024**2)\n",
    "        vram_usage.append(allocated)\n",
    "        pbar.set_postfix(loss=f'{loss.item():.4f}', step=step, vram=f'{allocated:.2f} MB')\n",
    "    return losses, step, vram_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbedfd61",
   "metadata": {
    "id": "cbedfd61"
   },
   "source": [
    "## Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e91075e8",
   "metadata": {
    "id": "e91075e8"
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, device, epochs):\n",
    "    all_train_losses = []\n",
    "    all_val_losses = []\n",
    "    all_vram_usages = [] # 에포크별 평균 VRAM 사용량 리스트로 변경 가능 또는 배치별 VRAM 사용량 리스트 유지\n",
    "    step = 0\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        # Training phase\n",
    "        # epoch_train_losses는 (step, epoch, loss_value) 튜플의 리스트\n",
    "        epoch_train_losses_list, step, epoch_vram_usage_list = train(model, train_loader, criterion, optimizer, device, epoch, step)\n",
    "        all_train_losses.extend(epoch_train_losses_list)\n",
    "        if epoch_vram_usage_list: # CUDA 사용 시에만 vram_usage가 채워짐\n",
    "             all_vram_usages.extend(epoch_vram_usage_list) # 배치별 VRAM 사용량 모두 저장\n",
    "\n",
    "        # 현재 에포크의 평균 훈련 로스 계산\n",
    "        avg_epoch_train_loss = 0\n",
    "        if epoch_train_losses_list:\n",
    "            avg_epoch_train_loss = np.mean([loss_tuple[2] for loss_tuple in epoch_train_losses_list])\n",
    "        \n",
    "        # Validation phase\n",
    "        # epoch_val_losses는 (step, epoch, loss_value) 튜플의 리스트\n",
    "        epoch_val_losses_list = validate(model, val_loader, criterion, device, epoch, step) # validate의 step은 해당 에포크 종료 시점의 step\n",
    "        all_val_losses.extend(epoch_val_losses_list)\n",
    "        \n",
    "        # 현재 에포크의 평균 검증 로스 계산\n",
    "        avg_epoch_val_loss = 0\n",
    "        if epoch_val_losses_list:\n",
    "            avg_epoch_val_loss = np.mean([loss_tuple[2] for loss_tuple in epoch_val_losses_list])\n",
    "            \n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        \n",
    "        avg_epoch_vram_str = \"\"\n",
    "        if epoch_vram_usage_list: # CUDA 사용 시\n",
    "            avg_epoch_vram = np.mean(epoch_vram_usage_list)\n",
    "            avg_epoch_vram_str = f', Avg VRAM: {avg_epoch_vram:.2f}MB'\n",
    "\n",
    "        print(f'Epoch {epoch}/{epochs}, Avg Train Loss: {avg_epoch_train_loss:.4f}, '\n",
    "              f'Avg Val Loss: {avg_epoch_val_loss:.4f}, Epoch Time: {epoch_time:.2f}s{avg_epoch_vram_str}')\n",
    "\n",
    "    train_losses_df = pd.DataFrame(all_train_losses, columns=['step', 'epoch', 'loss_value'])\n",
    "    val_losses_df = pd.DataFrame(all_val_losses, columns=['step', 'epoch', 'loss_value'])\n",
    "    \n",
    "    # all_vram_usages는 모든 배치의 VRAM 사용량을 담고 있으므로, 전체 평균을 계산할 수 있음\n",
    "    # overall_avg_vram = np.mean(all_vram_usages) if all_vram_usages else 0\n",
    "    # print(f\"Overall Average VRAM Usage: {overall_avg_vram:.2f} MB\")\n",
    "    \n",
    "    return model, train_losses_df, val_losses_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4535c886",
   "metadata": {
    "id": "4535c886"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, char_to_idx, idx_to_char, start_text, device, max_length=500):\n",
    "    model.eval()\n",
    "    current_text = start_text\n",
    "    hidden = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            x = torch.tensor([[char_to_idx[ch] for ch in current_text[-SEQUENCE_LENGTH:]]]).to(device)\n",
    "            output, hidden = model(x, hidden)\n",
    "            probs = torch.softmax(output[0, -1], dim=0)\n",
    "            next_char_idx = torch.multinomial(probs, 1).item()\n",
    "            next_char = idx_to_char[next_char_idx]\n",
    "            current_text += next_char\n",
    "\n",
    "    return current_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6aef1cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_comparison_dict = {}\n",
    "\n",
    "def add_loss_to_comparison(model_name, train_losses_df, val_losses_df):\n",
    "    \"\"\"\n",
    "    Adds training and validation losses from a model to the comparison dictionary.\n",
    "    \"\"\"\n",
    "    loss_comparison_dict[model_name] = {\n",
    "        'train': train_losses_df,\n",
    "        'val': val_losses_df\n",
    "    }\n",
    "\n",
    "def print_final_losses(loss_dict):\n",
    "    for model_name, losses in loss_dict.items():\n",
    "        train_df = losses['train']\n",
    "        val_df = losses['val']\n",
    "        final_train = train_df.groupby('epoch')['loss_value'].last().iloc[-1]\n",
    "        final_val = val_df.groupby('epoch')['loss_value'].last().iloc[-1]\n",
    "        print(f\"{model_name}: Final Train Loss: {final_train:.4f}, Final Val Loss: {final_val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "973b4a80",
   "metadata": {
    "id": "973b4a80"
   },
   "outputs": [],
   "source": [
    "# Function to plot loss curves\n",
    "def plot_loss(train_losses_df, val_losses_df):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    # Plot training losses\n",
    "    for epoch in train_losses_df['epoch'].unique():\n",
    "        epoch_train_losses = train_losses_df[train_losses_df['epoch'] == epoch]\n",
    "        plt.plot(epoch_train_losses['step'], epoch_train_losses['loss_value'],\n",
    "                 color='blue', alpha=0.3)\n",
    "\n",
    "    # scatter training loss at the end of each epoch\n",
    "    last_train_losses = train_losses_df.groupby('epoch').last().reset_index()\n",
    "    plt.scatter(last_train_losses['step'], last_train_losses['loss_value'],\n",
    "                color='blue')\n",
    "\n",
    "    # Plot and scatter validation loss at the end of each epoch\n",
    "    last_val_losses = val_losses_df.groupby('epoch').last().reset_index()\n",
    "    plt.plot(last_val_losses['step'], last_val_losses['loss_value'],\n",
    "             color='orange', label='Validation Loss')\n",
    "    plt.scatter(last_val_losses['step'], last_val_losses['loss_value'],\n",
    "                color='orange')\n",
    "\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Function to print final loss values\n",
    "def print_final_losses(train_losses_df, val_losses_df):\n",
    "    print(\"Final Training Loss:\", train_losses_df.groupby('epoch')['loss_value'].last().iloc[-1])\n",
    "    print(\"Final Validation Loss:\", val_losses_df.groupby('epoch')['loss_value'].last().iloc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5001e203",
   "metadata": {
    "id": "5001e203"
   },
   "outputs": [],
   "source": [
    "# Function to plot loss curves for multiple models stored in loss_comparison_dict\n",
    "def plot_loss_comparisons():\n",
    "    \"\"\"\n",
    "    Plots the training loss curves and average validation loss per epoch for multiple models added to the loss comparison dictionary.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    # Get the last model in the dictionary (for special final-point highlighting)\n",
    "    last_model_name = list(loss_comparison_dict.keys())[-1]\n",
    "\n",
    "    # Loop through each model in the loss dictionary\n",
    "    for model_name, losses in loss_comparison_dict.items():\n",
    "        train_losses_df = losses['train']\n",
    "        val_losses_df = losses['val']\n",
    "\n",
    "        # Plot training losses for each model\n",
    "        plt.plot(train_losses_df['step'], train_losses_df['loss_value'],\n",
    "                 label=f'{model_name} train', linestyle='-', alpha=0.7)\n",
    "\n",
    "        # Scatter training loss at the end of each epoch\n",
    "        last_train_losses = train_losses_df.groupby('epoch').last().reset_index()\n",
    "        plt.scatter(last_train_losses['step'], last_train_losses['loss_value'], marker='o', s=50)\n",
    "\n",
    "        # Compute average validation loss per epoch (using the last step of each epoch for x-axis)\n",
    "        avg_val_losses = val_losses_df.groupby('epoch').agg({'loss_value': 'mean', 'step': 'last'}).reset_index()\n",
    "        # Scatter the average validation loss for each epoch\n",
    "        plt.scatter(avg_val_losses['step'], avg_val_losses['loss_value'], marker='s', s=50,\n",
    "                    label=f'{model_name} val avg')\n",
    "\n",
    "        # For the last model, highlight the final training loss with a star\n",
    "        if model_name == last_model_name:\n",
    "            final_step = train_losses_df['step'].iloc[-1]\n",
    "            final_loss = train_losses_df['loss_value'].iloc[-1]\n",
    "            plt.scatter(final_step, final_loss, marker='*', s=100, color='red', zorder=5)\n",
    "\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Comparison')\n",
    "    plt.legend()  # Legend shows both training and validation average labels\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16777ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_separate_train_val(loss_dict):\n",
    "    \"\"\"\n",
    "    모델별 Training Loss와 Validation Loss를 각각 별도의 그래프로 그립니다.\n",
    "    단, Validation Loss는 에포크별 평균으로 계산합니다.\n",
    "    \"\"\"\n",
    "    # 1. Training Loss Plot (원본 그대로)\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    plt.subplot(1, 2, 1)  # 1행 2열 중 첫 번째\n",
    "    for model_name, losses in loss_dict.items():\n",
    "        train_df = losses['train']\n",
    "        steps_train = train_df['step'].values\n",
    "        loss_train = train_df['loss_value'].values\n",
    "        plt.plot(steps_train, loss_train, label=f'{model_name} Train')\n",
    "    plt.title('Training Loss Comparison')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # 2. Validation Loss Plot (에포크별 평균 처리)\n",
    "    plt.subplot(1, 2, 2)  # 1행 2열 중 두 번째\n",
    "    for model_name, losses in loss_dict.items():\n",
    "        val_df = losses['val']\n",
    "        # 에포크별 평균 loss와 마지막 step을 계산\n",
    "        val_avg = val_df.groupby('epoch').agg({'loss_value': 'mean', 'step': 'last'}).reset_index()\n",
    "        plt.plot(val_avg['step'], val_avg['loss_value'], label=f'{model_name} Val')\n",
    "    plt.title('Validation Loss (Epoch Avg) Comparison')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "396c469b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text generation using validation data\n",
    "val_sample, _ = next(iter(val_loader))\n",
    "start_text = ''.join([idx_to_char[idx.item()] for idx in val_sample[0][:SEQUENCE_LENGTH]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98b72703",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1732704396861,
     "user": {
      "displayName": "Hugman Sangkeun Jung",
      "userId": "08689291704194029524"
     },
     "user_tz": -540
    },
    "id": "98b72703",
    "outputId": "ece1620a-8690-4750-97c1-ada645d59a08"
   },
   "outputs": [],
   "source": [
    "# Decoder Input/Output Example\n",
    "sample_input, _ = next(iter(val_loader))\n",
    "sample_input = sample_input[0].unsqueeze(0).to(device)  # Add batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2d8a352",
   "metadata": {
    "id": "a2d8a352"
   },
   "outputs": [],
   "source": [
    "def generate_text_attention(model, char_to_idx, idx_to_char, start_text, device, max_length=500):\n",
    "    model.eval()\n",
    "    current_text = start_text\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            # Convert the last sequence of characters to indices and feed it to the model\n",
    "            x = torch.tensor([[char_to_idx[ch] for ch in current_text[-SEQUENCE_LENGTH:]]]).to(device)\n",
    "            output = model(x)[0]  # No hidden state needed for attention-based models\n",
    "            probs = torch.softmax(output[0, -1], dim=0)\n",
    "            next_char_idx = torch.multinomial(probs, 1).item()\n",
    "            next_char = idx_to_char[next_char_idx]\n",
    "            current_text += next_char\n",
    "\n",
    "    return current_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32c936b1",
   "metadata": {
    "id": "32c936b1"
   },
   "outputs": [],
   "source": [
    "def train_and_test(model_desc, model, start_text):\n",
    "    # Initialize the model\n",
    "    model = model.to(device)\n",
    "    # Use the same optimizer and criterion\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, fused=True)\n",
    "    \n",
    "    # Train the model\n",
    "    trained_model, train_losses_df, val_losses_df = train_model(\n",
    "        model, train_loader, val_loader, criterion, optimizer, device, EPOCHS\n",
    "    )\n",
    "\n",
    "    # Generate text\n",
    "    generated_text = generate_text_attention(trained_model, char_to_idx, idx_to_char, start_text, device)\n",
    "    print(f\"Generated text [{start_text}]:\")\n",
    "    print(\"-\"*50)\n",
    "    print(generated_text)\n",
    "    \n",
    "    add_loss_to_comparison(model_desc, train_losses_df, val_losses_df)\n",
    "\n",
    "    # Plot loss comparisons including this model\n",
    "    plot_loss_comparisons()\n",
    "    \n",
    "    plot_separate_train_val(loss_comparison_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228e462d",
   "metadata": {},
   "source": [
    "## Restructure minGRU with Triton\n",
    "- 기존 모델 구조는 torch.compile과 호환되기 어려운 구조이므로, 이전에 구현했던 구조 중 하나인 llama like structure를 이용\n",
    "- triton 병렬 스캔 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1401ae92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Triton 라이브러리 임포트\n",
    "try:\n",
    "    import triton\n",
    "    import triton.language as tl\n",
    "    HAS_TRITON = True\n",
    "except ImportError:\n",
    "    HAS_TRITON = False\n",
    "    print(\"Triton not found. Please install it to use the optimized GRU implementation.\")\n",
    "\n",
    "@triton.jit\n",
    "def _softplus(x):\n",
    "    \"\"\"\n",
    "    수치적으로 안정적인 softplus 함수를 Triton으로 구현합니다.\n",
    "    softplus(x) = log(1 + exp(x))\n",
    "    \"\"\"\n",
    "    return tl.maximum(x, 0.) + tl.log(1. + tl.exp(-tl.abs(x)))\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def _log_add_exp(a, b):\n",
    "    \"\"\" \n",
    "    수치적으로 안정적인 log(exp(a) + exp(b)) 계산 \n",
    "    \"\"\"\n",
    "    return tl.maximum(a, b) + tl.log(1. + tl.exp(-tl.abs(a - b)))\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def _log_g(x):\n",
    "    \"\"\" \n",
    "    Triton 커널 내에서 사용할 log_g 함수 \n",
    "    \"\"\"\n",
    "    safe_x = x + 0.5 + 1e-9\n",
    "    return tl.where(x >= 0, tl.log(safe_x), -_softplus(-x))\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def _gru_scan_op(log_a1, log_b1, log_a2, log_b2):\n",
    "    \"\"\"\n",
    "    GRU 순환 관계를 위한 결합 스캔 연산자입니다.\n",
    "    \"\"\"\n",
    "    new_log_a = log_a1 + log_a2\n",
    "    new_log_b = _log_add_exp(log_a2 + log_b1, log_b2)\n",
    "    return new_log_a, new_log_b\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def _parallel_scan_gru_kernel(\n",
    "    # --- 입력/출력 텐서 포인터 ---\n",
    "    h_candidate_input_ptr,\n",
    "    logits_z_ptr,\n",
    "    output_ptr,\n",
    "    # --- 텐서 차원 및 스트라이드 ---\n",
    "    B, L, H,\n",
    "    stride_h_cand_b, stride_h_cand_l, stride_h_cand_h,\n",
    "    stride_logits_z_b, stride_logits_z_l, stride_logits_z_h,\n",
    "    stride_out_b, stride_out_l, stride_out_h,\n",
    "    # --- 상수 ---\n",
    "    epsilon: float,\n",
    "    # --- Triton 메타 파라미터 ---\n",
    "    BLOCK_SIZE_L: tl.constexpr, \n",
    "):\n",
    "    # --- 프로그램 ID 계산 ---\n",
    "    pid = tl.program_id(axis=0)\n",
    "    b_idx = pid // H\n",
    "    h_idx = pid % H\n",
    "\n",
    "    # --- 시퀀스(L) 차원에 대한 오프셋 생성 ---\n",
    "    l_offsets = tl.arange(0, BLOCK_SIZE_L)\n",
    "    mask = l_offsets < L\n",
    "\n",
    "    # --- 처리할 전체 시퀀스의 시작 포인터 계산 ---\n",
    "    h_cand_seq_ptr = h_candidate_input_ptr + (b_idx * stride_h_cand_b + h_idx * stride_h_cand_h + l_offsets * stride_h_cand_l)\n",
    "    logits_z_seq_ptr = logits_z_ptr + (b_idx * stride_logits_z_b + h_idx * stride_logits_z_h + l_offsets * stride_logits_z_l)\n",
    "    \n",
    "    # --- 전역 메모리에서 전체 시퀀스 데이터 로드 ---\n",
    "    input_dtype = h_candidate_input_ptr.dtype.element_ty\n",
    "    h_cand_seq = tl.load(h_cand_seq_ptr, mask=mask, other=0.0).to(tl.float32)\n",
    "    logits_z_seq = tl.load(logits_z_seq_ptr, mask=mask, other=0.0).to(tl.float32)\n",
    "\n",
    "    # --- 1. 모든 타임스텝에 대한 log_a, log_b 계산 ---\n",
    "    log_a_seq = -_softplus(logits_z_seq)\n",
    "    log_b_seq = -_softplus(-logits_z_seq) + _log_g(h_cand_seq)\n",
    "\n",
    "    # --- 2. 병렬 스캔 수행 ---\n",
    "    scanned_log_a, scanned_log_b = tl.associative_scan((log_a_seq, log_b_seq), axis=0, combine_fn=_gru_scan_op)\n",
    "\n",
    "    # --- 3. 초기 은닉 상태(h0) 적용 ---\n",
    "    log_h0 = tl.log(epsilon)\n",
    "    log_hidden_states = _log_add_exp(scanned_log_a + log_h0, scanned_log_b)\n",
    "\n",
    "    # --- 4. 최종 결과 계산 및 저장 ---\n",
    "    # ❗️ NaN 방지를 위한 클램핑: tl.exp에 들어가기 전, 너무 작은 값(-inf) 방지\n",
    "    # torch.finfo(dtype).min 값과 유사한 역할을 하는 안전한 최소값 설정\n",
    "    MIN_LOG_VAL = -30.0 \n",
    "    log_hidden_states = tl.maximum(log_hidden_states, MIN_LOG_VAL)\n",
    "    \n",
    "    output = tl.exp(log_hidden_states)\n",
    "    output_seq_ptr = output_ptr + (b_idx * stride_out_b + h_idx * stride_out_h + l_offsets * stride_out_l)\n",
    "    tl.store(output_seq_ptr, output.to(input_dtype), mask=mask)\n",
    "\n",
    "\n",
    "def _parallel_log_min_gru_triton(\n",
    "    h_candidate_input: torch.Tensor,\n",
    "    logits_z: torch.Tensor,\n",
    "    epsilon: float\n",
    ") -> torch.Tensor:\n",
    "    \"\"\" Triton 커널을 실행하기 위한 Python 래퍼 함수 \"\"\"\n",
    "    B, L, H = h_candidate_input.shape\n",
    "    device = h_candidate_input.device\n",
    "    \n",
    "    output = torch.empty((B, L, H), device=device, dtype=h_candidate_input.dtype) \n",
    "    \n",
    "    # BLOCK_SIZE_L은 L보다 크거나 같은 2의 거듭제곱이어야 함\n",
    "    BLOCK_SIZE_L = triton.next_power_of_2(L)\n",
    "\n",
    "    grid = (B * H,)\n",
    "\n",
    "    _parallel_scan_gru_kernel[grid](\n",
    "        h_candidate_input, logits_z, output,\n",
    "        B, L, H,\n",
    "        h_candidate_input.stride(0), h_candidate_input.stride(1), h_candidate_input.stride(2),\n",
    "        logits_z.stride(0), logits_z.stride(1), logits_z.stride(2),\n",
    "        output.stride(0), output.stride(1), output.stride(2),\n",
    "        epsilon,\n",
    "        BLOCK_SIZE_L=BLOCK_SIZE_L,\n",
    "    )\n",
    "    return output\n",
    "\n",
    "\n",
    "class ParallelLogMinGRU(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, expansion_factor: float = 1.0, epsilon: float = 1e-7):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.expansion_factor = expansion_factor\n",
    "        self.internal_expanded_dim = int(hidden_size * self.expansion_factor)\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        self.use_triton = HAS_TRITON\n",
    "\n",
    "        self.to_hidden_and_gate = nn.Linear(input_size, self.internal_expanded_dim * 2)\n",
    "        if self.expansion_factor != 1.0:\n",
    "            self.to_out = nn.Linear(self.internal_expanded_dim, hidden_size)\n",
    "        else:\n",
    "            self.to_out = nn.Identity()\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'to_hidden_and_gate' in name or 'to_out' in name:\n",
    "                if 'weight' in name:\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "                elif 'bias' in name and param is not None:\n",
    "                    nn.init.zeros_(param)\n",
    "                    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, L, _ = x.size()\n",
    "        hidden_and_gate = self.to_hidden_and_gate(x)\n",
    "        h_candidate_input_expanded, logits_z_expanded = hidden_and_gate.chunk(2, dim=-1)\n",
    "\n",
    "        if self.use_triton and x.is_cuda and L > 0:\n",
    "            h_expanded_scan_out = _parallel_log_min_gru_triton(\n",
    "                h_candidate_input_expanded, \n",
    "                logits_z_expanded, \n",
    "                self.epsilon\n",
    "            )\n",
    "        else:\n",
    "            if L == 0:\n",
    "                return torch.empty(B, 0, self.hidden_size, device=x.device, dtype=x.dtype)\n",
    "            \n",
    "            log_A = F.logsigmoid(-logits_z_expanded)\n",
    "            log_Z_expanded = F.logsigmoid(logits_z_expanded)\n",
    "            log_h_candidate_contrib_expanded = self._original_log_g(h_candidate_input_expanded)\n",
    "            log_B = log_Z_expanded + log_h_candidate_contrib_expanded\n",
    "\n",
    "            log_h0_val = torch.full(\n",
    "                (B, 1, self.internal_expanded_dim),\n",
    "                math.log(self.epsilon),\n",
    "                device=x.device,\n",
    "                dtype=x.dtype\n",
    "            )\n",
    "            log_vals = torch.cat([log_h0_val, log_B], dim=1)\n",
    "            h_expanded_scan_out = self._original_parallel_scan_log(log_A, log_vals)\n",
    "\n",
    "        output = self.to_out(h_expanded_scan_out)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def _original_log_g(x: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.where(x >= 0, (F.relu(x) + 0.5).log(), -F.softplus(-x))\n",
    "\n",
    "    @staticmethod\n",
    "    def _original_parallel_scan_log(log_coeffs: torch.Tensor, log_values: torch.Tensor) -> torch.Tensor:\n",
    "        log_proda_coeffs_prefix = F.pad(torch.cumsum(log_coeffs, dim=1), (0, 0, 1, 0), value=0.0)\n",
    "        terms_for_logcumsumexp = log_values - log_proda_coeffs_prefix\n",
    "        log_sum_exp_terms = torch.logcumsumexp(terms_for_logcumsumexp, dim=1)\n",
    "        log_hidden_states = log_proda_coeffs_prefix + log_sum_exp_terms\n",
    "        output_hidden_states = torch.exp(log_hidden_states[:, 1:, :])\n",
    "        return output_hidden_states\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-8):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.gamma = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        input_dtype = x.dtype\n",
    "        variance = x.to(torch.float32).pow(2).mean(dim=-1, keepdim=True)\n",
    "        return (x * torch.rsqrt(variance + self.eps) * self.gamma).to(input_dtype)\n",
    "\n",
    "\n",
    "class CausalDepthWiseConv1d(nn.Module):\n",
    "    def __init__(self, dim: int, kernel_size: int):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(dim, dim, kernel_size=kernel_size, groups=dim),\n",
    "            nn.Conv1d(dim, dim, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x_orig_shape = x.shape\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(0)\n",
    "        x_transposed = x.transpose(1, 2)\n",
    "        x_padded = F.pad(x_transposed, (self.kernel_size - 1, 0), value=0.0)\n",
    "        x_conv_out = self.net(x_padded)\n",
    "        x_restored = x_conv_out.transpose(1, 2)\n",
    "        if len(x_orig_shape) == 2 and x_restored.shape[0] == 1:\n",
    "            x_restored = x_restored.squeeze(0)\n",
    "        return x_restored\n",
    "\n",
    "\n",
    "class SwiGLUFFN(nn.Module):\n",
    "    def __init__(self, dim: int, expansion_factor: float = 4.0, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        ffn_hidden_dim = int(dim * expansion_factor)\n",
    "\n",
    "        self.w1_w3 = nn.Linear(dim, ffn_hidden_dim * 2, bias=False)\n",
    "        self.w2 = nn.Linear(ffn_hidden_dim, dim, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x_proj = self.w1_w3(x)\n",
    "        x1, x3_gate = x_proj.chunk(2, dim=-1)\n",
    "        hidden_states = F.silu(x1) * x3_gate\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        return self.w2(hidden_states)\n",
    "\n",
    "\n",
    "class minGRULM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        embedding_dim: int,\n",
    "        hidden_dim: int,\n",
    "        num_layers: int = 2,\n",
    "        dropout: float = 0.1,\n",
    "        expansion_factor_gru: float = 1.0,\n",
    "        epsilon_gru: float = 1e-7,\n",
    "        rms_norm_eps: float = 1e-8,\n",
    "        ffn_expansion_factor: float = 2.0,\n",
    "        conv_kernel_size: int = 3\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.input_projection = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.conv = CausalDepthWiseConv1d(hidden_dim, conv_kernel_size)\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            ffn_layer = SwiGLUFFN(\n",
    "                dim=hidden_dim,\n",
    "                expansion_factor=ffn_expansion_factor,\n",
    "                dropout=dropout\n",
    "            )\n",
    "\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                RMSNorm(hidden_dim, eps=rms_norm_eps),\n",
    "                ParallelLogMinGRU( # <--- 최적화된 모듈이 여기서 사용됩니다.\n",
    "                    input_size=hidden_dim,\n",
    "                    hidden_size=hidden_dim,\n",
    "                    expansion_factor=expansion_factor_gru,\n",
    "                    epsilon=epsilon_gru\n",
    "                ),\n",
    "                RMSNorm(hidden_dim, eps=rms_norm_eps),\n",
    "                ffn_layer,\n",
    "                nn.Dropout(dropout) if dropout > 0. else nn.Identity()\n",
    "            ]))\n",
    "\n",
    "        self.final_norm = RMSNorm(hidden_dim, eps=rms_norm_eps)\n",
    "        self.to_vocab = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        h = self.embedding(x)\n",
    "        h = self.input_projection(h)\n",
    "        h = self.conv(h)\n",
    "\n",
    "        for block_idx, block_modules in enumerate(self.layers):\n",
    "            norm_gru_input, gru_cell, norm_ff_input, ffn_module, dropout_block_output = block_modules\n",
    "\n",
    "            h_gru_residual_source = h\n",
    "            h_normed_for_gru = norm_gru_input(h)\n",
    "            h_gru_out = gru_cell(h_normed_for_gru)\n",
    "            h = h_gru_residual_source + h_gru_out\n",
    "\n",
    "            h_ff_residual_source = h\n",
    "            h_normed_for_ff = norm_ff_input(h)\n",
    "            h_ff_out = ffn_module(h_normed_for_ff)\n",
    "            h = h_ff_residual_source + h_ff_out\n",
    "            \n",
    "            h = dropout_block_output(h)\n",
    "\n",
    "        h = self.final_norm(h)\n",
    "        logits = self.to_vocab(h)\n",
    "        return logits, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3f48538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Model Initialization\n",
    "# mingru2 = minGRULM(vocab_size, EMBEDDING_DIM, int(HIDDEN_DIM), NUM_LAYERS).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4deb080b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchinfo import summary\n",
    "\n",
    "# summary(mingru2, input_size=(BATCH_SIZE, SEQUENCE_LENGTH), dtypes=[torch.long])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ec215016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_and_test(\"minGRU_restructured\", mingru2, start_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5993e0",
   "metadata": {},
   "source": [
    "## Compiled minGRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2beb18c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fbd175c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test2(model_desc, model, start_text):\n",
    "    # Initialize the model\n",
    "    model = model.to(device)\n",
    "    # 다양한 컴파일 모드:\n",
    "        # \"default\": 좋은 성능과 컴파일 시간의 균형\n",
    "        # \"reduce-overhead\": 컴파일 오버헤드 감소 (작은 모델에 유리)\n",
    "        # \"max-autotune\": 가장 긴 컴파일 시간, 가장 높은 성능 목표\n",
    "    model = torch.compile(model, mode=\"reduce-overhead\")\n",
    "    # Use the same optimizer and criterion\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, fused=True)\n",
    "    \n",
    "    # Train the model\n",
    "    trained_model, train_losses_df, val_losses_df = train_model(\n",
    "        model, train_loader, val_loader, criterion, optimizer, device, EPOCHS\n",
    "    )\n",
    "\n",
    "    # Generate text\n",
    "    generated_text = generate_text_attention(trained_model, char_to_idx, idx_to_char, start_text, device)\n",
    "    print(f\"Generated text [{start_text}]:\")\n",
    "    print(\"-\"*50)\n",
    "    print(generated_text)\n",
    "    \n",
    "    add_loss_to_comparison(model_desc, train_losses_df, val_losses_df)\n",
    "\n",
    "    # Plot loss comparisons including this model\n",
    "    plot_loss_comparisons()\n",
    "    \n",
    "    plot_separate_train_val(loss_comparison_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "27931e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model Initialization\n",
    "mingru3 = minGRULM(vocab_size, EMBEDDING_DIM, int(HIDDEN_DIM), NUM_LAYERS).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "52bac9ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "minGRULM                                 [256, 1024, 62]           --\n",
       "├─Embedding: 1-1                         [256, 1024, 64]           3,968\n",
       "├─Linear: 1-2                            [256, 1024, 128]          8,320\n",
       "├─CausalDepthWiseConv1d: 1-3             [256, 1024, 128]          --\n",
       "│    └─Sequential: 2-1                   [256, 128, 1024]          --\n",
       "│    │    └─Conv1d: 3-1                  [256, 128, 1024]          512\n",
       "│    │    └─Conv1d: 3-2                  [256, 128, 1024]          16,512\n",
       "├─ModuleList: 1-4                        --                        --\n",
       "│    └─ModuleList: 2-2                   --                        --\n",
       "│    │    └─RMSNorm: 3-3                 [256, 1024, 128]          128\n",
       "│    │    └─ParallelLogMinGRU: 3-4       [256, 1024, 128]          33,024\n",
       "│    │    └─RMSNorm: 3-5                 [256, 1024, 128]          128\n",
       "│    │    └─SwiGLUFFN: 3-6               [256, 1024, 128]          98,304\n",
       "│    │    └─Dropout: 3-7                 [256, 1024, 128]          --\n",
       "│    └─ModuleList: 2-3                   --                        --\n",
       "│    │    └─RMSNorm: 3-8                 [256, 1024, 128]          128\n",
       "│    │    └─ParallelLogMinGRU: 3-9       [256, 1024, 128]          33,024\n",
       "│    │    └─RMSNorm: 3-10                [256, 1024, 128]          128\n",
       "│    │    └─SwiGLUFFN: 3-11              [256, 1024, 128]          98,304\n",
       "│    │    └─Dropout: 3-12                [256, 1024, 128]          --\n",
       "├─RMSNorm: 1-5                           [256, 1024, 128]          128\n",
       "├─Linear: 1-6                            [256, 1024, 62]           7,998\n",
       "==========================================================================================\n",
       "Total params: 300,606\n",
       "Trainable params: 300,606\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 4.54\n",
       "==========================================================================================\n",
       "Input size (MB): 2.10\n",
       "Forward/backward pass size (MB): 6169.82\n",
       "Params size (MB): 1.20\n",
       "Estimated Total Size (MB): 6173.12\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(mingru3, input_size=(BATCH_SIZE, SEQUENCE_LENGTH), dtypes=[torch.long])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be15695d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_and_test2\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mminGRU_compiled\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmingru3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_text\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[27], line 14\u001b[0m, in \u001b[0;36mtrain_and_test2\u001b[0;34m(model_desc, model, start_text)\u001b[0m\n\u001b[1;32m     11\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mLEARNING_RATE, fused\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m trained_model, train_losses_df, val_losses_df \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Generate text\u001b[39;00m\n\u001b[1;32m     19\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m generate_text_attention(trained_model, char_to_idx, idx_to_char, start_text, device)\n",
      "Cell \u001b[0;32mIn[12], line 12\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, device, epochs)\u001b[0m\n\u001b[1;32m      8\u001b[0m epoch_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Training phase\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# epoch_train_losses는 (step, epoch, loss_value) 튜플의 리스트\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m epoch_train_losses_list, step, epoch_vram_usage_list \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m all_train_losses\u001b[38;5;241m.\u001b[39mextend(epoch_train_losses_list)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch_vram_usage_list: \u001b[38;5;66;03m# CUDA 사용 시에만 vram_usage가 채워짐\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 14\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, criterion, optimizer, device, epoch, step)\u001b[0m\n\u001b[1;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16):\n\u001b[0;32m---> 14\u001b[0m     output, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, output\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), y\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     17\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m saved_dynamic_layer_stack_depth \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    570\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mget_dynamic_layer_stack_depth()\n\u001b[1;32m    571\u001b[0m )\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;66;03m# Restore the dynamic layer stack depth if necessary.\u001b[39;00m\n\u001b[1;32m    577\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mpop_dynamic_layer_stack_and_undo_to_depth(\n\u001b[1;32m    578\u001b[0m         saved_dynamic_layer_stack_depth\n\u001b[1;32m    579\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[22], line 302\u001b[0m, in \u001b[0;36mminGRULM.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_norm \u001b[38;5;241m=\u001b[39m RMSNorm(hidden_dim, eps\u001b[38;5;241m=\u001b[39mrms_norm_eps)\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_vocab \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(hidden_dim, vocab_size)\n\u001b[0;32m--> 302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    303\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x)\n\u001b[1;32m    304\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_projection(h)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:745\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    741\u001b[0m prior_skip_guard_eval_unsafe \u001b[38;5;241m=\u001b[39m set_skip_guard_eval_unsafe(\n\u001b[1;32m    742\u001b[0m     _is_skip_guard_eval_unsafe_stance()\n\u001b[1;32m    743\u001b[0m )\n\u001b[1;32m    744\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    747\u001b[0m     _maybe_set_eval_frame(prior)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py:1184\u001b[0m, in \u001b[0;36maot_module_simplified.<locals>.forward\u001b[0;34m(*runtime_args)\u001b[0m\n\u001b[1;32m   1182\u001b[0m full_args\u001b[38;5;241m.\u001b[39mextend(params_flat)\n\u001b[1;32m   1183\u001b[0m full_args\u001b[38;5;241m.\u001b[39mextend(runtime_args)\n\u001b[0;32m-> 1184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiled_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:310\u001b[0m, in \u001b[0;36m_create_runtime_wrapper.<locals>.runtime_wrapper\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;66;03m# It's possible to have trace_joint inside user specified with no_grad() region,\u001b[39;00m\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;66;03m# if there is a nested with enable_grad(), that forces some outputs to require gradients.\u001b[39;00m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;66;03m# Therefore, we unconditionally turn on enable_grad() for compiled_fn execution.\u001b[39;00m\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39m_force_original_view_tracking(\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    309\u001b[0m     ), torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m--> 310\u001b[0m         all_outs \u001b[38;5;241m=\u001b[39m \u001b[43mcall_func_at_runtime_with_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompiled_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_amp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteal_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;66;03m# When we have an inference graph, we run with grad disabled.\u001b[39;00m\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;66;03m# It's possible to get an inference graph with inputs that require grad,\u001b[39;00m\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;66;03m# in which case we want to make sure autograd is disabled\u001b[39;00m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;66;03m# (since e.g., inductor will generate aten.addmm.out calls which autograd will complain on)\u001b[39;00m\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;66;03m# NOTE: We use _set_grad_enabled directly to reduce runtime overhead\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     grad_enabled \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_grad_enabled()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/utils.py:126\u001b[0m, in \u001b[0;36mcall_func_at_runtime_with_args\u001b[0;34m(f, args, steal_args, disable_amp)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_boxed_call\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 126\u001b[0m         out \u001b[38;5;241m=\u001b[39m normalize_as_list(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;66;03m# TODO: Please remove soon\u001b[39;00m\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[39;00m\n\u001b[1;32m    130\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    131\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour compiler for AOTAutograd is returning a function that doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt take boxed arguments. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    132\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    133\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    134\u001b[0m         )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/utils.py:100\u001b[0m, in \u001b[0;36mmake_boxed_func.<locals>.g\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mg\u001b[39m(args):\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py:575\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    574\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    583\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:1585\u001b[0m, in \u001b[0;36mAOTDispatchAutograd.post_compile.<locals>.CompiledFunction.forward\u001b[0;34m(ctx, *deduped_flat_tensor_args)\u001b[0m\n\u001b[1;32m   1576\u001b[0m     ctx\u001b[38;5;241m.\u001b[39m_compiled_autograd_backward_state \u001b[38;5;241m=\u001b[39m bw_state\n\u001b[1;32m   1578\u001b[0m \u001b[38;5;66;03m# There is a pretty complicated calling convention around what the compiled fw returns.\u001b[39;00m\n\u001b[1;32m   1579\u001b[0m \u001b[38;5;66;03m# The full list of outputs and their relative order is:\u001b[39;00m\n\u001b[1;32m   1580\u001b[0m \u001b[38;5;66;03m# (*tokens, *mutated_inputs, *fw_outs, *fw_intermediate_bases, *saved_tensors, *saved_symints)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1583\u001b[0m \u001b[38;5;66;03m# - Note that donated buffer logic requires (*saved_tensors, *saved_symints) showing up last\u001b[39;00m\n\u001b[1;32m   1584\u001b[0m \u001b[38;5;66;03m#   in the fw output order.\u001b[39;00m\n\u001b[0;32m-> 1585\u001b[0m fw_outs \u001b[38;5;241m=\u001b[39m \u001b[43mcall_func_at_runtime_with_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1586\u001b[0m \u001b[43m    \u001b[49m\u001b[43mCompiledFunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompiled_fw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1587\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1588\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_amp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1589\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1591\u001b[0m num_outputs \u001b[38;5;241m=\u001b[39m CompiledFunction\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mnum_outputs\n\u001b[1;32m   1592\u001b[0m num_outputs_aliased \u001b[38;5;241m=\u001b[39m CompiledFunction\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mnum_outputs_aliased\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/utils.py:126\u001b[0m, in \u001b[0;36mcall_func_at_runtime_with_args\u001b[0;34m(f, args, steal_args, disable_amp)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_boxed_call\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 126\u001b[0m         out \u001b[38;5;241m=\u001b[39m normalize_as_list(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;66;03m# TODO: Please remove soon\u001b[39;00m\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[39;00m\n\u001b[1;32m    130\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    131\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour compiler for AOTAutograd is returning a function that doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt take boxed arguments. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    132\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    133\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    134\u001b[0m         )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:490\u001b[0m, in \u001b[0;36mFunctionalizedRngRuntimeWrapper.post_compile.<locals>.wrapper\u001b[0;34m(runtime_args)\u001b[0m\n\u001b[1;32m    483\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_functionalized_rng_runtime_epilogue(\n\u001b[1;32m    484\u001b[0m         runtime_metadata,\n\u001b[1;32m    485\u001b[0m         out,\n\u001b[1;32m    486\u001b[0m         \u001b[38;5;66;03m# TODO: this won't be right for the backward when we convert the call_compiled_backward to use the wrapper\u001b[39;00m\n\u001b[1;32m    487\u001b[0m         runtime_metadata\u001b[38;5;241m.\u001b[39mnum_forward_returns,\n\u001b[1;32m    488\u001b[0m     )\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m--> 490\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiled_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mruntime_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:672\u001b[0m, in \u001b[0;36mEffectTokensWrapper.post_compile.<locals>.inner_fn\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    669\u001b[0m     args \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m([\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m num_tokens), \u001b[38;5;241m*\u001b[39margs]\n\u001b[1;32m    670\u001b[0m     old_args\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m--> 672\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[43mcompiled_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;66;03m# Inductor cache DummyModule can return None\u001b[39;00m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m outs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/output_code.py:466\u001b[0m, in \u001b[0;36mCompiledFxGraph.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_callable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 466\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    468\u001b[0m     AutotuneCacheBundler\u001b[38;5;241m.\u001b[39mend_compile()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py:1208\u001b[0m, in \u001b[0;36mcudagraphify.<locals>.run\u001b[0;34m(new_inputs)\u001b[0m\n\u001b[1;32m   1203\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_utils\u001b[38;5;241m.\u001b[39mdynamo_timed(\n\u001b[1;32m   1204\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcudagraphify\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1205\u001b[0m         log_pt2_compile_event\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1206\u001b[0m     ), dynamo_utils\u001b[38;5;241m.\u001b[39mpreserve_rng_state():\n\u001b[1;32m   1207\u001b[0m         compiled_fn \u001b[38;5;241m=\u001b[39m cudagraphify_fn(model, new_inputs, static_input_idxs)\n\u001b[0;32m-> 1208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiled_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/cudagraph_trees.py:398\u001b[0m, in \u001b[0;36mcudagraphify_impl.<locals>.deferred_cudagraphify\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m    395\u001b[0m new_static_input_idxs \u001b[38;5;241m=\u001b[39m remove_unaligned_input_idxs(inputs, static_input_idxs)\n\u001b[1;32m    396\u001b[0m copy_misaligned_inputs(inputs, check_input_idxs)\n\u001b[0;32m--> 398\u001b[0m fn, out \u001b[38;5;241m=\u001b[39m \u001b[43mcudagraphify\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_static_input_idxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    399\u001b[0m fn \u001b[38;5;241m=\u001b[39m align_inputs_from_check_idxs(fn, inputs_to_check\u001b[38;5;241m=\u001b[39mcheck_input_idxs)\n\u001b[1;32m    400\u001b[0m fn_cache[int_key] \u001b[38;5;241m=\u001b[39m fn\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/cudagraph_trees.py:428\u001b[0m, in \u001b[0;36mcudagraphify\u001b[0;34m(model, inputs, static_input_idxs, device_index, is_backward, is_inference, stack_traces, constants, placeholders, mutated_input_idxs)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_backward \u001b[38;5;129;01mand\u001b[39;00m is_inference)\n\u001b[1;32m    422\u001b[0m mode \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    423\u001b[0m     CompilationMode\u001b[38;5;241m.\u001b[39mBACKWARD\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_backward\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (CompilationMode\u001b[38;5;241m.\u001b[39mINFERENCE \u001b[38;5;28;01mif\u001b[39;00m is_inference \u001b[38;5;28;01melse\u001b[39;00m CompilationMode\u001b[38;5;241m.\u001b[39mFORWARD)\n\u001b[1;32m    426\u001b[0m )\n\u001b[0;32m--> 428\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmanager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstatic_input_idxs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstack_traces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconstants\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplaceholders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmutated_input_idxs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/cudagraph_trees.py:2253\u001b[0m, in \u001b[0;36mCUDAGraphTreeManager.add_function\u001b[0;34m(self, model, inputs, static_input_idxs, stack_traces, mode, constants, placeholders, mutated_input_idxs)\u001b[0m\n\u001b[1;32m   2251\u001b[0m \u001b[38;5;66;03m# container needs to set clean up when fn dies\u001b[39;00m\n\u001b[1;32m   2252\u001b[0m get_container(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_index)\u001b[38;5;241m.\u001b[39madd_strong_reference(fn)\n\u001b[0;32m-> 2253\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn, \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/cudagraph_trees.py:1947\u001b[0m, in \u001b[0;36mCUDAGraphTreeManager.run\u001b[0;34m(self, new_inputs, function_id)\u001b[0m\n\u001b[1;32m   1945\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning CUDAGraph after shutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1946\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid_to_mode[function_id]\n\u001b[0;32m-> 1947\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1949\u001b[0m \u001b[38;5;66;03m# The forwards are only pending following invocation, not before\u001b[39;00m\n\u001b[1;32m   1950\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m CompilationMode\u001b[38;5;241m.\u001b[39mFORWARD:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/cudagraph_trees.py:2055\u001b[0m, in \u001b[0;36mCUDAGraphTreeManager._run\u001b[0;34m(self, new_inputs, function_id)\u001b[0m\n\u001b[1;32m   2049\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_checkpoint_execution_state_in_allocator()\n\u001b[1;32m   2051\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\n\u001b[1;32m   2052\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDAGraphTreeManager.run_eager\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2053\u001b[0m         log_pt2_compile_event\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   2054\u001b[0m     ):\n\u001b[0;32m-> 2055\u001b[0m         out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_eager\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2057\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m   2059\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_node, CUDAWarmupNode)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/cudagraph_trees.py:2219\u001b[0m, in \u001b[0;36mCUDAGraphTreeManager.run_eager\u001b[0;34m(self, new_inputs, function_id)\u001b[0m\n\u001b[1;32m   2217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath_state \u001b[38;5;241m=\u001b[39m ExecutionState\u001b[38;5;241m.\u001b[39mWARMUP\n\u001b[1;32m   2218\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_generation()\n\u001b[0;32m-> 2219\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/cudagraph_trees.py:643\u001b[0m, in \u001b[0;36mCUDAWarmupNode.run\u001b[0;34m(self, new_inputs)\u001b[0m\n\u001b[1;32m    636\u001b[0m     check_memory_pool(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_index, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcuda_graphs_pool, refs)\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice(\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_index\n\u001b[1;32m    640\u001b[0m ), disable_conv_cache_emptying(), clear_cublas_manager(), _use_cuda_memory_pool_manager(\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_index, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcuda_graphs_pool, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\n\u001b[1;32m    642\u001b[0m ), get_history_recording():\n\u001b[0;32m--> 643\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrapped_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;66;03m# We need to know which outputs are allocated within the cudagraph pool\u001b[39;00m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;66;03m# so that we can deallocate them at the beginning of the next cudagraph step,\u001b[39;00m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;66;03m# and set their access to error.\u001b[39;00m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;66;03m# We use a weakref to the inputs storage, in case a block which was previously\u001b[39;00m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;66;03m# allocated to the general caching allocator pool gets reallocated to a private pool.\u001b[39;00m\n\u001b[1;32m    651\u001b[0m non_cudagraph_inps_storage_ptrs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/tmp/torchinductor_root/2g/c2goqbra2wwpo3ehgrbyz65da5zq4pzxvdxviiqmfcb5fbgclh6g.py:1377\u001b[0m, in \u001b[0;36mcall\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m   1375\u001b[0m triton_poi_fused_add_11_ynumel \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m\u001b[38;5;241m*\u001b[39ms0\n\u001b[1;32m   1376\u001b[0m stream0 \u001b[38;5;241m=\u001b[39m get_raw_stream(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m-> 1377\u001b[0m \u001b[43mtriton_poi_fused_add_11\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf20\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf10\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtriton_poi_fused_add_11_ynumel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtriton_poi_fused_add_11_ynumel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1378\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m buf10\n\u001b[1;32m   1379\u001b[0m buf21 \u001b[38;5;241m=\u001b[39m empty_strided_cuda((s0, \u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m1\u001b[39m), (\u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1024\u001b[39m\u001b[38;5;241m*\u001b[39ms0), torch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py:1034\u001b[0m, in \u001b[0;36mCachingAutotuner.run\u001b[0;34m(self, grid, stream, benchmark_run, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecompile_time_taken_ns \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime_ns() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m   1033\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlaunchers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m-> 1034\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautotune_to_one_config\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlaunchers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mconfig, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_by_coordesc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m ) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minductor_meta\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoordinate_descent_tuning\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlaunchers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoordinate_descent_tuning(\n\u001b[1;32m   1041\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlaunchers[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39margs, grid\u001b[38;5;241m=\u001b[39mgrid, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m   1042\u001b[0m         )\n\u001b[1;32m   1043\u001b[0m     ]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py:911\u001b[0m, in \u001b[0;36mCachingAutotuner.autotune_to_one_config\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Do the actual autotuning\"\"\"\u001b[39;00m\n\u001b[1;32m    910\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime_ns()\n\u001b[0;32m--> 911\u001b[0m timings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbenchmark_all_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    912\u001b[0m benchmark_time_taken_ns \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime_ns() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlaunchers \u001b[38;5;241m=\u001b[39m [builtins\u001b[38;5;241m.\u001b[39mmin(timings, key\u001b[38;5;241m=\u001b[39mtimings\u001b[38;5;241m.\u001b[39mget)]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py:885\u001b[0m, in \u001b[0;36mCachingAutotuner.benchmark_all_configs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbenchmark_all_configs\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    882\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\n\u001b[1;32m    883\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCachingAutotuner.benchmark_all_configs\u001b[39m\u001b[38;5;124m\"\u001b[39m, log_pt2_compile_event\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    884\u001b[0m     ):\n\u001b[0;32m--> 885\u001b[0m         timings \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    886\u001b[0m             launcher: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbench(launcher, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    887\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m launcher \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlaunchers\n\u001b[1;32m    888\u001b[0m         }\n\u001b[1;32m    890\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m timings\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    891\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoordesc_tuner\u001b[38;5;241m.\u001b[39mcache_benchmark_result(k\u001b[38;5;241m.\u001b[39mconfig, v)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py:886\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbenchmark_all_configs\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    882\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\n\u001b[1;32m    883\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCachingAutotuner.benchmark_all_configs\u001b[39m\u001b[38;5;124m\"\u001b[39m, log_pt2_compile_event\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    884\u001b[0m     ):\n\u001b[1;32m    885\u001b[0m         timings \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m--> 886\u001b[0m             launcher: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbench\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlauncher\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    887\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m launcher \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlaunchers\n\u001b[1;32m    888\u001b[0m         }\n\u001b[1;32m    890\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m timings\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    891\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoordesc_tuner\u001b[38;5;241m.\u001b[39mcache_benchmark_result(k\u001b[38;5;241m.\u001b[39mconfig, v)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py:787\u001b[0m, in \u001b[0;36mCachingAutotuner.bench\u001b[0;34m(self, launcher, grid, with_profiler, *args, **kwargs)\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_props\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    785\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m benchmarker\u001b[38;5;241m.\u001b[39mbenchmark_cpu(kernel_call)\n\u001b[0;32m--> 787\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbenchmarker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbenchmark_gpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkernel_call\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/benchmarking.py:66\u001b[0m, in \u001b[0;36mcount.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m: Any, \u001b[38;5;241m*\u001b[39margs: P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m     63\u001b[0m     counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minductor\u001b[39m\u001b[38;5;124m\"\u001b[39m][\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbenchmarking.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m     65\u001b[0m     ] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/benchmarking.py:202\u001b[0m, in \u001b[0;36mTritonBenchmarker.benchmark_gpu\u001b[0;34m(self, _callable, **kwargs)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_mode\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtriton_do_bench(_callable, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtriton_do_bench\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmedian\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/triton/testing.py:118\u001b[0m, in \u001b[0;36mdo_bench\u001b[0;34m(fn, warmup, rep, grad_to_none, quantiles, return_mode)\u001b[0m\n\u001b[1;32m    115\u001b[0m di \u001b[38;5;241m=\u001b[39m runtime\u001b[38;5;241m.\u001b[39mdriver\u001b[38;5;241m.\u001b[39mactive\u001b[38;5;241m.\u001b[39mget_device_interface()\n\u001b[1;32m    117\u001b[0m fn()\n\u001b[0;32m--> 118\u001b[0m \u001b[43mdi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynchronize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m cache \u001b[38;5;241m=\u001b[39m runtime\u001b[38;5;241m.\u001b[39mdriver\u001b[38;5;241m.\u001b[39mactive\u001b[38;5;241m.\u001b[39mget_empty_cache_for_benchmark()\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# Estimate the runtime of the function\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py:985\u001b[0m, in \u001b[0;36msynchronize\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    983\u001b[0m _lazy_init()\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice(device):\n\u001b[0;32m--> 985\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_synchronize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "train_and_test2(\"minGRU_compiled\", mingru3, start_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad61f066",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
