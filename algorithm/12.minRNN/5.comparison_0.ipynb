{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20ec8bf0",
   "metadata": {
    "id": "20ec8bf0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3390f4",
   "metadata": {
    "id": "5a3390f4"
   },
   "source": [
    "## Hyperparameters and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0c61f36",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1732703737281,
     "user": {
      "displayName": "Hugman Sangkeun Jung",
      "userId": "08689291704194029524"
     },
     "user_tz": -540
    },
    "id": "d0c61f36",
    "outputId": "45d9894c-a14d-4164-9f2d-699025751e72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n"
     ]
    }
   ],
   "source": [
    "# Modified hyperparameters\n",
    "SEQUENCE_LENGTH = 64\n",
    "\n",
    "EMBEDDING_DIM = 32\n",
    "HIDDEN_DIM = EMBEDDING_DIM*2\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 1e-4\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "NUM_LAYERS = 2\n",
    "NUM_HEADS = 4\n",
    "FFN_DIM = 64\n",
    "DROPOUT = 0.1\n",
    "\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c9b962",
   "metadata": {
    "id": "89c9b962"
   },
   "source": [
    "## Data Preparation\n",
    "\n",
    "We are using the TinyShakespeare dataset, a small character-level text corpus consisting of a subset of Shakespeare's plays. It's often used for testing sequence models, as it includes a rich set of vocabulary and provides a challenging task for next-character prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "089830e2",
   "metadata": {
    "id": "089830e2"
   },
   "outputs": [],
   "source": [
    "## Utility Functions\n",
    "\n",
    "def load_data(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "def create_char_mappings(text):\n",
    "    chars = sorted(list(set(text)))\n",
    "    char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "    idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "    return chars, char_to_idx, idx_to_char"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fc6452",
   "metadata": {
    "id": "30fc6452"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f07670d9",
   "metadata": {
    "id": "f07670d9"
   },
   "outputs": [],
   "source": [
    "class CharDataset(Dataset):\n",
    "    def __init__(self, text, seq_length, char_to_idx):\n",
    "        self.text = text\n",
    "        self.seq_length = seq_length\n",
    "        self.char_to_idx = char_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text) - self.seq_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = [self.char_to_idx[ch] for ch in self.text[idx:idx+self.seq_length]]\n",
    "        y = [self.char_to_idx[ch] for ch in self.text[idx+1:idx+self.seq_length+1]]\n",
    "        return torch.tensor(x), torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9172da13",
   "metadata": {
    "id": "9172da13"
   },
   "outputs": [],
   "source": [
    "def prepare_data(text, seq_length, batch_size, val_split):\n",
    "    chars, char_to_idx, idx_to_char = create_char_mappings(text)\n",
    "\n",
    "    # Split data into train and validation\n",
    "    val_size = int(len(text) * val_split)\n",
    "    train_text, val_text = text[:-val_size], text[-val_size:]\n",
    "\n",
    "    train_dataset = CharDataset(train_text, seq_length, char_to_idx)\n",
    "    val_dataset = CharDataset(val_text, seq_length, char_to_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=12, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=12, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader, chars, char_to_idx, idx_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "nEwKFB_8L6AG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3706,
     "status": "ok",
     "timestamp": 1732704246464,
     "user": {
      "displayName": "Hugman Sangkeun Jung",
      "userId": "08689291704194029524"
     },
     "user_tz": -540
    },
    "id": "nEwKFB_8L6AG",
    "outputId": "e75cdff9-3775-461e-d930-0ced534bf74d"
   },
   "outputs": [],
   "source": [
    "# !wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=19zosLuU0z4MxIMKbGVYEGlg52QyfbTIy' -O input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47d03398",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 668,
     "status": "ok",
     "timestamp": 1732704255324,
     "user": {
      "displayName": "Hugman Sangkeun Jung",
      "userId": "08689291704194029524"
     },
     "user_tz": -540
    },
    "id": "47d03398",
    "outputId": "f247f5b3-88ff-4ecb-e8a3-4b2e42a2820b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 1115394\n",
      "Vocabulary size: 65\n",
      "Train dataset size: 1003791\n",
      "Validation dataset size: 111475\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "text = load_data('../data/input.txt')\n",
    "train_loader, val_loader, chars, char_to_idx, idx_to_char = prepare_data(text, SEQUENCE_LENGTH, BATCH_SIZE, VALIDATION_SPLIT)\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(f\"Total characters: {len(text)}\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Train dataset size: {len(train_loader.dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9442b104",
   "metadata": {
    "id": "9442b104"
   },
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1f0a6a1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 916,
     "status": "ok",
     "timestamp": 1732704257555,
     "user": {
      "displayName": "Hugman Sangkeun Jung",
      "userId": "08689291704194029524"
     },
     "user_tz": -540
    },
    "id": "f1f0a6a1",
    "outputId": "9ef6eb5a-0015-40ad-f143-236e3d38fb7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([512, 64])\n",
      "Target shape: torch.Size([512, 64])\n",
      "Sample 1: ------------------------------\n",
      "Input sequence : m them.BRUTUS:And when such time they have begun to cry,Let \n",
      "Target sequence:  them.BRUTUS:And when such time they have begun to cry,Let t\n",
      "\n",
      "Sample 2: ------------------------------\n",
      "Input sequence : nd of late,When corn was given them gratis, you repined;Scanda\n",
      "Target sequence: d of late,When corn was given them gratis, you repined;Scandal\n",
      "\n",
      "Sample 3: ------------------------------\n",
      "Input sequence : be here.This is a thing that Angelo knows not; for he thisvery\n",
      "Target sequence: e here.This is a thing that Angelo knows not; for he thisvery \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to convert index sequence to character sequence\n",
    "def indices_to_text(indices, idx_to_char):\n",
    "    return ''.join([idx_to_char[idx.item()] for idx in indices])\n",
    "\n",
    "# Get a batch of data\n",
    "dataiter = iter(train_loader)\n",
    "batch_x, batch_y = next(dataiter)\n",
    "\n",
    "print(f\"Input shape: {batch_x.shape}\")\n",
    "print(f\"Target shape: {batch_y.shape}\")\n",
    "\n",
    "# Print a few samples from the batch\n",
    "num_samples = 3\n",
    "for i in range(num_samples):\n",
    "    print(f\"Sample {i+1}: ------------------------------\" )\n",
    "    print(\"Input sequence :\", indices_to_text(batch_x[i], idx_to_char).replace('\\n',''))\n",
    "    print(\"Target sequence:\", indices_to_text(batch_y[i], idx_to_char).replace('\\n',''))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5486f73f",
   "metadata": {
    "id": "5486f73f"
   },
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61a52f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_vram_usage(device=\"cuda\"):\n",
    "    allocated = torch.cuda.memory_allocated(device) / (1024**2)  # in MB\n",
    "    reserved = torch.cuda.memory_reserved(device) / (1024**2)    # in MB\n",
    "    max_allocated = torch.cuda.max_memory_allocated(device) / (1024**2)  # in MB\n",
    "    print(f\"Allocated: {allocated:.2f} MB, Reserved: {reserved:.2f} MB, Max Allocated: {max_allocated:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59178a11",
   "metadata": {
    "id": "59178a11"
   },
   "outputs": [],
   "source": [
    "def validate(model, dataloader, criterion, device, epoch, step):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            output, _ = model(x)\n",
    "            loss = criterion(output.view(-1, output.size(-1)), y.view(-1))\n",
    "            losses.append((step, epoch, loss.item()))\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea5e61f5",
   "metadata": {
    "id": "ea5e61f5"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, dataloader, criterion, optimizer, device, epoch, step):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    vram_usage = []\n",
    "    scaler = torch.amp.GradScaler('cuda')\n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch}\", leave=False)\n",
    "    for batch, (x, y) in enumerate(pbar):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.amp.autocast('cuda'):\n",
    "            output, _ = model(x)\n",
    "            loss = criterion(output.view(-1, output.size(-1)), y.view(-1))\n",
    "            \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        step += 1\n",
    "        losses.append((step, epoch, loss.item()))\n",
    "        \n",
    "        # VRAM 사용량을 progress bar의 postfix로 업데이트\n",
    "        allocated = torch.cuda.memory_allocated(device) / (1024**2)\n",
    "        vram_usage.append(allocated)\n",
    "        pbar.set_postfix(loss=f'{loss.item():.4f}', step=step, vram=f'{allocated:.2f} MB')\n",
    "    return losses, step, vram_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbedfd61",
   "metadata": {
    "id": "cbedfd61"
   },
   "source": [
    "## Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e91075e8",
   "metadata": {
    "id": "e91075e8"
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, device, epochs):\n",
    "    all_train_losses = []\n",
    "    all_val_losses = []\n",
    "    all_vram_usages = []\n",
    "    step = 0\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        # Training phase with tqdm updates\n",
    "        epoch_train_losses, step, vram_usage = train(model, train_loader, criterion, optimizer, device, epoch, step)\n",
    "        all_train_losses.extend(epoch_train_losses)\n",
    "        all_vram_usages.append(vram_usage)\n",
    "        \n",
    "        # Validation phase\n",
    "        epoch_val_losses = validate(model, val_loader, criterion, device, epoch, step)\n",
    "        all_val_losses.extend(epoch_val_losses)\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        print(f'Epoch {epoch}/{epochs}, Train Loss: {epoch_train_losses[-1][2]:.4f}, '\n",
    "              f'Val Loss: {epoch_val_losses[-1][2]:.4f}, Epoch Time: {epoch_time:.2f}s',\n",
    "              f'Average Vram Usage: {np.mean(vram_usage):.2f}MB')\n",
    "\n",
    "    train_losses_df = pd.DataFrame(all_train_losses, columns=['step', 'epoch', 'loss_value'])\n",
    "    val_losses_df = pd.DataFrame(all_val_losses, columns=['step', 'epoch', 'loss_value'])\n",
    "    # average_vram_usage = np.mean(all_vram_usages)\n",
    "    return model, train_losses_df, val_losses_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4535c886",
   "metadata": {
    "id": "4535c886"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, char_to_idx, idx_to_char, start_text, device, max_length=500):\n",
    "    model.eval()\n",
    "    current_text = start_text\n",
    "    hidden = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            x = torch.tensor([[char_to_idx[ch] for ch in current_text[-SEQUENCE_LENGTH:]]]).to(device)\n",
    "            output, hidden = model(x, hidden)\n",
    "            probs = torch.softmax(output[0, -1], dim=0)\n",
    "            next_char_idx = torch.multinomial(probs, 1).item()\n",
    "            next_char = idx_to_char[next_char_idx]\n",
    "            current_text += next_char\n",
    "\n",
    "    return current_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6aef1cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_comparison_dict = {}\n",
    "\n",
    "def add_loss_to_comparison(model_name, train_losses_df, val_losses_df):\n",
    "    \"\"\"\n",
    "    Adds training and validation losses from a model to the comparison dictionary.\n",
    "    \"\"\"\n",
    "    loss_comparison_dict[model_name] = {\n",
    "        'train': train_losses_df,\n",
    "        'val': val_losses_df\n",
    "    }\n",
    "\n",
    "def print_final_losses(loss_dict):\n",
    "    for model_name, losses in loss_dict.items():\n",
    "        train_df = losses['train']\n",
    "        val_df = losses['val']\n",
    "        final_train = train_df.groupby('epoch')['loss_value'].last().iloc[-1]\n",
    "        final_val = val_df.groupby('epoch')['loss_value'].last().iloc[-1]\n",
    "        print(f\"{model_name}: Final Train Loss: {final_train:.4f}, Final Val Loss: {final_val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "973b4a80",
   "metadata": {
    "id": "973b4a80"
   },
   "outputs": [],
   "source": [
    "# Function to plot loss curves\n",
    "def plot_loss(train_losses_df, val_losses_df):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    # Plot training losses\n",
    "    for epoch in train_losses_df['epoch'].unique():\n",
    "        epoch_train_losses = train_losses_df[train_losses_df['epoch'] == epoch]\n",
    "        plt.plot(epoch_train_losses['step'], epoch_train_losses['loss_value'],\n",
    "                 color='blue', alpha=0.3)\n",
    "\n",
    "    # scatter training loss at the end of each epoch\n",
    "    last_train_losses = train_losses_df.groupby('epoch').last().reset_index()\n",
    "    plt.scatter(last_train_losses['step'], last_train_losses['loss_value'],\n",
    "                color='blue')\n",
    "\n",
    "    # Plot and scatter validation loss at the end of each epoch\n",
    "    last_val_losses = val_losses_df.groupby('epoch').last().reset_index()\n",
    "    plt.plot(last_val_losses['step'], last_val_losses['loss_value'],\n",
    "             color='orange', label='Validation Loss')\n",
    "    plt.scatter(last_val_losses['step'], last_val_losses['loss_value'],\n",
    "                color='orange')\n",
    "\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Function to print final loss values\n",
    "def print_final_losses(train_losses_df, val_losses_df):\n",
    "    print(\"Final Training Loss:\", train_losses_df.groupby('epoch')['loss_value'].last().iloc[-1])\n",
    "    print(\"Final Validation Loss:\", val_losses_df.groupby('epoch')['loss_value'].last().iloc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5001e203",
   "metadata": {
    "id": "5001e203"
   },
   "outputs": [],
   "source": [
    "# Function to plot loss curves for multiple models stored in loss_comparison_dict\n",
    "def plot_loss_comparisons():\n",
    "    \"\"\"\n",
    "    Plots the training loss curves and average validation loss per epoch for multiple models added to the loss comparison dictionary.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    # Get the last model in the dictionary (for special final-point highlighting)\n",
    "    last_model_name = list(loss_comparison_dict.keys())[-1]\n",
    "\n",
    "    # Loop through each model in the loss dictionary\n",
    "    for model_name, losses in loss_comparison_dict.items():\n",
    "        train_losses_df = losses['train']\n",
    "        val_losses_df = losses['val']\n",
    "\n",
    "        # Plot training losses for each model\n",
    "        plt.plot(train_losses_df['step'], train_losses_df['loss_value'],\n",
    "                 label=f'{model_name} train', linestyle='-', alpha=0.7)\n",
    "\n",
    "        # Scatter training loss at the end of each epoch\n",
    "        last_train_losses = train_losses_df.groupby('epoch').last().reset_index()\n",
    "        plt.scatter(last_train_losses['step'], last_train_losses['loss_value'], marker='o', s=50)\n",
    "\n",
    "        # Compute average validation loss per epoch (using the last step of each epoch for x-axis)\n",
    "        avg_val_losses = val_losses_df.groupby('epoch').agg({'loss_value': 'mean', 'step': 'last'}).reset_index()\n",
    "        # Scatter the average validation loss for each epoch\n",
    "        plt.scatter(avg_val_losses['step'], avg_val_losses['loss_value'], marker='s', s=50,\n",
    "                    label=f'{model_name} val avg')\n",
    "\n",
    "        # For the last model, highlight the final training loss with a star\n",
    "        if model_name == last_model_name:\n",
    "            final_step = train_losses_df['step'].iloc[-1]\n",
    "            final_loss = train_losses_df['loss_value'].iloc[-1]\n",
    "            plt.scatter(final_step, final_loss, marker='*', s=100, color='red', zorder=5)\n",
    "\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Comparison')\n",
    "    plt.legend()  # Legend shows both training and validation average labels\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16777ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_separate_train_val(loss_dict):\n",
    "    \"\"\"\n",
    "    모델별 Training Loss와 Validation Loss를 각각 별도의 그래프로 그립니다.\n",
    "    단, Validation Loss는 에포크별 평균으로 계산합니다.\n",
    "    \"\"\"\n",
    "    # 1. Training Loss Plot (원본 그대로)\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    plt.subplot(1, 2, 1)  # 1행 2열 중 첫 번째\n",
    "    for model_name, losses in loss_dict.items():\n",
    "        train_df = losses['train']\n",
    "        steps_train = train_df['step'].values\n",
    "        loss_train = train_df['loss_value'].values\n",
    "        plt.plot(steps_train, loss_train, label=f'{model_name} Train')\n",
    "    plt.title('Training Loss Comparison')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # 2. Validation Loss Plot (에포크별 평균 처리)\n",
    "    plt.subplot(1, 2, 2)  # 1행 2열 중 두 번째\n",
    "    for model_name, losses in loss_dict.items():\n",
    "        val_df = losses['val']\n",
    "        # 에포크별 평균 loss와 마지막 step을 계산\n",
    "        val_avg = val_df.groupby('epoch').agg({'loss_value': 'mean', 'step': 'last'}).reset_index()\n",
    "        plt.plot(val_avg['step'], val_avg['loss_value'], label=f'{model_name} Val')\n",
    "    plt.title('Validation Loss (Epoch Avg) Comparison')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "396c469b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text generation using validation data\n",
    "val_sample, _ = next(iter(val_loader))\n",
    "start_text = ''.join([idx_to_char[idx.item()] for idx in val_sample[0][:SEQUENCE_LENGTH]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98b72703",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1732704396861,
     "user": {
      "displayName": "Hugman Sangkeun Jung",
      "userId": "08689291704194029524"
     },
     "user_tz": -540
    },
    "id": "98b72703",
    "outputId": "ece1620a-8690-4750-97c1-ada645d59a08"
   },
   "outputs": [],
   "source": [
    "# Decoder Input/Output Example\n",
    "sample_input, _ = next(iter(val_loader))\n",
    "sample_input = sample_input[0].unsqueeze(0).to(device)  # Add batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2d8a352",
   "metadata": {
    "id": "a2d8a352"
   },
   "outputs": [],
   "source": [
    "def generate_text_attention(model, char_to_idx, idx_to_char, start_text, device, max_length=500):\n",
    "    model.eval()\n",
    "    current_text = start_text\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            # Convert the last sequence of characters to indices and feed it to the model\n",
    "            x = torch.tensor([[char_to_idx[ch] for ch in current_text[-SEQUENCE_LENGTH:]]]).to(device)\n",
    "            output = model(x)[0]  # No hidden state needed for attention-based models\n",
    "            probs = torch.softmax(output[0, -1], dim=0)\n",
    "            next_char_idx = torch.multinomial(probs, 1).item()\n",
    "            next_char = idx_to_char[next_char_idx]\n",
    "            current_text += next_char\n",
    "\n",
    "    return current_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32c936b1",
   "metadata": {
    "id": "32c936b1"
   },
   "outputs": [],
   "source": [
    "def train_and_test(model_desc, model, start_text):\n",
    "    # Initialize the model\n",
    "    model = model.to(device)\n",
    "    # Use the same optimizer and criterion\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, fused=True)\n",
    "    \n",
    "    # Train the model\n",
    "    trained_model, train_losses_df, val_losses_df = train_model(\n",
    "        model, train_loader, val_loader, criterion, optimizer, device, EPOCHS\n",
    "    )\n",
    "\n",
    "    # Generate text\n",
    "    generated_text = generate_text_attention(trained_model, char_to_idx, idx_to_char, start_text, device)\n",
    "    print(f\"Generated text [{start_text}]:\")\n",
    "    print(\"-\"*50)\n",
    "    print(generated_text)\n",
    "    \n",
    "    add_loss_to_comparison(model_desc, train_losses_df, val_losses_df)\n",
    "\n",
    "    # Plot loss comparisons including this model\n",
    "    plot_loss_comparisons()\n",
    "    \n",
    "    plot_separate_train_val(loss_comparison_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbb22cc",
   "metadata": {},
   "source": [
    "## mingru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d73586ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math # math.log 및 상수 사용을 위함\n",
    "from tqdm import tqdm # tqdm 임포트 추가\n",
    "\n",
    "# Lion 옵티마이저 임포트 (설치 필요: pip install lion-pytorch)\n",
    "# from lion_pytorch import Lion # 주석 처리 - 실제 사용 시 주석 해제\n",
    "\n",
    "def log_g(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    은닉 상태 후보를 로그 공간으로 변환하는 함수.\n",
    "    x >= 0 이면 log(x + 0.5)\n",
    "    x < 0 이면 log(sigmoid(x))\n",
    "    \"\"\"\n",
    "    return torch.where(x >= 0, (F.relu(x) + 0.5).log(), -F.softplus(-x))\n",
    "\n",
    "def parallel_scan_log(log_coeffs: torch.Tensor, log_values: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    로그 공간에서 병렬 스캔 연산을 수행합니다. (클램핑 제거됨)\n",
    "    Args:\n",
    "        log_coeffs (torch.Tensor): [B, L, H_internal] 형태의 텐서로, log(alpha_t)를 나타냅니다.\n",
    "        log_values (torch.Tensor): [B, L+1, H_internal] 형태의 텐서로, [log_h_initial, log(beta_1), ..., log(beta_L)]를 나타냅니다.\n",
    "    Returns:\n",
    "        torch.Tensor: [B, L, H_internal] 형태의 텐서로, 지수 함수가 적용된 은닉 상태 (h_1 부터 h_L까지)를 나타냅니다.\n",
    "    \"\"\"\n",
    "    log_proda_coeffs_prefix = F.pad(torch.cumsum(log_coeffs, dim=1), (0, 0, 1, 0), value=0.0)\n",
    "    terms_for_logcumsumexp = log_values - log_proda_coeffs_prefix\n",
    "    log_sum_exp_terms = torch.logcumsumexp(terms_for_logcumsumexp, dim=1) \n",
    "    log_hidden_states = log_proda_coeffs_prefix + log_sum_exp_terms\n",
    "    output_hidden_states = torch.exp(log_hidden_states[:, 1:, :]) \n",
    "    return output_hidden_states\n",
    "\n",
    "class ParallelLogMinGRU(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, expansion_factor: float = 1.0, epsilon: float = 1e-7):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size \n",
    "        self.expansion_factor = expansion_factor # GRU 셀 내부의 확장 계수\n",
    "        # GRU 셀 내부에서 사용될 확장된 차원 (hidden_size는 이 셀의 기본 출력 차원을 의미)\n",
    "        self.internal_expanded_dim = int(hidden_size * self.expansion_factor)\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        # 입력 input_size를 internal_expanded_dim * 2 (은닉 상태 후보용, 게이트용)로 프로젝션\n",
    "        self.to_hidden_and_gate = nn.Linear(input_size, self.internal_expanded_dim * 2)\n",
    "        \n",
    "        # GRU 셀 내부 확장이 있었다면, 다시 hidden_size로 프로젝션\n",
    "        if self.expansion_factor != 1.0:\n",
    "            self.to_out = nn.Linear(self.internal_expanded_dim, hidden_size)\n",
    "        else:\n",
    "            self.to_out = nn.Identity()\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'linear' in name or 'to_hidden_and_gate' in name or 'to_out' in name :\n",
    "                if 'weight' in name:\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "                elif 'bias' in name and param is not None: # bias가 있을 경우에만 초기화\n",
    "                    nn.init.zeros_(param)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: [B, L, input_size]\n",
    "        B, L, _ = x.size()\n",
    "        hidden_and_gate = self.to_hidden_and_gate(x) # [B, L, internal_expanded_dim * 2]\n",
    "        h_candidate_input_expanded, logits_z_expanded = hidden_and_gate.chunk(2, dim=-1) # 각각 [B, L, internal_expanded_dim]\n",
    "        \n",
    "        log_A = F.logsigmoid(-logits_z_expanded)\n",
    "        log_Z_expanded = F.logsigmoid(logits_z_expanded)\n",
    "        log_h_candidate_contrib_expanded = log_g(h_candidate_input_expanded)\n",
    "        log_B = log_Z_expanded + log_h_candidate_contrib_expanded\n",
    "\n",
    "        log_h0_val = torch.full((B, 1, self.internal_expanded_dim),\n",
    "                                math.log(self.epsilon),\n",
    "                                device=x.device, dtype=x.dtype)\n",
    "        log_vals = torch.cat([log_h0_val, log_B], dim=1) # [B, L+1, internal_expanded_dim]\n",
    "        h_expanded_scan_out = parallel_scan_log(log_A, log_vals) # [B, L, internal_expanded_dim]\n",
    "        \n",
    "        output = self.to_out(h_expanded_scan_out) # [B, L, hidden_size]\n",
    "        return output\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-8):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.gamma = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        variance = x.pow(2).mean(dim=-1, keepdim=True)\n",
    "        return x * torch.rsqrt(variance + self.eps) * self.gamma\n",
    "\n",
    "class CausalDepthWiseConv1d(nn.Module):\n",
    "    def __init__(self, dim: int, kernel_size: int):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(dim, dim, kernel_size=kernel_size, groups=dim), \n",
    "            nn.Conv1d(dim, dim, kernel_size=1)                       \n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x_orig_shape = x.shape\n",
    "        # x.ndim 대신 len(x.shape) 또는 x.dim() 사용\n",
    "        if x.dim() == 2: \n",
    "            x = x.unsqueeze(0) \n",
    "        x_transposed = x.transpose(1, 2) \n",
    "        x_padded = F.pad(x_transposed, (self.kernel_size - 1, 0), value=0.)\n",
    "        x_conv_out = self.net(x_padded)\n",
    "        x_restored = x_conv_out.transpose(1, 2) \n",
    "        # len(x_orig_shape) 사용\n",
    "        if len(x_orig_shape) == 2 and x_restored.shape[0] == 1:\n",
    "            x_restored = x_restored.squeeze(0) \n",
    "        return x_restored\n",
    "\n",
    "class SwiGLUFFN(nn.Module):\n",
    "    def __init__(self, dim: int, expansion_factor: float = 4.0, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        ffn_hidden_dim = int(dim * expansion_factor)\n",
    "        \n",
    "        self.w1_w3 = nn.Linear(dim, ffn_hidden_dim * 2, bias=False) \n",
    "        self.w2 = nn.Linear(ffn_hidden_dim, dim, bias=False)       \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x_proj = self.w1_w3(x)\n",
    "        x1, x3_gate = x_proj.chunk(2, dim=-1) \n",
    "        hidden_states = F.silu(x1) * x3_gate\n",
    "        hidden_states = self.dropout(hidden_states) \n",
    "        return self.w2(hidden_states)\n",
    "\n",
    "class MinGRUDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        embedding_dim: int,\n",
    "        hidden_dim: int,\n",
    "        num_layers: int = 2,\n",
    "        dropout: float = 0.1, \n",
    "        expansion_factor_gru: float = 1.0, \n",
    "        epsilon_gru: float = 1e-7,\n",
    "        rms_norm_eps: float = 1e-8,\n",
    "        enable_conv: bool = True, \n",
    "        conv_kernel_size: int = 3, \n",
    "        ffn_expansion_factor: float = 1.0 \n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.input_projection = nn.Linear(embedding_dim, hidden_dim) if embedding_dim != hidden_dim else nn.Identity()\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            ffn_layer = SwiGLUFFN(\n",
    "                dim=hidden_dim, \n",
    "                expansion_factor=ffn_expansion_factor, \n",
    "                dropout=dropout \n",
    "            )\n",
    "\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                CausalDepthWiseConv1d(hidden_dim, conv_kernel_size) if enable_conv else nn.Identity(), \n",
    "                RMSNorm(hidden_dim, eps=rms_norm_eps),                                                 \n",
    "                ParallelLogMinGRU(\n",
    "                    input_size=hidden_dim, \n",
    "                    hidden_size=hidden_dim, \n",
    "                    expansion_factor=expansion_factor_gru, \n",
    "                    epsilon=epsilon_gru\n",
    "                ),                                                                                     \n",
    "                RMSNorm(hidden_dim, eps=rms_norm_eps),                                                 \n",
    "                ffn_layer,                                                                             \n",
    "                nn.Dropout(dropout) if dropout > 0. else nn.Identity()                                 \n",
    "            ]))\n",
    "\n",
    "        self.final_norm = RMSNorm(hidden_dim, eps=rms_norm_eps) \n",
    "        self.output_fc1 = nn.Linear(hidden_dim, hidden_dim * 4) \n",
    "        self.output_fc2 = nn.Linear(hidden_dim * 4, vocab_size)\n",
    "        self.final_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, None]:\n",
    "        h = self.embedding(x)\n",
    "        h = self.input_projection(h)\n",
    "\n",
    "        for block_idx, block_modules in enumerate(self.layers):\n",
    "            conv_layer, norm_gru_input, gru_cell, norm_ff_input, ffn_module, dropout_block_output = block_modules\n",
    "\n",
    "            h_conv_input = h\n",
    "            if not isinstance(conv_layer, nn.Identity): \n",
    "                h_conv_out = conv_layer(h)\n",
    "                h = h_conv_input + h_conv_out \n",
    "            \n",
    "            h_gru_residual_source = h\n",
    "            h_normed_for_gru = norm_gru_input(h)\n",
    "            h_gru_out = gru_cell(h_normed_for_gru)\n",
    "            h = h_gru_residual_source + h_gru_out \n",
    "\n",
    "            h_ff_residual_source = h\n",
    "            h_normed_for_ff = norm_ff_input(h)\n",
    "            h_ff_out = ffn_module(h_normed_for_ff) \n",
    "            h = h_ff_residual_source + h_ff_out \n",
    "            \n",
    "            h = dropout_block_output(h)\n",
    "        \n",
    "        h_norm_final = self.final_norm(h)\n",
    "        h_dropped_final = self.final_dropout(h_norm_final) \n",
    "        \n",
    "        output_expanded = F.gelu(self.output_fc1(h_dropped_final)) \n",
    "        logits = self.output_fc2(output_expanded)\n",
    "        \n",
    "        return logits, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c86a2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model Initialization\n",
    "mingru = MinGRUDecoder(vocab_size, EMBEDDING_DIM, int(HIDDEN_DIM*0.8), NUM_LAYERS).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8199acd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "MinGRUDecoder                                 [512, 64, 65]             --\n",
       "├─Embedding: 1-1                              [512, 64, 32]             2,080\n",
       "├─Linear: 1-2                                 [512, 64, 51]             1,683\n",
       "├─ModuleList: 1-3                             --                        --\n",
       "│    └─ModuleList: 2-1                        --                        --\n",
       "│    │    └─CausalDepthWiseConv1d: 3-1        [512, 64, 51]             2,856\n",
       "│    │    └─RMSNorm: 3-2                      [512, 64, 51]             51\n",
       "│    │    └─ParallelLogMinGRU: 3-3            [512, 64, 51]             5,304\n",
       "│    │    └─RMSNorm: 3-4                      [512, 64, 51]             51\n",
       "│    │    └─SwiGLUFFN: 3-5                    [512, 64, 51]             7,803\n",
       "│    │    └─Dropout: 3-6                      [512, 64, 51]             --\n",
       "│    └─ModuleList: 2-2                        --                        --\n",
       "│    │    └─CausalDepthWiseConv1d: 3-7        [512, 64, 51]             2,856\n",
       "│    │    └─RMSNorm: 3-8                      [512, 64, 51]             51\n",
       "│    │    └─ParallelLogMinGRU: 3-9            [512, 64, 51]             5,304\n",
       "│    │    └─RMSNorm: 3-10                     [512, 64, 51]             51\n",
       "│    │    └─SwiGLUFFN: 3-11                   [512, 64, 51]             7,803\n",
       "│    │    └─Dropout: 3-12                     [512, 64, 51]             --\n",
       "├─RMSNorm: 1-4                                [512, 64, 51]             51\n",
       "├─Dropout: 1-5                                [512, 64, 51]             --\n",
       "├─Linear: 1-6                                 [512, 64, 204]            10,608\n",
       "├─Linear: 1-7                                 [512, 64, 65]             13,325\n",
       "===============================================================================================\n",
       "Total params: 59,877\n",
       "Trainable params: 59,877\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 214.77\n",
       "===============================================================================================\n",
       "Input size (MB): 0.26\n",
       "Forward/backward pass size (MB): 346.29\n",
       "Params size (MB): 0.24\n",
       "Estimated Total Size (MB): 346.79\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(mingru, input_size=(BATCH_SIZE, SEQUENCE_LENGTH), dtypes=[torch.long])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f66c397",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/1961 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "train_and_test(\"minGRU\", mingru, start_text)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
