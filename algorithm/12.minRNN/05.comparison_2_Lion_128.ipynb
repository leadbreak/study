{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40e00ce5",
   "metadata": {},
   "source": [
    "```\n",
    "Lion Optimizer test\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ec8bf0",
   "metadata": {
    "id": "20ec8bf0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3390f4",
   "metadata": {
    "id": "5a3390f4"
   },
   "source": [
    "## Hyperparameters and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c61f36",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1732703737281,
     "user": {
      "displayName": "Hugman Sangkeun Jung",
      "userId": "08689291704194029524"
     },
     "user_tz": -540
    },
    "id": "d0c61f36",
    "outputId": "45d9894c-a14d-4164-9f2d-699025751e72"
   },
   "outputs": [],
   "source": [
    "# Modified hyperparameters\n",
    "SEQUENCE_LENGTH = 128\n",
    "\n",
    "EMBEDDING_DIM = SEQUENCE_LENGTH\n",
    "HIDDEN_DIM = EMBEDDING_DIM*2\n",
    "\n",
    "BATCH_SIZE = 1024\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 1e-3   # Lion Optimizer에 맞게 더 큰 lr\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "NUM_LAYERS = 2\n",
    "NUM_HEADS = 4\n",
    "FFN_DIM = SEQUENCE_LENGTH\n",
    "DROPOUT = 0.1\n",
    "\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c9b962",
   "metadata": {
    "id": "89c9b962"
   },
   "source": [
    "## Data Preparation\n",
    "\n",
    "We are using the TinyShakespeare dataset, a small character-level text corpus consisting of a subset of Shakespeare's plays. It's often used for testing sequence models, as it includes a rich set of vocabulary and provides a challenging task for next-character prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089830e2",
   "metadata": {
    "id": "089830e2"
   },
   "outputs": [],
   "source": [
    "## Utility Functions\n",
    "\n",
    "def load_data(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "def create_char_mappings(text):\n",
    "    chars = sorted(list(set(text)))\n",
    "    char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "    idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "    return chars, char_to_idx, idx_to_char"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fc6452",
   "metadata": {
    "id": "30fc6452"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07670d9",
   "metadata": {
    "id": "f07670d9"
   },
   "outputs": [],
   "source": [
    "class CharDataset(Dataset):\n",
    "    def __init__(self, text, seq_length, char_to_idx):\n",
    "        self.text = text\n",
    "        self.seq_length = seq_length\n",
    "        self.char_to_idx = char_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text) - self.seq_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = [self.char_to_idx[ch] for ch in self.text[idx:idx+self.seq_length]]\n",
    "        y = [self.char_to_idx[ch] for ch in self.text[idx+1:idx+self.seq_length+1]]\n",
    "        return torch.tensor(x), torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9172da13",
   "metadata": {
    "id": "9172da13"
   },
   "outputs": [],
   "source": [
    "def prepare_data(text, seq_length, batch_size, val_split):\n",
    "    chars, char_to_idx, idx_to_char = create_char_mappings(text)\n",
    "\n",
    "    # Split data into train and validation\n",
    "    val_size = int(len(text) * val_split)\n",
    "    train_text, val_text = text[:-val_size], text[-val_size:]\n",
    "\n",
    "    train_dataset = CharDataset(train_text, seq_length, char_to_idx)\n",
    "    val_dataset = CharDataset(val_text, seq_length, char_to_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=12, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=12, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader, chars, char_to_idx, idx_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nEwKFB_8L6AG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3706,
     "status": "ok",
     "timestamp": 1732704246464,
     "user": {
      "displayName": "Hugman Sangkeun Jung",
      "userId": "08689291704194029524"
     },
     "user_tz": -540
    },
    "id": "nEwKFB_8L6AG",
    "outputId": "e75cdff9-3775-461e-d930-0ced534bf74d"
   },
   "outputs": [],
   "source": [
    "# !wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=19zosLuU0z4MxIMKbGVYEGlg52QyfbTIy' -O input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d03398",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 668,
     "status": "ok",
     "timestamp": 1732704255324,
     "user": {
      "displayName": "Hugman Sangkeun Jung",
      "userId": "08689291704194029524"
     },
     "user_tz": -540
    },
    "id": "47d03398",
    "outputId": "f247f5b3-88ff-4ecb-e8a3-4b2e42a2820b"
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "text = load_data('../data/input.txt')\n",
    "train_loader, val_loader, chars, char_to_idx, idx_to_char = prepare_data(text, SEQUENCE_LENGTH, BATCH_SIZE, VALIDATION_SPLIT)\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(f\"Total characters: {len(text)}\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Train dataset size: {len(train_loader.dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9442b104",
   "metadata": {
    "id": "9442b104"
   },
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f0a6a1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 916,
     "status": "ok",
     "timestamp": 1732704257555,
     "user": {
      "displayName": "Hugman Sangkeun Jung",
      "userId": "08689291704194029524"
     },
     "user_tz": -540
    },
    "id": "f1f0a6a1",
    "outputId": "9ef6eb5a-0015-40ad-f143-236e3d38fb7e"
   },
   "outputs": [],
   "source": [
    "# Function to convert index sequence to character sequence\n",
    "def indices_to_text(indices, idx_to_char):\n",
    "    return ''.join([idx_to_char[idx.item()] for idx in indices])\n",
    "\n",
    "# Get a batch of data\n",
    "dataiter = iter(train_loader)\n",
    "batch_x, batch_y = next(dataiter)\n",
    "\n",
    "print(f\"Input shape: {batch_x.shape}\")\n",
    "print(f\"Target shape: {batch_y.shape}\")\n",
    "\n",
    "# Print a few samples from the batch\n",
    "num_samples = 3\n",
    "for i in range(num_samples):\n",
    "    print(f\"Sample {i+1}: ------------------------------\" )\n",
    "    print(\"Input sequence :\", indices_to_text(batch_x[i], idx_to_char).replace('\\n',''))\n",
    "    print(\"Target sequence:\", indices_to_text(batch_y[i], idx_to_char).replace('\\n',''))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5486f73f",
   "metadata": {
    "id": "5486f73f"
   },
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a52f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_vram_usage(device=\"cuda\"):\n",
    "    allocated = torch.cuda.memory_allocated(device) / (1024**2)  # in MB\n",
    "    reserved = torch.cuda.memory_reserved(device) / (1024**2)    # in MB\n",
    "    max_allocated = torch.cuda.max_memory_allocated(device) / (1024**2)  # in MB\n",
    "    print(f\"Allocated: {allocated:.2f} MB, Reserved: {reserved:.2f} MB, Max Allocated: {max_allocated:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59178a11",
   "metadata": {
    "id": "59178a11"
   },
   "outputs": [],
   "source": [
    "def validate(model, dataloader, criterion, device, epoch, step):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            output, _ = model(x)\n",
    "            loss = criterion(output.view(-1, output.size(-1)), y.view(-1))\n",
    "            losses.append((step, epoch, loss.item()))\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5e61f5",
   "metadata": {
    "id": "ea5e61f5"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, dataloader, criterion, optimizer, device, epoch, step):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    vram_usage = []\n",
    "    scaler = torch.amp.GradScaler('cuda')\n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch}\", leave=False)\n",
    "    for batch, (x, y) in enumerate(pbar):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "            output, _ = model(x)\n",
    "            loss = criterion(output.view(-1, output.size(-1)), y.view(-1))\n",
    "            \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        step += 1\n",
    "        losses.append((step, epoch, loss.item()))\n",
    "        \n",
    "        # VRAM 사용량을 progress bar의 postfix로 업데이트\n",
    "        allocated = torch.cuda.memory_allocated(device) / (1024**2)\n",
    "        vram_usage.append(allocated)\n",
    "        pbar.set_postfix(loss=f'{loss.item():.4f}', step=step, vram=f'{allocated:.2f} MB')\n",
    "    return losses, step, vram_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbedfd61",
   "metadata": {
    "id": "cbedfd61"
   },
   "source": [
    "## Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91075e8",
   "metadata": {
    "id": "e91075e8"
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, device, epochs):\n",
    "    all_train_losses = []\n",
    "    all_val_losses = []\n",
    "    all_vram_usages = []\n",
    "    step = 0\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        # Training phase with tqdm updates\n",
    "        epoch_train_losses, step, vram_usage = train(model, train_loader, criterion, optimizer, device, epoch, step)\n",
    "        all_train_losses.extend(epoch_train_losses)\n",
    "        all_vram_usages.append(vram_usage)\n",
    "        \n",
    "        # Validation phase\n",
    "        epoch_val_losses = validate(model, val_loader, criterion, device, epoch, step)\n",
    "        all_val_losses.extend(epoch_val_losses)\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        print(f'Epoch {epoch}/{epochs}, Train Loss: {epoch_train_losses[-1][2]:.4f}, '\n",
    "              f'Val Loss: {epoch_val_losses[-1][2]:.4f}, Epoch Time: {epoch_time:.2f}s',\n",
    "              f'Average Vram Usage: {np.mean(vram_usage):.2f}MB')\n",
    "\n",
    "    train_losses_df = pd.DataFrame(all_train_losses, columns=['step', 'epoch', 'loss_value'])\n",
    "    val_losses_df = pd.DataFrame(all_val_losses, columns=['step', 'epoch', 'loss_value'])\n",
    "    # average_vram_usage = np.mean(all_vram_usages)\n",
    "    return model, train_losses_df, val_losses_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4535c886",
   "metadata": {
    "id": "4535c886"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, char_to_idx, idx_to_char, start_text, device, max_length=500):\n",
    "    model.eval()\n",
    "    current_text = start_text\n",
    "    hidden = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            x = torch.tensor([[char_to_idx[ch] for ch in current_text[-SEQUENCE_LENGTH:]]]).to(device)\n",
    "            output, hidden = model(x, hidden)\n",
    "            probs = torch.softmax(output[0, -1], dim=0)\n",
    "            next_char_idx = torch.multinomial(probs, 1).item()\n",
    "            next_char = idx_to_char[next_char_idx]\n",
    "            current_text += next_char\n",
    "\n",
    "    return current_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aef1cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_comparison_dict = {}\n",
    "\n",
    "def add_loss_to_comparison(model_name, train_losses_df, val_losses_df):\n",
    "    \"\"\"\n",
    "    Adds training and validation losses from a model to the comparison dictionary.\n",
    "    \"\"\"\n",
    "    loss_comparison_dict[model_name] = {\n",
    "        'train': train_losses_df,\n",
    "        'val': val_losses_df\n",
    "    }\n",
    "\n",
    "def print_final_losses(loss_dict):\n",
    "    for model_name, losses in loss_dict.items():\n",
    "        train_df = losses['train']\n",
    "        val_df = losses['val']\n",
    "        final_train = train_df.groupby('epoch')['loss_value'].last().iloc[-1]\n",
    "        final_val = val_df.groupby('epoch')['loss_value'].last().iloc[-1]\n",
    "        print(f\"{model_name}: Final Train Loss: {final_train:.4f}, Final Val Loss: {final_val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973b4a80",
   "metadata": {
    "id": "973b4a80"
   },
   "outputs": [],
   "source": [
    "# Function to plot loss curves\n",
    "def plot_loss(train_losses_df, val_losses_df):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    # Plot training losses\n",
    "    for epoch in train_losses_df['epoch'].unique():\n",
    "        epoch_train_losses = train_losses_df[train_losses_df['epoch'] == epoch]\n",
    "        plt.plot(epoch_train_losses['step'], epoch_train_losses['loss_value'],\n",
    "                 color='blue', alpha=0.3)\n",
    "\n",
    "    # scatter training loss at the end of each epoch\n",
    "    last_train_losses = train_losses_df.groupby('epoch').last().reset_index()\n",
    "    plt.scatter(last_train_losses['step'], last_train_losses['loss_value'],\n",
    "                color='blue')\n",
    "\n",
    "    # Plot and scatter validation loss at the end of each epoch\n",
    "    last_val_losses = val_losses_df.groupby('epoch').last().reset_index()\n",
    "    plt.plot(last_val_losses['step'], last_val_losses['loss_value'],\n",
    "             color='orange', label='Validation Loss')\n",
    "    plt.scatter(last_val_losses['step'], last_val_losses['loss_value'],\n",
    "                color='orange')\n",
    "\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Function to print final loss values\n",
    "def print_final_losses(train_losses_df, val_losses_df):\n",
    "    print(\"Final Training Loss:\", train_losses_df.groupby('epoch')['loss_value'].last().iloc[-1])\n",
    "    print(\"Final Validation Loss:\", val_losses_df.groupby('epoch')['loss_value'].last().iloc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5001e203",
   "metadata": {
    "id": "5001e203"
   },
   "outputs": [],
   "source": [
    "# Function to plot loss curves for multiple models stored in loss_comparison_dict\n",
    "def plot_loss_comparisons():\n",
    "    \"\"\"\n",
    "    Plots the training loss curves and average validation loss per epoch for multiple models added to the loss comparison dictionary.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    # Get the last model in the dictionary (for special final-point highlighting)\n",
    "    last_model_name = list(loss_comparison_dict.keys())[-1]\n",
    "\n",
    "    # Loop through each model in the loss dictionary\n",
    "    for model_name, losses in loss_comparison_dict.items():\n",
    "        train_losses_df = losses['train']\n",
    "        val_losses_df = losses['val']\n",
    "\n",
    "        # Plot training losses for each model\n",
    "        plt.plot(train_losses_df['step'], train_losses_df['loss_value'],\n",
    "                 label=f'{model_name} train', linestyle='-', alpha=0.7)\n",
    "\n",
    "        # Scatter training loss at the end of each epoch\n",
    "        last_train_losses = train_losses_df.groupby('epoch').last().reset_index()\n",
    "        plt.scatter(last_train_losses['step'], last_train_losses['loss_value'], marker='o', s=50)\n",
    "\n",
    "        # Compute average validation loss per epoch (using the last step of each epoch for x-axis)\n",
    "        avg_val_losses = val_losses_df.groupby('epoch').agg({'loss_value': 'mean', 'step': 'last'}).reset_index()\n",
    "        # Scatter the average validation loss for each epoch\n",
    "        plt.scatter(avg_val_losses['step'], avg_val_losses['loss_value'], marker='s', s=50,\n",
    "                    label=f'{model_name} val avg')\n",
    "\n",
    "        # For the last model, highlight the final training loss with a star\n",
    "        if model_name == last_model_name:\n",
    "            final_step = train_losses_df['step'].iloc[-1]\n",
    "            final_loss = train_losses_df['loss_value'].iloc[-1]\n",
    "            plt.scatter(final_step, final_loss, marker='*', s=100, color='red', zorder=5)\n",
    "\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Comparison')\n",
    "    plt.legend()  # Legend shows both training and validation average labels\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16777ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_separate_train_val(loss_dict):\n",
    "    \"\"\"\n",
    "    모델별 Training Loss와 Validation Loss를 각각 별도의 그래프로 그립니다.\n",
    "    단, Validation Loss는 에포크별 평균으로 계산합니다.\n",
    "    \"\"\"\n",
    "    # 1. Training Loss Plot (원본 그대로)\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    plt.subplot(1, 2, 1)  # 1행 2열 중 첫 번째\n",
    "    for model_name, losses in loss_dict.items():\n",
    "        train_df = losses['train']\n",
    "        steps_train = train_df['step'].values\n",
    "        loss_train = train_df['loss_value'].values\n",
    "        plt.plot(steps_train, loss_train, label=f'{model_name} Train')\n",
    "    plt.title('Training Loss Comparison')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # 2. Validation Loss Plot (에포크별 평균 처리)\n",
    "    plt.subplot(1, 2, 2)  # 1행 2열 중 두 번째\n",
    "    for model_name, losses in loss_dict.items():\n",
    "        val_df = losses['val']\n",
    "        # 에포크별 평균 loss와 마지막 step을 계산\n",
    "        val_avg = val_df.groupby('epoch').agg({'loss_value': 'mean', 'step': 'last'}).reset_index()\n",
    "        plt.plot(val_avg['step'], val_avg['loss_value'], label=f'{model_name} Val')\n",
    "    plt.title('Validation Loss (Epoch Avg) Comparison')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1eb8ad",
   "metadata": {
    "id": "aa1eb8ad"
   },
   "source": [
    "## Model 1: GRU Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e017e89b",
   "metadata": {
    "id": "e017e89b"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GRUDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=2, dropout=0.1):\n",
    "        super(GRUDecoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers=num_layers,\n",
    "                          dropout=dropout if num_layers > 1 else 0.0, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        embed = self.embedding(x)\n",
    "        output, hidden = self.gru(embed, hidden)\n",
    "        output = self.dropout(output)\n",
    "        output = self.layer_norm(output)\n",
    "        output = F.gelu(self.fc1(output))\n",
    "        output = self.fc2(output)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3581e4",
   "metadata": {
    "id": "9a3581e4"
   },
   "outputs": [],
   "source": [
    "from lion_pytorch import Lion\n",
    "\n",
    "### Model Initialization\n",
    "gru = GRUDecoder(vocab_size, EMBEDDING_DIM, int(HIDDEN_DIM*0.8), NUM_LAYERS).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Lion(gru.parameters(), lr=LEARNING_RATE, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe18507",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(gru, input_size=(BATCH_SIZE, SEQUENCE_LENGTH), dtypes=[torch.long])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec86dc8e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 121082,
     "status": "ok",
     "timestamp": 1732704395031,
     "user": {
      "displayName": "Hugman Sangkeun Jung",
      "userId": "08689291704194029524"
     },
     "user_tz": -540
    },
    "id": "ec86dc8e",
    "outputId": "d3a32ebf-e2ad-4e8b-fb04-f2651e0ac049"
   },
   "outputs": [],
   "source": [
    "## Training Loop\n",
    "trained_model, train_losses_df, val_losses_df = train_model(gru, train_loader, val_loader, criterion, optimizer, device, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396c469b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text generation using validation data\n",
    "val_sample, _ = next(iter(val_loader))\n",
    "start_text = ''.join([idx_to_char[idx.item()] for idx in val_sample[0][:SEQUENCE_LENGTH]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70664998",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 489,
     "status": "ok",
     "timestamp": 1732704395512,
     "user": {
      "displayName": "Hugman Sangkeun Jung",
      "userId": "08689291704194029524"
     },
     "user_tz": -540
    },
    "id": "70664998",
    "outputId": "a970b003-a23c-47b7-900e-5ed705866838"
   },
   "outputs": [],
   "source": [
    "generated_text = generate_text(trained_model, char_to_idx, idx_to_char, start_text, device)\n",
    "                      \n",
    "print(f\"Generated text (starting with validation data [{start_text}]):\")\n",
    "print(\"-\"*50)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcca4e52",
   "metadata": {
    "id": "fcca4e52"
   },
   "outputs": [],
   "source": [
    "# After training a model (e.g., LSTM without RMSNorm), add its losses\n",
    "add_loss_to_comparison('GRU', train_losses_df, val_losses_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b72703",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1732704396861,
     "user": {
      "displayName": "Hugman Sangkeun Jung",
      "userId": "08689291704194029524"
     },
     "user_tz": -540
    },
    "id": "98b72703",
    "outputId": "ece1620a-8690-4750-97c1-ada645d59a08"
   },
   "outputs": [],
   "source": [
    "# Decoder Input/Output Example\n",
    "sample_input, _ = next(iter(val_loader))\n",
    "sample_input = sample_input[0].unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "trained_model.eval()\n",
    "with torch.no_grad():\n",
    "    output, _ = trained_model(sample_input)\n",
    "\n",
    "print(\"\\nSample Input:\")\n",
    "print(''.join([idx_to_char[idx.item()] for idx in sample_input[0]]))\n",
    "\n",
    "print(\"\\nModel Output (logits for next character prediction):\")\n",
    "print(output.shape)\n",
    "print(output[0, 0, :10])  # Print first 10 logits of the first timestep\n",
    "\n",
    "print(\"\\nPredicted next character:\")\n",
    "predicted_char_idx = torch.argmax(output[0, -1]).item()\n",
    "print(idx_to_char[predicted_char_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23a4aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_comparisons()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868a5338",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "executionInfo": {
     "elapsed": 899,
     "status": "ok",
     "timestamp": 1732704397758,
     "user": {
      "displayName": "Hugman Sangkeun Jung",
      "userId": "08689291704194029524"
     },
     "user_tz": -540
    },
    "id": "868a5338",
    "outputId": "06397d97-49fb-4f85-b2b3-2b27ca481fe0"
   },
   "outputs": [],
   "source": [
    "plot_separate_train_val(loss_comparison_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d8a352",
   "metadata": {
    "id": "a2d8a352"
   },
   "outputs": [],
   "source": [
    "def generate_text_attention(model, char_to_idx, idx_to_char, start_text, device, max_length=500):\n",
    "    model.eval()\n",
    "    current_text = start_text\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            # Convert the last sequence of characters to indices and feed it to the model\n",
    "            x = torch.tensor([[char_to_idx[ch] for ch in current_text[-SEQUENCE_LENGTH:]]]).to(device)\n",
    "            output = model(x)[0]  # No hidden state needed for attention-based models\n",
    "            probs = torch.softmax(output[0, -1], dim=0)\n",
    "            next_char_idx = torch.multinomial(probs, 1).item()\n",
    "            next_char = idx_to_char[next_char_idx]\n",
    "            current_text += next_char\n",
    "\n",
    "    return current_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c936b1",
   "metadata": {
    "id": "32c936b1"
   },
   "outputs": [],
   "source": [
    "def train_and_test(model_desc, model, start_text):\n",
    "    # Initialize the model\n",
    "    model = model.to(device)\n",
    "    # Use the same optimizer and criterion\n",
    "    from lion_pytorch import Lion\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Lion(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-2)\n",
    "    \n",
    "    # Train the model\n",
    "    trained_model, train_losses_df, val_losses_df = train_model(\n",
    "        model, train_loader, val_loader, criterion, optimizer, device, EPOCHS\n",
    "    )\n",
    "\n",
    "    # Generate text\n",
    "    generated_text = generate_text_attention(trained_model, char_to_idx, idx_to_char, start_text, device)\n",
    "    print(f\"Generated text [{start_text}]:\")\n",
    "    print(\"-\"*50)\n",
    "    print(generated_text)\n",
    "    \n",
    "    add_loss_to_comparison(model_desc, train_losses_df, val_losses_df)\n",
    "\n",
    "    # Plot loss comparisons including this model\n",
    "    plot_loss_comparisons()\n",
    "    \n",
    "    plot_separate_train_val(loss_comparison_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39911f55",
   "metadata": {
    "id": "39911f55"
   },
   "source": [
    "## Model 2: Modern Transformer(LLaMA - 2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b43bc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    DIM = EMBEDDING_DIM \n",
    "    FFN_DIM = int(FFN_DIM * 4)\n",
    "    NUM_HEADS = NUM_HEADS \n",
    "    NUM_LAYERS = NUM_LAYERS\n",
    "\n",
    "    NUM_KV_HEADS = NUM_HEADS \n",
    "    VOCAB_SIZE = vocab_size\n",
    "    NORM_EPS = 1e-5 # LLaMA: 1e-5\n",
    "    ROPE_THETA = 10000 # LLaMA: 10000\n",
    "\n",
    "    MAX_BATCH_SIZE = BATCH_SIZE\n",
    "    MAX_SEQ_LEN = SEQUENCE_LENGTH # depending on the DATASET\n",
    "    NUM_KV_HEAD_REP = NUM_HEADS // NUM_KV_HEADS\n",
    "\n",
    "    HEAD_DIM = DIM // NUM_HEADS\n",
    "    DROPOUT = DROPOUT\n",
    "    DEVICE = device\n",
    "    \n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "    def _norm(self, x: torch.Tensor):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.weight.to(x.device) * self._norm(x.float()).type_as(x)\n",
    "    \n",
    "def precompute_freqs_cis(head_dim: int, seq_len: int, theta: float = 100.0, device: str = device):\n",
    "    if head_dim % 2 != 0:\n",
    "        raise ValueError(\"head_dim must be even for rotary embeddings.\")\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, head_dim, 2).float() / head_dim)).to(device)\n",
    "    t = torch.arange(seq_len, device=device, dtype=torch.float32)\n",
    "    freqs = torch.outer(t, freqs)  # [seq_len, head_dim//2]\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
    "    return freqs_cis  # [seq_len, head_dim // 2]\n",
    "\n",
    "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
    "    L = x.shape[1]\n",
    "    return freqs_cis.view(1, L, 1, x.shape[-1] // 2)  # [1, L, 1, head_dim]\n",
    "\n",
    "def apply_rotary_emb(x: torch.Tensor, device: str = device):\n",
    "    # x: [B, L, 2*heads, D] & D is even\n",
    "    _, L, _, D = x.shape\n",
    "    x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2)) # [B, L, 2*heads, D//2, 2]\n",
    "    freqs_cis = precompute_freqs_cis(D, L)\n",
    "    freqs = reshape_for_broadcast(freqs_cis, x)\n",
    "    x_rotated = x_complex * freqs\n",
    "    x_out = torch.view_as_real(x_rotated).reshape(x.shape)\n",
    "    return x_out.type_as(x).to(device)\n",
    "\n",
    "def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    B, L, nk, d = x.shape\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "    return x[:, :, :, None, :].expand(B, L, nk, n_rep, d).reshape(B, L, nk * n_rep, d)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, ffn_dim, dropout):\n",
    "        super().__init__()\n",
    "        hidden_dim = ffn_dim\n",
    "        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n",
    "        self.w3 = nn.Linear(dim, hidden_dim, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x: [B, L, D]\n",
    "        return self.w2(F.silu(self.w1(x)) * self.dropout(self.w3(x)))\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, n_heads, n_kv_heads, n_rep, dim, dropout, batch, seq_len, device):\n",
    "        super().__init__()\n",
    "        self.n_heads_q = n_heads\n",
    "        self.n_kv_heads = n_kv_heads\n",
    "        self.n_rep = n_rep\n",
    "        self.head_dim = dim // n_heads\n",
    "\n",
    "        self.wq = nn.Linear(dim, n_heads * self.head_dim, bias=False)\n",
    "        self.wk = nn.Linear(dim, n_kv_heads * self.head_dim, bias=False)\n",
    "        self.wv = nn.Linear(dim, n_kv_heads * self.head_dim, bias=False)\n",
    "        self.wo = nn.Linear(n_heads * self.head_dim, dim, bias=False)\n",
    "        self.attn_dropout = dropout\n",
    "        \n",
    "        self.norm = RMSNorm(self.head_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, start_pos, mask, return_attn=False):\n",
    "        B, L, _ = x.shape\n",
    "        src_len = trg_len = L\n",
    "        offset = start_pos\n",
    "        \n",
    "        xq = self.wq(x).view(B, L, self.n_heads_q, self.head_dim)\n",
    "        xk = self.wk(x).view(B, L, self.n_kv_heads, self.head_dim)\n",
    "        xv = self.wv(x).view(B, L, self.n_kv_heads, self.head_dim)\n",
    "        \n",
    "        # Apply rotary embeddings\n",
    "        xq = apply_rotary_emb(xq)\n",
    "        xk = apply_rotary_emb(xk)\n",
    "        \n",
    "        # GQA: Adjust dimensions for attention computation\n",
    "        xq = xq.transpose(1, 2)   # [B, n_heads, L, head_dim]\n",
    "        xk = repeat_kv(xk, self.n_rep).transpose(1, 2) # [B, n_heads, L, head_dim]\n",
    "        xv = repeat_kv(xv, self.n_rep).transpose(1, 2) # [B, n_heads, L, head_dim]\n",
    "\n",
    "        # Compute scaled dot-product attention manually to capture attention weights\n",
    "        scores = torch.matmul(xq, xk.transpose(-2, -1)) / math.sqrt(self.head_dim)  # [B, n_heads, L, L]\n",
    "        scores = torch.nan_to_num(scores)\n",
    "        if mask is None:\n",
    "            mask = torch.triu(\n",
    "                torch.zeros([L, L])\n",
    "                .float()\n",
    "                .fill_(float(\"-inf\"))\n",
    "                .type_as(attn_weights),\n",
    "                1 + offset,\n",
    "            )\n",
    "            \n",
    "        scores += mask\n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        attn_weights = F.dropout(attn_weights, self.attn_dropout, training=self.training)\n",
    "        attn_output = torch.matmul(attn_weights, xv)  # [B, n_heads, L, head_dim]\n",
    "        attn_output = self.norm(attn_output)\n",
    "        \n",
    "        # Reshape attention output and project\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(B, L, -1)\n",
    "        output = self.wo(attn_output)  # [B, L, D]\n",
    "        if return_attn:\n",
    "            return output, attn_weights\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, args: 'ModelArgs'):\n",
    "        super().__init__()\n",
    "        self.attention = SelfAttention(\n",
    "            args.NUM_HEADS, \n",
    "            args.NUM_KV_HEADS, \n",
    "            args.NUM_KV_HEAD_REP, \n",
    "            args.DIM, \n",
    "            args.DROPOUT, \n",
    "            args.MAX_BATCH_SIZE, \n",
    "            args.MAX_SEQ_LEN, \n",
    "            args.DEVICE\n",
    "        )\n",
    "        self.ffn = FeedForward(args.DIM, args.FFN_DIM, args.DROPOUT)\n",
    "        self.attention_norm = RMSNorm(args.DIM, args.NORM_EPS)\n",
    "        self.ffn_norm = RMSNorm(args.DIM, args.NORM_EPS)\n",
    "        self.res_dropout = nn.Dropout(args.DROPOUT)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, start_pos, mask, return_attn=False):\n",
    "        if return_attn:\n",
    "            attn_out, attn_map = self.attention(self.attention_norm(x), start_pos, mask, return_attn=True)\n",
    "            h = x + self.res_dropout(attn_out)\n",
    "            h = h + self.res_dropout(self.ffn(self.ffn_norm(h)))\n",
    "            return h, attn_map\n",
    "        else:\n",
    "            h = x + self.res_dropout(self.attention(self.attention_norm(x), start_pos, mask))\n",
    "            h = h + self.res_dropout(self.ffn(self.ffn_norm(h)))\n",
    "            return h\n",
    "\n",
    "class LlamaTransformer(nn.Module):\n",
    "    def __init__(self, args: 'ModelArgs'):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.tok_embeddings = nn.Embedding(args.VOCAB_SIZE, args.DIM)\n",
    "        self.layers = nn.ModuleList([TransformerBlock(args) for _ in range(args.NUM_LAYERS)])\n",
    "        self.norm = RMSNorm(args.DIM, args.NORM_EPS)\n",
    "        self.output = nn.Linear(args.DIM, args.VOCAB_SIZE, bias=False)\n",
    "        self.device = args.DEVICE\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, start_pos=0, return_attn=False):\n",
    "        B, L = x.shape\n",
    "        h = self.tok_embeddings(x)  # [B, L, D]\n",
    "        \n",
    "        mask = None\n",
    "        if L > 1:\n",
    "            mask = torch.triu(\n",
    "                torch.zeros([L, L])\n",
    "                .float()\n",
    "                .fill_(float(\"-inf\"))\n",
    "                .type_as(x),\n",
    "                1 + start_pos,\n",
    "            )\n",
    "        attn_maps = []\n",
    "        for layer in self.layers:\n",
    "            if return_attn:\n",
    "                h, attn_map = layer(h, start_pos, mask, return_attn=True)\n",
    "                attn_maps.append(attn_map)\n",
    "            else:\n",
    "                h = layer(h, start_pos, mask)\n",
    "        logits = self.output(self.norm(h)).float()\n",
    "        if return_attn:\n",
    "            return logits, attn_maps\n",
    "        return logits, _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b4f114",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = ModelArgs()\n",
    "llama = LlamaTransformer(PARAMS).to(device)\n",
    "llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087c3927",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(llama.to(device), input_size=(BATCH_SIZE, SEQUENCE_LENGTH), dtypes=[torch.long])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b361db8b",
   "metadata": {
    "id": "b361db8b",
    "outputId": "33c6269c-5979-4100-b6e7-a89b6b4ed962"
   },
   "outputs": [],
   "source": [
    "train_and_test(\"LLaMA\", llama, start_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8775d573",
   "metadata": {},
   "source": [
    "## Model 3: Mamba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd160c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "import math # For initialization if needed\n",
    "\n",
    "# --- Assume mamba_ssm is installed ---\n",
    "# pip install mamba-ssm causal-conv1d\n",
    "try:\n",
    "    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update # Need both for training/inference\n",
    "    # causal_conv1d_fn is the main function for training/full sequence processing\n",
    "except ImportError:\n",
    "    print(\"Warning: 'causal_conv1d' package not found. Falling back to nn.Conv1d simulation.\")\n",
    "    print(\"Install with: pip install causal-conv1d\")\n",
    "    causal_conv1d_fn = None # Placeholder\n",
    "\n",
    "try:\n",
    "    from mamba_ssm.ops.selective_scan_interface import selective_scan_fn, mamba_inner_fn\n",
    "    # selective_scan_fn wraps the CUDA/Triton kernel\n",
    "except ImportError:\n",
    "    print(\"Warning: 'mamba_ssm' package not found or compiled kernels unavailable.\")\n",
    "    print(\"SSM scan will use the less efficient PyTorch implementation.\")\n",
    "    print(\"Install with: pip install mamba-ssm\")\n",
    "    selective_scan_fn = None # Placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6be4f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat\n",
    "import math\n",
    "from causal_conv1d import causal_conv1d_fn\n",
    "from mamba_ssm.ops.selective_scan_interface import selective_scan_fn\n",
    "\n",
    "# --- RMSNorm (Root Mean Square Layer Normalization) ---\n",
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"RMS 정규화 레이어.\n",
    "\n",
    "    Args:\n",
    "        dim (int): 정규화할 벡터의 차원.\n",
    "        eps (float, optional): 분모에 더할 작은 값 (0으로 나누는 것 방지). 기본값: 1e-6.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        # 학습 가능한 스케일링 파라미터 (gamma)\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"RMS 정규화 계산 수행.\"\"\"\n",
    "        # 계산 안정성을 위해 float32 사용 후 원본 타입 복원\n",
    "        original_dtype = x.dtype\n",
    "        # 입력의 제곱 평균의 제곱근 역수 계산\n",
    "        rms = torch.rsqrt(x.to(torch.float32).pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "        return (x * rms).to(original_dtype)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"RMS 정규화 적용.\"\"\"\n",
    "        # 정규화 후 학습 가능한 가중치 적용\n",
    "        return self.weight * self._norm(x)\n",
    "\n",
    "# --- SSM (State Space Model) 모듈 ---\n",
    "class SSM(nn.Module):\n",
    "    \"\"\"선택적 스캔 메커니즘 (SSM). 최적화된 selective_scan_fn 커널 사용.\n",
    "       Mamba v1 논문의 파라미터화 및 계산 방식 (non-fused path) 기반.\n",
    "       selective_scan_fn이 기대하는 특정 텐서 레이아웃(B,D,L / B,N,L 등)을 따름.\n",
    "\n",
    "    Args:\n",
    "        d_inner (int): 내부 확장 차원 (D).\n",
    "        state_size (int): SSM 상태 벡터 크기 (N).\n",
    "        dt_rank (str or int, optional): Δ 계산 시 사용될 중간 랭크. \"auto\"시 d_inner / 16. 기본값: \"auto\".\n",
    "        dt_min (float, optional): Δ의 최소값 제한 (softplus 적용 후). 기본값: 0.001.\n",
    "        dt_max (float, optional): Δ의 최대값 제한 (softplus 적용 후). 기본값: 0.1.\n",
    "        dt_init (str, optional): dt_proj 가중치 초기화 방식 (\"random\" or \"constant\"). 기본값: \"random\".\n",
    "        dt_scale (float, optional): dt_proj 가중치 초기화 스케일. 기본값: 1.0.\n",
    "        dt_init_floor (float, optional): dt 초기값 하한선. 기본값: 1e-4.\n",
    "        bias (bool, optional): x_proj 레이어에 bias 사용 여부. 기본값: False.\n",
    "        device (str, optional): 연산 장치. 기본값: 'cuda'.\n",
    "        dtype (torch.dtype, optional): 연산 데이터 타입. 기본값: torch.float32.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_inner: int, state_size: int, dt_rank: str | int =\"auto\", dt_min: float =0.001, dt_max: float =0.1,\n",
    "                 dt_init: str =\"random\", dt_scale: float =1.0, dt_init_floor: float =1e-4, bias: bool =False,\n",
    "                 device: str ='cuda:2', dtype: torch.dtype =torch.float32):\n",
    "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
    "        super().__init__()\n",
    "        self.d_inner = d_inner\n",
    "        self.state_size = state_size\n",
    "        # dt_rank 자동 계산 또는 지정값 사용\n",
    "        self.dt_rank = math.ceil(d_inner / 16) if dt_rank == \"auto\" else dt_rank\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "\n",
    "        # 컨볼루션 출력(x)을 받아 dt_inter, B, C 계산용 프로젝션 (Mamba v1 non-fused 방식)\n",
    "        self.x_proj = nn.Linear(self.d_inner, self.dt_rank + self.state_size * 2, bias=bias, **factory_kwargs)\n",
    "\n",
    "        # dt_inter를 받아 dt 계산용 프로젝션\n",
    "        self.dt_proj = nn.Linear(self.dt_rank, self.d_inner, bias=True, **factory_kwargs)\n",
    "\n",
    "        # --- dt_proj bias 특별 초기화 (Mamba v1 공식 코드 참조) ---\n",
    "        # 초기화 시 softplus(bias) 결과가 [dt_min, dt_max] 범위에 있도록 조정\n",
    "        dt_init_std = self.dt_rank**-0.5 * dt_scale\n",
    "        if dt_init == \"constant\":\n",
    "            nn.init.constant_(self.dt_proj.weight, dt_init_std)\n",
    "        elif dt_init == \"random\":\n",
    "            nn.init.uniform_(self.dt_proj.weight, -dt_init_std, dt_init_std)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Invalid dt_init: {dt_init}\")\n",
    "\n",
    "        # dt bias 초기값 계산 (softplus의 역함수 활용)\n",
    "        dt = torch.exp(\n",
    "            torch.rand(self.d_inner, **factory_kwargs) * (math.log(dt_max) - math.log(dt_min))\n",
    "            + math.log(dt_min)\n",
    "        ).clamp(min=dt_init_floor)\n",
    "        inv_dt = dt + torch.log(-torch.expm1(-dt)) # softplus(inv_dt) ≈ dt\n",
    "        with torch.no_grad():\n",
    "            self.dt_proj.bias.copy_(inv_dt)\n",
    "        # 다른 초기화 루틴에서 이 bias를 덮어쓰지 않도록 플래그 설정 (선택적)\n",
    "        self.dt_proj.bias._no_reinit = True\n",
    "        # --- dt_proj bias 초기화 종료 ---\n",
    "\n",
    "        # --- SSM 파라미터 A (A_log) ---\n",
    "        # S4D-Real 방식 초기화: A 행렬의 대각 요소가 [1, 2, ..., N]이 되도록 A_log 설정\n",
    "        A = repeat(\n",
    "            torch.arange(1, self.state_size + 1, dtype=torch.float32, device=device),\n",
    "            \"n -> d n\", # 1차원 벡터를 d_inner 번 반복하여 (D, N) 행렬 생성\n",
    "            d=self.d_inner,\n",
    "        ).contiguous()\n",
    "        A_log = torch.log(A) # 로그 스케일에서 파라미터 학습 (float32 유지)\n",
    "        self.A_log = nn.Parameter(A_log)\n",
    "        self.A_log._no_weight_decay = True # 가중치 감쇠 제외\n",
    "\n",
    "        # --- SSM 파라미터 D (피드스루) ---\n",
    "        # 형태: (D)\n",
    "        self.D = nn.Parameter(torch.ones(self.d_inner, device=device)) # float32 유지 권장\n",
    "        self.D._no_weight_decay = True # 가중치 감쇠 제외\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"SSM 순방향 계산. 입력 x는 (B, D, L) 레이아웃을 가정.\"\"\"\n",
    "        B, D, L = x.shape # 입력 레이아웃 확인\n",
    "        if D != self.d_inner:\n",
    "            raise ValueError(f\"입력 차원 D({D})가 SSM 내부 차원({self.d_inner})과 불일치\")\n",
    "\n",
    "        # 1. 입력 x(컨볼루션 출력)로부터 dt, B, C 계산\n",
    "        #    선형 프로젝션을 위해 (B*L, D) 형태로 변환\n",
    "        x_reshaped = rearrange(x, \"b d l -> (b l) d\")\n",
    "        x_proj_out = self.x_proj(x_reshaped) # 결과: (B*L, dt_rank + 2*N)\n",
    "        dt_inter, B, C = torch.split(x_proj_out, [self.dt_rank, self.state_size, self.state_size], dim=-1)\n",
    "\n",
    "        # 2. dt 계산 (softplus 적용 전) -> 형태 (B, D, L)\n",
    "        #    dt_inter: (B*L, dt_rank)\n",
    "        #    Mamba v1 non-fused path 방식 적용\n",
    "        dt = self.dt_proj.weight @ dt_inter.t() # 결과: (D, B*L)\n",
    "        dt = rearrange(dt, \"d (b l) -> b d l\", l=L) # 최종 형태: (B, D, L)\n",
    "        # dt_bias는 selective_scan_fn 내부에서 delta_bias 인자로 전달되어 더해짐\n",
    "\n",
    "        # 3. A 행렬 계산, 형태 (D, N)\n",
    "        A = -torch.exp(self.A_log.float()) # float32에서 exp 계산\n",
    "\n",
    "        # 4. B, C 형태 변경 -> (B, N, L)\n",
    "        #    selective_scan_fn 커널이 요구하는 레이아웃\n",
    "        B = rearrange(B, \"(b l) n -> b n l\", l=L).contiguous()\n",
    "        C = rearrange(C, \"(b l) n -> b n l\", l=L).contiguous()\n",
    "\n",
    "        # 5. D 파라미터 준비, 형태 (D)\n",
    "        D_param = self.D.float().contiguous()\n",
    "\n",
    "        # 6. selective_scan_fn 입력 준비 (메모리 연속성 보장)\n",
    "        input_u = x.contiguous()             # u (SSM 입력): (B, D, L)\n",
    "        input_delta = dt.contiguous()        # delta (dt): (B, D, L)\n",
    "        input_A = A.contiguous()             # A: (D, N)\n",
    "        input_B = B                          # B: (B, N, L)\n",
    "        input_C = C                          # C: (B, N, L)\n",
    "        input_D = D_param                    # D: (D)\n",
    "        input_delta_bias = self.dt_proj.bias.float().contiguous() if self.dt_proj.bias is not None else None\n",
    "\n",
    "        # 7. 최적화된 selective_scan_fn 호출\n",
    "        #    입력 레이아웃 및 파라미터 형태는 Mamba v1 non-fused path 기준\n",
    "        y = selective_scan_fn(\n",
    "            u=input_u,\n",
    "            delta=input_delta,\n",
    "            A=input_A,\n",
    "            B=input_B,\n",
    "            C=input_C,\n",
    "            D=input_D,\n",
    "            z=None, # 게이트 z는 MambaBlock 레벨에서 처리\n",
    "            delta_bias=input_delta_bias, # dt_bias 전달\n",
    "            delta_softplus=True, # 내부에서 delta = softplus(dt + delta_bias) 계산\n",
    "        )\n",
    "\n",
    "        # 8. 결과 반환, 형태 (B, D, L)\n",
    "        #    후속 처리를 위해 이 레이아웃 유지\n",
    "        return y\n",
    "\n",
    "\n",
    "# --- Mamba 블록 (causal_conv1d_fn 사용 및 데이터 흐름 수정) ---\n",
    "class MambaBlock(nn.Module):\n",
    "    \"\"\"Mamba 핵심 블록. RMSNorm, 입력 프로젝션, 인과적 컨볼루션, SSM, 게이팅, 출력 프로젝션 구성.\n",
    "       내부적으로 (B, D, L) 텐서 레이아웃 사용.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, state_size: int, d_conv: int = 4, expand: int = 1,\n",
    "                 dropout_prob: float = 0.1, device: str = 'cuda:2', dtype: torch.dtype = torch.float32):\n",
    "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_inner = int(expand * d_model) # 내부 확장 차원 (D)\n",
    "        self.state_size = state_size\n",
    "        self.d_conv = d_conv\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "\n",
    "        # 입력 정규화 (RMSNorm)\n",
    "        self.norm = RMSNorm(d_model, eps=1e-5)\n",
    "\n",
    "        # 입력 프로젝션 (d_model -> 2 * d_inner)\n",
    "        self.in_proj = nn.Linear(d_model, 2 * self.d_inner, bias=False, **factory_kwargs)\n",
    "\n",
    "        # 인과적 컨볼루션 파라미터 (가중치 형태: D, K)\n",
    "        self.conv1d_weight = nn.Parameter(torch.empty(self.d_inner, d_conv, **factory_kwargs))\n",
    "        self.conv1d_bias = nn.Parameter(torch.empty(self.d_inner, **factory_kwargs))\n",
    "\n",
    "        # SSM 모듈 인스턴스화 (Mamba v1 파라미터 전달 옵션 추가 가능)\n",
    "        self.ssm = SSM(self.d_inner, state_size, device=device, dtype=dtype)\n",
    "                      # dt_rank, dt_min 등 SSM 파라미터 전달 가능\n",
    "\n",
    "        # 출력 프로젝션 (d_inner -> d_model)\n",
    "        self.out_proj = nn.Linear(self.d_inner, d_model, bias=False, **factory_kwargs)\n",
    "\n",
    "        # 잔차 연결 드롭아웃\n",
    "        self.dropout_res = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Mamba 블록 순방향 계산.\"\"\"\n",
    "        B, L, D_model = x.shape # 입력 형태: (B, L, d_model)\n",
    "        residual = x # 잔차 연결용 원본 저장\n",
    "\n",
    "        # 1. 입력 정규화\n",
    "        x_norm = self.norm(x)\n",
    "\n",
    "        # 2. 입력 프로젝션 및 분할 (x_in, z)\n",
    "        xz = self.in_proj(x_norm) # 결과: (B, L, 2*D)\n",
    "        x_in, z = xz.chunk(2, dim=-1) # 각각: (B, L, D)\n",
    "\n",
    "        # 3. 인과적 컨볼루션 브랜치\n",
    "        #    입력 레이아웃 변경: (B, L, D) -> (B, D, L)\n",
    "        x_conv_in = rearrange(x_in, 'b l d -> b d l').contiguous()\n",
    "        #    최적화된 causal_conv1d_fn 호출 (SiLU 활성화 포함)\n",
    "        #    출력 x_conv_out 형태: (B, D, L)\n",
    "        x_conv_out = causal_conv1d_fn(\n",
    "            x=x_conv_in, weight=self.conv1d_weight, bias=self.conv1d_bias, activation='silu'\n",
    "        )\n",
    "\n",
    "        # 4. SSM 브랜치\n",
    "        #    컨볼루션 출력(B, D, L)을 SSM에 직접 전달\n",
    "        #    SSM 출력 y_ssm 형태: (B, D, L)\n",
    "        y_ssm = self.ssm(x_conv_out)\n",
    "\n",
    "        # 5. 게이팅 메커니즘\n",
    "        #    y_ssm을 z와 곱하기 위해 (B, L, D) 형태로 변경\n",
    "        y_ssm_rearranged = rearrange(y_ssm, 'b d l -> b l d')\n",
    "        #    z에 SiLU 활성화 적용 후 요소별 곱셈\n",
    "        y_gated = y_ssm_rearranged * F.silu(z) # 결과: (B, L, D)\n",
    "\n",
    "        # 6. 출력 프로젝션\n",
    "        output = self.out_proj(y_gated) # 결과: (B, L, d_model)\n",
    "\n",
    "        # 7. 잔차 연결 및 드롭아웃\n",
    "        output = residual + self.dropout_res(output) # 최종 결과: (B, L, d_model)\n",
    "\n",
    "        return output\n",
    "\n",
    "# --- Mamba 모델 전체 ---\n",
    "class Mamba(nn.Module):\n",
    "    \"\"\"Mamba 언어 모델 전체 구조.\"\"\"\n",
    "    def __init__(self, d_model: int, n_layers: int, vocab_size: int, state_size: int = 16,\n",
    "                 d_conv: int = 4, expand: int = 2, dropout_prob: float = 0.1,\n",
    "                 device: str = 'cuda:2', dtype: torch.dtype = torch.float32):\n",
    "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_layers = n_layers\n",
    "        self.vocab_size = vocab_size\n",
    "        # 저장된 파라미터는 validation 등에 사용될 수 있음\n",
    "        self.state_size = state_size\n",
    "        self.d_conv = d_conv\n",
    "        self.expand = expand\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "\n",
    "        # 토큰 임베딩 레이어\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, **factory_kwargs)\n",
    "        self.dropout_emb = nn.Dropout(dropout_prob)\n",
    "\n",
    "        # Mamba 블록 스택\n",
    "        self.layers = nn.ModuleList([\n",
    "            MambaBlock(\n",
    "                d_model=d_model, state_size=state_size, d_conv=d_conv,\n",
    "                expand=expand, dropout_prob=dropout_prob, device=device, dtype=dtype\n",
    "            ) for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "        # 최종 정규화 레이어\n",
    "        self.norm_f = RMSNorm(d_model, eps=1e-5)\n",
    "        # 언어 모델링 헤드 (출력 레이어)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False, **factory_kwargs)\n",
    "\n",
    "        # 가중치 공유 (임베딩과 LM 헤드)\n",
    "        self.lm_head.weight = self.embedding.weight\n",
    "\n",
    "        # 모델 가중치 초기화 적용\n",
    "        self.apply(self._init_weights)\n",
    "        print(f\"Mamba 모델 초기화 완료. Device: {device}, Dtype: {dtype}\")\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"모델의 각 모듈 가중치 초기화.\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # dt_proj의 bias는 특별 초기화되므로 건너뜀\n",
    "            if hasattr(module.bias, '_no_reinit') and module.bias._no_reinit:\n",
    "                return\n",
    "            # Linear 레이어 가중치 초기화 (예: GPT-2 스타일)\n",
    "            std = 0.02\n",
    "            # 모델 깊이에 따른 스케일링 (옵션)\n",
    "            if self.n_layers > 0:\n",
    "                 # 입력/출력 프로젝션 등 특정 레이어에만 적용 고려 가능\n",
    "                 if module.weight.shape[0] == self.d_model or module.weight.shape[1] == self.d_model:\n",
    "                     std /= math.sqrt(2.0 * self.n_layers)\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            # 임베딩 가중치 초기화\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, MambaBlock):\n",
    "            # MambaBlock 내의 Conv1d 가중치 초기화\n",
    "            if hasattr(module, 'conv1d_weight'):\n",
    "                # Kaiming 초기화 (SiLU 활성화 함수에 적합)\n",
    "                nn.init.kaiming_normal_(module.conv1d_weight, nonlinearity='leaky_relu')\n",
    "            if hasattr(module, 'conv1d_bias'):\n",
    "                nn.init.zeros_(module.conv1d_bias)\n",
    "        # RMSNorm 가중치는 해당 클래스 생성자에서 1로 초기화됨\n",
    "        # SSM 파라미터 (A_log, D)는 해당 클래스 생성자에서 초기화됨\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor) -> tuple[torch.Tensor, None]:\n",
    "        \"\"\"Mamba 모델 순방향 계산.\"\"\"\n",
    "        B, L = input_ids.shape\n",
    "\n",
    "        # 1. 임베딩 및 드롭아웃\n",
    "        # 임베딩 레이어는 LongTensor 입력 필요\n",
    "        x = self.embedding(input_ids.long())\n",
    "        x = self.dropout_emb(x)\n",
    "\n",
    "        # 2. Mamba 블록 순차 적용\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        # 3. 최종 정규화\n",
    "        x = self.norm_f(x)\n",
    "\n",
    "        # 4. LM 헤드를 통해 로짓 계산\n",
    "        logits = self.lm_head(x) # 결과: [B, L, vocab_size]\n",
    "\n",
    "        # Loss 계산은 외부 학습 루프에서 처리 (labels 사용)\n",
    "        # 여기서는 로짓과 None 반환 (일반적인 Hugging Face 모델 스타일)\n",
    "        return logits, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf6a7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_SIZE = 8\n",
    "\n",
    "x = torch.randint(0, vocab_size, (BATCH_SIZE, SEQUENCE_LENGTH)).to(device)\n",
    "\n",
    "mamba = Mamba(HIDDEN_DIM, NUM_LAYERS, vocab_size, STATE_SIZE, d_conv=4, expand=1.2).to(device)\n",
    "\n",
    "test_output, _ = mamba(x)\n",
    "print(f\"test_output.shape = {test_output.shape}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31528f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(mamba.to(device), input_size=(BATCH_SIZE, SEQUENCE_LENGTH), dtypes=[torch.long])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a427c989",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_test(\"Mamba\", mamba, start_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbb22cc",
   "metadata": {},
   "source": [
    "## Model 4: minGRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73586ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Tuple, Optional, Union\n",
    "from torch.nn import RMSNorm # 사용자의 환경에 RMSNorm이 torch.nn에 있다고 가정합니다.\n",
    "\n",
    "class CausalDepthWiseConv1d(nn.Module):\n",
    "    \"\"\"\n",
    "    인과적(Causal) 1D 깊이별 분리형 컨볼루션(Depth-wise Separable Convolution) 모듈.\n",
    "    깊이별 컨볼루션과 점별 컨볼루션을 순차적으로 적용합니다.\n",
    "    인과성을 유지하기 위해 시퀀스의 왼쪽에 패딩을 추가하여,\n",
    "    시간 t의 출력은 시간 t까지의 입력에만 의존하도록 합니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, dimension: int, kernel_size: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dimension (int): 입력 및 출력 채널의 수.\n",
    "            kernel_size (int): 컨볼루션 커널의 크기.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        # 인과적 패딩 계산: (커널 크기 - 1) 만큼 왼쪽에, 0만큼 오른쪽에 패딩.\n",
    "        self.causal_padding = (self.kernel_size - 1, 0)\n",
    "\n",
    "        self.depthwise_pointwise_conv = nn.Sequential(\n",
    "            # 깊이별 컨볼루션: 각 입력 채널에 대해 독립적인 필터를 적용.\n",
    "            nn.Conv1d(dimension, dimension, kernel_size=kernel_size, groups=dimension),\n",
    "            # 점별 컨볼루션: 채널 정보를 혼합하기 위한 1x1 컨볼루션.\n",
    "            nn.Conv1d(dimension, dimension, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): 입력 텐서. 형태: (배치 크기, 시퀀스 길이, 채널 수).\n",
    "        Returns:\n",
    "            torch.Tensor: 출력 텐서. 형태: (배치 크기, 시퀀스 길이, 채널 수).\n",
    "        \"\"\"\n",
    "        # Conv1D를 위해 (배치 크기, 채널 수, 시퀀스 길이) 형태로 변경.\n",
    "        x_transposed = x.transpose(1, 2)\n",
    "        # 시퀀스 왼쪽에 인과적 패딩 적용.\n",
    "        x_padded = F.pad(x_transposed, self.causal_padding)\n",
    "        # 깊이별-점별 컨볼루션 적용.\n",
    "        x_conv_out = self.depthwise_pointwise_conv(x_padded)\n",
    "        # 다시 (배치 크기, 시퀀스 길이, 채널 수) 형태로 변경.\n",
    "        return x_conv_out.transpose(1, 2)\n",
    "\n",
    "\n",
    "class MinGRUCoreOps:\n",
    "    \"\"\"\n",
    "    MinGRU의 핵심 수학적 연산을 위한 정적 메소드들을 포함하는 클래스.\n",
    "    MinGRU의 수학적 공식에 기반한 함수들이며, 사용자 정의 활성화 함수 및\n",
    "    순환적 업데이트를 위한 병렬 스캔 연산 등을 포함합니다.\n",
    "    원문에서는 mF 네임스페이스로 사용되었습니다.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def g_activation(x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        MinGRU를 위한 사용자 정의 활성화 함수 'g'.\n",
    "        g(x) = x + 0.5  (x >= 0 인 경우)\n",
    "        g(x) = sigmoid(x) (x < 0 인 경우)\n",
    "        \"\"\"\n",
    "        output = torch.empty_like(x)\n",
    "        positive_mask = x >= 0\n",
    "        # 양수 또는 0인 경우\n",
    "        output[positive_mask] = x[positive_mask] + torch.tensor(0.5, dtype=x.dtype, device=x.device)\n",
    "        # 음수인 경우\n",
    "        output[~positive_mask] = torch.sigmoid(x[~positive_mask])\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def log_g_activation(x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        사용자 정의 활성화 함수 'g'의 로그(logarithm) 값.\n",
    "        log_g(x) = log(x + 0.5) (x >= 0 인 경우)\n",
    "        log_g(x) = log(sigmoid(x)) = -softplus(-x) (x < 0 인 경우)\n",
    "        수치적 안정성을 위해 사용됩니다.\n",
    "        \"\"\"\n",
    "        output = torch.empty_like(x)\n",
    "        positive_mask = x >= 0\n",
    "        # 양수 또는 0인 경우\n",
    "        output[positive_mask] = (x[positive_mask] + torch.tensor(0.5, dtype=x.dtype, device=x.device)).log().to(x.dtype)\n",
    "        # 음수인 경우 (log(sigmoid(x)) = -softplus(-x))\n",
    "        output[~positive_mask] = (-F.softplus(-x[~positive_mask])).to(x.dtype)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def _log_space_parallel_scan(log_a_terms: torch.Tensor, log_b_terms: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        로그 공간에서의 병렬 스캔(누적) 연산.\n",
    "        MinGRU의 순환 관계를 병렬적으로 계산하기 위해 사용됩니다.\n",
    "        x_t = a_t * x_{t-1} + b_t  형태의 연산을 로그 공간에서 수행합니다.\n",
    "        (원문: _parallel_scan_log)\n",
    "\n",
    "        Args:\n",
    "            log_a_terms (torch.Tensor): 로그 스케일의 'a' 항들. (log(a_t))\n",
    "            log_b_terms (torch.Tensor): 로그 스케일의 'b' 항들. (log(b_t))\n",
    "                                      첫 번째 요소는 log(x_0) * a_0 (초기 상태 관련 항)을 포함할 수 있음.\n",
    "        Returns:\n",
    "            torch.Tensor: 병렬 스캔 결과의 지수 값 (원래 스케일). (exp(log_x_t))\n",
    "        \"\"\"\n",
    "        # log_a_terms의 누적 합을 계산 (alpha_star). 시퀀스 차원(dim=1)을 따라 누적.\n",
    "        # 패딩은 초기 조건(x_0)을 처리하기 위함.\n",
    "        padding_dims = [0] * (log_a_terms.ndim - 2) * 2 + [1, 0] # (..., 1, 0) sequence dimension padding\n",
    "        alpha_star = F.pad(torch.cumsum(log_a_terms, dim=1), padding_dims)\n",
    "\n",
    "        # log_b_terms - alpha_star 항에 대해 로그-합-지수(logsumexp) 누적을 수행.\n",
    "        # 이는 x_0 항과 b_t / (a_1*...*a_t) 항들의 누적 합을 로그 공간에서 계산하는 것과 유사.\n",
    "        x0_plus_b_star_contribution = torch.logcumsumexp(log_b_terms - alpha_star, dim=1)\n",
    "\n",
    "        # 최종 로그 스케일 결과: alpha_star + x0_plus_b_star_contribution\n",
    "        log_x_sequence = alpha_star + x0_plus_b_star_contribution\n",
    "        return torch.exp(log_x_sequence)\n",
    "\n",
    "    @staticmethod\n",
    "    def _mingru_recurrent_update_log_space(log_previous_hidden_state: torch.Tensor,\n",
    "                                           gate_z: torch.Tensor,\n",
    "                                           candidate_hidden: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        MinGRU의 순환 업데이트를 로그 공간에서 병렬적으로 수행.\n",
    "        h_t = (1-z_t)*h_{t-1} + z_t*h_tilde_t\n",
    "        (원문: _mingru_parallel)\n",
    "\n",
    "        Args:\n",
    "            log_previous_hidden_state (torch.Tensor): 이전 은닉 상태의 로그 값 (log(h_{t-1})).\n",
    "                                                     형태: (배치, 1, 은닉 차원) - 시퀀스의 첫 번째 요소에 해당.\n",
    "            gate_z (torch.Tensor): 게이트 z_t 값. 형태: (배치, 시퀀스 길이, 은닉 차원).\n",
    "            candidate_hidden (torch.Tensor): 후보 은닉 상태 h_tilde_t 값. 형태: (배치, 시퀀스 길이, 은닉 차원).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: 계산된 은닉 상태 시퀀스 h_t. 형태: (배치, 시퀀스 길이, 은닉 차원).\n",
    "        \"\"\"\n",
    "        # log(z_t) = -softplus(-gate_z)  (log(sigmoid(gate_z)))\n",
    "        log_z = -F.softplus(-gate_z)\n",
    "        # log(1-z_t) = -softplus(gate_z) (log(sigmoid(-gate_z)))\n",
    "        log_one_minus_z = -F.softplus(gate_z)\n",
    "\n",
    "        # log(h_tilde_t) 계산\n",
    "        log_candidate_hidden = MinGRUCoreOps.log_g_activation(candidate_hidden)\n",
    "\n",
    "        # 병렬 스캔을 위한 log_b 항 구성:\n",
    "        # 첫 번째 항은 log(h_prev) (실제로는 log(h_0) * (1-z_0)에 대응하는 항으로 변환됨)\n",
    "        # 나머지 항은 log(z_t) + log(h_tilde_t)\n",
    "        # log_previous_hidden_state는 (B, 1, H) 이므로, dim=1로 concat\n",
    "        log_b_for_scan = torch.cat((log_previous_hidden_state, log_z + log_candidate_hidden), dim=1)\n",
    "\n",
    "        # log_a 항은 log(1-z_t)\n",
    "        # 병렬 스캔 실행\n",
    "        # h_sequence_plus_initial 결과는 h_0, h_1, ..., h_T-1 을 포함\n",
    "        h_sequence_plus_initial = MinGRUCoreOps._log_space_parallel_scan(log_one_minus_z, log_b_for_scan)\n",
    "\n",
    "        # 초기 상태 h_0를 제외한 h_1, ..., h_T-1 시퀀스 반환\n",
    "        return h_sequence_plus_initial[:, 1:]\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_output_sequence(gate_z: torch.Tensor,\n",
    "                                  candidate_hidden: torch.Tensor,\n",
    "                                  initial_hidden_state: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        주어진 게이트, 후보 은닉 상태, 초기 은닉 상태를 사용하여 MinGRU의 출력 시퀀스를 계산.\n",
    "        (원문: mingru_gate_hidden)\n",
    "\n",
    "        Args:\n",
    "            gate_z (torch.Tensor): 게이트 z_t 값.\n",
    "            candidate_hidden (torch.Tensor): 후보 은닉 상태 h_tilde_t 값.\n",
    "            initial_hidden_state (torch.Tensor): 초기 은닉 상태 h_0 값.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: 계산된 은닉 상태 시퀀스.\n",
    "        \"\"\"\n",
    "        # 수치적 안정성을 위해 작은 epsilon 값을 더한 후 로그 변환.\n",
    "        epsilon = torch.tensor(1e-12, dtype=initial_hidden_state.dtype, device=initial_hidden_state.device)\n",
    "        log_initial_hidden_state = (initial_hidden_state + epsilon).log()\n",
    "\n",
    "        return MinGRUCoreOps._mingru_recurrent_update_log_space(log_initial_hidden_state, gate_z, candidate_hidden)\n",
    "\n",
    "\n",
    "class MinGRU(nn.Module):\n",
    "    \"\"\"\n",
    "    MinGRU (Minimal Gated Recurrent Unit) 모델.\n",
    "    여러 MinGRU 레이어를 쌓아 구성할 수 있습니다.\n",
    "    \"\"\"\n",
    "    # 타입 힌트 (JIT 호환성을 위해 Final 제거)\n",
    "    layer_dims: Tuple[int, ...]\n",
    "    num_internal_layers: int\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_dims_list: List[int],\n",
    "        use_bias: bool = True,\n",
    "        apply_layer_norm: bool = True,\n",
    "        dropout_probability: float = 0.1,\n",
    "        use_residual: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim (int): 입력 특징의 차원.\n",
    "            hidden_dims_list (List[int]): 각 MinGRU 레이어의 은닉 차원 리스트.\n",
    "            use_bias (bool): 선형 계층에서 편향을 사용할지 여부.\n",
    "            apply_layer_norm (bool): 각 레이어 내부에 LayerNorm을 적용할지 여부.\n",
    "            dropout_probability (float): 드롭아웃 확률. 0이면 드롭아웃을 적용하지 않음.\n",
    "            use_residual (bool): 잔차 연결(residual connection)을 사용할지 여부.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layer_dims = tuple([input_dim] + hidden_dims_list) # 입력 차원 + 각 레이어 은닉 차원\n",
    "        self.num_internal_layers = len(hidden_dims_list) # MinGRU 레이어의 수\n",
    "        self.dropout_probability = dropout_probability\n",
    "        self.use_residual = use_residual\n",
    "        self.apply_internal_norm = apply_layer_norm\n",
    "\n",
    "        torch_layers_list = [] # nn.ModuleList로 관리될 레이어 모듈들\n",
    "        # 각 레이어의 입력 차원과 출력 차원을 순회\n",
    "        layer_dim_pairs = zip(self.layer_dims[:-1], self.layer_dims[1:])\n",
    "\n",
    "        for layer_idx, (current_input_dim, current_output_dim) in enumerate(layer_dim_pairs):\n",
    "            layer_modules_dict = {} # nn.ModuleDict로 관리될 특정 레이어의 모듈들\n",
    "\n",
    "            if self.apply_internal_norm:\n",
    "                layer_modules_dict[\"norm\"] = torch.nn.LayerNorm(current_input_dim)\n",
    "            else:\n",
    "                layer_modules_dict[\"norm\"] = torch.nn.Identity()\n",
    "\n",
    "            # 게이트(z_t)와 후보 은닉 상태(h_tilde_t)를 계산하기 위한 선형 계층\n",
    "            layer_modules_dict[\"gate_and_candidate_proj\"] = torch.nn.Linear(current_input_dim, current_output_dim * 2, bias=use_bias)\n",
    "\n",
    "            # 잔차 연결을 위한 차원 정렬용 선형 계층 (입력과 출력 차원이 다를 경우)\n",
    "            if use_residual and current_input_dim != current_output_dim:\n",
    "                layer_modules_dict[\"residual_align_proj\"] = torch.nn.Linear(current_input_dim, current_output_dim, bias=False)\n",
    "            else:\n",
    "                layer_modules_dict[\"residual_align_proj\"] = torch.nn.Identity()\n",
    "\n",
    "            # 드롭아웃 (마지막 레이어 제외)\n",
    "            if self.dropout_probability > 0.0 and layer_idx < (self.num_internal_layers - 1):\n",
    "                layer_modules_dict[\"dropout\"] = torch.nn.Dropout(p=self.dropout_probability)\n",
    "            else:\n",
    "                layer_modules_dict[\"dropout\"] = torch.nn.Identity()\n",
    "\n",
    "            torch_layers_list.append(torch.nn.ModuleDict(layer_modules_dict))\n",
    "\n",
    "        self.layers = torch.nn.ModuleList(torch_layers_list)\n",
    "        self.apply(self._init_weights)  # 가중치 초기화\n",
    "        \n",
    "    def _init_weights(self, module: nn.Module):\n",
    "        # 선형 레이어 가중치 초기화 (게이트 편향 특수 처리 포함)\n",
    "        for i in range(self.num_internal_layers):\n",
    "            layer_components = self.layers[i]\n",
    "            output_dim = self.layer_dims[i+1]\n",
    "            # 1. 잔차 연결 프로젝션 레이어 초기화 (일반)\n",
    "            if isinstance(layer_components[\"residual_align_proj\"], nn.Linear):\n",
    "                nn.init.normal_(layer_components[\"residual_align_proj\"].weight, mean=0.0, std=0.02)\n",
    "            \n",
    "            # 2. 게이트 및 후보 은닉 상태 프로젝션 레이어 특별 초기화\n",
    "            gate_hidden_proj_layer = layer_components[\"gate_and_candidate_proj\"]\n",
    "            \n",
    "            # 가중치는 표준 정규분포로 초기화\n",
    "            nn.init.normal_(gate_hidden_proj_layer.weight, mean=0.0, std=0.02)\n",
    "\n",
    "            # 편향(bias)은 특별하게 초기화\n",
    "            if gate_hidden_proj_layer.bias is not None:\n",
    "                # 편향 텐서는 (output_dim * 2) 크기\n",
    "                # 앞 절반은 게이트 로짓(gate_logits)용, 뒤 절반은 후보 은닉 상태(candidate_hidden_inputs)용\n",
    "                \n",
    "                # 게이트 로짓의 편향을 음수 값으로 초기화 (예: -1.0)\n",
    "                # sigmoid(음수)는 0에 가까워져, z_t가 작아짐\n",
    "                # h_t = (1-z_t)*h_{t-1} + z_t*h_tilde_t 에서 (1-z_t)가 커지므로\n",
    "                # 학습 초기에 이전 은닉 상태 h_{t-1}를 더 많이 유지하도록 유도하여 안정적인 학습에 도움을 줍니다.\n",
    "                nn.init.constant_(gate_hidden_proj_layer.bias[:output_dim], -1.0)\n",
    "                \n",
    "                # 후보 은닉 상태의 편향은 0으로 초기화\n",
    "                nn.init.zeros_(gate_hidden_proj_layer.bias[output_dim:])\n",
    "            \n",
    "\n",
    "    def init_hidden_states(self, input_tensor: torch.Tensor) -> List[torch.Tensor]:\n",
    "        \"\"\"\n",
    "        각 MinGRU 레이어의 초기 은닉 상태를 생성합니다.\n",
    "        MinGRU의 g 활성화 함수를 사용하여 0으로 초기화된 텐서를 변환합니다.\n",
    "\n",
    "        Args:\n",
    "            input_tensor (torch.Tensor): 입력 텐서. 배치 크기, 자료형, 장치 정보를 얻기 위해 사용.\n",
    "                                         형태: (배치 크기, 시퀀스 길이, 입력 차원).\n",
    "        Returns:\n",
    "            List[torch.Tensor]: 각 레이어의 초기 은닉 상태 리스트.\n",
    "                                각 텐서의 형태: (배치 크기, 1, 해당 레이어의 은닉 차원).\n",
    "        \"\"\"\n",
    "        batch_size = input_tensor.shape[0]\n",
    "        initial_states = []\n",
    "        for hidden_dim in self.layer_dims[1:]: # 각 레이어의 출력(은닉) 차원에 대해\n",
    "            # (배치 크기, 1, 은닉 차원) 형태의 0 텐서 생성 후 g_activation 적용\n",
    "            zeros = input_tensor.new_zeros(batch_size, 1, hidden_dim, dtype=input_tensor.dtype)\n",
    "            initial_states.append(MinGRUCoreOps.g_activation(zeros))\n",
    "        return initial_states\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_sequence: torch.Tensor,\n",
    "        initial_hidden_states_list: Optional[List[torch.Tensor]] = None,\n",
    "    ) -> Tuple[torch.Tensor, List[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        MinGRU 모델의 순방향 전파를 수행합니다.\n",
    "\n",
    "        Args:\n",
    "            input_sequence (torch.Tensor): 입력 시퀀스. 형태: (배치 크기, 시퀀스 길이, 입력 차원).\n",
    "            initial_hidden_states_list (Optional[List[torch.Tensor]]):\n",
    "                제공될 경우 사용할 초기 은닉 상태 리스트.\n",
    "                None이면 내부적으로 init_hidden_states를 호출하여 생성.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, List[torch.Tensor]]:\n",
    "                - 최종 레이어의 출력 시퀀스. 형태: (배치 크기, 시퀀스 길이, 마지막 레이어의 은닉 차원).\n",
    "                - 각 레이어의 다음 시퀀스를 위한 마지막 은닉 상태 리스트.\n",
    "                  각 텐서의 형태: (배치 크기, 1, 해당 레이어의 은닉 차원).\n",
    "        \"\"\"\n",
    "        if initial_hidden_states_list is None:\n",
    "            hidden_states_for_layers = self.init_hidden_states(input_sequence)\n",
    "        else:\n",
    "            hidden_states_for_layers = initial_hidden_states_list\n",
    "\n",
    "        current_layer_input_sequence = input_sequence\n",
    "        next_step_hidden_states_list: List[torch.Tensor] = []\n",
    "\n",
    "        for layer_idx in range(self.num_internal_layers):\n",
    "            layer_module_components = self.layers[layer_idx]\n",
    "\n",
    "            normalized_sequence = layer_module_components[\"norm\"](current_layer_input_sequence)\n",
    "            # 게이트와 후보 은닉 상태를 한 번에 계산 후 분리\n",
    "            gate_values, candidate_hidden_states = layer_module_components[\"gate_and_candidate_proj\"](normalized_sequence).chunk(2, dim=2)\n",
    "\n",
    "            # 현재 레이어의 초기(또는 이전 스텝의) 은닉 상태\n",
    "            previous_hidden_state_for_this_layer: torch.Tensor = hidden_states_for_layers[layer_idx]\n",
    "\n",
    "            # MinGRU 순환 연산\n",
    "            output_sequence_current_layer = MinGRUCoreOps.calculate_output_sequence(\n",
    "                gate_values, candidate_hidden_states, previous_hidden_state_for_this_layer\n",
    "            )\n",
    "            # 다음 스텝(또는 다음 시퀀스)을 위한 마지막 은닉 상태 저장\n",
    "            next_step_hidden_states_list.append(output_sequence_current_layer[:, -1:])\n",
    "\n",
    "            if self.use_residual:\n",
    "                residual_input = layer_module_components[\"residual_align_proj\"](current_layer_input_sequence)\n",
    "                output_sequence_current_layer = output_sequence_current_layer + residual_input\n",
    "\n",
    "            current_layer_input_sequence = layer_module_components[\"dropout\"](output_sequence_current_layer)\n",
    "\n",
    "        return current_layer_input_sequence, next_step_hidden_states_list\n",
    "\n",
    "\n",
    "class SwiGLUFFN(nn.Module):\n",
    "    \"\"\"\n",
    "    SwiGLU 활성화 함수를 사용하는 FFN (FeedForward Network) 모듈.\n",
    "    일반적으로 트랜스포머 모델에서 사용됩니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, dimension: int, expansion_factor: float = 2.0, dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dimension (int): 입력 및 출력 차원.\n",
    "            expansion_factor (float): 내부 은닉 차원을 결정하기 위한 확장 비율.\n",
    "                                      실제 은닉 차원은 dimension * expansion_factor * (2/3) 근사.\n",
    "            dropout (float): 드롭아웃 확률.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # 내부 은닉 차원 계산 (일반적으로 2/3는 GLU 계열에서 사용됨)\n",
    "        # 또한, 보통 하드웨어 가속을 위해 8의 배수로 맞춤.\n",
    "        hidden_dimension = int(dimension * expansion_factor * (2/3))\n",
    "        hidden_dimension = (hidden_dimension + 7) // 8 * 8 # 8의 배수로 올림\n",
    "\n",
    "        self.gate_proj = nn.Linear(dimension, hidden_dimension, bias=False) # W_gate\n",
    "        self.up_proj = nn.Linear(dimension, hidden_dimension, bias=False)   # W_up\n",
    "        self.down_proj = nn.Linear(hidden_dimension, dimension, bias=False) # W_down\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "        self.activation_fn = nn.SiLU() # Swish 또는 Sigmoid Linear Unit\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        SwiGLU FFN의 순방향 전파.\n",
    "        Output = Dropout(W_down(SiLU(W_gate(x)) * W_up(x)))\n",
    "        \"\"\"\n",
    "        gated_x = self.activation_fn(self.gate_proj(x))\n",
    "        activated_x = gated_x * self.up_proj(x)\n",
    "        return self.dropout_layer(self.down_proj(activated_x))\n",
    "\n",
    "class minGRULM(nn.Module):\n",
    "    \"\"\"\n",
    "    MinGRU를 핵심 순환 블록으로 사용하는 전체 언어 모델 아키텍처 (버전 3).\n",
    "    임베딩, 선택적 컨볼루션, MinGRU 스택, FFN, 최종 정규화 및 출력 프로젝션을 포함합니다.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        embedding_dim: int,\n",
    "        mingru_hidden_dims_config: Union[int, List[int]], # MinGRU 레이어들의 은닉 차원 설정\n",
    "        num_mingru_layers: Optional[int] = None, # hidden_dims_config가 int일 경우 레이어 수\n",
    "        dropout_rate: float = 0.1,\n",
    "        rms_norm_epsilon: float = 1e-8,\n",
    "        use_convolution_block: bool = True,\n",
    "        conv_kernel_size: int = 3,\n",
    "        ffn_expansion_factor: float = 1.0 # SwiGLUFFN의 확장 비율\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # MinGRU 내부 레이어들의 은닉 차원 리스트 결정\n",
    "        if isinstance(mingru_hidden_dims_config, int):\n",
    "            if num_mingru_layers is None or num_mingru_layers < 1:\n",
    "                raise ValueError(\"`num_mingru_layers`는 `mingru_hidden_dims_config`가 정수일 때 양의 정수여야 합니다.\")\n",
    "            actual_mingru_hidden_dims = [mingru_hidden_dims_config] * num_mingru_layers\n",
    "        elif isinstance(mingru_hidden_dims_config, list):\n",
    "            if not mingru_hidden_dims_config:\n",
    "                raise ValueError(\"`mingru_hidden_dims_config` 리스트는 비어있을 수 없습니다.\")\n",
    "            actual_mingru_hidden_dims = mingru_hidden_dims_config\n",
    "        else:\n",
    "            raise TypeError(\"`mingru_hidden_dims_config`는 정수 또는 정수 리스트여야 합니다.\")\n",
    "\n",
    "        # MinGRU 모듈의 입력 및 출력 차원\n",
    "        first_mingru_layer_dim = actual_mingru_hidden_dims[0]\n",
    "        last_mingru_layer_dim = actual_mingru_hidden_dims[-1]\n",
    "\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # 임베딩 차원을 첫 번째 MinGRU 레이어의 입력 차원으로 프로젝션\n",
    "        if embedding_dim != first_mingru_layer_dim:\n",
    "            self.embedding_to_mingru_projection = nn.Linear(embedding_dim, first_mingru_layer_dim)\n",
    "        else:\n",
    "            self.embedding_to_mingru_projection = nn.Identity()\n",
    "\n",
    "        # 선택적 인과적 깊이별 컨볼루션 블록\n",
    "        if use_convolution_block:\n",
    "            self.causal_conv_block = CausalDepthWiseConv1d(first_mingru_layer_dim, conv_kernel_size)\n",
    "        else:\n",
    "            self.causal_conv_block = nn.Identity()\n",
    "\n",
    "        # MinGRU 전 정규화\n",
    "        self.norm_before_mingru = RMSNorm(first_mingru_layer_dim, eps=rms_norm_epsilon)\n",
    "\n",
    "        self.mingru_stack = MinGRU(\n",
    "            input_dim=first_mingru_layer_dim,\n",
    "            hidden_dims_list=actual_mingru_hidden_dims,\n",
    "            apply_layer_norm=True, # MinGRU 내부 LayerNorm 사용\n",
    "            dropout_probability=dropout_rate,\n",
    "            use_residual=True      # MinGRU 내부 잔차 연결 사용\n",
    "        )\n",
    "\n",
    "        # MinGRU 블록의 입력과 출력 차원이 다를 경우 잔차 연결을 위한 프로젝션\n",
    "        if first_mingru_layer_dim != last_mingru_layer_dim:\n",
    "            self.mingru_residual_projection = nn.Linear(first_mingru_layer_dim, last_mingru_layer_dim)\n",
    "        else:\n",
    "            self.mingru_residual_projection = nn.Identity()\n",
    "\n",
    "        # MinGRU 후, FFN 전 정규화\n",
    "        self.norm_after_mingru_before_ffn = RMSNorm(last_mingru_layer_dim, eps=rms_norm_epsilon)\n",
    "        self.ffn_block = SwiGLUFFN(\n",
    "            dimension=last_mingru_layer_dim,\n",
    "            expansion_factor=ffn_expansion_factor,\n",
    "            dropout=dropout_rate\n",
    "        )\n",
    "\n",
    "        # 최종 정규화 및 로짓 프로젝션\n",
    "        self.final_output_norm = RMSNorm(last_mingru_layer_dim, eps=rms_norm_epsilon)\n",
    "        self.to_logits_projection = nn.Linear(last_mingru_layer_dim, vocab_size)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module: nn.Module): # module 인자를 명시적으로 타입 힌트\n",
    "        \"\"\"\n",
    "        모델 내 각 모듈의 가중치를 초기화합니다.\n",
    "        self.apply() 메소드에 의해 호출됩니다.\n",
    "        \"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02) \n",
    "        elif isinstance(module, nn.Conv1d):\n",
    "            nn.init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='leaky_relu') # mode='fan_in'이 일반적\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, RMSNorm):\n",
    "            if hasattr(module, 'weight') and module.weight is not None: # RMSNorm에 weight가 있을 경우\n",
    "                nn.init.ones_(module.weight)\n",
    "        elif isinstance(module, nn.LayerNorm): # MinGRU 내부의 LayerNorm 처리\n",
    "            if hasattr(module, 'weight') and module.weight is not None:\n",
    "                nn.init.ones_(module.weight)\n",
    "            if hasattr(module, 'bias') and module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor) -> torch.Tensor: # 반환 타입에서 None 제거, 일반적으로 로짓만 반환\n",
    "        \"\"\"\n",
    "        minGRULM 모델의 순방향 전파.\n",
    "\n",
    "        Args:\n",
    "            input_ids (torch.Tensor): 입력 토큰 ID 시퀀스. 형태: (배치 크기, 시퀀스 길이).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: 최종 로짓. 형태: (배치 크기, 시퀀스 길이, 어휘 크기).\n",
    "        \"\"\"\n",
    "        # 1. 임베딩 및 초기 프로젝션\n",
    "        current_hidden_repr = self.token_embedding(input_ids)\n",
    "        current_hidden_repr = self.embedding_to_mingru_projection(current_hidden_repr)\n",
    "\n",
    "        # 2. (선택적) 컨볼루션 블록 + 잔차 연결\n",
    "        residual_conv_input = current_hidden_repr\n",
    "        if not isinstance(self.causal_conv_block, nn.Identity):\n",
    "            # 컨볼루션 블록이 Identity가 아닐 때만 연산 (즉, use_convolution_block=True 일 때)\n",
    "            current_hidden_repr = residual_conv_input + self.causal_conv_block(residual_conv_input)\n",
    "        # else: current_hidden_repr은 residual_conv_input과 동일하게 유지됨\n",
    "\n",
    "        # 3. MinGRU 블록 + 잔차 연결\n",
    "        residual_mingru_input = current_hidden_repr\n",
    "        mingru_input_normalized = self.norm_before_mingru(residual_mingru_input)\n",
    "        mingru_output_sequence, _ = self.mingru_stack(mingru_input_normalized) # 다음 은닉 상태는 사용 안함\n",
    "        current_hidden_repr = self.mingru_residual_projection(residual_mingru_input) + mingru_output_sequence\n",
    "\n",
    "        # 4. FFN 블록 + 잔차 연결\n",
    "        residual_ffn_input = current_hidden_repr\n",
    "        ffn_input_normalized = self.norm_after_mingru_before_ffn(residual_ffn_input)\n",
    "        current_hidden_repr = residual_ffn_input + self.ffn_block(ffn_input_normalized)\n",
    "\n",
    "        # 5. 최종 정규화 및 로짓 계산\n",
    "        final_representation_normalized = self.final_output_norm(current_hidden_repr)\n",
    "        logits = self.to_logits_projection(final_representation_normalized)\n",
    "\n",
    "        return logits, _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c86a2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model Initialization\n",
    "mingru = minGRULM(vocab_size, EMBEDDING_DIM, int(HIDDEN_DIM*1.0), NUM_LAYERS).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8199acd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(mingru, input_size=(BATCH_SIZE, SEQUENCE_LENGTH), dtypes=[torch.long])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f66c397",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_test(\"minGRU\", mingru, start_text)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
