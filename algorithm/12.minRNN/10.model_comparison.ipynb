{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison: MinGRU vs Mamba vs Transformer\n",
    "\n",
    "이 노트북에서는 4가지 시퀀스 모델의 학습 및 추론 성능을 비교합니다.\n",
    "\n",
    "## 비교 모델\n",
    "\n",
    "1. **MinGRU (Triton)**: Triton 커널 기반 최적화\n",
    "2. **MinGRU (CUDA)**: CUDA C++ 커널 기반 최적화\n",
    "3. **Mamba**: State Space Model with selective scan\n",
    "4. **Transformer (LLaMA)**: Flash Attention 기반\n",
    "\n",
    "## 비교 항목\n",
    "\n",
    "- 학습 손실 (Training Loss)\n",
    "- 검증 손실 (Validation Loss)\n",
    "- 에포크당 학습 시간\n",
    "- VRAM 사용량\n",
    "- 파라미터 수\n",
    "\n",
    "## 참고\n",
    "\n",
    "- MinGRU: \"Were RNNs All We Needed?\" (arXiv:2410.01201)\n",
    "- Mamba: \"Mamba: Linear-Time Sequence Modeling\" (arXiv:2312.00752)\n",
    "- LLaMA: \"LLaMA: Open and Efficient Foundation Language Models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.8.0+cu128\n",
      "CUDA Available: True\n",
      "GPU: NVIDIA A100 80GB PCIe\n",
      "CUDA Version: 12.8\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Model Configuration (유사한 파라미터 수를 위한 설정)\n",
    "SEQUENCE_LENGTH = 256\n",
    "EMBEDDING_DIM = 64\n",
    "HIDDEN_DIM = 128\n",
    "NUM_LAYERS = 4\n",
    "NUM_HEADS = 4\n",
    "FFN_DIM = 256\n",
    "DROPOUT = 0.1\n",
    "\n",
    "# Mamba specific\n",
    "STATE_SIZE = 16\n",
    "D_CONV = 4\n",
    "EXPAND = 2\n",
    "\n",
    "# Training Configuration\n",
    "BATCH_SIZE = 512\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 1e-3\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "TinyShakespeare 데이터셋을 사용한 문자 수준 언어 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "def create_char_mappings(text):\n",
    "    chars = sorted(list(set(text)))\n",
    "    char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "    idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "    return chars, char_to_idx, idx_to_char\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, text, seq_length, char_to_idx):\n",
    "        self.text = text\n",
    "        self.seq_length = seq_length\n",
    "        self.char_to_idx = char_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text) - self.seq_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = [self.char_to_idx[ch] for ch in self.text[idx:idx+self.seq_length]]\n",
    "        y = [self.char_to_idx[ch] for ch in self.text[idx+1:idx+self.seq_length+1]]\n",
    "        return torch.tensor(x), torch.tensor(y)\n",
    "\n",
    "def prepare_data(text, seq_length, batch_size, val_split):\n",
    "    chars, char_to_idx, idx_to_char = create_char_mappings(text)\n",
    "    \n",
    "    val_size = int(len(text) * val_split)\n",
    "    train_text, val_text = text[:-val_size], text[-val_size:]\n",
    "    \n",
    "    train_dataset = CharDataset(train_text, seq_length, char_to_idx)\n",
    "    val_dataset = CharDataset(val_text, seq_length, char_to_idx)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    return train_loader, val_loader, chars, char_to_idx, idx_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 1,115,394\n",
      "Vocabulary size: 65\n",
      "Train batches: 1961\n",
      "Val batches: 218\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "text = load_data('../data/input.txt')\n",
    "train_loader, val_loader, chars, char_to_idx, idx_to_char = prepare_data(\n",
    "    text, SEQUENCE_LENGTH, BATCH_SIZE, VALIDATION_SPLIT\n",
    ")\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(f\"Total characters: {len(text):,}\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"Root Mean Square Layer Normalization\"\"\"\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        rms = torch.rsqrt(x.float().pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "        return (x * rms).type_as(x) * self.weight\n",
    "\n",
    "\n",
    "class SwiGLUFFN(nn.Module):\n",
    "    \"\"\"SwiGLU Feed-Forward Network (LLaMA style)\"\"\"\n",
    "    def __init__(self, dim: int, hidden_dim: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n",
    "        self.w3 = nn.Linear(dim, hidden_dim, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.w2(self.dropout(F.silu(self.w1(x)) * self.w3(x)))\n",
    "\n",
    "\n",
    "class CausalConv1d(nn.Module):\n",
    "    \"\"\"Causal 1D Convolution\"\"\"\n",
    "    def __init__(self, dim: int, kernel_size: int = 3):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv = nn.Conv1d(dim, dim, kernel_size, groups=dim)\n",
    "        self.pointwise = nn.Conv1d(dim, dim, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.pad(x, (self.kernel_size - 1, 0))\n",
    "        x = self.pointwise(self.conv(x))\n",
    "        return x.transpose(1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 & 2: MinGRU (Triton / CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triton available: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0202 15:39:52.874000 17606 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "W0202 15:39:52.874000 17606 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA kernel available: True\n"
     ]
    }
   ],
   "source": [
    "# Import MinGRU from backbone\n",
    "sys.path.insert(0, os.path.dirname(os.path.abspath('')))\n",
    "\n",
    "from backbone import (\n",
    "    MinGRUTriton, MinGRUCUDA,\n",
    "    mingru_scan_triton, mingru_scan_cuda,\n",
    "    is_triton_available, is_cuda_available,\n",
    ")\n",
    "\n",
    "print(f\"Triton available: {is_triton_available()}\")\n",
    "print(f\"CUDA kernel available: {is_cuda_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinGRUBlock(nn.Module):\n",
    "    \"\"\"MinGRU Block with Pre-Norm and Residual\"\"\"\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, use_cuda: bool = True):\n",
    "        super().__init__()\n",
    "        self.use_cuda = use_cuda and is_cuda_available()\n",
    "        \n",
    "        if self.use_cuda:\n",
    "            self.mingru = MinGRUCUDA(d_model, dropout=0.0)\n",
    "        else:\n",
    "            self.mingru = MinGRUTriton(d_model, dropout=0.0)\n",
    "        \n",
    "        self.norm1 = RMSNorm(d_model)\n",
    "        self.norm2 = RMSNorm(d_model)\n",
    "        self.ffn = SwiGLUFFN(d_model, d_model * 2, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # MinGRU with residual\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.mingru(x)\n",
    "        x = residual + self.dropout(x)\n",
    "        \n",
    "        # FFN with residual\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ffn(x)\n",
    "        x = residual + self.dropout(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class MinGRULM(nn.Module):\n",
    "    \"\"\"MinGRU Language Model\"\"\"\n",
    "    def __init__(self, vocab_size: int, d_model: int, n_layers: int, \n",
    "                 dropout: float = 0.1, use_cuda: bool = True):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.conv = CausalConv1d(d_model, kernel_size=3)\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            MinGRUBlock(d_model, dropout, use_cuda) \n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        self.norm = RMSNorm(d_model)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        self.lm_head.weight = self.embedding.weight  # Weight tying\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, None]:\n",
    "        h = self.embedding(x)\n",
    "        h = self.conv(h)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            h = layer(h)\n",
    "        \n",
    "        h = self.norm(h)\n",
    "        logits = self.lm_head(h)\n",
    "        return logits, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: Mamba (State Space Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mamba SSM library available\n"
     ]
    }
   ],
   "source": [
    "# Check Mamba availability\n",
    "try:\n",
    "    from mamba_ssm import Mamba as MambaModule\n",
    "    MAMBA_AVAILABLE = True\n",
    "    print(\"Mamba SSM library available\")\n",
    "except ImportError:\n",
    "    MAMBA_AVAILABLE = False\n",
    "    print(\"Mamba SSM library not available, using simplified version\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MAMBA_AVAILABLE:\n",
    "    class MambaBlock(nn.Module):\n",
    "        \"\"\"Mamba Block using official mamba-ssm\"\"\"\n",
    "        def __init__(self, d_model: int, d_state: int = 16, d_conv: int = 4, expand: int = 2, dropout: float = 0.1):\n",
    "            super().__init__()\n",
    "            self.norm = RMSNorm(d_model)\n",
    "            self.mamba = MambaModule(\n",
    "                d_model=d_model,\n",
    "                d_state=d_state,\n",
    "                d_conv=d_conv,\n",
    "                expand=expand,\n",
    "            )\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "            residual = x\n",
    "            x = self.norm(x)\n",
    "            x = self.mamba(x)\n",
    "            return residual + self.dropout(x)\n",
    "else:\n",
    "    # Simplified Mamba without selective scan\n",
    "    class MambaBlock(nn.Module):\n",
    "        \"\"\"Simplified Mamba Block (without mamba-ssm)\"\"\"\n",
    "        def __init__(self, d_model: int, d_state: int = 16, d_conv: int = 4, expand: int = 2, dropout: float = 0.1):\n",
    "            super().__init__()\n",
    "            d_inner = d_model * expand\n",
    "            self.norm = RMSNorm(d_model)\n",
    "            self.in_proj = nn.Linear(d_model, d_inner * 2, bias=False)\n",
    "            self.conv = nn.Conv1d(d_inner, d_inner, d_conv, groups=d_inner, padding=d_conv-1)\n",
    "            self.out_proj = nn.Linear(d_inner, d_model, bias=False)\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "            residual = x\n",
    "            x = self.norm(x)\n",
    "            xz = self.in_proj(x)\n",
    "            x, z = xz.chunk(2, dim=-1)\n",
    "            \n",
    "            x = x.transpose(1, 2)\n",
    "            x = self.conv(x)[:, :, :x.shape[2]]\n",
    "            x = x.transpose(1, 2)\n",
    "            x = F.silu(x) * z\n",
    "            x = self.out_proj(x)\n",
    "            return residual + self.dropout(x)\n",
    "\n",
    "\n",
    "class MambaLM(nn.Module):\n",
    "    \"\"\"Mamba Language Model\"\"\"\n",
    "    def __init__(self, vocab_size: int, d_model: int, n_layers: int,\n",
    "                 d_state: int = 16, d_conv: int = 4, expand: int = 2, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.dropout_emb = nn.Dropout(dropout)\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            MambaBlock(d_model, d_state, d_conv, expand, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        self.norm = RMSNorm(d_model)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        self.lm_head.weight = self.embedding.weight\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, None]:\n",
    "        h = self.embedding(x)\n",
    "        h = self.dropout_emb(h)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            h = layer(h)\n",
    "        \n",
    "        h = self.norm(h)\n",
    "        logits = self.lm_head(h)\n",
    "        return logits, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4: Transformer (LLaMA style with Flash Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flash Attention available\n"
     ]
    }
   ],
   "source": [
    "# Check Flash Attention availability\n",
    "try:\n",
    "    from flash_attn import flash_attn_func\n",
    "    FLASH_AVAILABLE = True\n",
    "    print(\"Flash Attention available\")\n",
    "except ImportError:\n",
    "    FLASH_AVAILABLE = False\n",
    "    print(\"Flash Attention not available, using standard attention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_freqs_cis(dim: int, max_seq_len: int, theta: float = 10000.0, device: str = 'cuda'):\n",
    "    \"\"\"Precompute RoPE frequencies\"\"\"\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2, device=device).float() / dim))\n",
    "    t = torch.arange(max_seq_len, device=device)\n",
    "    freqs = torch.outer(t, freqs)\n",
    "    return torch.polar(torch.ones_like(freqs), freqs)\n",
    "\n",
    "\n",
    "def apply_rotary_emb(x: torch.Tensor, freqs_cis: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Apply rotary positional embeddings\"\"\"\n",
    "    x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
    "    freqs_cis = freqs_cis[:x.shape[1]].view(1, x.shape[1], 1, -1)\n",
    "    x_rotated = x_complex * freqs_cis\n",
    "    return torch.view_as_real(x_rotated).reshape(x.shape).type_as(x)\n",
    "\n",
    "\n",
    "class TransformerAttention(nn.Module):\n",
    "    \"\"\"Multi-Head Attention with optional Flash Attention\"\"\"\n",
    "    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "        \n",
    "        self.wq = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.wk = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.wv = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.wo = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, freqs_cis: torch.Tensor) -> torch.Tensor:\n",
    "        B, L, D = x.shape\n",
    "        \n",
    "        q = self.wq(x).view(B, L, self.n_heads, self.head_dim)\n",
    "        k = self.wk(x).view(B, L, self.n_heads, self.head_dim)\n",
    "        v = self.wv(x).view(B, L, self.n_heads, self.head_dim)\n",
    "        \n",
    "        # Apply RoPE\n",
    "        q = apply_rotary_emb(q, freqs_cis)\n",
    "        k = apply_rotary_emb(k, freqs_cis)\n",
    "        \n",
    "        if FLASH_AVAILABLE:\n",
    "            # Flash Attention expects [B, L, n_heads, head_dim]\n",
    "            out = flash_attn_func(q.half(), k.half(), v.half(), causal=True).float()\n",
    "        else:\n",
    "            # Standard attention\n",
    "            q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
    "            scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "            mask = torch.triu(torch.ones(L, L, device=x.device), 1).bool()\n",
    "            scores.masked_fill_(mask, float('-inf'))\n",
    "            attn = F.softmax(scores, dim=-1)\n",
    "            attn = self.dropout(attn)\n",
    "            out = torch.matmul(attn, v).transpose(1, 2)\n",
    "        \n",
    "        out = out.reshape(B, L, D)\n",
    "        return self.wo(out)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Transformer Block (LLaMA style)\"\"\"\n",
    "    def __init__(self, d_model: int, n_heads: int, ffn_dim: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.attention = TransformerAttention(d_model, n_heads, dropout)\n",
    "        self.ffn = SwiGLUFFN(d_model, ffn_dim, dropout)\n",
    "        self.norm1 = RMSNorm(d_model)\n",
    "        self.norm2 = RMSNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, freqs_cis: torch.Tensor) -> torch.Tensor:\n",
    "        # Attention with residual\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.attention(x, freqs_cis)\n",
    "        x = residual + self.dropout(x)\n",
    "        \n",
    "        # FFN with residual\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ffn(x)\n",
    "        x = residual + self.dropout(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerLM(nn.Module):\n",
    "    \"\"\"Transformer Language Model (LLaMA style)\"\"\"\n",
    "    def __init__(self, vocab_size: int, d_model: int, n_layers: int, \n",
    "                 n_heads: int, ffn_dim: int, max_seq_len: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(d_model, n_heads, ffn_dim, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        self.norm = RMSNorm(d_model)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        self.lm_head.weight = self.embedding.weight\n",
    "        \n",
    "        # Precompute RoPE\n",
    "        self.register_buffer('freqs_cis', precompute_freqs_cis(\n",
    "            d_model // n_heads, max_seq_len * 2, device='cuda'\n",
    "        ))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, None]:\n",
    "        h = self.embedding(x)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            h = layer(h, self.freqs_cis)\n",
    "        \n",
    "        h = self.norm(h)\n",
    "        logits = self.lm_head(h)\n",
    "        return logits, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"Count trainable parameters\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device, epoch):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    losses = []\n",
    "    vram_usage = []\n",
    "    scaler = torch.amp.GradScaler('cuda')\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch}\", leave=False)\n",
    "    for x, y in pbar:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "            output, _ = model(x)\n",
    "            loss = criterion(output.view(-1, output.size(-1)), y.view(-1))\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        allocated = torch.cuda.memory_allocated(device) / (1024**2)\n",
    "        vram_usage.append(allocated)\n",
    "        pbar.set_postfix(loss=f'{loss.item():.4f}', vram=f'{allocated:.0f}MB')\n",
    "    \n",
    "    return np.mean(losses), np.mean(vram_usage)\n",
    "\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    \"\"\"Validate model\"\"\"\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            output, _ = model(x)\n",
    "            loss = criterion(output.view(-1, output.size(-1)), y.view(-1))\n",
    "            losses.append(loss.item())\n",
    "    \n",
    "    return np.mean(losses)\n",
    "\n",
    "\n",
    "def train_model(model, model_name, train_loader, val_loader, device, epochs, lr):\n",
    "    \"\"\"Full training loop\"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, fused=True)\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [], 'val_loss': [], \n",
    "        'epoch_time': [], 'vram_usage': []\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {model_name}\")\n",
    "    print(f\"Parameters: {count_parameters(model):,}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        train_loss, vram = train_epoch(model, train_loader, criterion, optimizer, device, epoch)\n",
    "        val_loss = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        epoch_time = time.time() - start_time\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['epoch_time'].append(epoch_time)\n",
    "        history['vram_usage'].append(vram)\n",
    "        \n",
    "        print(f\"Epoch {epoch}/{epochs} | \"\n",
    "              f\"Train: {train_loss:.4f} | Val: {val_loss:.4f} | \"\n",
    "              f\"Time: {epoch_time:.1f}s | VRAM: {vram:.0f}MB\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Models and Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Parameter Counts:\n",
      "----------------------------------------\n",
      "MinGRU-Triton: 551,808\n",
      "MinGRU-CUDA: 551,808\n",
      "Mamba: 474,880\n",
      "Transformer: 664,832\n"
     ]
    }
   ],
   "source": [
    "# Create all models\n",
    "models = {}\n",
    "\n",
    "# MinGRU (Triton)\n",
    "models['MinGRU-Triton'] = MinGRULM(\n",
    "    vocab_size=vocab_size, d_model=HIDDEN_DIM, n_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT, use_cuda=False\n",
    ").to(device)\n",
    "\n",
    "# MinGRU (CUDA)\n",
    "models['MinGRU-CUDA'] = MinGRULM(\n",
    "    vocab_size=vocab_size, d_model=HIDDEN_DIM, n_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT, use_cuda=True\n",
    ").to(device)\n",
    "\n",
    "# Mamba\n",
    "models['Mamba'] = MambaLM(\n",
    "    vocab_size=vocab_size, d_model=HIDDEN_DIM, n_layers=NUM_LAYERS,\n",
    "    d_state=STATE_SIZE, d_conv=D_CONV, expand=EXPAND, dropout=DROPOUT\n",
    ").to(device)\n",
    "\n",
    "# Transformer\n",
    "models['Transformer'] = TransformerLM(\n",
    "    vocab_size=vocab_size, d_model=HIDDEN_DIM, n_layers=NUM_LAYERS,\n",
    "    n_heads=NUM_HEADS, ffn_dim=FFN_DIM, max_seq_len=SEQUENCE_LENGTH, dropout=DROPOUT\n",
    ").to(device)\n",
    "\n",
    "# Print parameter counts\n",
    "print(\"\\nModel Parameter Counts:\")\n",
    "print(\"-\" * 40)\n",
    "for name, model in models.items():\n",
    "    print(f\"{name}: {count_parameters(model):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training MinGRU-Triton\n",
      "Parameters: 551,808\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 | Train: 2.0097 | Val: 1.6298 | Time: 99.7s | VRAM: 49MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 | Train: 1.4110 | Val: 1.5419 | Time: 95.8s | VRAM: 49MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 | Train: 1.2871 | Val: 1.5158 | Time: 95.8s | VRAM: 49MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 | Train: 1.2072 | Val: 1.5228 | Time: 96.1s | VRAM: 49MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 | Train: 1.1531 | Val: 1.5194 | Time: 96.2s | VRAM: 49MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 | Train: 1.1189 | Val: 1.5234 | Time: 96.4s | VRAM: 49MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 | Train: 1.0973 | Val: 1.5322 | Time: 96.3s | VRAM: 49MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 | Train: 1.0823 | Val: 1.5370 | Time: 96.4s | VRAM: 49MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 | Train: 1.0709 | Val: 1.5404 | Time: 96.2s | VRAM: 49MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 | Train: 1.0621 | Val: 1.5500 | Time: 96.5s | VRAM: 49MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 | Train: 1.0550 | Val: 1.5569 | Time: 96.3s | VRAM: 49MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 | Train: 1.0491 | Val: 1.5555 | Time: 96.4s | VRAM: 49MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 | Train: 1.0440 | Val: 1.5695 | Time: 96.4s | VRAM: 49MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 | Train: 1.0397 | Val: 1.5677 | Time: 96.7s | VRAM: 49MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 | Train: 1.0358 | Val: 1.5675 | Time: 96.6s | VRAM: 49MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 | Train: 1.0325 | Val: 1.5744 | Time: 96.4s | VRAM: 49MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 | Train: 1.0293 | Val: 1.5696 | Time: 96.3s | VRAM: 49MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    }
   ],
   "source": [
    "# Train all models\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    torch.cuda.empty_cache()\n",
    "    history = train_model(model, name, train_loader, val_loader, device, EPOCHS, LEARNING_RATE)\n",
    "    results[name] = history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison(results):\n",
    "    \"\"\"Plot training comparison\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    colors = ['#e74c3c', '#f39c12', '#3498db', '#9b59b6']\n",
    "    \n",
    "    # Training Loss\n",
    "    ax = axes[0, 0]\n",
    "    for (name, history), color in zip(results.items(), colors):\n",
    "        ax.plot(history['train_loss'], label=name, color=color, linewidth=2)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Training Loss')\n",
    "    ax.set_title('Training Loss Comparison')\n",
    "    ax.legend()\n",
    "    ax.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Validation Loss\n",
    "    ax = axes[0, 1]\n",
    "    for (name, history), color in zip(results.items(), colors):\n",
    "        ax.plot(history['val_loss'], label=name, color=color, linewidth=2)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Validation Loss')\n",
    "    ax.set_title('Validation Loss Comparison')\n",
    "    ax.legend()\n",
    "    ax.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Epoch Time\n",
    "    ax = axes[1, 0]\n",
    "    names = list(results.keys())\n",
    "    avg_times = [np.mean(h['epoch_time']) for h in results.values()]\n",
    "    bars = ax.bar(names, avg_times, color=colors)\n",
    "    ax.set_ylabel('Average Epoch Time (s)')\n",
    "    ax.set_title('Training Speed Comparison')\n",
    "    for bar, time in zip(bars, avg_times):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                f'{time:.1f}s', ha='center', va='bottom', fontweight='bold')\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # VRAM Usage\n",
    "    ax = axes[1, 1]\n",
    "    avg_vram = [np.mean(h['vram_usage']) for h in results.values()]\n",
    "    bars = ax.bar(names, avg_vram, color=colors)\n",
    "    ax.set_ylabel('Average VRAM Usage (MB)')\n",
    "    ax.set_title('Memory Usage Comparison')\n",
    "    for bar, vram in zip(bars, avg_vram):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 10,\n",
    "                f'{vram:.0f}MB', ha='center', va='bottom', fontweight='bold')\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.suptitle(f'Model Comparison (seq_len={SEQUENCE_LENGTH}, epochs={EPOCHS})',\n",
    "                 fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('model_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_comparison(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Table\n",
    "def print_summary(results, models):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    data = []\n",
    "    for name, history in results.items():\n",
    "        data.append({\n",
    "            'Model': name,\n",
    "            'Parameters': f\"{count_parameters(models[name]):,}\",\n",
    "            'Final Train Loss': f\"{history['train_loss'][-1]:.4f}\",\n",
    "            'Final Val Loss': f\"{history['val_loss'][-1]:.4f}\",\n",
    "            'Avg Epoch Time': f\"{np.mean(history['epoch_time']):.1f}s\",\n",
    "            'Avg VRAM': f\"{np.mean(history['vram_usage']):.0f}MB\",\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    print(df.to_string(index=False))\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return df\n",
    "\n",
    "summary_df = print_summary(results, models)\n",
    "summary_df.to_csv('model_comparison_summary.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_text, max_length=200):\n",
    "    \"\"\"Generate text from model\"\"\"\n",
    "    model.eval()\n",
    "    current_text = start_text\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            x = torch.tensor([[char_to_idx[ch] for ch in current_text[-SEQUENCE_LENGTH:]]]).to(device)\n",
    "            output, _ = model(x)\n",
    "            probs = F.softmax(output[0, -1], dim=0)\n",
    "            next_idx = torch.multinomial(probs, 1).item()\n",
    "            current_text += idx_to_char[next_idx]\n",
    "    \n",
    "    return current_text\n",
    "\n",
    "\n",
    "# Generate from each model\n",
    "start_text = \"ROMEO: \"\n",
    "print(\"\\nText Generation Samples:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for name, model in models.items():\n",
    "    generated = generate_text(model, start_text, max_length=150)\n",
    "    print(f\"\\n[{name}]\")\n",
    "    print(\"-\"*40)\n",
    "    print(generated[:200])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
