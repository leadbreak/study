{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd8158eb",
   "metadata": {},
   "source": [
    "```\n",
    "Adaptive Gradient Clip for Stable Calculation\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20ec8bf0",
   "metadata": {
    "id": "20ec8bf0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3390f4",
   "metadata": {
    "id": "5a3390f4"
   },
   "source": [
    "## Hyperparameters and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0c61f36",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1732703737281,
     "user": {
      "displayName": "Hugman Sangkeun Jung",
      "userId": "08689291704194029524"
     },
     "user_tz": -540
    },
    "id": "d0c61f36",
    "outputId": "45d9894c-a14d-4164-9f2d-699025751e72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Modified hyperparameters\n",
    "SEQUENCE_LENGTH = 64\n",
    "\n",
    "EMBEDDING_DIM = 32\n",
    "HIDDEN_DIM = EMBEDDING_DIM*2\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "EPOCHS = 2\n",
    "LEARNING_RATE = 1e-4\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "NUM_LAYERS = 2\n",
    "NUM_HEADS = 4\n",
    "FFN_DIM = 256\n",
    "DROPOUT = 0.1\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c9b962",
   "metadata": {
    "id": "89c9b962"
   },
   "source": [
    "## Data Preparation\n",
    "\n",
    "We are using the TinyShakespeare dataset, a small character-level text corpus consisting of a subset of Shakespeare's plays. It's often used for testing sequence models, as it includes a rich set of vocabulary and provides a challenging task for next-character prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "089830e2",
   "metadata": {
    "id": "089830e2"
   },
   "outputs": [],
   "source": [
    "## Utility Functions\n",
    "\n",
    "def load_data(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "def create_char_mappings(text):\n",
    "    chars = sorted(list(set(text)))\n",
    "    char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "    idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "    return chars, char_to_idx, idx_to_char"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fc6452",
   "metadata": {
    "id": "30fc6452"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f07670d9",
   "metadata": {
    "id": "f07670d9"
   },
   "outputs": [],
   "source": [
    "class CharDataset(Dataset):\n",
    "    def __init__(self, text, seq_length, char_to_idx):\n",
    "        self.text = text\n",
    "        self.seq_length = seq_length\n",
    "        self.char_to_idx = char_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text) - self.seq_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = [self.char_to_idx[ch] for ch in self.text[idx:idx+self.seq_length]]\n",
    "        y = [self.char_to_idx[ch] for ch in self.text[idx+1:idx+self.seq_length+1]]\n",
    "        return torch.tensor(x), torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9172da13",
   "metadata": {
    "id": "9172da13"
   },
   "outputs": [],
   "source": [
    "def prepare_data(text, seq_length, batch_size, val_split):\n",
    "    chars, char_to_idx, idx_to_char = create_char_mappings(text)\n",
    "\n",
    "    # Split data into train and validation\n",
    "    val_size = int(len(text) * val_split)\n",
    "    train_text, val_text = text[:-val_size], text[-val_size:]\n",
    "\n",
    "    train_dataset = CharDataset(train_text, seq_length, char_to_idx)\n",
    "    val_dataset = CharDataset(val_text, seq_length, char_to_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=12, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=12, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader, chars, char_to_idx, idx_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "nEwKFB_8L6AG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3706,
     "status": "ok",
     "timestamp": 1732704246464,
     "user": {
      "displayName": "Hugman Sangkeun Jung",
      "userId": "08689291704194029524"
     },
     "user_tz": -540
    },
    "id": "nEwKFB_8L6AG",
    "outputId": "e75cdff9-3775-461e-d930-0ced534bf74d"
   },
   "outputs": [],
   "source": [
    "# !wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=19zosLuU0z4MxIMKbGVYEGlg52QyfbTIy' -O input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47d03398",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 668,
     "status": "ok",
     "timestamp": 1732704255324,
     "user": {
      "displayName": "Hugman Sangkeun Jung",
      "userId": "08689291704194029524"
     },
     "user_tz": -540
    },
    "id": "47d03398",
    "outputId": "f247f5b3-88ff-4ecb-e8a3-4b2e42a2820b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 1115394\n",
      "Vocabulary size: 65\n",
      "Train dataset size: 1003791\n",
      "Validation dataset size: 111475\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "text = load_data('../data/input.txt')\n",
    "train_loader, val_loader, chars, char_to_idx, idx_to_char = prepare_data(text, SEQUENCE_LENGTH, BATCH_SIZE, VALIDATION_SPLIT)\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(f\"Total characters: {len(text)}\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Train dataset size: {len(train_loader.dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9442b104",
   "metadata": {
    "id": "9442b104"
   },
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1f0a6a1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 916,
     "status": "ok",
     "timestamp": 1732704257555,
     "user": {
      "displayName": "Hugman Sangkeun Jung",
      "userId": "08689291704194029524"
     },
     "user_tz": -540
    },
    "id": "f1f0a6a1",
    "outputId": "9ef6eb5a-0015-40ad-f143-236e3d38fb7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([512, 64])\n",
      "Target shape: torch.Size([512, 64])\n",
      "Sample 1: ------------------------------\n",
      "Input sequence : urses!If not by war, by surfeit die your king,As ours by murde\n",
      "Target sequence: rses!If not by war, by surfeit die your king,As ours by murder\n",
      "\n",
      "Sample 2: ------------------------------\n",
      "Input sequence : the stars; then let the mutinous windsStrike the proud cedars '\n",
      "Target sequence: he stars; then let the mutinous windsStrike the proud cedars 'g\n",
      "\n",
      "Sample 3: ------------------------------\n",
      "Input sequence : fill your grave up: stir, nay, come away,Bequeath to death your\n",
      "Target sequence: ill your grave up: stir, nay, come away,Bequeath to death your \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to convert index sequence to character sequence\n",
    "def indices_to_text(indices, idx_to_char):\n",
    "    return ''.join([idx_to_char[idx.item()] for idx in indices])\n",
    "\n",
    "# Get a batch of data\n",
    "dataiter = iter(train_loader)\n",
    "batch_x, batch_y = next(dataiter)\n",
    "\n",
    "print(f\"Input shape: {batch_x.shape}\")\n",
    "print(f\"Target shape: {batch_y.shape}\")\n",
    "\n",
    "# Print a few samples from the batch\n",
    "num_samples = 3\n",
    "for i in range(num_samples):\n",
    "    print(f\"Sample {i+1}: ------------------------------\" )\n",
    "    print(\"Input sequence :\", indices_to_text(batch_x[i], idx_to_char).replace('\\n',''))\n",
    "    print(\"Target sequence:\", indices_to_text(batch_y[i], idx_to_char).replace('\\n',''))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5486f73f",
   "metadata": {
    "id": "5486f73f"
   },
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61a52f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_vram_usage(device=\"cuda\"):\n",
    "    allocated = torch.cuda.memory_allocated(device) / (1024**2)  # in MB\n",
    "    reserved = torch.cuda.memory_reserved(device) / (1024**2)    # in MB\n",
    "    max_allocated = torch.cuda.max_memory_allocated(device) / (1024**2)  # in MB\n",
    "    print(f\"Allocated: {allocated:.2f} MB, Reserved: {reserved:.2f} MB, Max Allocated: {max_allocated:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59178a11",
   "metadata": {
    "id": "59178a11"
   },
   "outputs": [],
   "source": [
    "def validate(model, dataloader, criterion, device, epoch, step):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            output, _ = model(x)\n",
    "            loss = criterion(output.view(-1, output.size(-1)), y.view(-1))\n",
    "            losses.append((step, epoch, loss.item()))\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea5e61f5",
   "metadata": {
    "id": "ea5e61f5"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.utils as nn_utils\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    dataloader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    device,\n",
    "    epoch,\n",
    "    step,\n",
    "    ema_decay: float = 0.9,\n",
    "    clip_factor: float = 0.8,\n",
    "):\n",
    "    \"\"\"\n",
    "    Adaptive gradient clipping based on EMA of past gradient norms.\n",
    "    ema_decay: decay rate for EMA (close to 1.0 → slow update)\n",
    "    clip_factor: multiply EMA norm by this to get clipping threshold\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    scaler = torch.amp.GradScaler()\n",
    "    losses = []\n",
    "    ema_grad_norm = None\n",
    "\n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch}\", leave=False)\n",
    "    for batch, (x, y) in enumerate(pbar):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.amp.autocast(device_type='cuda'):\n",
    "            output, _ = model(x)\n",
    "            loss = criterion(output.view(-1, output.size(-1)), y.view(-1))\n",
    "\n",
    "        # backward\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # unscale grads before clipping\n",
    "        scaler.unscale_(optimizer)\n",
    "\n",
    "        # compute current total grad norm\n",
    "        total_norm = 0.0\n",
    "        for p in model.parameters():\n",
    "            if p.grad is not None:\n",
    "                param_norm = p.grad.data.norm(2)\n",
    "                total_norm += param_norm.item() ** 2\n",
    "        total_norm = total_norm ** 0.5\n",
    "\n",
    "        # initialize or update EMA of grad norms\n",
    "        if ema_grad_norm is None:\n",
    "            ema_grad_norm = total_norm\n",
    "        else:\n",
    "            ema_grad_norm = ema_decay * ema_grad_norm + (1.0 - ema_decay) * total_norm\n",
    "\n",
    "        # set clipping threshold\n",
    "        max_grad_norm = ema_grad_norm * clip_factor\n",
    "\n",
    "        # clip gradients\n",
    "        nn_utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\n",
    "        # optimizer step\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        step += 1\n",
    "\n",
    "        # update progress bar\n",
    "        allocated = torch.cuda.memory_allocated(device) / (1024**2)\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'grad_norm': f'{total_norm:.2f}',\n",
    "            'ema_norm': f'{ema_grad_norm:.2f}',\n",
    "            'clip_thr': f'{max_grad_norm:.2f}',\n",
    "            'step': step,\n",
    "            'vram': f'{allocated:.1f}MB'\n",
    "        })\n",
    "        losses.append((step, epoch, loss.item()))\n",
    "\n",
    "    return losses, step, allocated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbedfd61",
   "metadata": {
    "id": "cbedfd61"
   },
   "source": [
    "## Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e91075e8",
   "metadata": {
    "id": "e91075e8"
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, device, epochs):\n",
    "    all_train_losses = []\n",
    "    all_val_losses = []\n",
    "    all_vram_usages = []\n",
    "    step = 0\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        # Training phase with tqdm updates\n",
    "        epoch_train_losses, step, vram_usage = train(model, train_loader, criterion, optimizer, device, epoch, step)\n",
    "        all_train_losses.extend(epoch_train_losses)\n",
    "        all_vram_usages.append(vram_usage)\n",
    "        \n",
    "        # Validation phase\n",
    "        epoch_val_losses = validate(model, val_loader, criterion, device, epoch, step)\n",
    "        all_val_losses.extend(epoch_val_losses)\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        \n",
    "        print(f'Epoch {epoch}/{epochs}, Train Loss: {epoch_train_losses[-1][2]:.4f}, '\n",
    "              f'Val Loss: {epoch_val_losses[-1][2]:.4f}, Epoch Time: {epoch_time:.2f}s',\n",
    "              f'Average Vram Usage: {np.mean(vram_usage):.2f}MB')\n",
    "\n",
    "    train_losses_df = pd.DataFrame(all_train_losses, columns=['step', 'epoch', 'loss_value'])\n",
    "    val_losses_df = pd.DataFrame(all_val_losses, columns=['step', 'epoch', 'loss_value'])\n",
    "    # average_vram_usage = np.mean(all_vram_usages)\n",
    "    return model, train_losses_df, val_losses_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4535c886",
   "metadata": {
    "id": "4535c886"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, char_to_idx, idx_to_char, start_text, device, max_length=500):\n",
    "    model.eval()\n",
    "    current_text = start_text\n",
    "    hidden = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            x = torch.tensor([[char_to_idx[ch] for ch in current_text[-SEQUENCE_LENGTH:]]]).to(device)\n",
    "            output, hidden = model(x, hidden)\n",
    "            probs = torch.softmax(output[0, -1], dim=0)\n",
    "            next_char_idx = torch.multinomial(probs, 1).item()\n",
    "            next_char = idx_to_char[next_char_idx]\n",
    "            current_text += next_char\n",
    "\n",
    "    return current_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6aef1cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_comparison_dict = {}\n",
    "\n",
    "def add_loss_to_comparison(model_name, train_losses_df, val_losses_df):\n",
    "    \"\"\"\n",
    "    Adds training and validation losses from a model to the comparison dictionary.\n",
    "    \"\"\"\n",
    "    loss_comparison_dict[model_name] = {\n",
    "        'train': train_losses_df,\n",
    "        'val': val_losses_df\n",
    "    }\n",
    "\n",
    "def print_final_losses(loss_dict):\n",
    "    for model_name, losses in loss_dict.items():\n",
    "        train_df = losses['train']\n",
    "        val_df = losses['val']\n",
    "        final_train = train_df.groupby('epoch')['loss_value'].last().iloc[-1]\n",
    "        final_val = val_df.groupby('epoch')['loss_value'].last().iloc[-1]\n",
    "        print(f\"{model_name}: Final Train Loss: {final_train:.4f}, Final Val Loss: {final_val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "973b4a80",
   "metadata": {
    "id": "973b4a80"
   },
   "outputs": [],
   "source": [
    "# Function to plot loss curves\n",
    "def plot_loss(train_losses_df, val_losses_df):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    # Plot training losses\n",
    "    for epoch in train_losses_df['epoch'].unique():\n",
    "        epoch_train_losses = train_losses_df[train_losses_df['epoch'] == epoch]\n",
    "        plt.plot(epoch_train_losses['step'], epoch_train_losses['loss_value'],\n",
    "                 color='blue', alpha=0.3)\n",
    "\n",
    "    # scatter training loss at the end of each epoch\n",
    "    last_train_losses = train_losses_df.groupby('epoch').last().reset_index()\n",
    "    plt.scatter(last_train_losses['step'], last_train_losses['loss_value'],\n",
    "                color='blue')\n",
    "\n",
    "    # Plot and scatter validation loss at the end of each epoch\n",
    "    last_val_losses = val_losses_df.groupby('epoch').last().reset_index()\n",
    "    plt.plot(last_val_losses['step'], last_val_losses['loss_value'],\n",
    "             color='orange', label='Validation Loss')\n",
    "    plt.scatter(last_val_losses['step'], last_val_losses['loss_value'],\n",
    "                color='orange')\n",
    "\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Function to print final loss values\n",
    "def print_final_losses(train_losses_df, val_losses_df):\n",
    "    print(\"Final Training Loss:\", train_losses_df.groupby('epoch')['loss_value'].last().iloc[-1])\n",
    "    print(\"Final Validation Loss:\", val_losses_df.groupby('epoch')['loss_value'].last().iloc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5001e203",
   "metadata": {
    "id": "5001e203"
   },
   "outputs": [],
   "source": [
    "# Function to plot loss curves for multiple models stored in loss_comparison_dict\n",
    "def plot_loss_comparisons():\n",
    "    \"\"\"\n",
    "    Plots the training loss curves and average validation loss per epoch for multiple models added to the loss comparison dictionary.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    # Get the last model in the dictionary (for special final-point highlighting)\n",
    "    last_model_name = list(loss_comparison_dict.keys())[-1]\n",
    "\n",
    "    # Loop through each model in the loss dictionary\n",
    "    for model_name, losses in loss_comparison_dict.items():\n",
    "        train_losses_df = losses['train']\n",
    "        val_losses_df = losses['val']\n",
    "\n",
    "        # Plot training losses for each model\n",
    "        plt.plot(train_losses_df['step'], train_losses_df['loss_value'],\n",
    "                 label=f'{model_name} train', linestyle='-', alpha=0.7)\n",
    "\n",
    "        # Scatter training loss at the end of each epoch\n",
    "        last_train_losses = train_losses_df.groupby('epoch').last().reset_index()\n",
    "        plt.scatter(last_train_losses['step'], last_train_losses['loss_value'], marker='o', s=50)\n",
    "\n",
    "        # Compute average validation loss per epoch (using the last step of each epoch for x-axis)\n",
    "        avg_val_losses = val_losses_df.groupby('epoch').agg({'loss_value': 'mean', 'step': 'last'}).reset_index()\n",
    "        # Scatter the average validation loss for each epoch\n",
    "        plt.scatter(avg_val_losses['step'], avg_val_losses['loss_value'], marker='s', s=50,\n",
    "                    label=f'{model_name} val avg')\n",
    "\n",
    "        # For the last model, highlight the final training loss with a star\n",
    "        if model_name == last_model_name:\n",
    "            final_step = train_losses_df['step'].iloc[-1]\n",
    "            final_loss = train_losses_df['loss_value'].iloc[-1]\n",
    "            plt.scatter(final_step, final_loss, marker='*', s=100, color='red', zorder=5)\n",
    "\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Comparison')\n",
    "    plt.legend()  # Legend shows both training and validation average labels\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16777ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_separate_train_val(loss_dict):\n",
    "    \"\"\"\n",
    "    모델별 Training Loss와 Validation Loss를 각각 별도의 그래프로 그립니다.\n",
    "    단, Validation Loss는 에포크별 평균으로 계산합니다.\n",
    "    \"\"\"\n",
    "    # 1. Training Loss Plot (원본 그대로)\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    plt.subplot(1, 2, 1)  # 1행 2열 중 첫 번째\n",
    "    for model_name, losses in loss_dict.items():\n",
    "        train_df = losses['train']\n",
    "        steps_train = train_df['step'].values\n",
    "        loss_train = train_df['loss_value'].values\n",
    "        plt.plot(steps_train, loss_train, label=f'{model_name} Train')\n",
    "    plt.title('Training Loss Comparison')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # 2. Validation Loss Plot (에포크별 평균 처리)\n",
    "    plt.subplot(1, 2, 2)  # 1행 2열 중 두 번째\n",
    "    for model_name, losses in loss_dict.items():\n",
    "        val_df = losses['val']\n",
    "        # 에포크별 평균 loss와 마지막 step을 계산\n",
    "        val_avg = val_df.groupby('epoch').agg({'loss_value': 'mean', 'step': 'last'}).reset_index()\n",
    "        plt.plot(val_avg['step'], val_avg['loss_value'], label=f'{model_name} Val')\n",
    "    plt.title('Validation Loss (Epoch Avg) Comparison')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1eb8ad",
   "metadata": {
    "id": "aa1eb8ad"
   },
   "source": [
    "## Model 1: GRU Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e017e89b",
   "metadata": {
    "id": "e017e89b"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GRUCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.linear_ih = nn.Linear(input_size, 3 * hidden_size)\n",
    "        self.linear_hh = nn.Linear(hidden_size, 3 * hidden_size)\n",
    "\n",
    "    def forward(self, x_t, h_prev):\n",
    "        gates_ih = self.linear_ih(x_t)     # (batch, 3*hidden)\n",
    "        gates_hh = self.linear_hh(h_prev)  # (batch, 3*hidden)\n",
    "        i_r, i_z, i_n = gates_ih.chunk(3, dim=1)\n",
    "        h_r, h_z, h_n = gates_hh.chunk(3, dim=1)\n",
    "        r_t = torch.sigmoid(i_r + h_r)\n",
    "        z_t = torch.sigmoid(i_z + h_z)\n",
    "        n_t = torch.tanh(i_n + r_t * h_n)\n",
    "        h_t = (1 - z_t) * n_t + z_t * h_prev\n",
    "        return h_t\n",
    "\n",
    "class GRUDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        embedding_dim: int,\n",
    "        hidden_dim: int,\n",
    "        ffn_dim: int,\n",
    "        num_layers: int,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.cells = nn.ModuleList()\n",
    "        for layer in range(num_layers):\n",
    "            in_dim = embedding_dim if layer == 0 else hidden_dim\n",
    "            self.cells.append(GRUCell(in_dim, hidden_dim))\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.fc1 = nn.Linear(hidden_dim, ffn_dim)\n",
    "        self.fc2 = nn.Linear(ffn_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        \"\"\"\n",
    "        x: LongTensor of shape (batch, seq_len)\n",
    "        hidden: FloatTensor of shape (num_layers, batch, hidden_dim), or None\n",
    "        \"\"\"\n",
    "        embed = self.embedding(x)        # (batch, seq_len, embedding_dim)\n",
    "        batch_size, seq_len, _ = embed.size()\n",
    "\n",
    "        # initialize hidden states if not provided\n",
    "        if hidden is None:\n",
    "            hiddens = [\n",
    "                torch.zeros(batch_size, self.cells[0].hidden_size, device=embed.device)\n",
    "                for _ in range(self.num_layers)\n",
    "            ]\n",
    "        else:\n",
    "            # split provided hidden tensor into list per layer\n",
    "            hiddens = [hidden[layer] for layer in range(self.num_layers)]\n",
    "\n",
    "        layer_input = embed\n",
    "        # pass through each layer of GRUCell\n",
    "        for layer_idx, cell in enumerate(self.cells):\n",
    "            h_prev = hiddens[layer_idx]\n",
    "            outputs = []\n",
    "            for t in range(seq_len):\n",
    "                h_prev = cell(layer_input[:, t, :], h_prev)\n",
    "                outputs.append(h_prev.unsqueeze(1))\n",
    "            layer_output = torch.cat(outputs, dim=1)  # (batch, seq_len, hidden_dim)\n",
    "            layer_input = layer_output\n",
    "            hiddens[layer_idx] = h_prev\n",
    "\n",
    "        # apply dropout, layer norm, and feed-forward\n",
    "        out = self.dropout(layer_output)\n",
    "        out = self.layer_norm(out)\n",
    "        out = F.gelu(self.fc1(out))\n",
    "        out = self.fc2(out)  # (batch, seq_len, vocab_size)\n",
    "\n",
    "        # stack hiddens back into tensor of shape (num_layers, batch, hidden_dim)\n",
    "        hidden_out = torch.stack(hiddens, dim=0)\n",
    "        return out, hidden_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a3581e4",
   "metadata": {
    "id": "9a3581e4"
   },
   "outputs": [],
   "source": [
    "### Model Initialization\n",
    "gru = GRUDecoder(vocab_size, EMBEDDING_DIM, HIDDEN_DIM, FFN_DIM, NUM_LAYERS).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(gru.parameters(), lr=LEARNING_RATE, fused=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2fe18507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "GRUDecoder                               [512, 64, 65]             --\n",
       "├─Embedding: 1-1                         [512, 64, 32]             2,080\n",
       "├─ModuleList: 1-2                        --                        --\n",
       "│    └─GRUCell: 2-1                      [512, 64]                 --\n",
       "│    │    └─Linear: 3-1                  [512, 192]                6,336\n",
       "│    │    └─Linear: 3-2                  [512, 192]                12,480\n",
       "│    └─GRUCell: 2-2                      [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-3                  [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-4                  [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-3                      [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-5                  [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-6                  [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-4                      [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-7                  [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-8                  [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-5                      [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-9                  [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-10                 [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-6                      [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-11                 [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-12                 [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-7                      [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-13                 [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-14                 [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-8                      [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-15                 [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-16                 [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-9                      [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-17                 [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-18                 [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-10                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-19                 [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-20                 [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-11                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-21                 [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-22                 [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-12                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-23                 [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-24                 [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-13                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-25                 [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-26                 [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-14                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-27                 [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-28                 [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-15                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-29                 [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-30                 [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-16                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-31                 [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-32                 [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-17                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-33                 [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-34                 [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-18                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-35                 [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-36                 [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-19                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-37                 [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-38                 [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-20                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-39                 [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-40                 [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-21                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-41                 [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-42                 [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-22                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-43                 [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-44                 [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-23                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-45                 [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-46                 [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-24                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-47                 [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-48                 [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-25                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-49                 [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-50                 [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-26                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-51                 [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-52                 [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-27                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-53                 [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-54                 [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-28                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-55                 [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-56                 [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-29                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-57                 [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-58                 [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-30                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-59                 [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-60                 [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-31                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-61                 [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-62                 [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-32                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-63                 [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-64                 [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-33                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-65                 [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-66                 [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-34                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-67                 [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-68                 [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-35                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-69                 [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-70                 [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-36                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-71                 [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-72                 [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-37                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-73                 [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-74                 [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-38                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-75                 [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-76                 [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-39                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-77                 [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-78                 [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-40                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-79                 [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-80                 [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-41                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-81                 [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-82                 [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-42                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-83                 [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-84                 [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-43                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-85                 [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-86                 [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-44                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-87                 [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-88                 [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-45                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-89                 [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-90                 [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-46                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-91                 [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-92                 [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-47                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-93                 [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-94                 [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-48                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-95                 [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-96                 [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-49                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-97                 [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-98                 [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-50                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-99                 [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-100                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-51                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-101                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-102                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-52                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-103                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-104                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-53                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-105                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-106                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-54                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-107                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-108                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-55                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-109                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-110                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-56                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-111                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-112                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-57                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-113                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-114                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-58                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-115                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-116                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-59                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-117                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-118                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-60                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-119                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-120                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-61                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-121                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-122                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-62                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-123                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-124                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-63                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-125                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-126                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-64                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-127                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-128                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-65                     [512, 64]                 --\n",
       "│    │    └─Linear: 3-129                [512, 192]                12,480\n",
       "│    │    └─Linear: 3-130                [512, 192]                12,480\n",
       "│    └─GRUCell: 2-66                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-131                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-132                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-67                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-133                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-134                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-68                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-135                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-136                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-69                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-137                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-138                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-70                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-139                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-140                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-71                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-141                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-142                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-72                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-143                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-144                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-73                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-145                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-146                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-74                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-147                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-148                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-75                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-149                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-150                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-76                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-151                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-152                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-77                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-153                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-154                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-78                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-155                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-156                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-79                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-157                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-158                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-80                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-159                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-160                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-81                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-161                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-162                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-82                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-163                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-164                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-83                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-165                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-166                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-84                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-167                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-168                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-85                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-169                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-170                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-86                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-171                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-172                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-87                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-173                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-174                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-88                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-175                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-176                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-89                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-177                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-178                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-90                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-179                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-180                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-91                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-181                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-182                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-92                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-183                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-184                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-93                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-185                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-186                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-94                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-187                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-188                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-95                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-189                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-190                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-96                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-191                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-192                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-97                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-193                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-194                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-98                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-195                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-196                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-99                     [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-197                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-198                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-100                    [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-199                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-200                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-101                    [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-201                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-202                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-102                    [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-203                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-204                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-103                    [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-205                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-206                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-104                    [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-207                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-208                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-105                    [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-209                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-210                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-106                    [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-211                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-212                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-107                    [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-213                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-214                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-108                    [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-215                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-216                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-109                    [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-217                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-218                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-110                    [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-219                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-220                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-111                    [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-221                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-222                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-112                    [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-223                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-224                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-113                    [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-225                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-226                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-114                    [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-227                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-228                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-115                    [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-229                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-230                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-116                    [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-231                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-232                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-117                    [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-233                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-234                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-118                    [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-235                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-236                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-119                    [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-237                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-238                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-120                    [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-239                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-240                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-121                    [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-241                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-242                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-122                    [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-243                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-244                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-123                    [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-245                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-246                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-124                    [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-247                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-248                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-125                    [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-249                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-250                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-126                    [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-251                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-252                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-127                    [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-253                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-254                [512, 192]                (recursive)\n",
       "│    └─GRUCell: 2-128                    [512, 64]                 (recursive)\n",
       "│    │    └─Linear: 3-255                [512, 192]                (recursive)\n",
       "│    │    └─Linear: 3-256                [512, 192]                (recursive)\n",
       "├─Dropout: 1-3                           [512, 64, 64]             --\n",
       "├─LayerNorm: 1-4                         [512, 64, 64]             128\n",
       "├─Linear: 1-5                            [512, 64, 256]            16,640\n",
       "├─Linear: 1-6                            [512, 64, 65]             16,705\n",
       "==========================================================================================\n",
       "Total params: 79,329\n",
       "Trainable params: 79,329\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 1.45\n",
       "==========================================================================================\n",
       "Input size (MB): 0.26\n",
       "Forward/backward pass size (MB): 310.64\n",
       "Params size (MB): 0.32\n",
       "Estimated Total Size (MB): 311.22\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(gru, input_size=(BATCH_SIZE, SEQUENCE_LENGTH), dtypes=[torch.long])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec86dc8e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 121082,
     "status": "ok",
     "timestamp": 1732704395031,
     "user": {
      "displayName": "Hugman Sangkeun Jung",
      "userId": "08689291704194029524"
     },
     "user_tz": -540
    },
    "id": "ec86dc8e",
    "outputId": "d3a32ebf-e2ad-4e8b-fb04-f2651e0ac049"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2, Train Loss: 1.8861, Val Loss: 1.9602, Epoch Time: 266.61s Average Vram Usage: 20.03MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2, Train Loss: 1.7271, Val Loss: 1.8599, Epoch Time: 244.91s Average Vram Usage: 20.03MB\n"
     ]
    }
   ],
   "source": [
    "## Training Loop\n",
    "trained_model, train_losses_df, val_losses_df = train_model(gru, train_loader, val_loader, criterion, optimizer, device, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "396c469b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text generation using validation data\n",
    "val_sample, _ = next(iter(val_loader))\n",
    "start_text = ''.join([idx_to_char[idx.item()] for idx in val_sample[0][:SEQUENCE_LENGTH]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70664998",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 489,
     "status": "ok",
     "timestamp": 1732704395512,
     "user": {
      "displayName": "Hugman Sangkeun Jung",
      "userId": "08689291704194029524"
     },
     "user_tz": -540
    },
    "id": "70664998",
    "outputId": "a970b003-a23c-47b7-900e-5ed705866838"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text (starting with validation data [\n",
      "\n",
      "GREMIO:\n",
      "Good morrow, neighbour Baptista.\n",
      "\n",
      "BAPTISTA:\n",
      "Good morro]):\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "GREMIO:\n",
      "Good morrow, neighbour Baptista.\n",
      "\n",
      "BAPTISTA:\n",
      "Good morrow\n",
      "But all desper weach, and Wordy ding if those re's again, if like.\n",
      "\n",
      "BUCKINGHARD II:\n",
      "Which no exerself\n",
      "I pray, I do me him grees goe; of this be dave\n",
      "I way sound liegent that his clancofait, and help not dids,\n",
      "Now?\n",
      "\n",
      "HENS ELONE:\n",
      "O, sir,\n",
      "'it Rame master your depat,\n",
      "Bectent cannot son\n",
      "Enourse no see to must brother's what oncelfed we sy;\n",
      "Come the beath?\n",
      "\n",
      "VEDWIUS:\n",
      "Acherd o'eprothed, were?\n",
      "Thus is like any may of Herrows in thems of Mechard\n",
      "Thy lack it on ut, here I thant;\n",
      "And to make bloody earth.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generated_text = generate_text(trained_model, char_to_idx, idx_to_char, start_text, device)\n",
    "                      \n",
    "print(f\"Generated text (starting with validation data [{start_text}]):\")\n",
    "print(\"-\"*50)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fcca4e52",
   "metadata": {
    "id": "fcca4e52"
   },
   "outputs": [],
   "source": [
    "# After training a model (e.g., LSTM without RMSNorm), add its losses\n",
    "add_loss_to_comparison('GRU', train_losses_df, val_losses_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "98b72703",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1732704396861,
     "user": {
      "displayName": "Hugman Sangkeun Jung",
      "userId": "08689291704194029524"
     },
     "user_tz": -540
    },
    "id": "98b72703",
    "outputId": "ece1620a-8690-4750-97c1-ada645d59a08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Input:\n",
      "\n",
      "\n",
      "GREMIO:\n",
      "Good morrow, neighbour Baptista.\n",
      "\n",
      "BAPTISTA:\n",
      "Good morro\n",
      "\n",
      "Model Output (logits for next character prediction):\n",
      "torch.Size([1, 64, 65])\n",
      "tensor([ 1.7180, -2.2560, -4.8033, -4.7796, -4.8691,  0.2543, -5.2286, -4.7724,\n",
      "        -5.2995, -3.0716], device='cuda:0')\n",
      "\n",
      "Predicted next character:\n",
      "w\n"
     ]
    }
   ],
   "source": [
    "# Decoder Input/Output Example\n",
    "sample_input, _ = next(iter(val_loader))\n",
    "sample_input = sample_input[0].unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "trained_model.eval()\n",
    "with torch.no_grad():\n",
    "    output, _ = trained_model(sample_input)\n",
    "\n",
    "print(\"\\nSample Input:\")\n",
    "print(''.join([idx_to_char[idx.item()] for idx in sample_input[0]]))\n",
    "\n",
    "print(\"\\nModel Output (logits for next character prediction):\")\n",
    "print(output.shape)\n",
    "print(output[0, 0, :10])  # Print first 10 logits of the first timestep\n",
    "\n",
    "print(\"\\nPredicted next character:\")\n",
    "predicted_char_idx = torch.argmax(output[0, -1]).item()\n",
    "print(idx_to_char[predicted_char_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f23a4aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_loss_comparisons()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "868a5338",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "executionInfo": {
     "elapsed": 899,
     "status": "ok",
     "timestamp": 1732704397758,
     "user": {
      "displayName": "Hugman Sangkeun Jung",
      "userId": "08689291704194029524"
     },
     "user_tz": -540
    },
    "id": "868a5338",
    "outputId": "06397d97-49fb-4f85-b2b3-2b27ca481fe0"
   },
   "outputs": [],
   "source": [
    "# plot_separate_train_val(loss_comparison_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a2d8a352",
   "metadata": {
    "id": "a2d8a352"
   },
   "outputs": [],
   "source": [
    "def generate_text_attention(model, char_to_idx, idx_to_char, start_text, device, max_length=500):\n",
    "    model.eval()\n",
    "    current_text = start_text\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            # Convert the last sequence of characters to indices and feed it to the model\n",
    "            x = torch.tensor([[char_to_idx[ch] for ch in current_text[-SEQUENCE_LENGTH:]]]).to(device)\n",
    "            output = model(x)[0]  # No hidden state needed for attention-based models\n",
    "            probs = torch.softmax(output[0, -1], dim=0)\n",
    "            next_char_idx = torch.multinomial(probs, 1).item()\n",
    "            next_char = idx_to_char[next_char_idx]\n",
    "            current_text += next_char\n",
    "\n",
    "    return current_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "32c936b1",
   "metadata": {
    "id": "32c936b1"
   },
   "outputs": [],
   "source": [
    "def train_and_test(model_desc, model, start_text):\n",
    "    # Initialize the model\n",
    "    model = model.to(device)\n",
    "    # Use the same optimizer and criterion\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    # Train the model\n",
    "    trained_model, train_losses_df, val_losses_df = train_model(\n",
    "        model, train_loader, val_loader, criterion, optimizer, device, EPOCHS\n",
    "    )\n",
    "\n",
    "    # # Generate text\n",
    "    # generated_text = generate_text_attention(trained_model, char_to_idx, idx_to_char, start_text, device)\n",
    "    # print(f\"Generated text [{start_text}]:\")\n",
    "    # print(\"-\"*50)\n",
    "    # print(generated_text)\n",
    "    \n",
    "    # add_loss_to_comparison(model_desc, train_losses_df, val_losses_df)\n",
    "\n",
    "    # # Plot loss comparisons including this model\n",
    "    # plot_loss_comparisons()\n",
    "    \n",
    "    # plot_separate_train_val(loss_comparison_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39911f55",
   "metadata": {
    "id": "39911f55"
   },
   "source": [
    "## Model 2: Modern Transformer(LLaMA - 2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6b43bc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    DIM = EMBEDDING_DIM \n",
    "    FFN_DIM = FFN_DIM\n",
    "    NUM_HEADS = NUM_HEADS \n",
    "    NUM_LAYERS = NUM_LAYERS\n",
    "\n",
    "    NUM_KV_HEADS = NUM_HEADS \n",
    "    VOCAB_SIZE = vocab_size\n",
    "    NORM_EPS = 1e-5 # LLaMA: 1e-5\n",
    "    ROPE_THETA = 10000 # LLaMA: 10000\n",
    "\n",
    "    MAX_BATCH_SIZE = BATCH_SIZE\n",
    "    MAX_SEQ_LEN = SEQUENCE_LENGTH # depending on the DATASET\n",
    "    NUM_KV_HEAD_REP = NUM_HEADS // NUM_KV_HEADS\n",
    "\n",
    "    HEAD_DIM = DIM // NUM_HEADS\n",
    "    DROPOUT = DROPOUT\n",
    "    DEVICE = device\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "    def _norm(self, x: torch.Tensor):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.weight.to(x.device) * self._norm(x.float()).type_as(x)\n",
    "    \n",
    "def precompute_freqs_cis(head_dim: int, seq_len: int, theta: float = 100.0, device: str = \"cuda\"):\n",
    "    if head_dim % 2 != 0:\n",
    "        raise ValueError(\"head_dim must be even for rotary embeddings.\")\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, head_dim, 2).float() / head_dim)).to(device)\n",
    "    t = torch.arange(seq_len, device=device, dtype=torch.float32)\n",
    "    freqs = torch.outer(t, freqs)  # [seq_len, head_dim//2]\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
    "    return freqs_cis  # [seq_len, head_dim // 2]\n",
    "\n",
    "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
    "    L = x.shape[1]\n",
    "    return freqs_cis.view(1, L, 1, x.shape[-1] // 2)  # [1, L, 1, head_dim]\n",
    "\n",
    "def apply_rotary_emb(x: torch.Tensor, device: str = \"cuda\"):\n",
    "    # x: [B, L, 2*heads, D] & D is even\n",
    "    _, L, _, D = x.shape\n",
    "    x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2)) # [B, L, 2*heads, D//2, 2]\n",
    "    freqs_cis = precompute_freqs_cis(D, L)\n",
    "    freqs = reshape_for_broadcast(freqs_cis, x)\n",
    "    x_rotated = x_complex * freqs\n",
    "    x_out = torch.view_as_real(x_rotated).reshape(x.shape)\n",
    "    return x_out.type_as(x).to(device)\n",
    "\n",
    "def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    B, L, nk, d = x.shape\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "    return x[:, :, :, None, :].expand(B, L, nk, n_rep, d).reshape(B, L, nk * n_rep, d)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, ffn_dim, dropout):\n",
    "        super().__init__()\n",
    "        hidden_dim = ffn_dim\n",
    "        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n",
    "        self.w3 = nn.Linear(dim, hidden_dim, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x: [B, L, D]\n",
    "        return self.w2(F.silu(self.w1(x)) * self.dropout(self.w3(x)))\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, n_heads, n_kv_heads, n_rep, dim, dropout, batch, seq_len, device):\n",
    "        super().__init__()\n",
    "        self.n_heads_q = n_heads\n",
    "        self.n_kv_heads = n_kv_heads\n",
    "        self.n_rep = n_rep\n",
    "        self.head_dim = dim // n_heads\n",
    "\n",
    "        self.wq = nn.Linear(dim, n_heads * self.head_dim, bias=False)\n",
    "        self.wk = nn.Linear(dim, n_kv_heads * self.head_dim, bias=False)\n",
    "        self.wv = nn.Linear(dim, n_kv_heads * self.head_dim, bias=False)\n",
    "        self.wo = nn.Linear(n_heads * self.head_dim, dim, bias=False)\n",
    "        self.attn_dropout = dropout\n",
    "        \n",
    "        self.norm = RMSNorm(self.head_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, start_pos, mask, return_attn=False):\n",
    "        B, L, _ = x.shape\n",
    "        src_len = trg_len = L\n",
    "        offset = start_pos\n",
    "        \n",
    "        xq = self.wq(x).view(B, L, self.n_heads_q, self.head_dim)\n",
    "        xk = self.wk(x).view(B, L, self.n_kv_heads, self.head_dim)\n",
    "        xv = self.wv(x).view(B, L, self.n_kv_heads, self.head_dim)\n",
    "        \n",
    "        # Apply rotary embeddings\n",
    "        xq = apply_rotary_emb(xq)\n",
    "        xk = apply_rotary_emb(xk)\n",
    "        \n",
    "        # GQA: Adjust dimensions for attention computation\n",
    "        xq = xq.transpose(1, 2)   # [B, n_heads, L, head_dim]\n",
    "        xk = repeat_kv(xk, self.n_rep).transpose(1, 2) # [B, n_heads, L, head_dim]\n",
    "        xv = repeat_kv(xv, self.n_rep).transpose(1, 2) # [B, n_heads, L, head_dim]\n",
    "\n",
    "        # Compute scaled dot-product attention manually to capture attention weights\n",
    "        scores = torch.matmul(xq, xk.transpose(-2, -1)) / math.sqrt(self.head_dim)  # [B, n_heads, L, L]\n",
    "        scores = torch.nan_to_num(scores)\n",
    "        if mask is None:\n",
    "            mask = torch.triu(\n",
    "                torch.zeros([L, L])\n",
    "                .float()\n",
    "                .fill_(float(\"-inf\"))\n",
    "                .type_as(attn_weights),\n",
    "                1 + offset,\n",
    "            )\n",
    "            \n",
    "        scores += mask\n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        attn_weights = F.dropout(attn_weights, self.attn_dropout, training=self.training)\n",
    "        attn_output = torch.matmul(attn_weights, xv)  # [B, n_heads, L, head_dim]\n",
    "        attn_output = self.norm(attn_output)\n",
    "        \n",
    "        # Reshape attention output and project\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(B, L, -1)\n",
    "        output = self.wo(attn_output)  # [B, L, D]\n",
    "        if return_attn:\n",
    "            return output, attn_weights\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, args: 'ModelArgs'):\n",
    "        super().__init__()\n",
    "        self.attention = SelfAttention(\n",
    "            args.NUM_HEADS, \n",
    "            args.NUM_KV_HEADS, \n",
    "            args.NUM_KV_HEAD_REP, \n",
    "            args.DIM, \n",
    "            args.DROPOUT, \n",
    "            args.MAX_BATCH_SIZE, \n",
    "            args.MAX_SEQ_LEN, \n",
    "            args.DEVICE\n",
    "        )\n",
    "        self.ffn = FeedForward(args.DIM, args.FFN_DIM, args.DROPOUT)\n",
    "        self.attention_norm = RMSNorm(args.DIM, args.NORM_EPS)\n",
    "        self.ffn_norm = RMSNorm(args.DIM, args.NORM_EPS)\n",
    "        self.res_dropout = nn.Dropout(args.DROPOUT)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, start_pos, mask, return_attn=False):\n",
    "        if return_attn:\n",
    "            attn_out, attn_map = self.attention(self.attention_norm(x), start_pos, mask, return_attn=True)\n",
    "            h = x + self.res_dropout(attn_out)\n",
    "            h = h + self.res_dropout(self.ffn(self.ffn_norm(h)))\n",
    "            return h, attn_map\n",
    "        else:\n",
    "            h = x + self.res_dropout(self.attention(self.attention_norm(x), start_pos, mask))\n",
    "            h = h + self.res_dropout(self.ffn(self.ffn_norm(h)))\n",
    "            return h\n",
    "\n",
    "class LlamaTransformer(nn.Module):\n",
    "    def __init__(self, args: 'ModelArgs'):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.tok_embeddings = nn.Embedding(args.VOCAB_SIZE, args.DIM)\n",
    "        self.layers = nn.ModuleList([TransformerBlock(args) for _ in range(args.NUM_LAYERS)])\n",
    "        self.norm = RMSNorm(args.DIM, args.NORM_EPS)\n",
    "        self.output = nn.Linear(args.DIM, args.VOCAB_SIZE, bias=False)\n",
    "        self.device = args.DEVICE\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, start_pos=0, return_attn=False):\n",
    "        B, L = x.shape\n",
    "        h = self.tok_embeddings(x)  # [B, L, D]\n",
    "        \n",
    "        mask = None\n",
    "        if L > 1:\n",
    "            mask = torch.triu(\n",
    "                torch.zeros([L, L])\n",
    "                .float()\n",
    "                .fill_(float(\"-inf\"))\n",
    "                .type_as(x),\n",
    "                1 + start_pos,\n",
    "            )\n",
    "        attn_maps = []\n",
    "        for layer in self.layers:\n",
    "            if return_attn:\n",
    "                h, attn_map = layer(h, start_pos, mask, return_attn=True)\n",
    "                attn_maps.append(attn_map)\n",
    "            else:\n",
    "                h = layer(h, start_pos, mask)\n",
    "        logits = self.output(self.norm(h)).float()\n",
    "        if return_attn:\n",
    "            return logits, attn_maps\n",
    "        return logits, _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "66b4f114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTransformer(\n",
       "  (tok_embeddings): Embedding(65, 32)\n",
       "  (layers): ModuleList(\n",
       "    (0-1): 2 x TransformerBlock(\n",
       "      (attention): SelfAttention(\n",
       "        (wq): Linear(in_features=32, out_features=32, bias=False)\n",
       "        (wk): Linear(in_features=32, out_features=32, bias=False)\n",
       "        (wv): Linear(in_features=32, out_features=32, bias=False)\n",
       "        (wo): Linear(in_features=32, out_features=32, bias=False)\n",
       "        (norm): RMSNorm()\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (w1): Linear(in_features=32, out_features=256, bias=False)\n",
       "        (w2): Linear(in_features=256, out_features=32, bias=False)\n",
       "        (w3): Linear(in_features=32, out_features=256, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (attention_norm): RMSNorm()\n",
       "      (ffn_norm): RMSNorm()\n",
       "      (res_dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (norm): RMSNorm()\n",
       "  (output): Linear(in_features=32, out_features=65, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PARAMS = ModelArgs()\n",
    "llama = LlamaTransformer(PARAMS).to(device)\n",
    "llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "087c3927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "LlamaTransformer                         [512, 64, 65]             --\n",
       "├─Embedding: 1-1                         [512, 64, 32]             2,080\n",
       "├─ModuleList: 1-2                        --                        --\n",
       "│    └─TransformerBlock: 2-1             [512, 64, 32]             --\n",
       "│    │    └─RMSNorm: 3-1                 [512, 64, 32]             32\n",
       "│    │    └─SelfAttention: 3-2           [512, 64, 32]             4,104\n",
       "│    │    └─Dropout: 3-3                 [512, 64, 32]             --\n",
       "│    │    └─RMSNorm: 3-4                 [512, 64, 32]             32\n",
       "│    │    └─FeedForward: 3-5             [512, 64, 32]             24,576\n",
       "│    │    └─Dropout: 3-6                 [512, 64, 32]             --\n",
       "│    └─TransformerBlock: 2-2             [512, 64, 32]             --\n",
       "│    │    └─RMSNorm: 3-7                 [512, 64, 32]             32\n",
       "│    │    └─SelfAttention: 3-8           [512, 64, 32]             4,104\n",
       "│    │    └─Dropout: 3-9                 [512, 64, 32]             --\n",
       "│    │    └─RMSNorm: 3-10                [512, 64, 32]             32\n",
       "│    │    └─FeedForward: 3-11            [512, 64, 32]             24,576\n",
       "│    │    └─Dropout: 3-12                [512, 64, 32]             --\n",
       "├─RMSNorm: 1-3                           [512, 64, 32]             32\n",
       "├─Linear: 1-4                            [512, 64, 65]             2,080\n",
       "==========================================================================================\n",
       "Total params: 61,680\n",
       "Trainable params: 61,680\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 31.58\n",
       "==========================================================================================\n",
       "Input size (MB): 0.26\n",
       "Forward/backward pass size (MB): 436.47\n",
       "Params size (MB): 0.25\n",
       "Estimated Total Size (MB): 436.98\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(llama.to(device), input_size=(BATCH_SIZE, SEQUENCE_LENGTH), dtypes=[torch.long])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b361db8b",
   "metadata": {
    "id": "b361db8b",
    "outputId": "33c6269c-5979-4100-b6e7-a89b6b4ed962"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2, Train Loss: 2.3016, Val Loss: 2.2635, Epoch Time: 38.13s Average Vram Usage: 23.02MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2, Train Loss: 2.0619, Val Loss: 2.0352, Epoch Time: 38.19s Average Vram Usage: 23.02MB\n"
     ]
    }
   ],
   "source": [
    "train_and_test(\"LLaMA\", llama, start_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8775d573",
   "metadata": {},
   "source": [
    "## Model 3: Mamba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dd160c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-21 05:01:20,027] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 05:01:21.718748: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-21 05:01:22.600647: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "import math # For initialization if needed\n",
    "\n",
    "# --- Assume mamba_ssm is installed ---\n",
    "# pip install mamba-ssm causal-conv1d\n",
    "try:\n",
    "    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update # Need both for training/inference\n",
    "    # causal_conv1d_fn is the main function for training/full sequence processing\n",
    "except ImportError:\n",
    "    print(\"Warning: 'causal_conv1d' package not found. Falling back to nn.Conv1d simulation.\")\n",
    "    print(\"Install with: pip install causal-conv1d\")\n",
    "    causal_conv1d_fn = None # Placeholder\n",
    "\n",
    "try:\n",
    "    from mamba_ssm.ops.selective_scan_interface import selective_scan_fn, mamba_inner_fn\n",
    "    # selective_scan_fn wraps the CUDA/Triton kernel\n",
    "except ImportError:\n",
    "    print(\"Warning: 'mamba_ssm' package not found or compiled kernels unavailable.\")\n",
    "    print(\"SSM scan will use the less efficient PyTorch implementation.\")\n",
    "    print(\"Install with: pip install mamba-ssm\")\n",
    "    selective_scan_fn = None # Placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c6be4f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat\n",
    "import math\n",
    "from causal_conv1d import causal_conv1d_fn\n",
    "from mamba_ssm.ops.selective_scan_interface import selective_scan_fn\n",
    "\n",
    "# --- RMSNorm (Root Mean Square Layer Normalization) ---\n",
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"RMS 정규화 레이어.\n",
    "\n",
    "    Args:\n",
    "        dim (int): 정규화할 벡터의 차원.\n",
    "        eps (float, optional): 분모에 더할 작은 값 (0으로 나누는 것 방지). 기본값: 1e-6.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        # 학습 가능한 스케일링 파라미터 (gamma)\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"RMS 정규화 계산 수행.\"\"\"\n",
    "        # 계산 안정성을 위해 float32 사용 후 원본 타입 복원\n",
    "        original_dtype = x.dtype\n",
    "        # 입력의 제곱 평균의 제곱근 역수 계산\n",
    "        rms = torch.rsqrt(x.to(torch.float32).pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "        return (x * rms).to(original_dtype)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"RMS 정규화 적용.\"\"\"\n",
    "        # 정규화 후 학습 가능한 가중치 적용\n",
    "        return self.weight * self._norm(x)\n",
    "\n",
    "# --- SSM (State Space Model) 모듈 ---\n",
    "class SSM(nn.Module):\n",
    "    \"\"\"선택적 스캔 메커니즘 (SSM). 최적화된 selective_scan_fn 커널 사용.\n",
    "       Mamba v1 논문의 파라미터화 및 계산 방식 (non-fused path) 기반.\n",
    "       selective_scan_fn이 기대하는 특정 텐서 레이아웃(B,D,L / B,N,L 등)을 따름.\n",
    "\n",
    "    Args:\n",
    "        d_inner (int): 내부 확장 차원 (D).\n",
    "        state_size (int): SSM 상태 벡터 크기 (N).\n",
    "        dt_rank (str or int, optional): Δ 계산 시 사용될 중간 랭크. \"auto\"시 d_inner / 16. 기본값: \"auto\".\n",
    "        dt_min (float, optional): Δ의 최소값 제한 (softplus 적용 후). 기본값: 0.001.\n",
    "        dt_max (float, optional): Δ의 최대값 제한 (softplus 적용 후). 기본값: 0.1.\n",
    "        dt_init (str, optional): dt_proj 가중치 초기화 방식 (\"random\" or \"constant\"). 기본값: \"random\".\n",
    "        dt_scale (float, optional): dt_proj 가중치 초기화 스케일. 기본값: 1.0.\n",
    "        dt_init_floor (float, optional): dt 초기값 하한선. 기본값: 1e-4.\n",
    "        bias (bool, optional): x_proj 레이어에 bias 사용 여부. 기본값: False.\n",
    "        device (str, optional): 연산 장치. 기본값: 'cuda'.\n",
    "        dtype (torch.dtype, optional): 연산 데이터 타입. 기본값: torch.float32.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_inner: int, state_size: int, dt_rank: str | int =\"auto\", dt_min: float =0.001, dt_max: float =0.1,\n",
    "                 dt_init: str =\"random\", dt_scale: float =1.0, dt_init_floor: float =1e-4, bias: bool =False,\n",
    "                 device: str ='cuda', dtype: torch.dtype =torch.float32):\n",
    "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
    "        super().__init__()\n",
    "        self.d_inner = d_inner\n",
    "        self.state_size = state_size\n",
    "        # dt_rank 자동 계산 또는 지정값 사용\n",
    "        self.dt_rank = math.ceil(d_inner / 16) if dt_rank == \"auto\" else dt_rank\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "\n",
    "        # 컨볼루션 출력(x)을 받아 dt_inter, B, C 계산용 프로젝션 (Mamba v1 non-fused 방식)\n",
    "        self.x_proj = nn.Linear(self.d_inner, self.dt_rank + self.state_size * 2, bias=bias, **factory_kwargs)\n",
    "\n",
    "        # dt_inter를 받아 dt 계산용 프로젝션\n",
    "        self.dt_proj = nn.Linear(self.dt_rank, self.d_inner, bias=True, **factory_kwargs)\n",
    "\n",
    "        # --- dt_proj bias 특별 초기화 (Mamba v1 공식 코드 참조) ---\n",
    "        # 초기화 시 softplus(bias) 결과가 [dt_min, dt_max] 범위에 있도록 조정\n",
    "        dt_init_std = self.dt_rank**-0.5 * dt_scale\n",
    "        if dt_init == \"constant\":\n",
    "            nn.init.constant_(self.dt_proj.weight, dt_init_std)\n",
    "        elif dt_init == \"random\":\n",
    "            nn.init.uniform_(self.dt_proj.weight, -dt_init_std, dt_init_std)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Invalid dt_init: {dt_init}\")\n",
    "\n",
    "        # dt bias 초기값 계산 (softplus의 역함수 활용)\n",
    "        dt = torch.exp(\n",
    "            torch.rand(self.d_inner, **factory_kwargs) * (math.log(dt_max) - math.log(dt_min))\n",
    "            + math.log(dt_min)\n",
    "        ).clamp(min=dt_init_floor)\n",
    "        inv_dt = dt + torch.log(-torch.expm1(-dt)) # softplus(inv_dt) ≈ dt\n",
    "        with torch.no_grad():\n",
    "            self.dt_proj.bias.copy_(inv_dt)\n",
    "        # 다른 초기화 루틴에서 이 bias를 덮어쓰지 않도록 플래그 설정 (선택적)\n",
    "        self.dt_proj.bias._no_reinit = True\n",
    "        # --- dt_proj bias 초기화 종료 ---\n",
    "\n",
    "        # --- SSM 파라미터 A (A_log) ---\n",
    "        # S4D-Real 방식 초기화: A 행렬의 대각 요소가 [1, 2, ..., N]이 되도록 A_log 설정\n",
    "        A = repeat(\n",
    "            torch.arange(1, self.state_size + 1, dtype=torch.float32, device=device),\n",
    "            \"n -> d n\", # 1차원 벡터를 d_inner 번 반복하여 (D, N) 행렬 생성\n",
    "            d=self.d_inner,\n",
    "        ).contiguous()\n",
    "        A_log = torch.log(A) # 로그 스케일에서 파라미터 학습 (float32 유지)\n",
    "        self.A_log = nn.Parameter(A_log)\n",
    "        self.A_log._no_weight_decay = True # 가중치 감쇠 제외\n",
    "\n",
    "        # --- SSM 파라미터 D (피드스루) ---\n",
    "        # 형태: (D)\n",
    "        self.D = nn.Parameter(torch.ones(self.d_inner, device=device)) # float32 유지 권장\n",
    "        self.D._no_weight_decay = True # 가중치 감쇠 제외\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"SSM 순방향 계산. 입력 x는 (B, D, L) 레이아웃을 가정.\"\"\"\n",
    "        B, D, L = x.shape # 입력 레이아웃 확인\n",
    "        if D != self.d_inner:\n",
    "            raise ValueError(f\"입력 차원 D({D})가 SSM 내부 차원({self.d_inner})과 불일치\")\n",
    "\n",
    "        # 1. 입력 x(컨볼루션 출력)로부터 dt, B, C 계산\n",
    "        #    선형 프로젝션을 위해 (B*L, D) 형태로 변환\n",
    "        x_reshaped = rearrange(x, \"b d l -> (b l) d\")\n",
    "        x_proj_out = self.x_proj(x_reshaped) # 결과: (B*L, dt_rank + 2*N)\n",
    "        dt_inter, B, C = torch.split(x_proj_out, [self.dt_rank, self.state_size, self.state_size], dim=-1)\n",
    "\n",
    "        # 2. dt 계산 (softplus 적용 전) -> 형태 (B, D, L)\n",
    "        #    dt_inter: (B*L, dt_rank)\n",
    "        #    Mamba v1 non-fused path 방식 적용\n",
    "        dt = self.dt_proj.weight @ dt_inter.t() # 결과: (D, B*L)\n",
    "        dt = rearrange(dt, \"d (b l) -> b d l\", l=L) # 최종 형태: (B, D, L)\n",
    "        # dt_bias는 selective_scan_fn 내부에서 delta_bias 인자로 전달되어 더해짐\n",
    "\n",
    "        # 3. A 행렬 계산, 형태 (D, N)\n",
    "        A = -torch.exp(self.A_log.float()) # float32에서 exp 계산\n",
    "\n",
    "        # 4. B, C 형태 변경 -> (B, N, L)\n",
    "        #    selective_scan_fn 커널이 요구하는 레이아웃\n",
    "        B = rearrange(B, \"(b l) n -> b n l\", l=L).contiguous()\n",
    "        C = rearrange(C, \"(b l) n -> b n l\", l=L).contiguous()\n",
    "\n",
    "        # 5. D 파라미터 준비, 형태 (D)\n",
    "        D_param = self.D.float().contiguous()\n",
    "\n",
    "        # 6. selective_scan_fn 입력 준비 (메모리 연속성 보장)\n",
    "        input_u = x.contiguous()             # u (SSM 입력): (B, D, L)\n",
    "        input_delta = dt.contiguous()        # delta (dt): (B, D, L)\n",
    "        input_A = A.contiguous()             # A: (D, N)\n",
    "        input_B = B                          # B: (B, N, L)\n",
    "        input_C = C                          # C: (B, N, L)\n",
    "        input_D = D_param                    # D: (D)\n",
    "        input_delta_bias = self.dt_proj.bias.float().contiguous() if self.dt_proj.bias is not None else None\n",
    "\n",
    "        # 7. 최적화된 selective_scan_fn 호출\n",
    "        #    입력 레이아웃 및 파라미터 형태는 Mamba v1 non-fused path 기준\n",
    "        y = selective_scan_fn(\n",
    "            u=input_u,\n",
    "            delta=input_delta,\n",
    "            A=input_A,\n",
    "            B=input_B,\n",
    "            C=input_C,\n",
    "            D=input_D,\n",
    "            z=None, # 게이트 z는 MambaBlock 레벨에서 처리\n",
    "            delta_bias=input_delta_bias, # dt_bias 전달\n",
    "            delta_softplus=True, # 내부에서 delta = softplus(dt + delta_bias) 계산\n",
    "        )\n",
    "\n",
    "        # 8. 결과 반환, 형태 (B, D, L)\n",
    "        #    후속 처리를 위해 이 레이아웃 유지\n",
    "        return y\n",
    "\n",
    "\n",
    "# --- Mamba 블록 (causal_conv1d_fn 사용 및 데이터 흐름 수정) ---\n",
    "class MambaBlock(nn.Module):\n",
    "    \"\"\"Mamba 핵심 블록. RMSNorm, 입력 프로젝션, 인과적 컨볼루션, SSM, 게이팅, 출력 프로젝션 구성.\n",
    "       내부적으로 (B, D, L) 텐서 레이아웃 사용.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, state_size: int, d_conv: int = 4, expand: int = 2,\n",
    "                 dropout_prob: float = 0.1, device: str = 'cuda', dtype: torch.dtype = torch.float32):\n",
    "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_inner = int(expand * d_model) # 내부 확장 차원 (D)\n",
    "        self.state_size = state_size\n",
    "        self.d_conv = d_conv\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "\n",
    "        # 입력 정규화 (RMSNorm)\n",
    "        self.norm = RMSNorm(d_model, eps=1e-5)\n",
    "\n",
    "        # 입력 프로젝션 (d_model -> 2 * d_inner)\n",
    "        self.in_proj = nn.Linear(d_model, 2 * self.d_inner, bias=False, **factory_kwargs)\n",
    "\n",
    "        # 인과적 컨볼루션 파라미터 (가중치 형태: D, K)\n",
    "        self.conv1d_weight = nn.Parameter(torch.empty(self.d_inner, d_conv, **factory_kwargs))\n",
    "        self.conv1d_bias = nn.Parameter(torch.empty(self.d_inner, **factory_kwargs))\n",
    "\n",
    "        # SSM 모듈 인스턴스화 (Mamba v1 파라미터 전달 옵션 추가 가능)\n",
    "        self.ssm = SSM(self.d_inner, state_size, device=device, dtype=dtype)\n",
    "                      # dt_rank, dt_min 등 SSM 파라미터 전달 가능\n",
    "\n",
    "        # 출력 프로젝션 (d_inner -> d_model)\n",
    "        self.out_proj = nn.Linear(self.d_inner, d_model, bias=False, **factory_kwargs)\n",
    "\n",
    "        # 잔차 연결 드롭아웃\n",
    "        self.dropout_res = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Mamba 블록 순방향 계산.\"\"\"\n",
    "        B, L, D_model = x.shape # 입력 형태: (B, L, d_model)\n",
    "        residual = x # 잔차 연결용 원본 저장\n",
    "\n",
    "        # 1. 입력 정규화\n",
    "        x_norm = self.norm(x)\n",
    "\n",
    "        # 2. 입력 프로젝션 및 분할 (x_in, z)\n",
    "        xz = self.in_proj(x_norm) # 결과: (B, L, 2*D)\n",
    "        x_in, z = xz.chunk(2, dim=-1) # 각각: (B, L, D)\n",
    "\n",
    "        # 3. 인과적 컨볼루션 브랜치\n",
    "        #    입력 레이아웃 변경: (B, L, D) -> (B, D, L)\n",
    "        x_conv_in = rearrange(x_in, 'b l d -> b d l').contiguous()\n",
    "        #    최적화된 causal_conv1d_fn 호출 (SiLU 활성화 포함)\n",
    "        #    출력 x_conv_out 형태: (B, D, L)\n",
    "        x_conv_out = causal_conv1d_fn(\n",
    "            x=x_conv_in, weight=self.conv1d_weight, bias=self.conv1d_bias, activation='silu'\n",
    "        )\n",
    "\n",
    "        # 4. SSM 브랜치\n",
    "        #    컨볼루션 출력(B, D, L)을 SSM에 직접 전달\n",
    "        #    SSM 출력 y_ssm 형태: (B, D, L)\n",
    "        y_ssm = self.ssm(x_conv_out)\n",
    "\n",
    "        # 5. 게이팅 메커니즘\n",
    "        #    y_ssm을 z와 곱하기 위해 (B, L, D) 형태로 변경\n",
    "        y_ssm_rearranged = rearrange(y_ssm, 'b d l -> b l d')\n",
    "        #    z에 SiLU 활성화 적용 후 요소별 곱셈\n",
    "        y_gated = y_ssm_rearranged * F.silu(z) # 결과: (B, L, D)\n",
    "\n",
    "        # 6. 출력 프로젝션\n",
    "        output = self.out_proj(y_gated) # 결과: (B, L, d_model)\n",
    "\n",
    "        # 7. 잔차 연결 및 드롭아웃\n",
    "        output = residual + self.dropout_res(output) # 최종 결과: (B, L, d_model)\n",
    "\n",
    "        return output\n",
    "\n",
    "# --- Mamba 모델 전체 ---\n",
    "class Mamba(nn.Module):\n",
    "    \"\"\"Mamba 언어 모델 전체 구조.\"\"\"\n",
    "    def __init__(self, d_model: int, n_layers: int, vocab_size: int, state_size: int = 16,\n",
    "                 d_conv: int = 4, expand: int = 2, dropout_prob: float = 0.1,\n",
    "                 device: str = 'cuda', dtype: torch.dtype = torch.float32):\n",
    "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_layers = n_layers\n",
    "        self.vocab_size = vocab_size\n",
    "        # 저장된 파라미터는 validation 등에 사용될 수 있음\n",
    "        self.state_size = state_size\n",
    "        self.d_conv = d_conv\n",
    "        self.expand = expand\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "\n",
    "        # 토큰 임베딩 레이어\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, **factory_kwargs)\n",
    "        self.dropout_emb = nn.Dropout(dropout_prob)\n",
    "\n",
    "        # Mamba 블록 스택\n",
    "        self.layers = nn.ModuleList([\n",
    "            MambaBlock(\n",
    "                d_model=d_model, state_size=state_size, d_conv=d_conv,\n",
    "                expand=expand, dropout_prob=dropout_prob, device=device, dtype=dtype\n",
    "            ) for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "        # 최종 정규화 레이어\n",
    "        self.norm_f = RMSNorm(d_model, eps=1e-5)\n",
    "        # 언어 모델링 헤드 (출력 레이어)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False, **factory_kwargs)\n",
    "\n",
    "        # 가중치 공유 (임베딩과 LM 헤드)\n",
    "        self.lm_head.weight = self.embedding.weight\n",
    "\n",
    "        # 모델 가중치 초기화 적용\n",
    "        self.apply(self._init_weights)\n",
    "        print(f\"Mamba 모델 초기화 완료. Device: {device}, Dtype: {dtype}\")\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"모델의 각 모듈 가중치 초기화.\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # dt_proj의 bias는 특별 초기화되므로 건너뜀\n",
    "            if hasattr(module.bias, '_no_reinit') and module.bias._no_reinit:\n",
    "                return\n",
    "            # Linear 레이어 가중치 초기화 (예: GPT-2 스타일)\n",
    "            std = 0.02\n",
    "            # 모델 깊이에 따른 스케일링 (옵션)\n",
    "            if self.n_layers > 0:\n",
    "                 # 입력/출력 프로젝션 등 특정 레이어에만 적용 고려 가능\n",
    "                 if module.weight.shape[0] == self.d_model or module.weight.shape[1] == self.d_model:\n",
    "                     std /= math.sqrt(2.0 * self.n_layers)\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            # 임베딩 가중치 초기화\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, MambaBlock):\n",
    "            # MambaBlock 내의 Conv1d 가중치 초기화\n",
    "            if hasattr(module, 'conv1d_weight'):\n",
    "                # Kaiming 초기화 (SiLU 활성화 함수에 적합)\n",
    "                nn.init.kaiming_normal_(module.conv1d_weight, nonlinearity='leaky_relu')\n",
    "            if hasattr(module, 'conv1d_bias'):\n",
    "                nn.init.zeros_(module.conv1d_bias)\n",
    "        # RMSNorm 가중치는 해당 클래스 생성자에서 1로 초기화됨\n",
    "        # SSM 파라미터 (A_log, D)는 해당 클래스 생성자에서 초기화됨\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor) -> tuple[torch.Tensor, None]:\n",
    "        \"\"\"Mamba 모델 순방향 계산.\"\"\"\n",
    "        B, L = input_ids.shape\n",
    "\n",
    "        # 1. 임베딩 및 드롭아웃\n",
    "        # 임베딩 레이어는 LongTensor 입력 필요\n",
    "        x = self.embedding(input_ids.long())\n",
    "        x = self.dropout_emb(x)\n",
    "\n",
    "        # 2. Mamba 블록 순차 적용\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        # 3. 최종 정규화\n",
    "        x = self.norm_f(x)\n",
    "\n",
    "        # 4. LM 헤드를 통해 로짓 계산\n",
    "        logits = self.lm_head(x) # 결과: [B, L, vocab_size]\n",
    "\n",
    "        # Loss 계산은 외부 학습 루프에서 처리 (labels 사용)\n",
    "        # 여기서는 로짓과 None 반환 (일반적인 Hugging Face 모델 스타일)\n",
    "        return logits, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "014a9b40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n- pip install causal-conv1d\\n- mamba github clone > setup.py build > setup.py install\\n- sudo apt-get update && sudo apt-get install -y libaio-dev\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "- pip install causal-conv1d\n",
    "- mamba github clone > setup.py build > setup.py install\n",
    "- sudo apt-get update && sudo apt-get install -y libaio-dev\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7cf6a7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mamba 모델 초기화 완료. Device: cuda, Dtype: torch.float32\n",
      "test_output.shape = torch.Size([512, 64, 65])\n"
     ]
    }
   ],
   "source": [
    "STATE_SIZE = 8\n",
    "\n",
    "x = torch.randint(0, vocab_size, (BATCH_SIZE, SEQUENCE_LENGTH)).to(device)\n",
    "\n",
    "mamba = Mamba(HIDDEN_DIM, NUM_LAYERS, vocab_size, STATE_SIZE, d_conv=4, expand=3).to(device)\n",
    "\n",
    "test_output, _ = mamba(x)\n",
    "print(f\"test_output.shape = {test_output.shape}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e31528f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Mamba                                    [512, 64, 65]             --\n",
       "├─Embedding: 1-1                         [512, 64, 64]             4,160\n",
       "├─Dropout: 1-2                           [512, 64, 64]             --\n",
       "├─ModuleList: 1-3                        --                        --\n",
       "│    └─MambaBlock: 2-1                   [512, 64, 64]             960\n",
       "│    │    └─RMSNorm: 3-1                 [512, 64, 64]             64\n",
       "│    │    └─Linear: 3-2                  [512, 64, 384]            24,576\n",
       "│    │    └─SSM: 3-3                     [512, 192, 64]            9,600\n",
       "│    │    └─Linear: 3-4                  [512, 64, 64]             12,288\n",
       "│    │    └─Dropout: 3-5                 [512, 64, 64]             --\n",
       "│    └─MambaBlock: 2-2                   [512, 64, 64]             960\n",
       "│    │    └─RMSNorm: 3-6                 [512, 64, 64]             64\n",
       "│    │    └─Linear: 3-7                  [512, 64, 384]            24,576\n",
       "│    │    └─SSM: 3-8                     [512, 192, 64]            9,600\n",
       "│    │    └─Linear: 3-9                  [512, 64, 64]             12,288\n",
       "│    │    └─Dropout: 3-10                [512, 64, 64]             --\n",
       "├─RMSNorm: 1-4                           [512, 64, 64]             64\n",
       "├─Linear: 1-5                            [512, 64, 65]             4,160\n",
       "==========================================================================================\n",
       "Total params: 103,360\n",
       "Trainable params: 103,360\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 394.43\n",
       "==========================================================================================\n",
       "Input size (MB): 0.26\n",
       "Forward/backward pass size (MB): 333.71\n",
       "Params size (MB): 0.37\n",
       "Estimated Total Size (MB): 334.34\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(mamba.to(device), input_size=(BATCH_SIZE, SEQUENCE_LENGTH), dtypes=[torch.long])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a427c989",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2, Train Loss: 1.8402, Val Loss: 1.8921, Epoch Time: 30.89s Average Vram Usage: 565.10MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2, Train Loss: 1.6439, Val Loss: 1.7853, Epoch Time: 31.67s Average Vram Usage: 565.10MB\n"
     ]
    }
   ],
   "source": [
    "train_and_test(\"Mamba\", mamba, start_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbb22cc",
   "metadata": {},
   "source": [
    "## mingru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7aa2d519",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_CLAMP_MAX = 25.0\n",
    "LOG_CLAMP_MIN = -50.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "14396094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math # math.log 및 상수 사용을 위함\n",
    "\n",
    "# 로그 공간 연산 후 지수 함수 적용 전 클램핑 값 정의\n",
    "# 매우 큰 값(exp(large_num) -> inf) 또는 매우 작은 값(exp(very_small_num) -> 0)으로 인한\n",
    "# 오버플로우 또는 언더플로우 방지에 도움을 줍니다.\n",
    "# 이전 값: LOG_CLAMP_MAX = 20.0, LOG_CLAMP_MIN = -25.0\n",
    "LOG_CLAMP_MAX = 20.0  # exp(20) 은 약 4.8e8\n",
    "LOG_CLAMP_MIN = -25.0 # exp(-25) 은 약 1.3e-11\n",
    "\n",
    "def log_g(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    은닉 상태 후보를 로그 공간으로 변환하는 함수.\n",
    "    x >= 0 이면 log(x + 0.5)\n",
    "    x < 0 이면 log(sigmoid(x))\n",
    "    \"\"\"\n",
    "    # F.relu(x)는 x < 0 일 때 0을 반환합니다.\n",
    "    # -F.softplus(-x)는 F.logsigmoid(x)와 동일하며, log(sigmoid(x))를 계산합니다.\n",
    "    return torch.where(x >= 0, (F.relu(x) + 0.5).log(), -F.softplus(-x))\n",
    "\n",
    "def parallel_scan_log(log_coeffs: torch.Tensor, log_values: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    로그 공간에서 병렬 스캔 연산을 수행합니다.\n",
    "    이 함수는 다음 점화식을 위한 Blelloch 스캔 알고리즘을 기반으로 합니다:\n",
    "    h_t = exp(log_coeffs_t) * h_{t-1} + exp(log_values_{t+1})\n",
    "    이는 h_t = alpha_t * h_{t-1} + beta_t 로 변환되며,\n",
    "    함수는 log(h_t)를 계산합니다.\n",
    "\n",
    "    Args:\n",
    "        log_coeffs (torch.Tensor): [B, L, H] 형태의 텐서로, log(alpha_t)를 나타냅니다.\n",
    "                                   은닉 상태 점화식의 계수의 로그 값입니다.\n",
    "        log_values (torch.Tensor): [B, L+1, H] 형태의 텐서로,\n",
    "                                   [log_h_initial, log(beta_1), log(beta_2), ..., log(beta_L)]를 나타냅니다.\n",
    "                                   log_h_initial은 초기 은닉 상태의 로그 값입니다.\n",
    "                                   log(beta_t)는 각 단계에서 더해지는 항의 로그 값입니다.\n",
    "    Returns:\n",
    "        torch.Tensor: [B, L, H] 형태의 텐서로, 지수 함수가 적용된 은닉 상태 (h_1 부터 h_L까지)를 나타냅니다.\n",
    "    \"\"\"\n",
    "    # log_proda_coeffs_prefix_t = log( Product_{i=0}^{t-1} alpha_i )\n",
    "    # log_coeffs의 누적 합은 로그의 합이므로, 곱의 로그가 됩니다.\n",
    "    # F.pad는 t=0 경우 (h_initial의 계수)를 위해 0 (log(1))을 앞에 추가합니다.\n",
    "    # 입력 log_coeffs는 [B, L, H]입니다. cumsum 결과는 [B, L, H]입니다.\n",
    "    # L 차원(dim=1)에 (1,0)으로 패딩하여 [B, L+1, H]로 만듭니다.\n",
    "    log_proda_coeffs_prefix = F.pad(torch.cumsum(log_coeffs, dim=1), (0, 0, 1, 0), value=0.0)\n",
    "    # (H 차원은 0,0, L 차원은 1,0) -> L 차원의 왼쪽에 1만큼 패딩합니다.\n",
    "\n",
    "    # terms_for_logcumsumexp_t = log(beta_t) - log(Product_{i=0}^{t-1} alpha_i)\n",
    "    # = log ( beta_t / (Product_{i=0}^{t-1} alpha_i) )\n",
    "    terms_for_logcumsumexp = log_values - log_proda_coeffs_prefix\n",
    "    # logcumsumexp 전에 극단적인 값을 방지하기 위해 이 항들을 클램핑합니다.\n",
    "    # (logcumsumexp 자체는 일반적으로 안정적입니다.)\n",
    "    terms_for_logcumsumexp_clamped = torch.clamp(terms_for_logcumsumexp, min=LOG_CLAMP_MIN, max=LOG_CLAMP_MAX)\n",
    "\n",
    "    # log_sum_exp_terms_t = log( Sum_{j=0 to t} exp(terms_for_logcumsumexp_clamped_j) )\n",
    "    # 이는 log( Sum_{j=0 to t} beta_j / (Product_{i=0}^{j-1} alpha_i) )를 계산합니다.\n",
    "    log_sum_exp_terms = torch.logcumsumexp(terms_for_logcumsumexp_clamped, dim=1)\n",
    "\n",
    "    # log_hidden_states_t = log(Product_{i=0}^{t-1} alpha_i) + log( Sum_{j=0 to t} beta_j / (Product_{i=0}^{j-1} alpha_i) )\n",
    "    # 이는 log( Sum_{j=0 to t} beta_j * (Product_{i=j}^{t-1} alpha_i) )로 단순화되며, 이것이 log(h_t)입니다.\n",
    "    log_hidden_states = log_proda_coeffs_prefix + log_sum_exp_terms\n",
    "\n",
    "    # 지수 함수 적용 전에 최종 로그 은닉 상태를 클램핑합니다.\n",
    "    log_hidden_states_clamped = torch.clamp(log_hidden_states, min=LOG_CLAMP_MIN, max=LOG_CLAMP_MAX)\n",
    "\n",
    "    # h_1, ..., h_L을 반환하고자 합니다.\n",
    "    # log_hidden_states_clamped는 [B, L+1, H]이며, 인덱스 0은 log(h_initial)입니다.\n",
    "    # 따라서 인덱스 1부터 슬라이싱합니다.\n",
    "    output_hidden_states = torch.exp(log_hidden_states_clamped[:, 1:, :])\n",
    "\n",
    "    return output_hidden_states\n",
    "\n",
    "class ParallelLogMinGRU(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, epsilon: float = 1e-7):\n",
    "        super().__init__()\n",
    "        self.linear_z = nn.Linear(input_size, hidden_size)\n",
    "        self.linear_h_candidate = nn.Linear(input_size, hidden_size) # 명확성을 위해 이름 변경 (이전: linear_h)\n",
    "        self.epsilon = epsilon # log(0) 방지를 위한 작은 값\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        # 선형 레이어에 Xavier 초기화 적용\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'linear' in name:\n",
    "                if 'weight' in name:\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "                elif 'bias' in name:\n",
    "                    nn.init.zeros_(param)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: [B, L, D_in] (배치 크기, 시퀀스 길이, 입력 차원)\n",
    "        B, L, _ = x.size()\n",
    "\n",
    "        logits_z = self.linear_z(x)  # [B, L, H] - 업데이트 게이트 Z를 위한 로짓\n",
    "\n",
    "        # log_A = log(1-Z)는 점화식에서 log(alpha_t)에 해당합니다.\n",
    "        # log(1 - sigmoid(val))을 수치적으로 안정하게 계산하는 방법: log(sigmoid(-val)) = F.logsigmoid(-val)\n",
    "        log_A = F.logsigmoid(-logits_z)  # 형태: [B, L, H]\n",
    "\n",
    "        # 은닉 상태 후보 (예: W_h * x_t + b_h)\n",
    "        h_candidate_input = self.linear_h_candidate(x) # 형태: [B, L, H]\n",
    "        # log_g 함수를 사용하여 은닉 상태 후보를 로그 공간으로 변환\n",
    "        log_h_candidate_contrib = log_g(h_candidate_input) # 형태: [B, L, H]\n",
    "\n",
    "\n",
    "        # log_B = log(Z * h_candidate_processed)\n",
    "        # log_B = log(Z) + log_h_candidate_contrib\n",
    "        # log(Z) = F.logsigmoid(logits_z)\n",
    "        log_Z = F.logsigmoid(logits_z)\n",
    "        log_B = log_Z + log_h_candidate_contrib # 형태: [B, L, H]\n",
    "\n",
    "        # 초기 은닉 상태 h_0은 종종 0입니다. log(h_0) = log(0 + epsilon).\n",
    "        # 정확한 형태와 장치로 log_h0를 위한 텐서를 생성합니다.\n",
    "        log_h0_val = torch.full((B, 1, self.linear_h_candidate.out_features),\n",
    "                                math.log(self.epsilon), # 스칼라 epsilon에는 math.log 사용\n",
    "                                device=x.device, dtype=x.dtype) # 형태: [B, 1, H]\n",
    "\n",
    "        # log_h0_val과 log_B를 결합하여 parallel_scan_log를 위한 log_values를 형성합니다.\n",
    "        # log_values는 [log_h_initial, log_beta_1, ..., log_beta_L] 형태여야 합니다.\n",
    "        log_vals = torch.cat([log_h0_val, log_B], dim=1)  # 형태: [B, L+1, H]\n",
    "\n",
    "        # log_A는 계수(log_coeffs), log_vals는 더해지는 항(log_values)입니다.\n",
    "        return parallel_scan_log(log_A, log_vals)  # 은닉 상태 h_1부터 h_L까지 반환, 형태 [B, L, H]\n",
    "\n",
    "\n",
    "class MinGRUDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        embedding_dim: int,\n",
    "        hidden_dim: int,\n",
    "        num_layers: int = 2,\n",
    "        dropout: float = 0.1,\n",
    "        epsilon_gru: float = 1e-7 # GRU 셀을 위한 Epsilon 값\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layer_norms = nn.ModuleList() # 각 GRU 레이어 이후 LayerNorm\n",
    "\n",
    "        # 첫 번째 레이어: Embedding_dim -> Hidden_dim\n",
    "        self.layers.append(ParallelLogMinGRU(embedding_dim, hidden_dim, epsilon=epsilon_gru))\n",
    "        self.layer_norms.append(nn.LayerNorm(hidden_dim))\n",
    "\n",
    "        # 추가 레이어: Hidden_dim -> Hidden_dim\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.layers.append(ParallelLogMinGRU(hidden_dim, hidden_dim, epsilon=epsilon_gru))\n",
    "            self.layer_norms.append(nn.LayerNorm(hidden_dim))\n",
    "\n",
    "        self.fc1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.dropout_final = nn.Dropout(dropout) # 최종 FC 레이어 전 드롭아웃\n",
    "        self.dropout_inter_layer = nn.Dropout(dropout) if num_layers > 1 else nn.Identity()\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, None]:\n",
    "        \"\"\"\n",
    "        x: (batch, seq_len) 형태의 LongTensor, 입력 토큰 ID를 나타냄\n",
    "        반환값: logits (batch, seq_len, vocab_size)\n",
    "        \"\"\"\n",
    "        h = self.embedding(x)  # [B, L, E] (배치, 시퀀스 길이, 임베딩 차원)\n",
    "\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            # 잔차 연결을 위해 이전 h를 저장할 수 있으나, 여기서는 구현되지 않음\n",
    "            # h_prev = h\n",
    "            \n",
    "            h = layer(h)      # ParallelLogMinGRU 레이어 적용 -> [B, L, H]\n",
    "            h = self.layer_norms[i](h) # LayerNorm 적용 -> [B, L, H]\n",
    "\n",
    "            # GRU 레이어 간 드롭아웃 적용 (이 루프의 마지막 GRU 레이어 이후에는 적용 안 함)\n",
    "            if i < len(self.layers) - 1:\n",
    "                h = self.dropout_inter_layer(h)\n",
    "        \n",
    "        # h는 이제 마지막 GRU 레이어(와 해당 LayerNorm)의 출력입니다.\n",
    "\n",
    "        # 완전 연결 계층 전에 최종 드롭아웃을 적용합니다.\n",
    "        h = self.dropout_final(h)\n",
    "\n",
    "        h = F.gelu(self.fc1(h))    # 첫 번째 FC 레이어 + GELU 활성화 -> [B, L, H]\n",
    "        logits = self.fc2(h)       # 최종 FC 레이어, vocab_size 로짓 생성 -> [B, L, V]\n",
    "        \n",
    "        # 원본 코드는 (logits, _)를 반환하여 두 번째 반환 값(예: 마지막 은닉 상태)의 가능성을 암시했습니다.\n",
    "        # 현재로서는 train 함수가 이를 사용할 경우 예상되는 튜플 출력을 맞추기 위해 두 번째 값으로 None을 반환합니다.\n",
    "        return logits, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4c86a2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model Initialization\n",
    "mingru = MinGRUDecoder(vocab_size, EMBEDDING_DIM, HIDDEN_DIM, NUM_LAYERS).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(mingru.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8199acd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "MinGRUDecoder                            [512, 64, 65]             --\n",
       "├─Embedding: 1-1                         [512, 64, 32]             2,080\n",
       "├─ModuleList: 1-5                        --                        (recursive)\n",
       "│    └─ParallelLogMinGRU: 2-1            [512, 64, 64]             --\n",
       "│    │    └─Linear: 3-1                  [512, 64, 64]             2,112\n",
       "│    │    └─Linear: 3-2                  [512, 64, 64]             2,112\n",
       "├─ModuleList: 1-6                        --                        (recursive)\n",
       "│    └─LayerNorm: 2-2                    [512, 64, 64]             128\n",
       "├─Dropout: 1-4                           [512, 64, 64]             --\n",
       "├─ModuleList: 1-5                        --                        (recursive)\n",
       "│    └─ParallelLogMinGRU: 2-3            [512, 64, 64]             --\n",
       "│    │    └─Linear: 3-3                  [512, 64, 64]             4,160\n",
       "│    │    └─Linear: 3-4                  [512, 64, 64]             4,160\n",
       "├─ModuleList: 1-6                        --                        (recursive)\n",
       "│    └─LayerNorm: 2-4                    [512, 64, 64]             128\n",
       "├─Dropout: 1-7                           [512, 64, 64]             --\n",
       "├─Linear: 1-8                            [512, 64, 64]             4,160\n",
       "├─Linear: 1-9                            [512, 64, 65]             4,225\n",
       "==========================================================================================\n",
       "Total params: 23,265\n",
       "Trainable params: 23,265\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 11.91\n",
       "==========================================================================================\n",
       "Input size (MB): 0.26\n",
       "Forward/backward pass size (MB): 142.87\n",
       "Params size (MB): 0.09\n",
       "Estimated Total Size (MB): 143.22\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(mingru, input_size=(BATCH_SIZE, SEQUENCE_LENGTH), dtypes=[torch.long])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "284bd8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9f66c397",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Train Loss: 2.8209, Val Loss: 2.8024, Epoch Time: 26.61s Average Vram Usage: 563.17MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20, Train Loss: 2.5395, Val Loss: 2.4750, Epoch Time: 26.71s Average Vram Usage: 563.17MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20, Train Loss: 2.4459, Val Loss: 2.3421, Epoch Time: 26.79s Average Vram Usage: 563.17MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20, Train Loss: 2.3477, Val Loss: 2.2892, Epoch Time: 27.32s Average Vram Usage: 563.17MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20, Train Loss: 2.3039, Val Loss: 2.2598, Epoch Time: 26.44s Average Vram Usage: 563.17MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20, Train Loss: 2.2598, Val Loss: 2.2440, Epoch Time: 26.74s Average Vram Usage: 563.17MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20, Train Loss: 2.2378, Val Loss: 2.2328, Epoch Time: 27.19s Average Vram Usage: 563.17MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20, Train Loss: 2.2086, Val Loss: 2.2303, Epoch Time: 26.27s Average Vram Usage: 563.17MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20, Train Loss: 2.2025, Val Loss: 2.2274, Epoch Time: 26.43s Average Vram Usage: 563.17MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20, Train Loss: 2.1847, Val Loss: 2.2237, Epoch Time: 25.29s Average Vram Usage: 563.17MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20, Train Loss: 2.1799, Val Loss: 2.2204, Epoch Time: 25.52s Average Vram Usage: 563.17MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20, Train Loss: 2.1866, Val Loss: 2.2097, Epoch Time: 26.12s Average Vram Usage: 563.17MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20, Train Loss: 2.1576, Val Loss: 2.2088, Epoch Time: 26.47s Average Vram Usage: 563.17MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20, Train Loss: 2.1439, Val Loss: 2.2040, Epoch Time: 26.86s Average Vram Usage: 563.17MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20, Train Loss: 2.1344, Val Loss: 2.1981, Epoch Time: 25.91s Average Vram Usage: 563.17MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20, Train Loss: 2.1675, Val Loss: 2.1903, Epoch Time: 26.68s Average Vram Usage: 563.17MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20, Train Loss: 2.1237, Val Loss: 2.1888, Epoch Time: 26.16s Average Vram Usage: 563.17MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20, Train Loss: 2.1337, Val Loss: 2.1838, Epoch Time: 26.47s Average Vram Usage: 563.17MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20, Train Loss: 2.1255, Val Loss: 2.1855, Epoch Time: 25.82s Average Vram Usage: 563.17MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20, Train Loss: 2.1168, Val Loss: 2.1816, Epoch Time: 26.65s Average Vram Usage: 563.17MB\n"
     ]
    }
   ],
   "source": [
    "train_and_test(\"minGRU\", mingru, start_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b912f66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "def count_params(m): return sum(p.numel() for p in m.parameters())\n",
    "def model_size_kb(m):\n",
    "    buf=io.BytesIO(); torch.save(m.state_dict(),buf)\n",
    "    return round(len(buf.getvalue())/1024)\n",
    "def measure_time_ms(m,inp,iters=20):\n",
    "    m.eval()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(5): m(inp)\n",
    "        ts=[]\n",
    "        for _ in range(iters):\n",
    "            t0=time.time(); m(inp); ts.append((time.time()-t0)*1000)\n",
    "    return sum(ts)/len(ts)\n",
    "\n",
    "# settings\n",
    "device='cuda'\n",
    "\n",
    "models={\n",
    "    \"Transformer\":llama,\n",
    "    \"Mamba\": mamba,\n",
    "    \"GRU\": gru,\n",
    "    \"MinGRU\": mingru\n",
    "}\n",
    "\n",
    "# 1) fixed length\n",
    "inputs=torch.randint(vocab_size,(BATCH_SIZE,SEQUENCE_LENGTH),dtype=torch.long,device=device)\n",
    "records=[]\n",
    "for name,m in models.items():\n",
    "    records.append({\n",
    "        \"Model\": name,\n",
    "        \"Params\": count_params(m),\n",
    "        \"Size_KB\": model_size_kb(m),\n",
    "        \"Time_512_ms\": round(measure_time_ms(m,inputs), 2)\n",
    "    })\n",
    "df_fixed=pd.DataFrame(records).set_index(\"Model\")\n",
    "\n",
    "# 2) speed vs length\n",
    "seq_lens=[128,256,512,768,1024]\n",
    "speed_data={name:[round(measure_time_ms(m,torch.randint(vocab_size,(BATCH_SIZE,L),dtype=torch.long,device=device)), 2) for L in seq_lens]\n",
    "            for name,m in models.items()}\n",
    "df_speed=pd.DataFrame(speed_data,index=seq_lens)\n",
    "df_speed.index.name=\"Seq_Len\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bf8ee361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Comparison ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Params</th>\n",
       "      <th>Size_KB</th>\n",
       "      <th>Time_512_ms</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Transformer</th>\n",
       "      <td>61680</td>\n",
       "      <td>249</td>\n",
       "      <td>2.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mamba</th>\n",
       "      <td>99200</td>\n",
       "      <td>395</td>\n",
       "      <td>1.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GRU</th>\n",
       "      <td>79329</td>\n",
       "      <td>315</td>\n",
       "      <td>24.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MinGRU</th>\n",
       "      <td>23265</td>\n",
       "      <td>96</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Params  Size_KB  Time_512_ms\n",
       "Model                                    \n",
       "Transformer   61680      249         2.62\n",
       "Mamba         99200      395         1.35\n",
       "GRU           79329      315        24.16\n",
       "MinGRU        23265       96         0.86"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"=== Comparison ===\")\n",
    "display(df_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fd1c5791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Transformer</th>\n",
       "      <th>Mamba</th>\n",
       "      <th>GRU</th>\n",
       "      <th>MinGRU</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Seq_Len</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>5.29</td>\n",
       "      <td>4.81</td>\n",
       "      <td>48.06</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>14.11</td>\n",
       "      <td>4.71</td>\n",
       "      <td>95.86</td>\n",
       "      <td>4.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>46.84</td>\n",
       "      <td>9.48</td>\n",
       "      <td>190.72</td>\n",
       "      <td>8.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768</th>\n",
       "      <td>99.98</td>\n",
       "      <td>17.34</td>\n",
       "      <td>285.68</td>\n",
       "      <td>14.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>173.97</td>\n",
       "      <td>26.49</td>\n",
       "      <td>380.65</td>\n",
       "      <td>20.26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Transformer  Mamba     GRU  MinGRU\n",
       "Seq_Len                                    \n",
       "128             5.29   4.81   48.06    1.00\n",
       "256            14.11   4.71   95.86    4.11\n",
       "512            46.84   9.48  190.72    8.25\n",
       "768            99.98  17.34  285.68   14.27\n",
       "1024          173.97  26.49  380.65   20.26"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6ece8e22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# df_speed: DataFrame with index=sequence lengths and columns=model names, values=time in ms\n",
    "plt.figure()\n",
    "for model in df_speed.columns:\n",
    "    plt.plot(df_speed.index, df_speed[model], marker='o', label=model)\n",
    "plt.xlabel(\"Sequence Length\")\n",
    "plt.ylabel(\"Inference Time (ms)\")\n",
    "plt.title(\"Inference Time vs Sequence Length\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
