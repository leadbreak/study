{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83fe85c7",
   "metadata": {},
   "source": [
    "```Performance Optimization with torch compiler```\n",
    "- Model Size: 0.05M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20ec8bf0",
   "metadata": {
    "id": "20ec8bf0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3390f4",
   "metadata": {
    "id": "5a3390f4"
   },
   "source": [
    "## Hyperparameters and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0c61f36",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1732703737281,
     "user": {
      "displayName": "Hugman Sangkeun Jung",
      "userId": "08689291704194029524"
     },
     "user_tz": -540
    },
    "id": "d0c61f36",
    "outputId": "45d9894c-a14d-4164-9f2d-699025751e72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Modified hyperparameters\n",
    "SEQUENCE_LENGTH = 64\n",
    "\n",
    "EMBEDDING_DIM = 32\n",
    "HIDDEN_DIM = EMBEDDING_DIM*2\n",
    "\n",
    "BATCH_SIZE = 1024\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-3\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.1\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c9b962",
   "metadata": {
    "id": "89c9b962"
   },
   "source": [
    "## Data Preparation\n",
    "\n",
    "We are using the TinyShakespeare dataset, a small character-level text corpus consisting of a subset of Shakespeare's plays. It's often used for testing sequence models, as it includes a rich set of vocabulary and provides a challenging task for next-character prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "089830e2",
   "metadata": {
    "id": "089830e2"
   },
   "outputs": [],
   "source": [
    "## Utility Functions\n",
    "\n",
    "def load_data(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "def create_char_mappings(text):\n",
    "    chars = sorted(list(set(text)))\n",
    "    char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "    idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "    return chars, char_to_idx, idx_to_char"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fc6452",
   "metadata": {
    "id": "30fc6452"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f07670d9",
   "metadata": {
    "id": "f07670d9"
   },
   "outputs": [],
   "source": [
    "class CharDataset(Dataset):\n",
    "    def __init__(self, text, seq_length, char_to_idx):\n",
    "        self.text = text\n",
    "        self.seq_length = seq_length\n",
    "        self.char_to_idx = char_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text) - self.seq_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = [self.char_to_idx[ch] for ch in self.text[idx:idx+self.seq_length]]\n",
    "        y = [self.char_to_idx[ch] for ch in self.text[idx+1:idx+self.seq_length+1]]\n",
    "        return torch.tensor(x), torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9172da13",
   "metadata": {
    "id": "9172da13"
   },
   "outputs": [],
   "source": [
    "def prepare_data(text, seq_length, batch_size, val_split):\n",
    "    chars, char_to_idx, idx_to_char = create_char_mappings(text)\n",
    "\n",
    "    # Split data into train and validation\n",
    "    val_size = int(len(text) * val_split)\n",
    "    train_text, val_text = text[:-val_size], text[-val_size:]\n",
    "\n",
    "    train_dataset = CharDataset(train_text, seq_length, char_to_idx)\n",
    "    val_dataset = CharDataset(val_text, seq_length, char_to_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=12, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=12, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader, chars, char_to_idx, idx_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "nEwKFB_8L6AG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3706,
     "status": "ok",
     "timestamp": 1732704246464,
     "user": {
      "displayName": "Hugman Sangkeun Jung",
      "userId": "08689291704194029524"
     },
     "user_tz": -540
    },
    "id": "nEwKFB_8L6AG",
    "outputId": "e75cdff9-3775-461e-d930-0ced534bf74d"
   },
   "outputs": [],
   "source": [
    "# !wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=19zosLuU0z4MxIMKbGVYEGlg52QyfbTIy' -O input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47d03398",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 668,
     "status": "ok",
     "timestamp": 1732704255324,
     "user": {
      "displayName": "Hugman Sangkeun Jung",
      "userId": "08689291704194029524"
     },
     "user_tz": -540
    },
    "id": "47d03398",
    "outputId": "f247f5b3-88ff-4ecb-e8a3-4b2e42a2820b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 1115394\n",
      "Vocabulary size: 65\n",
      "Train dataset size: 1003791\n",
      "Validation dataset size: 111475\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "text = load_data('../data/input.txt')\n",
    "train_loader, val_loader, chars, char_to_idx, idx_to_char = prepare_data(text, SEQUENCE_LENGTH, BATCH_SIZE, VALIDATION_SPLIT)\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(f\"Total characters: {len(text)}\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Train dataset size: {len(train_loader.dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9442b104",
   "metadata": {
    "id": "9442b104"
   },
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1f0a6a1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 916,
     "status": "ok",
     "timestamp": 1732704257555,
     "user": {
      "displayName": "Hugman Sangkeun Jung",
      "userId": "08689291704194029524"
     },
     "user_tz": -540
    },
    "id": "f1f0a6a1",
    "outputId": "9ef6eb5a-0015-40ad-f143-236e3d38fb7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1024, 64])\n",
      "Target shape: torch.Size([1024, 64])\n",
      "Sample 1: ------------------------------\n",
      "Input sequence : part;An she agree, within her scope of choiceLies my consent a\n",
      "Target sequence: art;An she agree, within her scope of choiceLies my consent an\n",
      "\n",
      "Sample 2: ------------------------------\n",
      "Input sequence : rry words.QUEEN ELIZABETH:If he were dead, what would betide \n",
      "Target sequence: ry words.QUEEN ELIZABETH:If he were dead, what would betide o\n",
      "\n",
      "Sample 3: ------------------------------\n",
      "Input sequence : CINIUS:There, Coriolanus.CORIOLANUS:May I change these garme\n",
      "Target sequence: INIUS:There, Coriolanus.CORIOLANUS:May I change these garmen\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to convert index sequence to character sequence\n",
    "def indices_to_text(indices, idx_to_char):\n",
    "    return ''.join([idx_to_char[idx.item()] for idx in indices])\n",
    "\n",
    "# Get a batch of data\n",
    "dataiter = iter(train_loader)\n",
    "batch_x, batch_y = next(dataiter)\n",
    "\n",
    "print(f\"Input shape: {batch_x.shape}\")\n",
    "print(f\"Target shape: {batch_y.shape}\")\n",
    "\n",
    "# Print a few samples from the batch\n",
    "num_samples = 3\n",
    "for i in range(num_samples):\n",
    "    print(f\"Sample {i+1}: ------------------------------\" )\n",
    "    print(\"Input sequence :\", indices_to_text(batch_x[i], idx_to_char).replace('\\n',''))\n",
    "    print(\"Target sequence:\", indices_to_text(batch_y[i], idx_to_char).replace('\\n',''))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5486f73f",
   "metadata": {
    "id": "5486f73f"
   },
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61a52f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_vram_usage(device=\"cuda\"):\n",
    "    allocated = torch.cuda.memory_allocated(device) / (1024**2)  # in MB\n",
    "    reserved = torch.cuda.memory_reserved(device) / (1024**2)    # in MB\n",
    "    max_allocated = torch.cuda.max_memory_allocated(device) / (1024**2)  # in MB\n",
    "    print(f\"Allocated: {allocated:.2f} MB, Reserved: {reserved:.2f} MB, Max Allocated: {max_allocated:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59178a11",
   "metadata": {
    "id": "59178a11"
   },
   "outputs": [],
   "source": [
    "def validate(model, dataloader, criterion, device, epoch, step):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            output, _ = model(x)\n",
    "            loss = criterion(output.view(-1, output.size(-1)), y.view(-1))\n",
    "            losses.append((step, epoch, loss.item()))\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea5e61f5",
   "metadata": {
    "id": "ea5e61f5"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, dataloader, criterion, optimizer, device, epoch, step):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    vram_usage = []\n",
    "    scaler = torch.amp.GradScaler('cuda')\n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch}\", leave=False)\n",
    "    for batch, (x, y) in enumerate(pbar):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "            output, _ = model(x)\n",
    "            loss = criterion(output.view(-1, output.size(-1)), y.view(-1))\n",
    "            \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        step += 1\n",
    "        losses.append((step, epoch, loss.item()))\n",
    "        \n",
    "        # VRAM 사용량을 progress bar의 postfix로 업데이트\n",
    "        allocated = torch.cuda.memory_allocated(device) / (1024**2)\n",
    "        vram_usage.append(allocated)\n",
    "        pbar.set_postfix(loss=f'{loss.item():.4f}', step=step, vram=f'{allocated:.2f} MB')\n",
    "    return losses, step, vram_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbedfd61",
   "metadata": {
    "id": "cbedfd61"
   },
   "source": [
    "## Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e91075e8",
   "metadata": {
    "id": "e91075e8"
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, device, epochs):\n",
    "    all_train_losses = []\n",
    "    all_val_losses = []\n",
    "    all_vram_usages = [] # 에포크별 평균 VRAM 사용량 리스트로 변경 가능 또는 배치별 VRAM 사용량 리스트 유지\n",
    "    step = 0\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        # Training phase\n",
    "        # epoch_train_losses는 (step, epoch, loss_value) 튜플의 리스트\n",
    "        epoch_train_losses_list, step, epoch_vram_usage_list = train(model, train_loader, criterion, optimizer, device, epoch, step)\n",
    "        all_train_losses.extend(epoch_train_losses_list)\n",
    "        if epoch_vram_usage_list: # CUDA 사용 시에만 vram_usage가 채워짐\n",
    "             all_vram_usages.extend(epoch_vram_usage_list) # 배치별 VRAM 사용량 모두 저장\n",
    "\n",
    "        # 현재 에포크의 평균 훈련 로스 계산\n",
    "        avg_epoch_train_loss = 0\n",
    "        if epoch_train_losses_list:\n",
    "            avg_epoch_train_loss = np.mean([loss_tuple[2] for loss_tuple in epoch_train_losses_list])\n",
    "        \n",
    "        # Validation phase\n",
    "        # epoch_val_losses는 (step, epoch, loss_value) 튜플의 리스트\n",
    "        epoch_val_losses_list = validate(model, val_loader, criterion, device, epoch, step) # validate의 step은 해당 에포크 종료 시점의 step\n",
    "        all_val_losses.extend(epoch_val_losses_list)\n",
    "        \n",
    "        # 현재 에포크의 평균 검증 로스 계산\n",
    "        avg_epoch_val_loss = 0\n",
    "        if epoch_val_losses_list:\n",
    "            avg_epoch_val_loss = np.mean([loss_tuple[2] for loss_tuple in epoch_val_losses_list])\n",
    "            \n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        \n",
    "        avg_epoch_vram_str = \"\"\n",
    "        if epoch_vram_usage_list: # CUDA 사용 시\n",
    "            avg_epoch_vram = np.mean(epoch_vram_usage_list)\n",
    "            avg_epoch_vram_str = f', Avg VRAM: {avg_epoch_vram:.2f}MB'\n",
    "\n",
    "        print(f'Epoch {epoch}/{epochs}, Avg Train Loss: {avg_epoch_train_loss:.4f}, '\n",
    "              f'Avg Val Loss: {avg_epoch_val_loss:.4f}, Epoch Time: {epoch_time:.2f}s{avg_epoch_vram_str}')\n",
    "\n",
    "    train_losses_df = pd.DataFrame(all_train_losses, columns=['step', 'epoch', 'loss_value'])\n",
    "    val_losses_df = pd.DataFrame(all_val_losses, columns=['step', 'epoch', 'loss_value'])\n",
    "    \n",
    "    # all_vram_usages는 모든 배치의 VRAM 사용량을 담고 있으므로, 전체 평균을 계산할 수 있음\n",
    "    # overall_avg_vram = np.mean(all_vram_usages) if all_vram_usages else 0\n",
    "    # print(f\"Overall Average VRAM Usage: {overall_avg_vram:.2f} MB\")\n",
    "    \n",
    "    return model, train_losses_df, val_losses_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4535c886",
   "metadata": {
    "id": "4535c886"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, char_to_idx, idx_to_char, start_text, device, max_length=500):\n",
    "    model.eval()\n",
    "    current_text = start_text\n",
    "    hidden = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            x = torch.tensor([[char_to_idx[ch] for ch in current_text[-SEQUENCE_LENGTH:]]]).to(device)\n",
    "            output, hidden = model(x, hidden)\n",
    "            probs = torch.softmax(output[0, -1], dim=0)\n",
    "            next_char_idx = torch.multinomial(probs, 1).item()\n",
    "            next_char = idx_to_char[next_char_idx]\n",
    "            current_text += next_char\n",
    "\n",
    "    return current_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6aef1cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_comparison_dict = {}\n",
    "\n",
    "def add_loss_to_comparison(model_name, train_losses_df, val_losses_df):\n",
    "    \"\"\"\n",
    "    Adds training and validation losses from a model to the comparison dictionary.\n",
    "    \"\"\"\n",
    "    loss_comparison_dict[model_name] = {\n",
    "        'train': train_losses_df,\n",
    "        'val': val_losses_df\n",
    "    }\n",
    "\n",
    "def print_final_losses(loss_dict):\n",
    "    for model_name, losses in loss_dict.items():\n",
    "        train_df = losses['train']\n",
    "        val_df = losses['val']\n",
    "        final_train = train_df.groupby('epoch')['loss_value'].last().iloc[-1]\n",
    "        final_val = val_df.groupby('epoch')['loss_value'].last().iloc[-1]\n",
    "        print(f\"{model_name}: Final Train Loss: {final_train:.4f}, Final Val Loss: {final_val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "973b4a80",
   "metadata": {
    "id": "973b4a80"
   },
   "outputs": [],
   "source": [
    "# Function to plot loss curves\n",
    "def plot_loss(train_losses_df, val_losses_df):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    # Plot training losses\n",
    "    for epoch in train_losses_df['epoch'].unique():\n",
    "        epoch_train_losses = train_losses_df[train_losses_df['epoch'] == epoch]\n",
    "        plt.plot(epoch_train_losses['step'], epoch_train_losses['loss_value'],\n",
    "                 color='blue', alpha=0.3)\n",
    "\n",
    "    # scatter training loss at the end of each epoch\n",
    "    last_train_losses = train_losses_df.groupby('epoch').last().reset_index()\n",
    "    plt.scatter(last_train_losses['step'], last_train_losses['loss_value'],\n",
    "                color='blue')\n",
    "\n",
    "    # Plot and scatter validation loss at the end of each epoch\n",
    "    last_val_losses = val_losses_df.groupby('epoch').last().reset_index()\n",
    "    plt.plot(last_val_losses['step'], last_val_losses['loss_value'],\n",
    "             color='orange', label='Validation Loss')\n",
    "    plt.scatter(last_val_losses['step'], last_val_losses['loss_value'],\n",
    "                color='orange')\n",
    "\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Function to print final loss values\n",
    "def print_final_losses(train_losses_df, val_losses_df):\n",
    "    print(\"Final Training Loss:\", train_losses_df.groupby('epoch')['loss_value'].last().iloc[-1])\n",
    "    print(\"Final Validation Loss:\", val_losses_df.groupby('epoch')['loss_value'].last().iloc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5001e203",
   "metadata": {
    "id": "5001e203"
   },
   "outputs": [],
   "source": [
    "# Function to plot loss curves for multiple models stored in loss_comparison_dict\n",
    "def plot_loss_comparisons():\n",
    "    \"\"\"\n",
    "    Plots the training loss curves and average validation loss per epoch for multiple models added to the loss comparison dictionary.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    # Get the last model in the dictionary (for special final-point highlighting)\n",
    "    last_model_name = list(loss_comparison_dict.keys())[-1]\n",
    "\n",
    "    # Loop through each model in the loss dictionary\n",
    "    for model_name, losses in loss_comparison_dict.items():\n",
    "        train_losses_df = losses['train']\n",
    "        val_losses_df = losses['val']\n",
    "\n",
    "        # Plot training losses for each model\n",
    "        plt.plot(train_losses_df['step'], train_losses_df['loss_value'],\n",
    "                 label=f'{model_name} train', linestyle='-', alpha=0.7)\n",
    "\n",
    "        # Scatter training loss at the end of each epoch\n",
    "        last_train_losses = train_losses_df.groupby('epoch').last().reset_index()\n",
    "        plt.scatter(last_train_losses['step'], last_train_losses['loss_value'], marker='o', s=50)\n",
    "\n",
    "        # Compute average validation loss per epoch (using the last step of each epoch for x-axis)\n",
    "        avg_val_losses = val_losses_df.groupby('epoch').agg({'loss_value': 'mean', 'step': 'last'}).reset_index()\n",
    "        # Scatter the average validation loss for each epoch\n",
    "        plt.scatter(avg_val_losses['step'], avg_val_losses['loss_value'], marker='s', s=50,\n",
    "                    label=f'{model_name} val avg')\n",
    "\n",
    "        # For the last model, highlight the final training loss with a star\n",
    "        if model_name == last_model_name:\n",
    "            final_step = train_losses_df['step'].iloc[-1]\n",
    "            final_loss = train_losses_df['loss_value'].iloc[-1]\n",
    "            plt.scatter(final_step, final_loss, marker='*', s=100, color='red', zorder=5)\n",
    "\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Comparison')\n",
    "    plt.legend()  # Legend shows both training and validation average labels\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16777ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_separate_train_val(loss_dict):\n",
    "    \"\"\"\n",
    "    모델별 Training Loss와 Validation Loss를 각각 별도의 그래프로 그립니다.\n",
    "    단, Validation Loss는 에포크별 평균으로 계산합니다.\n",
    "    \"\"\"\n",
    "    # 1. Training Loss Plot (원본 그대로)\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    plt.subplot(1, 2, 1)  # 1행 2열 중 첫 번째\n",
    "    for model_name, losses in loss_dict.items():\n",
    "        train_df = losses['train']\n",
    "        steps_train = train_df['step'].values\n",
    "        loss_train = train_df['loss_value'].values\n",
    "        plt.plot(steps_train, loss_train, label=f'{model_name} Train')\n",
    "    plt.title('Training Loss Comparison')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # 2. Validation Loss Plot (에포크별 평균 처리)\n",
    "    plt.subplot(1, 2, 2)  # 1행 2열 중 두 번째\n",
    "    for model_name, losses in loss_dict.items():\n",
    "        val_df = losses['val']\n",
    "        # 에포크별 평균 loss와 마지막 step을 계산\n",
    "        val_avg = val_df.groupby('epoch').agg({'loss_value': 'mean', 'step': 'last'}).reset_index()\n",
    "        plt.plot(val_avg['step'], val_avg['loss_value'], label=f'{model_name} Val')\n",
    "    plt.title('Validation Loss (Epoch Avg) Comparison')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "396c469b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text generation using validation data\n",
    "val_sample, _ = next(iter(val_loader))\n",
    "start_text = ''.join([idx_to_char[idx.item()] for idx in val_sample[0][:SEQUENCE_LENGTH]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98b72703",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1732704396861,
     "user": {
      "displayName": "Hugman Sangkeun Jung",
      "userId": "08689291704194029524"
     },
     "user_tz": -540
    },
    "id": "98b72703",
    "outputId": "ece1620a-8690-4750-97c1-ada645d59a08"
   },
   "outputs": [],
   "source": [
    "# Decoder Input/Output Example\n",
    "sample_input, _ = next(iter(val_loader))\n",
    "sample_input = sample_input[0].unsqueeze(0).to(device)  # Add batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2d8a352",
   "metadata": {
    "id": "a2d8a352"
   },
   "outputs": [],
   "source": [
    "def generate_text_attention(model, char_to_idx, idx_to_char, start_text, device, max_length=500):\n",
    "    model.eval()\n",
    "    current_text = start_text\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            # Convert the last sequence of characters to indices and feed it to the model\n",
    "            x = torch.tensor([[char_to_idx[ch] for ch in current_text[-SEQUENCE_LENGTH:]]]).to(device)\n",
    "            output = model(x)[0]  # No hidden state needed for attention-based models\n",
    "            probs = torch.softmax(output[0, -1], dim=0)\n",
    "            next_char_idx = torch.multinomial(probs, 1).item()\n",
    "            next_char = idx_to_char[next_char_idx]\n",
    "            current_text += next_char\n",
    "\n",
    "    return current_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32c936b1",
   "metadata": {
    "id": "32c936b1"
   },
   "outputs": [],
   "source": [
    "def train_and_test(model_desc, model, start_text):\n",
    "    # Initialize the model\n",
    "    model = model.to(device)\n",
    "    # Use the same optimizer and criterion\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, fused=True)\n",
    "    \n",
    "    # Train the model\n",
    "    trained_model, train_losses_df, val_losses_df = train_model(\n",
    "        model, train_loader, val_loader, criterion, optimizer, device, EPOCHS\n",
    "    )\n",
    "\n",
    "    # Generate text\n",
    "    generated_text = generate_text_attention(trained_model, char_to_idx, idx_to_char, start_text, device)\n",
    "    print(f\"Generated text [{start_text}]:\")\n",
    "    print(\"-\"*50)\n",
    "    print(generated_text)\n",
    "    \n",
    "    add_loss_to_comparison(model_desc, train_losses_df, val_losses_df)\n",
    "\n",
    "    # Plot loss comparisons including this model\n",
    "    plot_loss_comparisons()\n",
    "    \n",
    "    plot_separate_train_val(loss_comparison_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbb22cc",
   "metadata": {},
   "source": [
    "## Original minGRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73586ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Triton 라이브러리 임포트\n",
    "try:\n",
    "    import triton\n",
    "    import triton.language as tl\n",
    "    HAS_TRITON = True\n",
    "except ImportError:\n",
    "    HAS_TRITON = False\n",
    "    print(\"Triton not found. Please install it to use the optimized GRU implementation.\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# │ │\n",
    "# │ Triton 커널 및 래퍼 함수 (병렬 스캔 알고리즘 적용) │\n",
    "# │ │\n",
    "# ---------------------------------------------------------------------------------\n",
    "\n",
    "@triton.jit\n",
    "def _softplus(x):\n",
    "    \"\"\"\n",
    "    수치적으로 안정적인 softplus 함수를 Triton으로 구현합니다.\n",
    "    softplus(x) = log(1 + exp(x))\n",
    "    \"\"\"\n",
    "    return tl.maximum(x, 0) + tl.log(1 + tl.exp(-tl.abs(x)))\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def _log_add_exp(a, b):\n",
    "    \"\"\" \n",
    "    수치적으로 안정적인 log(exp(a) + exp(b)) 계산 \n",
    "    \"\"\"\n",
    "    return tl.maximum(a, b) + tl.log(1 + tl.exp(-tl.abs(a - b)))\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def _log_g(x):\n",
    "    \"\"\" Triton 커널 내에서 사용할 log_g 함수 \"\"\"\n",
    "    # x >= 0 이면 log(x + 0.5)\n",
    "    # x < 0 이면 log(sigmoid(x)) = -softplus(-x)\n",
    "    return tl.where(x >= 0, tl.log(x + 0.5), -_softplus(-x))\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def _gru_scan_op(log_a1, log_b1, log_a2, log_b2):\n",
    "    \"\"\"\n",
    "    GRU 순환 관계를 위한 결합 스캔 연산자입니다.\n",
    "    ❗️오류 수정: tl.associative_scan의 combine_fn 시그니처에 맞게 인자를 4개 받도록 수정\n",
    "    \n",
    "    Args:\n",
    "        log_a1, log_b1: 첫 번째 요소의 (a, b) 계수\n",
    "        log_a2, log_b2: 두 번째 요소의 (a, b) 계수\n",
    "    \n",
    "    Returns:\n",
    "        합성된 함수의 (log_a', log_b') 튜플\n",
    "    \"\"\"\n",
    "    # 합성된 'a' 계수: a' = a2 * a1 -> log(a') = log(a2) + log(a1)\n",
    "    new_log_a = log_a1 + log_a2\n",
    "    \n",
    "    # 합성된 'b' 계수: b' = a2 * b1 + b2 -> log(b') = log_add_exp(log(a2) + log(b1), log(b2))\n",
    "    new_log_b = _log_add_exp(log_a2 + log_b1, log_b2)\n",
    "    \n",
    "    return new_log_a, new_log_b\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def _parallel_scan_gru_kernel(\n",
    "    # --- 입력/출력 텐서 포인터 ---\n",
    "    h_candidate_input_ptr,\n",
    "    logits_z_ptr,\n",
    "    output_ptr,\n",
    "    # --- 텐서 차원 및 스트라이드 ---\n",
    "    B, L, H,\n",
    "    stride_h_cand_b, stride_h_cand_l, stride_h_cand_h,\n",
    "    stride_logits_z_b, stride_logits_z_l, stride_logits_z_h,\n",
    "    stride_out_b, stride_out_l, stride_out_h,\n",
    "    # --- 상수 ---\n",
    "    epsilon: float,\n",
    "    # --- Triton 메타 파라미터 ---\n",
    "    BLOCK_SIZE_L: tl.constexpr, \n",
    "):\n",
    "    # --- 프로그램 ID 계산 ---\n",
    "    pid = tl.program_id(axis=0)\n",
    "    b_idx = pid // H\n",
    "    h_idx = pid % H\n",
    "\n",
    "    # --- 시퀀스(L) 차원에 대한 오프셋 생성 ---\n",
    "    l_offsets = tl.arange(0, BLOCK_SIZE_L)\n",
    "    mask = l_offsets < L\n",
    "\n",
    "    # --- 처리할 전체 시퀀스의 시작 포인터 계산 ---\n",
    "    h_cand_seq_ptr = h_candidate_input_ptr + (b_idx * stride_h_cand_b + h_idx * stride_h_cand_h + l_offsets * stride_h_cand_l)\n",
    "    logits_z_seq_ptr = logits_z_ptr + (b_idx * stride_logits_z_b + h_idx * stride_logits_z_h + l_offsets * stride_logits_z_l)\n",
    "    \n",
    "    # --- 전역 메모리에서 전체 시퀀스 데이터 로드 ---\n",
    "    h_cand_seq = tl.load(h_cand_seq_ptr, mask=mask, other=0.0).to(tl.float32)\n",
    "    logits_z_seq = tl.load(logits_z_seq_ptr, mask=mask, other=0.0).to(tl.float32)\n",
    "\n",
    "    # --- 1. 모든 타임스텝에 대한 log_a, log_b 계산 ---\n",
    "    log_a_seq = -_softplus(logits_z_seq)\n",
    "    log_b_seq = -_softplus(-logits_z_seq) + _log_g(h_cand_seq)\n",
    "\n",
    "    # --- 2. 병렬 스캔 수행 ---\n",
    "    # `tl.associative_scan`을 사용하여 순차 루프 없이 전체 시퀀스를 병렬로 계산합니다.\n",
    "    # 입력은 (log_a, log_b) 쌍의 시퀀스입니다.\n",
    "    scanned_log_a, scanned_log_b = tl.associative_scan((log_a_seq, log_b_seq), axis=0, combine_fn=_gru_scan_op)\n",
    "\n",
    "    # --- 3. 초기 은닉 상태(h0) 적용 ---\n",
    "    # h_t = (scanned_a_t) * h_0 + scanned_b_t\n",
    "    # 로그 공간에서 계산: log(h_t) = log_add_exp(scanned_log_a_t + log_h0, scanned_log_b_t)\n",
    "    log_h0 = tl.log(epsilon)\n",
    "    log_hidden_states = _log_add_exp(scanned_log_a + log_h0, scanned_log_b)\n",
    "\n",
    "    # --- 4. 최종 결과 계산 및 저장 ---\n",
    "    output = tl.exp(log_hidden_states)\n",
    "    output_seq_ptr = output_ptr + (b_idx * stride_out_b + h_idx * stride_out_h + l_offsets * stride_out_l)\n",
    "    tl.store(output_seq_ptr, output, mask=mask)\n",
    "\n",
    "\n",
    "def _parallel_log_min_gru_triton(\n",
    "    h_candidate_input: torch.Tensor,\n",
    "    logits_z: torch.Tensor,\n",
    "    epsilon: float\n",
    ") -> torch.Tensor:\n",
    "    \"\"\" Triton 커널을 실행하기 위한 Python 래퍼 함수 \"\"\"\n",
    "    B, L, H = h_candidate_input.shape\n",
    "    device = h_candidate_input.device\n",
    "    \n",
    "    output = torch.empty((B, L, H), device=device, dtype=h_candidate_input.dtype) \n",
    "    \n",
    "    # BLOCK_SIZE_L은 L보다 크거나 같은 2의 거듭제곱이어야 함\n",
    "    BLOCK_SIZE_L = triton.next_power_of_2(L)\n",
    "\n",
    "    grid = (B * H,)\n",
    "\n",
    "    _parallel_scan_gru_kernel[grid](\n",
    "        h_candidate_input, logits_z, output,\n",
    "        B, L, H,\n",
    "        h_candidate_input.stride(0), h_candidate_input.stride(1), h_candidate_input.stride(2),\n",
    "        logits_z.stride(0), logits_z.stride(1), logits_z.stride(2),\n",
    "        output.stride(0), output.stride(1), output.stride(2),\n",
    "        epsilon,\n",
    "        BLOCK_SIZE_L=BLOCK_SIZE_L,\n",
    "    )\n",
    "    return output\n",
    "\n",
    "\n",
    "class ParallelLogMinGRU(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, expansion_factor: float = 1.0, epsilon: float = 1e-7):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.expansion_factor = expansion_factor\n",
    "        self.internal_expanded_dim = int(hidden_size * self.expansion_factor)\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        self.use_triton = HAS_TRITON\n",
    "\n",
    "        self.to_hidden_and_gate = nn.Linear(input_size, self.internal_expanded_dim * 2)\n",
    "        if self.expansion_factor != 1.0:\n",
    "            self.to_out = nn.Linear(self.internal_expanded_dim, hidden_size)\n",
    "        else:\n",
    "            self.to_out = nn.Identity()\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'to_hidden_and_gate' in name or 'to_out' in name:\n",
    "                if 'weight' in name:\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "                elif 'bias' in name and param is not None:\n",
    "                    nn.init.zeros_(param)\n",
    "                    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, L, _ = x.size()\n",
    "        hidden_and_gate = self.to_hidden_and_gate(x)\n",
    "        h_candidate_input_expanded, logits_z_expanded = hidden_and_gate.chunk(2, dim=-1)\n",
    "\n",
    "        if self.use_triton and x.is_cuda and L > 0:\n",
    "            h_expanded_scan_out = _parallel_log_min_gru_triton(\n",
    "                h_candidate_input_expanded, \n",
    "                logits_z_expanded, \n",
    "                self.epsilon\n",
    "            )\n",
    "        else:\n",
    "            if L == 0:\n",
    "                return torch.empty(B, 0, self.hidden_size, device=x.device, dtype=x.dtype)\n",
    "            \n",
    "            log_A = F.logsigmoid(-logits_z_expanded)\n",
    "            log_Z_expanded = F.logsigmoid(logits_z_expanded)\n",
    "            log_h_candidate_contrib_expanded = self._original_log_g(h_candidate_input_expanded)\n",
    "            log_B = log_Z_expanded + log_h_candidate_contrib_expanded\n",
    "\n",
    "            log_h0_val = torch.full(\n",
    "                (B, 1, self.internal_expanded_dim),\n",
    "                math.log(self.epsilon),\n",
    "                device=x.device,\n",
    "                dtype=x.dtype\n",
    "            )\n",
    "            log_vals = torch.cat([log_h0_val, log_B], dim=1)\n",
    "            h_expanded_scan_out = self._original_parallel_scan_log(log_A, log_vals)\n",
    "\n",
    "        output = self.to_out(h_expanded_scan_out)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def _original_log_g(x: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.where(x >= 0, (F.relu(x) + 0.5).log(), -F.softplus(-x))\n",
    "\n",
    "    @staticmethod\n",
    "    def _original_parallel_scan_log(log_coeffs: torch.Tensor, log_values: torch.Tensor) -> torch.Tensor:\n",
    "        log_proda_coeffs_prefix = F.pad(torch.cumsum(log_coeffs, dim=1), (0, 0, 1, 0), value=0.0)\n",
    "        terms_for_logcumsumexp = log_values - log_proda_coeffs_prefix\n",
    "        log_sum_exp_terms = torch.logcumsumexp(terms_for_logcumsumexp, dim=1)\n",
    "        log_hidden_states = log_proda_coeffs_prefix + log_sum_exp_terms\n",
    "        output_hidden_states = torch.exp(log_hidden_states[:, 1:, :])\n",
    "        return output_hidden_states\n",
    "\n",
    "\n",
    "# =================================================================================\n",
    "# │ │\n",
    "# │ 아래는 사용자께서 제공한 나머지 코드 (변경 없음) │\n",
    "# │ │\n",
    "# =================================================================================\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-8):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.gamma = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        input_dtype = x.dtype\n",
    "        variance = x.to(torch.float32).pow(2).mean(dim=-1, keepdim=True)\n",
    "        return (x * torch.rsqrt(variance + self.eps) * self.gamma).to(input_dtype)\n",
    "\n",
    "\n",
    "class CausalDepthWiseConv1d(nn.Module):\n",
    "    def __init__(self, dim: int, kernel_size: int):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(dim, dim, kernel_size=kernel_size, groups=dim),\n",
    "            nn.Conv1d(dim, dim, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x_orig_shape = x.shape\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(0)\n",
    "        x_transposed = x.transpose(1, 2)\n",
    "        x_padded = F.pad(x_transposed, (self.kernel_size - 1, 0), value=0.0)\n",
    "        x_conv_out = self.net(x_padded)\n",
    "        x_restored = x_conv_out.transpose(1, 2)\n",
    "        if len(x_orig_shape) == 2 and x_restored.shape[0] == 1:\n",
    "            x_restored = x_restored.squeeze(0)\n",
    "        return x_restored\n",
    "\n",
    "\n",
    "class SwiGLUFFN(nn.Module):\n",
    "    def __init__(self, dim: int, expansion_factor: float = 4.0, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        ffn_hidden_dim = int(dim * expansion_factor)\n",
    "\n",
    "        self.w1_w3 = nn.Linear(dim, ffn_hidden_dim * 2, bias=False)\n",
    "        self.w2 = nn.Linear(ffn_hidden_dim, dim, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x_proj = self.w1_w3(x)\n",
    "        x1, x3_gate = x_proj.chunk(2, dim=-1)\n",
    "        hidden_states = F.silu(x1) * x3_gate\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        return self.w2(hidden_states)\n",
    "\n",
    "\n",
    "class minGRULM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        embedding_dim: int,\n",
    "        hidden_dim: int,\n",
    "        num_layers: int = 2,\n",
    "        dropout: float = 0.1,\n",
    "        expansion_factor_gru: float = 1.0,\n",
    "        epsilon_gru: float = 1e-7,\n",
    "        rms_norm_eps: float = 1e-8,\n",
    "        ffn_expansion_factor: float = 2.0,\n",
    "        conv_kernel_size: int = 3\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.input_projection = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.conv = CausalDepthWiseConv1d(hidden_dim, conv_kernel_size)\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            ffn_layer = SwiGLUFFN(\n",
    "                dim=hidden_dim,\n",
    "                expansion_factor=ffn_expansion_factor,\n",
    "                dropout=dropout\n",
    "            )\n",
    "\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                RMSNorm(hidden_dim, eps=rms_norm_eps),\n",
    "                ParallelLogMinGRU( # <--- 최적화된 모듈이 여기서 사용됩니다.\n",
    "                    input_size=hidden_dim,\n",
    "                    hidden_size=hidden_dim,\n",
    "                    expansion_factor=expansion_factor_gru,\n",
    "                    epsilon=epsilon_gru\n",
    "                ),\n",
    "                RMSNorm(hidden_dim, eps=rms_norm_eps),\n",
    "                ffn_layer,\n",
    "                nn.Dropout(dropout) if dropout > 0. else nn.Identity()\n",
    "            ]))\n",
    "\n",
    "        self.final_norm = RMSNorm(hidden_dim, eps=rms_norm_eps)\n",
    "        self.to_vocab = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        h = self.embedding(x)\n",
    "        h = self.input_projection(h)\n",
    "        h = self.conv(h)\n",
    "\n",
    "        for block_idx, block_modules in enumerate(self.layers):\n",
    "            norm_gru_input, gru_cell, norm_ff_input, ffn_module, dropout_block_output = block_modules\n",
    "\n",
    "            h_gru_residual_source = h\n",
    "            h_normed_for_gru = norm_gru_input(h)\n",
    "            h_gru_out = gru_cell(h_normed_for_gru)\n",
    "            h = h_gru_residual_source + h_gru_out\n",
    "\n",
    "            h_ff_residual_source = h\n",
    "            h_normed_for_ff = norm_ff_input(h)\n",
    "            h_ff_out = ffn_module(h_normed_for_ff)\n",
    "            h = h_ff_residual_source + h_ff_out\n",
    "            \n",
    "            h = dropout_block_output(h)\n",
    "\n",
    "        h = self.final_norm(h)\n",
    "        logits = self.to_vocab(h)\n",
    "        return logits, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4c86a2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model Initialization\n",
    "mingru = minGRULM(vocab_size, EMBEDDING_DIM, int(HIDDEN_DIM*1.2), NUM_LAYERS).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8199acd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "minGRULM                                 [1024, 64, 65]            --\n",
       "├─Embedding: 1-1                         [1024, 64, 32]            2,080\n",
       "├─Linear: 1-2                            [1024, 64, 76]            2,508\n",
       "├─CausalDepthWiseConv1d: 1-3             [1024, 64, 76]            --\n",
       "│    └─Sequential: 2-1                   [1024, 76, 64]            --\n",
       "│    │    └─Conv1d: 3-1                  [1024, 76, 64]            304\n",
       "│    │    └─Conv1d: 3-2                  [1024, 76, 64]            5,852\n",
       "├─ModuleList: 1-4                        --                        --\n",
       "│    └─ModuleList: 2-2                   --                        --\n",
       "│    │    └─RMSNorm: 3-3                 [1024, 64, 76]            76\n",
       "│    │    └─ParallelLogMinGRU: 3-4       [1024, 64, 76]            11,704\n",
       "│    │    └─RMSNorm: 3-5                 [1024, 64, 76]            76\n",
       "│    │    └─SwiGLUFFN: 3-6               [1024, 64, 76]            34,656\n",
       "│    │    └─Dropout: 3-7                 [1024, 64, 76]            --\n",
       "│    └─ModuleList: 2-3                   --                        --\n",
       "│    │    └─RMSNorm: 3-8                 [1024, 64, 76]            76\n",
       "│    │    └─ParallelLogMinGRU: 3-9       [1024, 64, 76]            11,704\n",
       "│    │    └─RMSNorm: 3-10                [1024, 64, 76]            76\n",
       "│    │    └─SwiGLUFFN: 3-11              [1024, 64, 76]            34,656\n",
       "│    │    └─Dropout: 3-12                [1024, 64, 76]            --\n",
       "├─RMSNorm: 1-5                           [1024, 64, 76]            76\n",
       "├─Linear: 1-6                            [1024, 64, 65]            5,005\n",
       "==========================================================================================\n",
       "Total params: 108,849\n",
       "Trainable params: 108,849\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 484.24\n",
       "==========================================================================================\n",
       "Input size (MB): 0.52\n",
       "Forward/backward pass size (MB): 768.08\n",
       "Params size (MB): 0.34\n",
       "Estimated Total Size (MB): 768.95\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(mingru, input_size=(BATCH_SIZE, SEQUENCE_LENGTH), dtypes=[torch.long])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018353f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_and_test\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mminGRU\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmingru\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_text\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 9\u001b[0m, in \u001b[0;36mtrain_and_test\u001b[0;34m(model_desc, model, start_text)\u001b[0m\n\u001b[1;32m      6\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mLEARNING_RATE, fused\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m trained_model, train_losses_df, val_losses_df \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Generate text\u001b[39;00m\n\u001b[1;32m     14\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m generate_text_attention(trained_model, char_to_idx, idx_to_char, start_text, device)\n",
      "Cell \u001b[0;32mIn[12], line 12\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, device, epochs)\u001b[0m\n\u001b[1;32m      8\u001b[0m epoch_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Training phase\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# epoch_train_losses는 (step, epoch, loss_value) 튜플의 리스트\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m epoch_train_losses_list, step, epoch_vram_usage_list \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m all_train_losses\u001b[38;5;241m.\u001b[39mextend(epoch_train_losses_list)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch_vram_usage_list: \u001b[38;5;66;03m# CUDA 사용 시에만 vram_usage가 채워짐\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 14\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, criterion, optimizer, device, epoch, step)\u001b[0m\n\u001b[1;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16):\n\u001b[0;32m---> 14\u001b[0m     output, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, output\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), y\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     17\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[34], line 365\u001b[0m, in \u001b[0;36mminGRULM.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    362\u001b[0m h \u001b[38;5;241m=\u001b[39m h_gru_residual_source \u001b[38;5;241m+\u001b[39m h_gru_out\n\u001b[1;32m    364\u001b[0m h_ff_residual_source \u001b[38;5;241m=\u001b[39m h\n\u001b[0;32m--> 365\u001b[0m h_normed_for_ff \u001b[38;5;241m=\u001b[39m \u001b[43mnorm_ff_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    366\u001b[0m h_ff_out \u001b[38;5;241m=\u001b[39m ffn_module(h_normed_for_ff)\n\u001b[1;32m    367\u001b[0m h \u001b[38;5;241m=\u001b[39m h_ff_residual_source \u001b[38;5;241m+\u001b[39m h_ff_out\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[34], line 266\u001b[0m, in \u001b[0;36mRMSNorm.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    264\u001b[0m input_dtype \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    265\u001b[0m variance \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrsqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariance\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "train_and_test(\"minGRU\", mingru, start_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f66c397",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Avg Train Loss: 1.9610, Avg Val Loss: 1.7954, Epoch Time: 14.91s, Avg VRAM: 26.79MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Avg Train Loss: 1.6660, Avg Val Loss: 1.7422, Epoch Time: 14.19s, Avg VRAM: 26.79MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Avg Train Loss: 1.6281, Avg Val Loss: 1.7226, Epoch Time: 14.48s, Avg VRAM: 26.79MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Avg Train Loss: 1.6082, Avg Val Loss: 1.7082, Epoch Time: 14.27s, Avg VRAM: 26.79MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Avg Train Loss: 1.5957, Avg Val Loss: 1.7092, Epoch Time: 14.41s, Avg VRAM: 26.79MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Avg Train Loss: 1.5858, Avg Val Loss: 1.6997, Epoch Time: 14.45s, Avg VRAM: 26.79MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Avg Train Loss: 1.5784, Avg Val Loss: 1.7049, Epoch Time: 14.38s, Avg VRAM: 26.79MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_and_test\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mminGRU\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmingru\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_text\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 9\u001b[0m, in \u001b[0;36mtrain_and_test\u001b[0;34m(model_desc, model, start_text)\u001b[0m\n\u001b[1;32m      6\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mLEARNING_RATE, fused\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m trained_model, train_losses_df, val_losses_df \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Generate text\u001b[39;00m\n\u001b[1;32m     14\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m generate_text_attention(trained_model, char_to_idx, idx_to_char, start_text, device)\n",
      "Cell \u001b[0;32mIn[12], line 24\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, device, epochs)\u001b[0m\n\u001b[1;32m     20\u001b[0m     avg_epoch_train_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean([loss_tuple[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m loss_tuple \u001b[38;5;129;01min\u001b[39;00m epoch_train_losses_list])\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Validation phase\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# epoch_val_losses는 (step, epoch, loss_value) 튜플의 리스트\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m epoch_val_losses_list \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# validate의 step은 해당 에포크 종료 시점의 step\u001b[39;00m\n\u001b[1;32m     25\u001b[0m all_val_losses\u001b[38;5;241m.\u001b[39mextend(epoch_val_losses_list)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# 현재 에포크의 평균 검증 로스 계산\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m, in \u001b[0;36mvalidate\u001b[0;34m(model, dataloader, criterion, device, epoch, step)\u001b[0m\n\u001b[1;32m      3\u001b[0m losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m      6\u001b[0m         x, y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m         output, _ \u001b[38;5;241m=\u001b[39m model(x)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:1446\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1443\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1444\u001b[0m     \u001b[38;5;66;03m# no valid `self._rcvd_idx` is found (i.e., didn't break)\u001b[39;00m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persistent_workers:\n\u001b[0;32m-> 1446\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_shutdown_workers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[1;32m   1449\u001b[0m \u001b[38;5;66;03m# Now `self._rcvd_idx` is the batch index we want to fetch\u001b[39;00m\n\u001b[1;32m   1450\u001b[0m \n\u001b[1;32m   1451\u001b[0m \u001b[38;5;66;03m# Check if the next sample has already been generated\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:1582\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._shutdown_workers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1577\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mark_worker_as_unavailable(worker_id, shutdown\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1578\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers:\n\u001b[1;32m   1579\u001b[0m     \u001b[38;5;66;03m# We should be able to join here, but in case anything went\u001b[39;00m\n\u001b[1;32m   1580\u001b[0m     \u001b[38;5;66;03m# wrong, we set a timeout and if the workers fail to join,\u001b[39;00m\n\u001b[1;32m   1581\u001b[0m     \u001b[38;5;66;03m# they are killed in the `finally` block.\u001b[39;00m\n\u001b[0;32m-> 1582\u001b[0m     \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMP_STATUS_CHECK_INTERVAL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues:\n\u001b[1;32m   1584\u001b[0m     q\u001b[38;5;241m.\u001b[39mcancel_join_thread()\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/process.py:149\u001b[0m, in \u001b[0;36mBaseProcess.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_pid \u001b[38;5;241m==\u001b[39m os\u001b[38;5;241m.\u001b[39mgetpid(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan only join a child process\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan only join a started process\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 149\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_popen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     _children\u001b[38;5;241m.\u001b[39mdiscard(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/popen_fork.py:40\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmultiprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wait\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentinel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# This shouldn't block if wait() returned successfully.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m/usr/lib/python3.10/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_and_test(\"minGRU\", mingru, start_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228e462d",
   "metadata": {},
   "source": [
    "## Restructure minGRU\n",
    "- 기존 모델 구조는 torch.compile과 호환되기 어려운 구조이므로, 이전에 구현했던 구조 중 하나인 llama like structure를 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1401ae92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math  # math.log 및 상수 사용을 위함\n",
    "from tqdm import tqdm  # tqdm 임포트 추가\n",
    "\n",
    "\n",
    "def log_g(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    은닉 상태 후보를 로그 공간으로 변환하는 함수.\n",
    "    x >= 0 이면 log(x + 0.5)\n",
    "    x < 0 이면 log(sigmoid(x))\n",
    "    \"\"\"\n",
    "    return torch.where(x >= 0, (F.relu(x) + 0.5).log(), -F.softplus(-x))\n",
    "\n",
    "\n",
    "def parallel_scan_log(log_coeffs: torch.Tensor, log_values: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    로그 공간에서 병렬 스캔 연산을 수행합니다. (클램핑 제거됨)\n",
    "    Args:\n",
    "        log_coeffs (torch.Tensor): [B, L, H_internal] 형태의 텐서로, log(alpha_t)를 나타냅니다.\n",
    "        log_values (torch.Tensor): [B, L+1, H_internal] 형태의 텐서로, [log_h_initial, log(beta_1), ..., log(beta_L)]를 나타냅니다.\n",
    "    Returns:\n",
    "        torch.Tensor: [B, L, H_internal] 형태의 텐서로, 지수 함수가 적용된 은닉 상태 (h_1 부터 h_L까지)를 나타냅니다.\n",
    "    \"\"\"\n",
    "    log_proda_coeffs_prefix = F.pad(torch.cumsum(log_coeffs, dim=1), (0, 0, 1, 0), value=0.0)\n",
    "    terms_for_logcumsumexp = log_values - log_proda_coeffs_prefix\n",
    "    log_sum_exp_terms = torch.logcumsumexp(terms_for_logcumsumexp, dim=1)\n",
    "    log_hidden_states = log_proda_coeffs_prefix + log_sum_exp_terms\n",
    "    output_hidden_states = torch.exp(log_hidden_states[:, 1:, :])\n",
    "    return output_hidden_states\n",
    "\n",
    "\n",
    "class ParallelLogMinGRU(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, expansion_factor: float = 1.0, epsilon: float = 1e-7):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.expansion_factor = expansion_factor  # GRU 셀 내부의 확장 계수\n",
    "        self.internal_expanded_dim = int(hidden_size * self.expansion_factor)\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.to_hidden_and_gate = nn.Linear(input_size, self.internal_expanded_dim * 2)\n",
    "        if self.expansion_factor != 1.0:\n",
    "            self.to_out = nn.Linear(self.internal_expanded_dim, hidden_size)\n",
    "        else:\n",
    "            self.to_out = nn.Identity()\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'to_hidden_and_gate' in name or 'to_out' in name:\n",
    "                if 'weight' in name:\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "                elif 'bias' in name and param is not None:\n",
    "                    nn.init.zeros_(param)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: [B, L, input_size]\n",
    "        B, L, _ = x.size()\n",
    "        hidden_and_gate = self.to_hidden_and_gate(x)  # [B, L, internal_expanded_dim * 2]\n",
    "        h_candidate_input_expanded, logits_z_expanded = hidden_and_gate.chunk(2, dim=-1)\n",
    "\n",
    "        log_A = F.logsigmoid(-logits_z_expanded)\n",
    "        log_Z_expanded = F.logsigmoid(logits_z_expanded)\n",
    "        log_h_candidate_contrib_expanded = log_g(h_candidate_input_expanded)\n",
    "        log_B = log_Z_expanded + log_h_candidate_contrib_expanded\n",
    "\n",
    "        log_h0_val = torch.full(\n",
    "            (B, 1, self.internal_expanded_dim),\n",
    "            math.log(self.epsilon),\n",
    "            device=x.device,\n",
    "            dtype=x.dtype\n",
    "        )\n",
    "        log_vals = torch.cat([log_h0_val, log_B], dim=1)  # [B, L+1, internal_expanded_dim]\n",
    "        h_expanded_scan_out = parallel_scan_log(log_A, log_vals)  # [B, L, internal_expanded_dim]\n",
    "\n",
    "        output = self.to_out(h_expanded_scan_out)  # [B, L, hidden_size]\n",
    "        return output\n",
    "\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-8):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.gamma = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        variance = x.pow(2).mean(dim=-1, keepdim=True)\n",
    "        return x * torch.rsqrt(variance + self.eps) * self.gamma\n",
    "\n",
    "\n",
    "class CausalDepthWiseConv1d(nn.Module):\n",
    "    def __init__(self, dim: int, kernel_size: int):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(dim, dim, kernel_size=kernel_size, groups=dim),\n",
    "            nn.Conv1d(dim, dim, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x_orig_shape = x.shape\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(0)\n",
    "        x_transposed = x.transpose(1, 2)\n",
    "        x_padded = F.pad(x_transposed, (self.kernel_size - 1, 0), value=0.0)\n",
    "        x_conv_out = self.net(x_padded)\n",
    "        x_restored = x_conv_out.transpose(1, 2)\n",
    "        if len(x_orig_shape) == 2 and x_restored.shape[0] == 1:\n",
    "            x_restored = x_restored.squeeze(0)\n",
    "        return x_restored\n",
    "\n",
    "\n",
    "class SwiGLUFFN(nn.Module):\n",
    "    def __init__(self, dim: int, expansion_factor: float = 4.0, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        ffn_hidden_dim = int(dim * expansion_factor)\n",
    "\n",
    "        self.w1_w3 = nn.Linear(dim, ffn_hidden_dim * 2, bias=False)\n",
    "        self.w2 = nn.Linear(ffn_hidden_dim, dim, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x_proj = self.w1_w3(x)\n",
    "        x1, x3_gate = x_proj.chunk(2, dim=-1)\n",
    "        hidden_states = F.silu(x1) * x3_gate\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        return self.w2(hidden_states)\n",
    "\n",
    "\n",
    "class minGRULM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        embedding_dim: int,\n",
    "        hidden_dim: int,\n",
    "        num_layers: int = 2,\n",
    "        dropout: float = 0.1,\n",
    "        expansion_factor_gru: float = 1.0,\n",
    "        epsilon_gru: float = 1e-7,\n",
    "        rms_norm_eps: float = 1e-8,\n",
    "        ffn_expansion_factor: float = 2.0,\n",
    "        conv_kernel_size: int = 3            # ← 여기에 컨볼루션 커널 크기 파라미터 추가\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.input_projection = nn.Linear(embedding_dim, hidden_dim)\n",
    "        # ─── 입력 프로젝션 이후에 CausalDepthWiseConv1d 모듈을 추가 ───\n",
    "        self.conv = CausalDepthWiseConv1d(hidden_dim, conv_kernel_size)\n",
    "        # ─────────────────────────────────────────────────────────\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            ffn_layer = SwiGLUFFN(\n",
    "                dim=hidden_dim,\n",
    "                expansion_factor=ffn_expansion_factor,\n",
    "                dropout=dropout\n",
    "            )\n",
    "\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                RMSNorm(hidden_dim, eps=rms_norm_eps),\n",
    "                ParallelLogMinGRU(\n",
    "                    input_size=hidden_dim,\n",
    "                    hidden_size=hidden_dim,\n",
    "                    expansion_factor=expansion_factor_gru,\n",
    "                    epsilon=epsilon_gru\n",
    "                ),\n",
    "                RMSNorm(hidden_dim, eps=rms_norm_eps),\n",
    "                ffn_layer,\n",
    "                nn.Dropout(dropout) if dropout > 0. else nn.Identity()\n",
    "            ]))\n",
    "\n",
    "        self.final_norm = RMSNorm(hidden_dim, eps=rms_norm_eps)\n",
    "        self.to_vocab = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        h = self.embedding(x)                    # [B, L, embedding_dim]\n",
    "        h = self.input_projection(h)             # [B, L, hidden_dim]\n",
    "\n",
    "        # ─── input_projection 직후에 causal conv1d 적용 ───\n",
    "        h = self.conv(h)                         # [B, L, hidden_dim]\n",
    "        # ───────────────────────────────────────────────────\n",
    "\n",
    "        for block_idx, block_modules in enumerate(self.layers):\n",
    "            norm_gru_input, gru_cell, norm_ff_input, ffn_module, dropout_block_output = block_modules\n",
    "\n",
    "            # ┌── GRU 블록 + 잔차\n",
    "            h_gru_residual_source = h\n",
    "            h_normed_for_gru = norm_gru_input(h)\n",
    "            h_gru_out = gru_cell(h_normed_for_gru)\n",
    "            h = h_gru_residual_source + h_gru_out\n",
    "            # └───────────────────────\n",
    "\n",
    "            # ┌── FFN 블록 + 잔차\n",
    "            h_ff_residual_source = h\n",
    "            h_normed_for_ff = norm_ff_input(h)\n",
    "            h_ff_out = ffn_module(h_normed_for_ff)\n",
    "            h = h_ff_residual_source + h_ff_out\n",
    "            # └───────────────────────\n",
    "\n",
    "            h = dropout_block_output(h)\n",
    "\n",
    "        h = self.final_norm(h)\n",
    "        logits = self.to_vocab(h)\n",
    "        return logits, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f48538",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model Initialization\n",
    "mingru2 = minGRULM(vocab_size, EMBEDDING_DIM, int(HIDDEN_DIM*1.2), NUM_LAYERS).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4deb080b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(mingru2, input_size=(BATCH_SIZE, SEQUENCE_LENGTH), dtypes=[torch.long])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec215016",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_test(\"minGRU_restructured\", mingru2, start_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5993e0",
   "metadata": {},
   "source": [
    "## Compiled minGRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2beb18c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd175c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test2(model_desc, model, start_text):\n",
    "    # Initialize the model\n",
    "    model = model.to(device)\n",
    "    # 다양한 컴파일 모드:\n",
    "        # \"default\": 좋은 성능과 컴파일 시간의 균형\n",
    "        # \"reduce-overhead\": 컴파일 오버헤드 감소 (작은 모델에 유리)\n",
    "        # \"max-autotune\": 가장 긴 컴파일 시간, 가장 높은 성능 목표\n",
    "    model = torch.compile(model, mode=\"reduce-overhead\")\n",
    "    # Use the same optimizer and criterion\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, fused=True)\n",
    "    \n",
    "    # Train the model\n",
    "    trained_model, train_losses_df, val_losses_df = train_model(\n",
    "        model, train_loader, val_loader, criterion, optimizer, device, EPOCHS\n",
    "    )\n",
    "\n",
    "    # Generate text\n",
    "    generated_text = generate_text_attention(trained_model, char_to_idx, idx_to_char, start_text, device)\n",
    "    print(f\"Generated text [{start_text}]:\")\n",
    "    print(\"-\"*50)\n",
    "    print(generated_text)\n",
    "    \n",
    "    add_loss_to_comparison(model_desc, train_losses_df, val_losses_df)\n",
    "\n",
    "    # Plot loss comparisons including this model\n",
    "    plot_loss_comparisons()\n",
    "    \n",
    "    plot_separate_train_val(loss_comparison_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27931e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model Initialization\n",
    "mingru3 = minGRULM(vocab_size, EMBEDDING_DIM, int(HIDDEN_DIM*1.2), NUM_LAYERS).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bac9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(mingru3, input_size=(BATCH_SIZE, SEQUENCE_LENGTH), dtypes=[torch.long])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be15695d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_test2(\"minGRU_compiled\", mingru3, start_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfafe2e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
