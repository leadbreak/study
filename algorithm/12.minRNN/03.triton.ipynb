{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f679879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardGRU  → time: 6.91s, accuracy: 99.80%\n",
      "LoopMinGRU   → time: 10.42s, accuracy: 100.00%\n",
      "ParScanGRU   → time: 1.70s, accuracy: 97.00%\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Triton imports for parallel scan\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "# --- Triton kernel for prefix-scan: h_t = a_t * h_{t-1} + b_t ---\n",
    "@triton.jit\n",
    "def parallel_scan_kernel(\n",
    "    a_ptr, b_ptr, h0_ptr, out_ptr,\n",
    "    seq_len: tl.constexpr, stride_seq: tl.constexpr, stride_s: tl.constexpr\n",
    "):\n",
    "    pid = tl.program_id(0)\n",
    "    # initial hidden state\n",
    "    h = tl.load(h0_ptr + pid)\n",
    "    for t in range(seq_len):\n",
    "        off = t * stride_seq + pid * stride_s\n",
    "        a_t = tl.load(a_ptr + off)\n",
    "        b_t = tl.load(b_ptr + off)\n",
    "        h = a_t * h + b_t\n",
    "        tl.store(out_ptr + off, h)\n",
    "\n",
    "# Python prefix-scan (for backward)\n",
    "def python_scan(a: torch.Tensor, b: torch.Tensor, h0: torch.Tensor) -> torch.Tensor:\n",
    "    P = torch.cumprod(a, dim=0)\n",
    "    P_inv = 1.0 / P\n",
    "    C = torch.cumsum(b * P_inv, dim=0)\n",
    "    return P * (h0.unsqueeze(0) + C)\n",
    "\n",
    "# Autograd-enabled Triton scan\n",
    "class _TritonScanFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, a, b, h0):\n",
    "        # Save tensors for backward\n",
    "        ctx.save_for_backward(a, b, h0)\n",
    "        T, B, H = a.shape\n",
    "        S = B * H\n",
    "        # Flatten tensors for Triton\n",
    "        a_flat = a.reshape(T, S).contiguous()\n",
    "        b_flat = b.reshape(T, S).contiguous()\n",
    "        out_flat = torch.empty_like(a_flat)\n",
    "        h0_flat = h0.reshape(S).contiguous()\n",
    "        # Strides for pointer arithmetic\n",
    "        stride_seq, stride_s = a_flat.stride()\n",
    "        # Launch Triton kernel\n",
    "        parallel_scan_kernel[(S,)](\n",
    "            a_flat, b_flat, h0_flat, out_flat,\n",
    "            T, stride_seq, stride_s\n",
    "        )\n",
    "        # Reshape back to (T, B, H)\n",
    "        return out_flat.view(T, B, H)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        a, b, h0 = ctx.saved_tensors\n",
    "        # Compute gradients for a and b using Python scan under grad\n",
    "        with torch.enable_grad():\n",
    "            a_ = a.detach().requires_grad_(True)\n",
    "            b_ = b.detach().requires_grad_(True)\n",
    "            H_py = python_scan(a_, b_, h0)\n",
    "        # Compute gradients\n",
    "        grad_a, grad_b = torch.autograd.grad(\n",
    "            outputs=H_py,\n",
    "            inputs=(a_, b_),\n",
    "            grad_outputs=grad_output,\n",
    "            retain_graph=False,\n",
    "            allow_unused=True\n",
    "        )\n",
    "        return grad_a, grad_b, None\n",
    "\n",
    "def triton_scan(a: torch.Tensor, b: torch.Tensor, h0: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Wrapper for Triton-enabled prefix scan\"\"\"\n",
    "    return _TritonScanFunction.apply(a, b, h0)\n",
    "\n",
    "# --- Toy dataset generation ---\n",
    "def make_toy_data(seq_len=10, n_samples=500, input_size=16, n_classes=2):\n",
    "    X = torch.randn(seq_len, n_samples, input_size)\n",
    "    y = (X[0].sum(dim=1) > 0).long()\n",
    "    dataset = TensorDataset(X.permute(1,0,2), y)\n",
    "    def collate(batch):\n",
    "        xs, ys = zip(*batch)\n",
    "        xs = torch.stack(xs).permute(1,0,2)\n",
    "        ys = torch.tensor(ys)\n",
    "        return xs, ys\n",
    "    return DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate)\n",
    "\n",
    "loader = make_toy_data()\n",
    "\n",
    "# --- Custom MinGRU layer ---\n",
    "class MinGRULayer(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_size, dropout=0.1, is_last=False):\n",
    "        super().__init__()\n",
    "        self.W_z = nn.Linear(in_dim, hidden_size)\n",
    "        self.W_h = nn.Linear(in_dim, hidden_size)\n",
    "        self.b_h = nn.Parameter(torch.zeros(hidden_size))\n",
    "        self.ln = nn.LayerNorm(hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout) if not is_last else None\n",
    "\n",
    "    def forward(self, inp, h_prev):\n",
    "        z = torch.sigmoid(self.W_z(inp))\n",
    "        h_tilde = torch.tanh(self.W_h(inp) + self.b_h)\n",
    "        h = (1 - z) * h_prev + z * h_tilde\n",
    "        h = self.ln(h)\n",
    "        if self.dropout:\n",
    "            h = self.dropout(h)\n",
    "        return h\n",
    "\n",
    "# --- Model definitions ---\n",
    "class StandardMultiLayerGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.cells = nn.ModuleList([\n",
    "            nn.GRUCell(input_size if i==0 else hidden_size, hidden_size)\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "        self.lns = nn.ModuleList([nn.LayerNorm(hidden_size) for _ in range(num_layers)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        T, B, _ = x.size()\n",
    "        h = [torch.zeros(B, self.cells[0].hidden_size, device=x.device) for _ in self.cells]\n",
    "        for t in range(T):\n",
    "            inp = x[t]\n",
    "            for i, cell in enumerate(self.cells):\n",
    "                h[i] = cell(inp, h[i])\n",
    "                h[i] = self.lns[i](h[i])\n",
    "                if i < len(self.cells) - 1:\n",
    "                    h[i] = self.dropout(h[i])\n",
    "                inp = h[i]\n",
    "        return self.fc(h[-1])\n",
    "\n",
    "class MultiLayerMinGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout=0.1):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            in_dim = input_size if i==0 else hidden_size\n",
    "            layers.append(MinGRULayer(in_dim, hidden_size, dropout, is_last=(i==num_layers-1)))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        T, B, _ = x.size()\n",
    "        h = [torch.zeros(B, self.layers[0].W_z.out_features, device=x.device) for _ in self.layers]\n",
    "        for t in range(T):\n",
    "            inp = x[t]\n",
    "            for i, layer in enumerate(self.layers):\n",
    "                h[i] = layer(inp, h[i])\n",
    "                inp = h[i]\n",
    "        return self.fc(h[-1])\n",
    "\n",
    "class ParallelScanMinGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout=0.1):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            in_dim = input_size if i==0 else hidden_size\n",
    "            layers.append(\n",
    "                nn.ModuleDict({\n",
    "                    'W_z': nn.Linear(in_dim, hidden_size),\n",
    "                    'W_h': nn.Linear(in_dim, hidden_size),\n",
    "                    'ln': nn.LayerNorm(hidden_size)\n",
    "                })\n",
    "            )\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        T, B, _ = x.size()\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            W_z, W_h, ln = layer['W_z'], layer['W_h'], layer['ln']\n",
    "            x_flat = x.reshape(T*B, -1).contiguous()\n",
    "            z = torch.sigmoid(W_z(x_flat)).view(T, B, -1)\n",
    "            a = 1 - z\n",
    "            h_tilde = torch.tanh(W_h(x_flat)).view(T, B, -1)\n",
    "            b = z * h_tilde\n",
    "            h0 = torch.zeros(B, z.size(2), device=x.device)\n",
    "            H = triton_scan(a, b, h0)\n",
    "            H = ln(H)\n",
    "            if i < len(self.layers) - 1:\n",
    "                H = self.dropout(H)\n",
    "            x = H\n",
    "        return self.fc(x[-1])\n",
    "\n",
    "# --- Training & evaluation ---\n",
    "def train_and_eval(model, loader, epochs=20, lr=1e-3, device='cuda'):\n",
    "    model.to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "    start = time.time()\n",
    "    for _ in range(epochs):\n",
    "        model.train()\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            loss = crit(logits, y)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "    train_time = time.time() - start\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            pred = model(x).argmax(dim=1)\n",
    "            correct += (pred == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return train_time, correct/total\n",
    "\n",
    "# --- Benchmarking ---\n",
    "if __name__ == '__main__':\n",
    "    INPUT_DIM, HIDDEN, LAYERS, CLASSES = 16, 32, 4, 2\n",
    "    models = {\n",
    "        'StandardGRU': StandardMultiLayerGRU(INPUT_DIM, HIDDEN, LAYERS, CLASSES),\n",
    "        'LoopMinGRU':  MultiLayerMinGRU(INPUT_DIM, HIDDEN, LAYERS, CLASSES),\n",
    "        'ParScanGRU':  ParallelScanMinGRU(INPUT_DIM, HIDDEN, LAYERS, CLASSES),\n",
    "    }\n",
    "    loader = make_toy_data()\n",
    "    for name, m in models.items():\n",
    "        t, acc = train_and_eval(m, loader)\n",
    "        print(f\"{name:12s} → time: {t:.2f}s, accuracy: {acc*100:5.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f2eedc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
