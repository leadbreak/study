{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68b084ae",
   "metadata": {},
   "source": [
    "## RNN History"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64250601",
   "metadata": {},
   "source": [
    "- RNN은 시퀀스 길이가 길어질 때, 앞쪽의 정보가 뒤쪽까지 효과적으로 전달되지 못하는 장기 의존성 문제(기울기 소실/폭주)를 가짐\n",
    "- 이를 해결하기 위한 전략으로 제시된 것이 바로 게이트 매커니즘(Gate Mechanism)이며, 이를 통해 정보의 흐름을 제어하는 것이 가능해졌으며,  \n",
    "  대표적인 게이트 매커니즘 모델이 LSTM과 GRU임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0748e4",
   "metadata": {},
   "source": [
    "## LSTM (Long Short-Term Memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39318bb",
   "metadata": {},
   "source": [
    "- LSTM은 '셀 상태(Cell State)'라는 별도의 정보 흐름 경로와 3개의 게이트(Forget, Input, Output)를 통해 장기 기억을 효과적으로 관리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54fd6da",
   "metadata": {},
   "source": [
    "### 1. 핵심 개념:\n",
    "\n",
    "* **셀 상태 ($c_t$)**: 정보가 장기간 보존될 수 있는 '메모리 라인'. 게이트에 의해 정보가 추가되거나 제거됨.\n",
    "* **은닉 상태 ($h_t$)**: 현재 시점의 출력을 만들고 다음 시점으로 전달되는 정보. 셀 상태를 가공하여 생성됨.\n",
    "* **게이트**: Sigmoid 함수를 사용하여 0~1 사이의 값을 출력, 정보의 통과 비율을 조절.\n",
    "    * **망각 게이트 ($f_t$)**: 과거 셀 상태($c_{t-1}$)에서 어떤 정보를 잊을지 결정.\n",
    "    * **입력 게이트 ($i_t$)**: 현재 입력($x_t$)과 이전 은닉 상태($h_{t-1}$)를 바탕으로 어떤 새로운 정보($\\tilde{c}_t$)를 셀 상태에 추가할지 결정.\n",
    "    * **출력 게이트 ($o_t$)**: 업데이트된 셀 상태($c_t$)에서 어떤 정보를 현재 은닉 상태($h_t$)로 출력할지 결정.\n",
    "\n",
    "### 2. 수식:\n",
    "\n",
    "현재 시점 $t$의 입력 $x_t$, 이전 은닉 상태 $h_{t-1}$, 이전 셀 상태 $c_{t-1}$가 주어졌을 때:\n",
    "\n",
    "* 망각 게이트: $f_t = \\sigma(W_f [h_{t-1}, x_t] + b_f)$\n",
    "* 입력 게이트: $i_t = \\sigma(W_i [h_{t-1}, x_t] + b_i)$\n",
    "* 셀 상태 후보: $\\tilde{c}_t = \\tanh(W_c [h_{t-1}, x_t] + b_c)$\n",
    "* 셀 상태 업데이트: $c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t$\n",
    "* 출력 게이트: $o_t = \\sigma(W_o [h_{t-1}, x_t] + b_o)$\n",
    "* 은닉 상태 업데이트: $h_t = o_t \\odot \\tanh(c_t)$\n",
    "\n",
    "(표기: $\\sigma$=Sigmoid, $\\tanh$=Hyperbolic Tangent, $\\odot$=원소별 곱셈, $[h_{t-1}, x_t]$=벡터 연결)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb664df1",
   "metadata": {},
   "source": [
    "### 3. 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f10b9fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class LSTMCellScratch(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        \"\"\"LSTM Cell 초기화 (PyTorch nn.Module 상속)\"\"\"\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # 가중치와 편향을 nn.Parameter로 등록 (학습 대상)\n",
    "        # 입력과 이전 은닉 상태를 합친 크기\n",
    "        concat_size = input_size + hidden_size\n",
    "\n",
    "        # 각 게이트 및 셀 후보를 위한 하나의 큰 가중치 행렬과 편향 벡터\n",
    "        # (계산 효율성을 위해 실제 라이브러리들은 이렇게 구현하는 경우가 많음)\n",
    "        # 여기서는 이해를 돕기 위해 개별적으로 정의 (개념적 분리)\n",
    "        # 망각 게이트 (Forget Gate)\n",
    "        self.Wf = nn.Parameter(torch.Tensor(concat_size, hidden_size))\n",
    "        self.bf = nn.Parameter(torch.Tensor(hidden_size))\n",
    "        # 입력 게이트 (Input Gate)\n",
    "        self.Wi = nn.Parameter(torch.Tensor(concat_size, hidden_size))\n",
    "        self.bi = nn.Parameter(torch.Tensor(hidden_size))\n",
    "        # 셀 상태 후보 (Candidate Cell State)\n",
    "        self.Wc = nn.Parameter(torch.Tensor(concat_size, hidden_size))\n",
    "        self.bc = nn.Parameter(torch.Tensor(hidden_size))\n",
    "        # 출력 게이트 (Output Gate)\n",
    "        self.Wo = nn.Parameter(torch.Tensor(concat_size, hidden_size))\n",
    "        self.bo = nn.Parameter(torch.Tensor(hidden_size))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"가중치와 편향 초기화\"\"\"\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            # 모든 파라미터(Wf, bf, Wi, bi 등)에 대해 초기화 수행\n",
    "             nn.init.uniform_(weight, -stdv, stdv)\n",
    "        # 또는 nn.init.xavier_uniform_ 등 다른 초기화 방법 사용 가능\n",
    "\n",
    "    def forward(self, xt, prev_state):\n",
    "        \"\"\"\n",
    "        LSTM Cell의 단일 스텝 순전파\n",
    "\n",
    "        Args:\n",
    "            xt (torch.Tensor): 현재 시점 t의 입력 (batch_size, input_size)\n",
    "            prev_state (tuple): 이전 시점의 상태 (h_{t-1}, c_{t-1})\n",
    "                                h_{t-1}: (batch_size, hidden_size)\n",
    "                                c_{t-1}: (batch_size, hidden_size)\n",
    "\n",
    "        Returns:\n",
    "            tuple: 현재 시점의 상태 (ht, ct)\n",
    "                   ht: (batch_size, hidden_size)\n",
    "                   ct: (batch_size, hidden_size)\n",
    "        \"\"\"\n",
    "        prev_h, prev_c = prev_state\n",
    "\n",
    "        # 1. 이전 은닉 상태와 현재 입력을 연결 (concatenate)\n",
    "        # dim=1 : 피처(열) 방향으로 합침\n",
    "        combined = torch.cat((prev_h, xt), dim=1)\n",
    "\n",
    "        # 2. 망각 게이트 계산 (ft)\n",
    "        ft = torch.sigmoid(torch.matmul(combined, self.Wf) + self.bf)\n",
    "\n",
    "        # 3. 입력 게이트 계산 (it)\n",
    "        it = torch.sigmoid(torch.matmul(combined, self.Wi) + self.bi)\n",
    "\n",
    "        # 4. 셀 상태 후보 계산 (c_tilde_t)\n",
    "        c_tilde_t = torch.tanh(torch.matmul(combined, self.Wc) + self.bc)\n",
    "\n",
    "        # 5. 셀 상태 업데이트 (ct)\n",
    "        ct = (ft * prev_c) + (it * c_tilde_t) # 원소별 곱셈\n",
    "\n",
    "        # 6. 출력 게이트 계산 (ot)\n",
    "        ot = torch.sigmoid(torch.matmul(combined, self.Wo) + self.bo)\n",
    "\n",
    "        # 7. 은닉 상태 업데이트 (ht)\n",
    "        ht = ot * torch.tanh(ct) # 원소별 곱셈\n",
    "\n",
    "        return ht, ct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e75621f",
   "metadata": {},
   "source": [
    "## GRU(Gated Recurrent Unit)\n",
    "- GRU는 LSTM을 단순화한 구조로, 셀 상태 없이 은닉 상태만 사용하며 2개의 게이트(Reset, Update)로 정보 흐름을 제어"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b5ae4e",
   "metadata": {},
   "source": [
    "### 1. 핵심 개념:\n",
    "\n",
    "* **은닉 상태 ($h_t$)**: 정보 저장과 출력을 모두 담당.\n",
    "* **게이트**:\n",
    "    * **리셋 게이트 ($r_t$)**: 과거 정보($h_{t-1}$) 중 현재와 관련 없는 정보를 얼마나 무시할지 결정하여 후보 은닉 상태($\\tilde{h}_t$) 계산에 반영.\n",
    "    * **업데이트 게이트 ($z_t$)**: 과거 정보($h_{t-1}$)와 현재 계산된 후보 정보($\\tilde{h}_t$)를 어떤 비율로 조합하여 최종 은닉 상태($h_t$)를 만들지 결정. (LSTM의 망각+입력 게이트 역할 통합)\n",
    "\n",
    "### 2. 수식:\n",
    "\n",
    "현재 시점 $t$의 입력 $x_t$, 이전 은닉 상태 $h_{t-1}$가 주어졌을 때:\n",
    "\n",
    "* 리셋 게이트: $r_t = \\sigma(W_r [h_{t-1}, x_t] + b_r)$\n",
    "* 업데이트 게이트: $z_t = \\sigma(W_z [h_{t-1}, x_t] + b_z)$\n",
    "* 후보 은닉 상태: $\\tilde{h}_t = \\tanh(W_h [r_t \\odot h_{t-1}, x_t] + b_h)$\n",
    "* 은닉 상태 업데이트: $h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08789eaa",
   "metadata": {},
   "source": [
    "### 3. 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcd503d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class GRUCellScratch(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        \"\"\"GRU Cell 초기화 (PyTorch nn.Module 상속)\"\"\"\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        concat_size = input_size + hidden_size\n",
    "\n",
    "        # 리셋 게이트 (Reset Gate)\n",
    "        self.Wr = nn.Parameter(torch.Tensor(concat_size, hidden_size))\n",
    "        self.br = nn.Parameter(torch.Tensor(hidden_size))\n",
    "        # 업데이트 게이트 (Update Gate)\n",
    "        self.Wz = nn.Parameter(torch.Tensor(concat_size, hidden_size))\n",
    "        self.bz = nn.Parameter(torch.Tensor(hidden_size))\n",
    "        # 후보 은닉 상태 (Candidate Hidden State)\n",
    "        self.Wh = nn.Parameter(torch.Tensor(concat_size, hidden_size))\n",
    "        self.bh = nn.Parameter(torch.Tensor(hidden_size))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"가중치와 편향 초기화\"\"\"\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "             nn.init.uniform_(weight, -stdv, stdv)\n",
    "\n",
    "    def forward(self, xt, prev_h):\n",
    "        \"\"\"\n",
    "        GRU Cell의 단일 스텝 순전파\n",
    "\n",
    "        Args:\n",
    "            xt (torch.Tensor): 현재 시점 t의 입력 (batch_size, input_size)\n",
    "            prev_h (torch.Tensor): 이전 시점의 은닉 상태 h_{t-1} (batch_size, hidden_size)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: 현재 시점의 은닉 상태 ht (batch_size, hidden_size)\n",
    "        \"\"\"\n",
    "        # 1. 이전 은닉 상태와 현재 입력을 연결\n",
    "        combined = torch.cat((prev_h, xt), dim=1)\n",
    "\n",
    "        # 2. 리셋 게이트 계산 (rt)\n",
    "        rt = torch.sigmoid(torch.matmul(combined, self.Wr) + self.br)\n",
    "\n",
    "        # 3. 업데이트 게이트 계산 (zt)\n",
    "        zt = torch.sigmoid(torch.matmul(combined, self.Wz) + self.bz)\n",
    "\n",
    "        # 4. 후보 은닉 상태 계산 (h_tilde_t)\n",
    "        # 리셋 게이트를 적용한 이전 은닉 상태와 현재 입력을 연결\n",
    "        combined_reset = torch.cat((rt * prev_h, xt), dim=1) # 원소별 곱셈\n",
    "        h_tilde_t = torch.tanh(torch.matmul(combined_reset, self.Wh) + self.bh)\n",
    "\n",
    "        # 5. 은닉 상태 업데이트 (ht)\n",
    "        ht = (1 - zt) * prev_h + zt * h_tilde_t # 원소별 곱셈\n",
    "\n",
    "        return ht"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37559c3f",
   "metadata": {},
   "source": [
    "## Test #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91a3c938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- LSTM Cell Scratch Test ---\n",
      "Input shape (xt): torch.Size([5, 10])\n",
      "Previous hidden state shape (h_prev): torch.Size([5, 20])\n",
      "Previous cell state shape (c_prev): torch.Size([5, 20])\n",
      "Output hidden state shape (ht_lstm): torch.Size([5, 20])\n",
      "Output cell state shape (ct_lstm): torch.Size([5, 20])\n",
      "\n",
      "--- GRU Cell Scratch Test ---\n",
      "Input shape (xt): torch.Size([5, 10])\n",
      "Previous hidden state shape (h_prev): torch.Size([5, 20])\n",
      "Output hidden state shape (ht_gru): torch.Size([5, 20])\n",
      "\n",
      "--- PyTorch Built-in Cell Comparison ---\n",
      "Built-in LSTM output hidden shape: torch.Size([5, 20])\n",
      "Built-in GRU output hidden shape: torch.Size([5, 20])\n"
     ]
    }
   ],
   "source": [
    "# 파라미터 설정\n",
    "input_size = 10\n",
    "hidden_size = 20\n",
    "batch_size = 5\n",
    "\n",
    "# 디바이스 설정 (GPU 사용 가능하면 GPU, 아니면 CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 입력 데이터 생성 (batch_size, input_size)\n",
    "xt = torch.randn(batch_size, input_size).to(device)\n",
    "\n",
    "# 초기 상태 생성 (batch_size, hidden_size)\n",
    "h_prev = torch.zeros(batch_size, hidden_size).to(device)\n",
    "c_prev = torch.zeros(batch_size, hidden_size).to(device) # LSTM만 필요\n",
    "\n",
    "# --- LSTM Cell 테스트 ---\n",
    "print(\"--- LSTM Cell Scratch Test ---\")\n",
    "lstm_cell_scratch = LSTMCellScratch(input_size, hidden_size).to(device)\n",
    "# 모델 파라미터도 같은 디바이스로 이동\n",
    "# lstm_cell_scratch.to(device) # 위 라인에서 .to(device)로 대체 가능\n",
    "\n",
    "# 순전파\n",
    "ht_lstm, ct_lstm = lstm_cell_scratch(xt, (h_prev, c_prev))\n",
    "\n",
    "print(f\"Input shape (xt): {xt.shape}\")\n",
    "print(f\"Previous hidden state shape (h_prev): {h_prev.shape}\")\n",
    "print(f\"Previous cell state shape (c_prev): {c_prev.shape}\")\n",
    "print(f\"Output hidden state shape (ht_lstm): {ht_lstm.shape}\")\n",
    "print(f\"Output cell state shape (ct_lstm): {ct_lstm.shape}\")\n",
    "\n",
    "# --- GRU Cell 테스트 ---\n",
    "print(\"\\n--- GRU Cell Scratch Test ---\")\n",
    "gru_cell_scratch = GRUCellScratch(input_size, hidden_size).to(device)\n",
    "# gru_cell_scratch.to(device)\n",
    "\n",
    "# 순전파\n",
    "ht_gru = gru_cell_scratch(xt, h_prev)\n",
    "\n",
    "print(f\"Input shape (xt): {xt.shape}\")\n",
    "print(f\"Previous hidden state shape (h_prev): {h_prev.shape}\")\n",
    "print(f\"Output hidden state shape (ht_gru): {ht_gru.shape}\")\n",
    "\n",
    "# --- PyTorch 내장 Cell과 비교 ---\n",
    "print(\"\\n--- PyTorch Built-in Cell Comparison ---\")\n",
    "lstm_cell_builtin = nn.LSTMCell(input_size, hidden_size).to(device)\n",
    "gru_cell_builtin = nn.GRUCell(input_size, hidden_size).to(device)\n",
    "\n",
    "# 직접 구현한 Cell과 동일한 가중치로 설정해야 정확한 비교 가능\n",
    "# 예: lstm_cell_builtin.weight_ih = lstm_cell_scratch.W_combined_input 등 (가중치 매핑 필요)\n",
    "\n",
    "ht_lstm_builtin, ct_lstm_builtin = lstm_cell_builtin(xt, (h_prev, c_prev))\n",
    "ht_gru_builtin = gru_cell_builtin(xt, h_prev)\n",
    "\n",
    "print(f\"Built-in LSTM output hidden shape: {ht_lstm_builtin.shape}\")\n",
    "print(f\"Built-in GRU output hidden shape: {ht_gru_builtin.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d73d938",
   "metadata": {},
   "source": [
    "## Update\n",
    "- 해당 논문('Were RNN all you need')에서 제시된 핵심 개념은 다음의 두 개임\n",
    "    1. **단순화**: 핵심 기능을 유지하면서 게이트와 파라미터 수를 줄임\n",
    "    2. **병렬화**: 순환적 업데이트 단계를 병렬 연관 스캔(parallel association scan) 연산으로 표현할 수 있도록 재구성해, 학습 및 추론 속드를 대폭 향상시킴  \n",
    "                   이러한 방식은 Mamba 등의 영향을 받은 것으로 보임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27acbed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, Identity, Module\n",
    "import math\n",
    "import time\n",
    "import numpy as np # 데이터 생성을 위해 필요\n",
    "\n",
    "# --- 헬퍼 함수들 (minLSTM/minGRU 코드 및 부록 B 맥락 기반) ---\n",
    "def exists(v):\n",
    "    \"\"\"값이 None이 아닌지 확인\"\"\"\n",
    "    return v is not None\n",
    "\n",
    "def default(v, d):\n",
    "    \"\"\"값이 존재하면 값을 반환하고, 아니면 기본값 d를 반환\"\"\"\n",
    "    return v if exists(v) else d\n",
    "\n",
    "# 안정적인 로그-누적합-지수 함수 (log-cumsum-exp)\n",
    "def logcumsumexp(x, dim: int):\n",
    "    \"\"\" numerically stable log-cumsum-exp\n",
    "    from https://github.com/pytorch/pytorch/issues/31827#issuecomment-1027611157\n",
    "    \"\"\"\n",
    "    # 최신 PyTorch 버전에는 torch.logcumsumexp 가 내장됨\n",
    "    if hasattr(torch, \"logcumsumexp\"):\n",
    "        #  print(\"내장 torch.logcumsumexp 사용\")\n",
    "         return torch.logcumsumexp(x, dim=dim)\n",
    "    else:\n",
    "         print(\"수동 logcumsumexp 사용\")\n",
    "         # 구버전 PyTorch를 위한 수동 구현\n",
    "         max_val = torch.max(x, dim=dim, keepdim=True).values\n",
    "         # max_val이 -inf일 때 (모든 요소가 -inf) 안정성 확보\n",
    "         max_val = torch.where(torch.isinf(max_val), torch.zeros_like(max_val), max_val)\n",
    "         x_adjusted = x - max_val\n",
    "         cumulative_exp = torch.cumsum(x_adjusted.exp(), dim=dim)\n",
    "         # 작은 값 클램핑으로 log(0) 가능성 처리 (또는 cumulative_exp 확인)\n",
    "         log_cumulative_exp = torch.log(cumulative_exp.clamp(min=1e-38)) # 작은 값 클램핑\n",
    "         return max_val + log_cumulative_exp\n",
    "\n",
    "def heinsen_associative_scan_log(log_coeffs, log_values):\n",
    "    \"\"\"로그 공간에서의 연관 스캔 (Heinsen, Appendix B)\"\"\"\n",
    "    # 구버전 PyTorch에서 필요할 수 있으므로 입력 텐서가 contiguous인지 확인\n",
    "    log_coeffs = log_coeffs.contiguous()\n",
    "    log_values = log_values.contiguous()\n",
    "\n",
    "    # a*(t) = log_coeffs.cumsum(dim=1) = log(prod_{i=1...t} coeffs_i)\n",
    "    a_star = log_coeffs.cumsum(dim = 1)\n",
    "\n",
    "    # b*(t) = log_values(t) - a*(t) = log(values_t / prod_{i=1...t} coeffs_i)\n",
    "    # log H0 + b*(t) = (log_values - a_star).logcumsumexp(dim = 1)\n",
    "    # 여기서 H0는 초기 은닉 상태 항으로, 필요시 log_values 앞에 추가하여 효과적으로 처리\n",
    "    log_h0_plus_b_star = logcumsumexp(log_values - a_star, dim = 1)\n",
    "\n",
    "    # log h(t) = a*(t) + (log H0 + b*(t))\n",
    "    log_h = a_star + log_h0_plus_b_star\n",
    "    return log_h.exp() # exp()를 반환하여 양수 은닉 상태 얻기\n",
    "\n",
    "# 사용자 정의 활성화 함수 g와 그 로그 버전 (Appendix B.3)\n",
    "def g(x):\n",
    "    \"\"\"사용자 정의 활성화 함수 g\"\"\"\n",
    "    return torch.where(x >= 0, x + 0.5, x.sigmoid())\n",
    "\n",
    "def log_g(x):\n",
    "    \"\"\"사용자 정의 활성화 함수 g의 로그 버전\"\"\"\n",
    "    # 로그 입력이 양수인지 확인\n",
    "    relu_x_plus_0_5 = F.relu(x) + 0.5 # 항상 >= 0.5\n",
    "    # sigmoid는 항상 > 0, softplus는 항상 > 0\n",
    "    # log(sigmoid(x)) = -softplus(-x)\n",
    "    return torch.where(x >= 0, relu_x_plus_0_5.log(), -F.softplus(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cfcbe2",
   "metadata": {},
   "source": [
    "## minLSTM (Minimal LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b433582",
   "metadata": {},
   "source": [
    "\n",
    "minLSTM은 스캔을 통한 병렬 실행을 위해 설계된 단순화된 LSTM 변형으로 간주되며. 출력 게이트를 생략하고 망각/입력 게이트 상호작용에 다른 공식을 사용할 수 있음.\n",
    "\n",
    "### 1. 핵심 개념:\n",
    "\n",
    "* **축소된 게이트:** 단일 선형 투사(linear projection)를 통해 망각($f_t$) 및 입력($i_t$) 게이트 관련 값과 은닉 후보($\\tilde{h}_t$)만 명시적으로 계산합니다. 출력 게이트($o_t$)는 입력 투사로부터 직접 계산되지 않습니다.\n",
    "* **암시적 출력:** 업데이트된 은닉 상태가 직접적으로 출력 역할을 하며, 선택적인 최종 투사(`to_out`)를 거칠 수 있습니다.\n",
    "* **정규화된 업데이트 (순차 경로):** 단일 스텝 처리 시, 망각 게이트와 입력 게이트를 정규화하여($f'_t, i'_t$) 합이 1이 되도록 만들고, 이전 은닉 상태와 현재 후보 상태 사이의 선형 보간(linear interpolation)을 수행합니다: $h_t = h_{t-1} f'_{t} + \\tilde{h}_t i'_{t}$. 여기서 $\\tilde{h}_t$는 사용자 정의 활성화 함수 $g$를 사용하여 계산됩니다.\n",
    "* **병렬 스캔 공식 (병렬 경로):** 시퀀스 처리 시, 게이트 및 은닉 후보 값들을 로그 공간(`log_f`, `log_i`, `log_tilde_h`)으로 변환하고, 연관 스캔 연산(`heinsen_associative_scan_log`)을 사용하여 모든 은닉 상태를 병렬로 계산합니다. 이는 표준 RNN의 순차적 의존성을 우회합니다.\n",
    "* **활성화 함수 `g`:** 은닉 후보 계산에 사용자 정의 활성화 함수가 사용됩니다.\n",
    "\n",
    "### 2. 수식:\n",
    "\n",
    "결합된 투사를 위한 가중치 행렬을 $W_{hfi}$라고 가정합시다.\n",
    "$ [\\text{hidden}_t, \\text{f\\_gate}_t^{\\text{raw}}, \\text{i\\_gate}_t^{\\text{raw}}] = W_{hfi} x_t $\n",
    "\n",
    "* 은닉 후보: $\\tilde{h}_t = g(\\text{hidden}_t)$\n",
    "* 망각 게이트 (Sigmoid): $f_t = \\sigma(\\text{f\\_gate}_t^{\\text{raw}})$\n",
    "* 입력 게이트 (Sigmoid): $i_t = \\sigma(\\text{i\\_gate}_t^{\\text{raw}})$\n",
    "* 정규화된 망각 게이트: $f'_t = \\frac{f_t}{f_t + i_t}$\n",
    "* 정규화된 입력 게이트: $i'_t = \\frac{i_t}{f_t + i_t}$\n",
    "* 은닉 상태 업데이트: $h_t = h_{t-1} f'_{t} + \\tilde{h}_t i'_{t}$ ($h_{t-1}$ 존재 시, 없으면 $h_t = \\tilde{h}_t i'_{t}$)\n",
    "\n",
    "*(참고: 병렬 경로는 스캔을 위해 `softplus`와 로그 공간 연산을 포함하는 다른 공식을 사용합니다.)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1ab9ce",
   "metadata": {},
   "source": [
    "###  3. 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5bb7393",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, Identity, Module\n",
    "\n",
    "class minLSTM(Module):\n",
    "    def __init__(self, dim, expansion_factor = 1., proj_out = None):\n",
    "        super().__init__()\n",
    "        dim_inner = int(dim * expansion_factor) # 내부 차원 계산\n",
    "        proj_out = default(proj_out, expansion_factor != 1.) # 출력 투사 여부 결정\n",
    "\n",
    "        # 단일 선형 레이어가 입력 'x'를 hidden_candidate, raw_f_gate, raw_i_gate로 투사\n",
    "        self.to_hidden_and_f_i_gate = Linear(dim, dim_inner * 3, bias = False)\n",
    "        # 선택적 출력 투사 레이어\n",
    "        self.to_out = Linear(dim_inner, dim, bias = False) if proj_out else Identity()\n",
    "\n",
    "    def forward(self, x, prev_hidden = None, return_next_prev_hidden = False):\n",
    "        seq_len = x.shape[1] # 시퀀스 길이 얻기\n",
    "\n",
    "        # 입력 투사 후 세 부분으로 나누기\n",
    "        hidden, f_gate_raw, i_gate_raw = self.to_hidden_and_f_i_gate(x).chunk(3, dim = -1)\n",
    "\n",
    "        if seq_len == 1:\n",
    "            # --- 순차 경로 (단일 타임 스텝) ---\n",
    "            hidden_candidate = g(hidden)    # 사용자 정의 활성화 함수 g 적용\n",
    "            f_gate = f_gate_raw.sigmoid() # Sigmoid 적용하여 망각 게이트 얻기\n",
    "            i_gate = i_gate_raw.sigmoid() # Sigmoid 적용하여 입력 게이트 얻기\n",
    "\n",
    "            # 게이트 정규화 (합이 1이 되도록 보장, 분모 0 방지 위해 작은 epsilon 추가)\n",
    "            # 참고: f_gate + i_gate가 0이 될 수 있는 경우 안정성을 위해 작은 epsilon 필요\n",
    "            f_gate_prime = f_gate / (f_gate + i_gate + 1e-8)\n",
    "            i_gate_prime = i_gate / (f_gate + i_gate + 1e-8)\n",
    "\n",
    "            # 정규화된 게이트를 사용한 선형 보간으로 은닉 상태 업데이트\n",
    "            if exists(prev_hidden):\n",
    "                out = (prev_hidden * f_gate_prime) + (hidden_candidate * i_gate_prime)\n",
    "            else: # 초기 스텝 처리\n",
    "                out = hidden_candidate * i_gate_prime\n",
    "        else:\n",
    "            # --- 병렬 경로 (스캔을 사용한 시퀀스 처리) ---\n",
    "            # 망각 및 입력 영향에 대한 로그 공간 구성 요소 계산\n",
    "            # diff = softplus(-f) - softplus(-i) = log(1+exp(-f)) - log(1+exp(-i)) = log((1+exp(-f))/(1+exp(-i)))\n",
    "            diff = F.softplus(-f_gate_raw) - F.softplus(-i_gate_raw)\n",
    "            # log_f = -softplus(diff) 는 안정적인 방식으로 log(f / (f+i))에 해당\n",
    "            log_f = -F.softplus(diff)\n",
    "            # log_i = -softplus(-diff) 는 안정적인 방식으로 log(i / (f+i))에 해당\n",
    "            log_i = -F.softplus(-diff)\n",
    "\n",
    "            # 사용자 정의 log_g를 사용하여 후보 은닉 상태의 로그 계산\n",
    "            log_tilde_h = log_g(hidden)\n",
    "            # 로그 입력 영향과 로그 후보 상태 결합\n",
    "            log_values = log_i + log_tilde_h\n",
    "\n",
    "            # 이전 은닉 상태가 제공된 경우 통합 (시퀀스 청킹/추론용)\n",
    "            if exists(prev_hidden):\n",
    "                # 일관성을 위해 log_g 사용? 논문/코드 맥락 필요.\n",
    "                # minGRU 코드 구조상 단순 log 사용이 암시된 듯\n",
    "                log_h_0 = prev_hidden.log() # prev_hidden이 양수라고 가정\n",
    "                log_values = torch.cat((log_h_0, log_values), dim = 1)\n",
    "                # 스캔을 위해 log_f 차원 정렬용 패딩\n",
    "                log_f = F.pad(log_f, (0, 0, 1, 0), value=0.) # 시간 차원(dim 1) 패딩\n",
    "\n",
    "            # 로그 공간에서 병렬 연관 스캔 적용\n",
    "            # exp(scan_output) 반환, 양수 은닉 상태 제공\n",
    "            out = heinsen_associative_scan_log(log_f, log_values)\n",
    "\n",
    "            # prev_hidden이 앞에 추가된 경우 첫 번째 출력 스텝 제거\n",
    "            if exists(prev_hidden):\n",
    "                out = out[:, 1:] # 패딩이 달랐다면 조정 필요\n",
    "            # 패딩 사용 시 원본 시퀀스 길이와 출력 길이 일치 확인 (필요시 재검토)\n",
    "            # 원본 코드의 out = out[:, -seq_len:] 줄은 길이 변경 없다고 가정? 로직 재검토.\n",
    "            # 여기서는 스캔이 입력 shape 기반으로 길이를 올바르게 처리한다고 가정.\n",
    "\n",
    "        # 다음 호출을 위해 마지막 은닉 상태 저장\n",
    "        next_prev_hidden = out[:, -1:]\n",
    "\n",
    "        # 선택적 최종 투사 적용\n",
    "        out = self.to_out(out)\n",
    "\n",
    "        if not return_next_prev_hidden:\n",
    "            return out\n",
    "        return out, next_prev_hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238f50c8",
   "metadata": {},
   "source": [
    "## minGRU (Minimal GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ffa014",
   "metadata": {},
   "source": [
    "minGRU는 매우 단순화된 GRU로, 단일 게이트(업데이트 게이트와 유사)를 사용하고 병렬 스캔 메커니즘에 의존.  \n",
    "표준 GRU의 리셋 게이트 기능은 생략되거나 단순화\n",
    "\n",
    "### 1. 핵심 개념:\n",
    "\n",
    "* **단일 게이트:** 단일 선형 투사로부터 은닉 후보($\\tilde{h}_t$)와 함께 단 하나의 게이트 값(`gate`)만 명시적으로 계산합니다.\n",
    "* **업데이트 게이트 역할:** 이 단일 `gate`는 주로 표준 GRU의 업데이트 게이트($z_t$)처럼 작동하여 이전 상태와 후보 상태 간의 보간을 제어합니다.\n",
    "* **단순화된 리셋:** 리셋 게이트($r_t$)는 없거나 암시적으로 처리되는 것으로 보입니다 (예: 효과적으로 $r_t=1$). 후보 $\\tilde{h}_t$는 $h_{t-1}$ 기반의 리셋 게이트에 의한 명시적 조절 없이 $g(\\text{hidden}_t)$를 사용하여 직접 계산됩니다.\n",
    "* **선형 보간 (순차 경로):** 업데이트는 직접적인 선형 보간(lerp)입니다: $h_t = (1 - z_t) h_{t-1} + z_t \\tilde{h}_t$, 여기서 $z_t = \\sigma(\\text{gate}_t)$.\n",
    "* **병렬 스캔 공식 (병렬 경로):** minLSTM과 유사하게 로그 공간 계산(`log_coeffs`, `log_z`, `log_tilde_h`)과 연관 스캔(`heinsen_associative_scan_log`)을 사용하여 병렬 시퀀스 처리를 수행합니다.\n",
    "* **양수 은닉 상태:** 코드 주석에서 양수 은닉 상태를 강제한다고 언급하며, 이는 `exp()`를 출력하는 로그 공간 스캔을 통해 달성될 가능성이 높습니다.\n",
    "\n",
    "### 2. 수식:\n",
    "\n",
    "결합된 투사를 위한 가중치 행렬을 $W_{hg}$라고 가정합시다.\n",
    "$ [\\text{hidden}_t, \\text{gate}_t^{\\text{raw}}] = W_{hg} x_t $\n",
    "\n",
    "* 은닉 후보: $\\tilde{h}_t = g(\\text{hidden}_t)$\n",
    "* 업데이트 게이트 (Sigmoid): $z_t = \\sigma(\\text{gate}_t^{\\text{raw}})$\n",
    "* 은닉 상태 업데이트: $h_t = (1 - z_t) h_{t-1} + z_t \\tilde{h}_t$ ($h_{t-1}$ 존재 시, 없으면 $h_t = z_t \\tilde{h}_t$)\n",
    "\n",
    "*(참고: 병렬 경로는 스캔을 위해 로그 공간 공식을 사용합니다.)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf36105",
   "metadata": {},
   "source": [
    "### 3. 코드:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "575c2445",
   "metadata": {},
   "outputs": [],
   "source": [
    "class minGRU(Module):\n",
    "    def __init__(self, dim, expansion_factor = 1., proj_out = None):\n",
    "        super().__init__()\n",
    "        dim_inner = int(dim * expansion_factor) # 내부 차원 계산\n",
    "        proj_out = default(proj_out, expansion_factor != 1.) # 출력 투사 여부 결정\n",
    "\n",
    "        # 단일 선형 레이어가 입력 'x'를 hidden_candidate, raw_gate로 투사\n",
    "        self.to_hidden_and_gate = Linear(dim, dim_inner * 2, bias = False)\n",
    "        # 선택적 출력 투사 레이어\n",
    "        self.to_out = Linear(dim_inner, dim, bias = False) if proj_out else Identity()\n",
    "\n",
    "    def forward(self, x, prev_hidden = None, return_next_prev_hidden = False):\n",
    "        seq_len = x.shape[1] # 시퀀스 길이 얻기\n",
    "\n",
    "        # 입력 투사 후 두 부분으로 나누기\n",
    "        hidden, gate_raw = self.to_hidden_and_gate(x).chunk(2, dim = -1)\n",
    "\n",
    "        if seq_len == 1:\n",
    "            # --- 순차 경로 (단일 타임 스텝) ---\n",
    "            hidden_candidate = g(hidden) # 사용자 정의 활성화 함수 g 적용\n",
    "            gate = gate_raw.sigmoid()    # Sigmoid 적용하여 업데이트 게이트 (z_t) 얻기\n",
    "\n",
    "            # 선형 보간(lerp)을 사용하여 은닉 상태 업데이트\n",
    "            if exists(prev_hidden):\n",
    "                # out = (1 - gate) * prev_hidden + gate * hidden_candidate\n",
    "                out = torch.lerp(prev_hidden, hidden_candidate, gate) # lerp 사용\n",
    "            else: # 초기 스텝 처리\n",
    "                out = hidden_candidate * gate\n",
    "        else:\n",
    "            # --- 병렬 경로 (스캔을 사용한 시퀀스 처리) ---\n",
    "            # log_coeffs = -softplus(gate) 는 log(1-sigmoid(gate)) = log(1 - z_t)에 해당\n",
    "            log_coeffs = -F.softplus(gate_raw) # (1 - z_t)의 로그\n",
    "\n",
    "            # log_z = -softplus(-gate) 는 log(sigmoid(gate)) = log(z_t)에 해당\n",
    "            log_z = -F.softplus(-gate_raw)    # z_t의 로그\n",
    "            # 후보 은닉 상태의 로그 계산\n",
    "            log_tilde_h = log_g(hidden)\n",
    "            # 로그 업데이트 영향과 로그 후보 상태 결합 = log(z_t * h_tilde_t)\n",
    "            log_values = log_z + log_tilde_h\n",
    "\n",
    "            # 이전 은닉 상태가 제공된 경우 통합\n",
    "            if exists(prev_hidden):\n",
    "                 # prev_hidden이 양수라고 가정 (이전 스텝에서 강제됨)\n",
    "                log_h_0 = prev_hidden.log()\n",
    "                log_values = torch.cat((log_h_0, log_values), dim = 1)\n",
    "                 # 스캔을 위해 log_coeffs 차원 정렬용 패딩\n",
    "                log_coeffs = F.pad(log_coeffs, (0, 0, 1, 0), value=0.) # 시간 차원 패딩\n",
    "\n",
    "            # 로그 공간에서 병렬 연관 스캔 적용\n",
    "            # exp(scan_output) 반환, 양수 은닉 상태 강제\n",
    "            out = heinsen_associative_scan_log(log_coeffs, log_values)\n",
    "\n",
    "            # prev_hidden이 앞에 추가된 경우 출력 조정\n",
    "            if exists(prev_hidden):\n",
    "                 out = out[:, 1:] # 스캔 출력이 h_0의 영향을 포함한다고 가정\n",
    "            # 올바른 길이 보장 (필요시 재검토)\n",
    "            # out = out[:, -seq_len:]\n",
    "\n",
    "        # 마지막 은닉 상태 저장\n",
    "        next_prev_hidden = out[:, -1:]\n",
    "\n",
    "        # 선택적 최종 투사 적용\n",
    "        out = self.to_out(out)\n",
    "\n",
    "        if not return_next_prev_hidden:\n",
    "            return out\n",
    "        return out, next_prev_hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b0fb81",
   "metadata": {},
   "source": [
    "## 비교 및 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d157ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 디바이스: cuda\n",
      "\n",
      "--- 파라미터 수 비교 (샘플 테스트 모델 기준) ---\n",
      "LSTM 직접 구현 레이어 : 5,504\n",
      "GRU 직접 구현 레이어  : 4,128\n",
      "minLSTM (proj_out=True) : 1,280\n",
      "minGRU  (proj_out=True) : 960\n",
      "\n",
      "--- 샘플 데이터 통과 테스트 ---\n",
      "LSTM Scratch 출력 Shape: torch.Size([64, 20, 32])\n",
      "GRU Scratch 출력 Shape : torch.Size([64, 20, 32])\n",
      "minLSTM 출력 Shape (proj_out=True) : torch.Size([64, 20, 10])\n",
      "minGRU 출력 Shape  (proj_out=True) : torch.Size([64, 20, 10])\n",
      "\n",
      "--- 간단한 학습 태스크 (패리티 예측) ---\n",
      "\n",
      "LSTM_Scratch 학습 시작...\n",
      "에폭 [5/10], 손실: 0.6933, 정확도: 0.5034\n",
      "에폭 [10/10], 손실: 0.6934, 정확도: 0.4930\n",
      "LSTM_Scratch - 최종 평균 손실: 0.6933, 최종 평균 정확도: 0.5005, 소요 시간: 21.98s\n",
      "\n",
      "GRU_Scratch 학습 시작...\n",
      "에폭 [5/10], 손실: 0.6931, 정확도: 0.5083\n",
      "에폭 [10/10], 손실: 0.6933, 정확도: 0.5014\n",
      "GRU_Scratch - 최종 평균 손실: 0.6934, 최종 평균 정확도: 0.4998, 소요 시간: 19.82s\n",
      "\n",
      "minLSTM 학습 시작...\n",
      "에폭 [5/10], 손실: 0.6950, 정확도: 0.4972\n",
      "에폭 [10/10], 손실: 0.6946, 정확도: 0.5030\n",
      "minLSTM - 최종 평균 손실: 0.6945, 최종 평균 정확도: 0.5023, 소요 시간: 2.67s\n",
      "\n",
      "minGRU 학습 시작...\n",
      "에폭 [5/10], 손실: 0.6937, 정확도: 0.5069\n",
      "에폭 [10/10], 손실: 0.6961, 정확도: 0.4997\n",
      "minGRU - 최종 평균 손실: 0.6950, 최종 평균 정확도: 0.5033, 소요 시간: 2.43s\n",
      "\n",
      "--- 학습 결과 요약 ---\n",
      "모델명           | 손실   | 정확도 | 시간(s)\n",
      "-----------------|--------|--------|--------\n",
      "LSTM_Scratch    | 0.6933 | 0.5005 | 21.98  \n",
      "GRU_Scratch     | 0.6934 | 0.4998 | 19.82  \n",
      "minLSTM         | 0.6945 | 0.5023 | 2.67   \n",
      "minGRU          | 0.6950 | 0.5033 | 2.43   \n"
     ]
    }
   ],
   "source": [
    "# --- 이전 직접 구현 Cell (이전과 동일) ---\n",
    "class LSTMCellScratch(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        concat_size = input_size + hidden_size\n",
    "        self.Wf = nn.Parameter(torch.Tensor(concat_size, hidden_size))\n",
    "        self.bf = nn.Parameter(torch.Tensor(hidden_size))\n",
    "        self.Wi = nn.Parameter(torch.Tensor(concat_size, hidden_size))\n",
    "        self.bi = nn.Parameter(torch.Tensor(hidden_size))\n",
    "        self.Wc = nn.Parameter(torch.Tensor(concat_size, hidden_size))\n",
    "        self.bc = nn.Parameter(torch.Tensor(hidden_size))\n",
    "        self.Wo = nn.Parameter(torch.Tensor(concat_size, hidden_size))\n",
    "        self.bo = nn.Parameter(torch.Tensor(hidden_size))\n",
    "        self.reset_parameters()\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "             nn.init.uniform_(weight, -stdv, stdv)\n",
    "    def forward(self, xt, prev_state):\n",
    "        prev_h, prev_c = prev_state\n",
    "        combined = torch.cat((prev_h, xt), dim=1)\n",
    "        ft = torch.sigmoid(torch.matmul(combined, self.Wf) + self.bf)\n",
    "        it = torch.sigmoid(torch.matmul(combined, self.Wi) + self.bi)\n",
    "        c_tilde_t = torch.tanh(torch.matmul(combined, self.Wc) + self.bc)\n",
    "        ct = (ft * prev_c) + (it * c_tilde_t)\n",
    "        ot = torch.sigmoid(torch.matmul(combined, self.Wo) + self.bo)\n",
    "        ht = ot * torch.tanh(ct)\n",
    "        return ht, ct\n",
    "\n",
    "class GRUCellScratch(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        concat_size = input_size + hidden_size\n",
    "        self.Wr = nn.Parameter(torch.Tensor(concat_size, hidden_size))\n",
    "        self.br = nn.Parameter(torch.Tensor(hidden_size))\n",
    "        self.Wz = nn.Parameter(torch.Tensor(concat_size, hidden_size))\n",
    "        self.bz = nn.Parameter(torch.Tensor(hidden_size))\n",
    "        self.Wh = nn.Parameter(torch.Tensor(concat_size, hidden_size))\n",
    "        self.bh = nn.Parameter(torch.Tensor(hidden_size))\n",
    "        self.reset_parameters()\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "             nn.init.uniform_(weight, -stdv, stdv)\n",
    "    def forward(self, xt, prev_h):\n",
    "        combined = torch.cat((prev_h, xt), dim=1)\n",
    "        rt = torch.sigmoid(torch.matmul(combined, self.Wr) + self.br)\n",
    "        zt = torch.sigmoid(torch.matmul(combined, self.Wz) + self.bz)\n",
    "        combined_reset = torch.cat((rt * prev_h, xt), dim=1)\n",
    "        h_tilde_t = torch.tanh(torch.matmul(combined_reset, self.Wh) + self.bh)\n",
    "        ht = (1 - zt) * prev_h + zt * h_tilde_t\n",
    "        return ht\n",
    "\n",
    "\n",
    "class SimpleRNNLayer(nn.Module):\n",
    "    def __init__(self, cell, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.cell = cell(input_size, hidden_size)\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.is_lstm = isinstance(self.cell, LSTMCellScratch)\n",
    "    def forward(self, x, initial_state=None):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        device = x.device\n",
    "        if initial_state is None:\n",
    "            h = torch.zeros(batch_size, self.hidden_size, device=device)\n",
    "            c = torch.zeros(batch_size, self.hidden_size, device=device) if self.is_lstm else None\n",
    "        else:\n",
    "            if self.is_lstm: h, c = initial_state\n",
    "            else: h = initial_state; c = None\n",
    "        outputs = []\n",
    "        for t in range(seq_len):\n",
    "            xt = x[:, t, :]\n",
    "            if self.is_lstm:\n",
    "                h, c = self.cell(xt, (h, c))\n",
    "                outputs.append(h.unsqueeze(1))\n",
    "            else:\n",
    "                h = self.cell(xt, h)\n",
    "                outputs.append(h.unsqueeze(1))\n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "        final_state = (h, c) if self.is_lstm else h\n",
    "        return outputs, final_state\n",
    "\n",
    "\n",
    "class minLSTM(Module):\n",
    "    def __init__(self, dim, expansion_factor = 1., proj_out = None):\n",
    "        super().__init__()\n",
    "        dim_inner = int(dim * expansion_factor)\n",
    "        # *** proj_out 기본값 결정 수정 (명시적으로 전달받도록) ***\n",
    "        # proj_out = default(proj_out, expansion_factor != 1.) -> 아래처럼 변경\n",
    "        self.proj_out_active = default(proj_out, expansion_factor != 1.)\n",
    "\n",
    "        self.to_hidden_and_f_i_gate = Linear(dim, dim_inner * 3, bias = False)\n",
    "        # *** proj_out_active 플래그 사용 ***\n",
    "        self.to_out = Linear(dim_inner, dim, bias = False) if self.proj_out_active else Identity()\n",
    "\n",
    "    def forward(self, x, prev_hidden = None, return_next_prev_hidden = False):\n",
    "        seq_len = x.shape[1]\n",
    "        hidden, f_gate_raw, i_gate_raw = self.to_hidden_and_f_i_gate(x).chunk(3, dim = -1)\n",
    "\n",
    "        if seq_len == 1:\n",
    "            # --- 순차 경로 (단일 타임 스텝) ---\n",
    "            hidden_candidate = hidden # g() 제거됨\n",
    "            f_gate = f_gate_raw.sigmoid()\n",
    "            i_gate = i_gate_raw.sigmoid()\n",
    "            f_gate_prime = f_gate / (f_gate + i_gate + 1e-8)\n",
    "            i_gate_prime = i_gate / (f_gate + i_gate + 1e-8)\n",
    "            if exists(prev_hidden):\n",
    "                out = (prev_hidden * f_gate_prime) + (hidden_candidate * i_gate_prime)\n",
    "            else:\n",
    "                out = hidden_candidate * i_gate_prime\n",
    "        else:\n",
    "            # --- 병렬 경로 (스캔) ---\n",
    "            diff = F.softplus(-f_gate_raw) - F.softplus(-i_gate_raw)\n",
    "            log_f = -F.softplus(diff)\n",
    "            log_i = -F.softplus(-diff)\n",
    "            log_tilde_h = log_g(hidden) # 병렬 경로는 log_g 유지 (주의사항 참고)\n",
    "            log_values = log_i + log_tilde_h\n",
    "            if exists(prev_hidden):\n",
    "                try: log_h_0 = prev_hidden.log()\n",
    "                except Exception as e: log_h_0 = torch.zeros_like(log_values[:, 0:1, :])\n",
    "                log_values = torch.cat((log_h_0, log_values), dim = 1)\n",
    "                log_f = F.pad(log_f, (0, 0, 1, 0), value=0.)\n",
    "            out = heinsen_associative_scan_log(log_f, log_values)\n",
    "            if exists(prev_hidden):\n",
    "                 out = out[:, 1:]\n",
    "\n",
    "        next_prev_hidden = out[:, -1:]\n",
    "        # *** 최종 출력 투사 적용 ***\n",
    "        out = self.to_out(out)\n",
    "\n",
    "        if not return_next_prev_hidden:\n",
    "            return out\n",
    "        return out, next_prev_hidden\n",
    "\n",
    "\n",
    "class minGRU(Module):\n",
    "    def __init__(self, dim, expansion_factor = 1., proj_out = None):\n",
    "        super().__init__()\n",
    "        dim_inner = int(dim * expansion_factor)\n",
    "        # *** proj_out 기본값 결정 수정 (명시적으로 전달받도록) ***\n",
    "        self.proj_out_active = default(proj_out, expansion_factor != 1.)\n",
    "\n",
    "        self.to_hidden_and_gate = Linear(dim, dim_inner * 2, bias = False)\n",
    "        # *** proj_out_active 플래그 사용 ***\n",
    "        self.to_out = Linear(dim_inner, dim, bias = False) if self.proj_out_active else Identity()\n",
    "\n",
    "    def forward(self, x, prev_hidden = None, return_next_prev_hidden = False):\n",
    "        seq_len = x.shape[1]\n",
    "        hidden, gate_raw = self.to_hidden_and_gate(x).chunk(2, dim = -1)\n",
    "\n",
    "        if seq_len == 1:\n",
    "            # --- 순차 경로 (단일 타임 스텝) ---\n",
    "            hidden_candidate = hidden # g() 제거됨\n",
    "            gate = gate_raw.sigmoid() # z_t\n",
    "            if exists(prev_hidden):\n",
    "                out = torch.lerp(prev_hidden, hidden_candidate, gate)\n",
    "            else:\n",
    "                out = hidden_candidate * gate\n",
    "        else:\n",
    "            # --- 병렬 경로 (스캔) ---\n",
    "            log_coeffs = -F.softplus(gate_raw) # log(1 - z_t)\n",
    "            log_z = -F.softplus(-gate_raw)    # log(z_t)\n",
    "            log_tilde_h = log_g(hidden) # 병렬 경로는 log_g 유지 (주의사항 참고)\n",
    "            log_values = log_z + log_tilde_h\n",
    "            if exists(prev_hidden):\n",
    "                try: log_h_0 = prev_hidden.log()\n",
    "                except Exception as e: log_h_0 = torch.zeros_like(log_values[:, 0:1, :])\n",
    "                log_values = torch.cat((log_h_0, log_values), dim = 1)\n",
    "                log_coeffs = F.pad(log_coeffs, (0, 0, 1, 0), value=0.)\n",
    "            out = heinsen_associative_scan_log(log_coeffs, log_values)\n",
    "            if exists(prev_hidden):\n",
    "                 out = out[:, 1:]\n",
    "\n",
    "        next_prev_hidden = out[:, -1:]\n",
    "        # *** 최종 출력 투사 적용 ***\n",
    "        out = self.to_out(out)\n",
    "\n",
    "        if not return_next_prev_hidden:\n",
    "            return out\n",
    "        return out, next_prev_hidden\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "input_dim = 10\n",
    "hidden_dim = 32\n",
    "seq_len = 20\n",
    "batch_size = 64\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"사용 디바이스: {device}\")\n",
    "lstm_scratch_layer = SimpleRNNLayer(LSTMCellScratch, input_dim, hidden_dim).to(device)\n",
    "gru_scratch_layer = SimpleRNNLayer(GRUCellScratch, input_dim, hidden_dim).to(device)\n",
    "# 샘플 테스트용 min 모델 생성 시 proj_out 기본값 사용 (True가 됨)\n",
    "min_lstm_model_sample = minLSTM(input_dim, expansion_factor=hidden_dim/input_dim).to(device)\n",
    "min_gru_model_sample = minGRU(input_dim, expansion_factor=hidden_dim/input_dim).to(device)\n",
    "print(\"\\n--- 파라미터 수 비교 (샘플 테스트 모델 기준) ---\")\n",
    "print(f\"LSTM 직접 구현 레이어 : {count_parameters(lstm_scratch_layer):,}\")\n",
    "print(f\"GRU 직접 구현 레이어  : {count_parameters(gru_scratch_layer):,}\")\n",
    "print(f\"minLSTM (proj_out=True) : {count_parameters(min_lstm_model_sample):,}\") # 출력 투사 포함\n",
    "print(f\"minGRU  (proj_out=True) : {count_parameters(min_gru_model_sample):,}\") # 출력 투사 포함\n",
    "\n",
    "print(\"\\n--- 샘플 데이터 통과 테스트 ---\")\n",
    "x_sample = torch.randn(batch_size, seq_len, input_dim).to(device)\n",
    "out_lstm_scratch, state_lstm_scratch = lstm_scratch_layer(x_sample)\n",
    "out_gru_scratch, state_gru_scratch = gru_scratch_layer(x_sample)\n",
    "# 샘플 테스트에서는 proj_out=True인 모델 사용\n",
    "out_min_lstm = min_lstm_model_sample(x_sample)\n",
    "out_min_gru = min_gru_model_sample(x_sample)\n",
    "print(f\"LSTM Scratch 출력 Shape: {out_lstm_scratch.shape}\")\n",
    "print(f\"GRU Scratch 출력 Shape : {out_gru_scratch.shape}\")\n",
    "print(f\"minLSTM 출력 Shape (proj_out=True) : {out_min_lstm.shape}\") # (..., input_dim)\n",
    "print(f\"minGRU 출력 Shape  (proj_out=True) : {out_min_gru.shape}\") # (..., input_dim)\n",
    "\n",
    "\n",
    "print(\"\\n--- 간단한 학습 태스크 (패리티 예측) ---\")\n",
    "def generate_parity_data(batch_size, seq_len, device):\n",
    "    data = torch.randint(0, 2, (batch_size, seq_len, 1), dtype=torch.float32).to(device)\n",
    "    labels = (data.sum(dim=1) % 2).float().to(device)\n",
    "    return data, labels\n",
    "\n",
    "class ParityClassifier(nn.Module):\n",
    "    def __init__(self, rnn_layer, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.rnn = rnn_layer\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "    def forward(self, x):\n",
    "        if isinstance(self.rnn, SimpleRNNLayer):\n",
    "             outputs, final_state = self.rnn(x)\n",
    "             final_hidden = final_state[0] if self.rnn.is_lstm else final_state\n",
    "        else:\n",
    "             # 학습 시에는 proj_out=False인 모델이 전달될 것임\n",
    "             outputs = self.rnn(x, prev_hidden=None)\n",
    "             # 이 경우 outputs의 마지막 차원은 hidden_dim이 됨\n",
    "             final_hidden = outputs[:, -1, :]\n",
    "        return self.fc(final_hidden)\n",
    "\n",
    "# 학습 파라미터 설정\n",
    "learning_rate = 0.005\n",
    "num_epochs = 10 # 에폭 수 확인/조정\n",
    "train_steps_per_epoch = 100\n",
    "parity_input_dim = 1\n",
    "hidden_dim = 32\n",
    "\n",
    "# 패리티 예측용 모델 인스턴스 생성 (*** 수정됨: proj_out=False 명시 ***)\n",
    "lstm_parity = ParityClassifier(SimpleRNNLayer(LSTMCellScratch, parity_input_dim, hidden_dim), hidden_dim).to(device)\n",
    "gru_parity = ParityClassifier(SimpleRNNLayer(GRUCellScratch, parity_input_dim, hidden_dim), hidden_dim).to(device)\n",
    "min_lstm_parity = ParityClassifier(minLSTM(parity_input_dim, expansion_factor=hidden_dim/parity_input_dim, proj_out=False), hidden_dim).to(device)\n",
    "min_gru_parity = ParityClassifier(minGRU(parity_input_dim, expansion_factor=hidden_dim/parity_input_dim, proj_out=False), hidden_dim).to(device)\n",
    "\n",
    "# 학습시킬 모델 딕셔너리\n",
    "models = {\n",
    "    \"LSTM_Scratch\": lstm_parity,\n",
    "    \"GRU_Scratch\": gru_parity,\n",
    "    \"minLSTM\": min_lstm_parity,\n",
    "    \"minGRU\": min_gru_parity\n",
    "}\n",
    "\n",
    "# 손실 함수 정의\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "results = {}\n",
    "\n",
    "# 학습 루프 (내용 동일)\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{name} 학습 시작...\")\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        epoch_correct = 0\n",
    "        epoch_samples = 0\n",
    "        for _ in range(train_steps_per_epoch):\n",
    "            optimizer.zero_grad()\n",
    "            data, labels = generate_parity_data(batch_size, seq_len, device)\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) # 필요시 클리핑\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * data.size(0)\n",
    "            preds = (outputs > 0).float()\n",
    "            epoch_correct += (preds == labels).sum().item()\n",
    "            epoch_samples += data.size(0)\n",
    "        avg_epoch_loss = epoch_loss / epoch_samples\n",
    "        avg_epoch_acc = epoch_correct / epoch_samples\n",
    "        if (epoch + 1) % 5 == 0 or epoch == num_epochs - 1: # 진행 및 마지막 결과 출력\n",
    "            print(f\"에폭 [{epoch+1}/{num_epochs}], 손실: {avg_epoch_loss:.4f}, 정확도: {avg_epoch_acc:.4f}\")\n",
    "        total_loss += epoch_loss\n",
    "        total_correct += epoch_correct\n",
    "        total_samples += epoch_samples\n",
    "    end_time = time.time()\n",
    "    avg_loss = total_loss / total_samples\n",
    "    avg_acc = total_correct / total_samples\n",
    "    duration = end_time - start_time\n",
    "    results[name] = {\"loss\": avg_loss, \"accuracy\": avg_acc, \"time\": duration}\n",
    "    print(f\"{name} - 최종 평균 손실: {avg_loss:.4f}, 최종 평균 정확도: {avg_acc:.4f}, 소요 시간: {duration:.2f}s\")\n",
    "\n",
    "# --- 최종 결과 비교 ---\n",
    "print(\"\\n--- 학습 결과 요약 ---\")\n",
    "print(\"모델명           | 손실   | 정확도 | 시간(s)\")\n",
    "print(\"-----------------|--------|--------|--------\")\n",
    "for name, res in results.items():\n",
    "    print(f\"{name:<15} | {res['loss']:.4f} | {res['accuracy']:.4f} | {res['time']:<7.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09da7c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- AG_NEWS 데이터 로딩 및 전처리 (수동 토크나이저/단어집합) ---\n",
      "사용 디바이스: cuda\n",
      "AG_NEWS 데이터셋 로드 완료.\n",
      "기본 토크나이저 (str.lower().split()) 사용\n",
      "단어 집합 구축 시작...\n",
      "단어 집합 크기 (min_freq=5): 39546\n",
      "Unk 인덱스: 0, Pad 인덱스: 1\n",
      "단어 집합 구축 완료.\n",
      "데이터셋 전처리 적용 중...\n",
      "데이터셋 전처리 완료.\n",
      "데이터 로더 생성 완료.\n",
      "\n",
      "--- 모델 구조 시각화 (minLSTM 예시) ---\n",
      "\n",
      "--- 텍스트 분류 학습 및 평가 ---\n",
      "\n",
      "===== LSTM_Scratch 학습 시작 =====\n",
      "--- 에폭 1/20 ---\n",
      "에폭 [1/20] 결과 | Train 손실: 1.3857 | Train 정확도: 0.2513 | Val 손실: 1.3856 | Val 정확도: 0.2514\n",
      "--- 에폭 2/20 ---\n",
      "에폭 [2/20] 결과 | Train 손실: 1.3853 | Train 정확도: 0.2521 | Val 손실: 1.3855 | Val 정확도: 0.2499\n",
      "--- 에폭 3/20 ---\n",
      "에폭 [3/20] 결과 | Train 손실: 1.3850 | Train 정확도: 0.2520 | Val 손실: 1.3855 | Val 정확도: 0.2514\n",
      "--- 에폭 4/20 ---\n",
      "에폭 [4/20] 결과 | Train 손실: 1.3874 | Train 정확도: 0.2532 | Val 손실: 1.3856 | Val 정확도: 0.2509\n",
      "--- 에폭 5/20 ---\n",
      "에폭 [5/20] 결과 | Train 손실: 1.3850 | Train 정확도: 0.2514 | Val 손실: 1.3853 | Val 정확도: 0.2513\n",
      "--- 에폭 6/20 ---\n",
      "에폭 [6/20] 결과 | Train 손실: 1.3855 | Train 정확도: 0.2507 | Val 손실: 1.3856 | Val 정확도: 0.2496\n",
      "--- 에폭 7/20 ---\n",
      "에폭 [7/20] 결과 | Train 손실: 1.3846 | Train 정확도: 0.2519 | Val 손실: 1.3854 | Val 정확도: 0.2514\n",
      "--- 에폭 8/20 ---\n",
      "에폭 [8/20] 결과 | Train 손실: 1.3845 | Train 정확도: 0.2511 | Val 손실: 1.3856 | Val 정확도: 0.2513\n",
      "--- 에폭 9/20 ---\n",
      "에폭 [9/20] 결과 | Train 손실: 1.3862 | Train 정확도: 0.2513 | Val 손실: 1.3859 | Val 정확도: 0.2511\n",
      "--- 에폭 10/20 ---\n",
      "에폭 [10/20] 결과 | Train 손실: 1.3847 | Train 정확도: 0.2507 | Val 손실: 1.3853 | Val 정확도: 0.2500\n",
      "--- 에폭 11/20 ---\n",
      "에폭 [11/20] 결과 | Train 손실: 1.3844 | Train 정확도: 0.2499 | Val 손실: 1.3854 | Val 정확도: 0.2497\n",
      "--- 에폭 12/20 ---\n",
      "에폭 [12/20] 결과 | Train 손실: 1.3840 | Train 정확도: 0.2520 | Val 손실: 1.3853 | Val 정확도: 0.2511\n",
      "--- 에폭 13/20 ---\n",
      "에폭 [13/20] 결과 | Train 손실: 1.3832 | Train 정확도: 0.2560 | Val 손실: 1.3852 | Val 정확도: 0.2508\n",
      "--- 에폭 14/20 ---\n",
      "에폭 [14/20] 결과 | Train 손실: 1.3748 | Train 정확도: 0.2799 | Val 손실: 1.3555 | Val 정확도: 0.2947\n",
      "--- 에폭 15/20 ---\n",
      "에폭 [15/20] 결과 | Train 손실: 1.2162 | Train 정확도: 0.3891 | Val 손실: 1.0684 | Val 정확도: 0.4457\n",
      "--- 에폭 16/20 ---\n",
      "에폭 [16/20] 결과 | Train 손실: 1.0228 | Train 정확도: 0.4822 | Val 손실: 0.9508 | Val 정확도: 0.5704\n",
      "--- 에폭 17/20 ---\n",
      "에폭 [17/20] 결과 | Train 손실: 0.7661 | Train 정확도: 0.6473 | Val 손실: 0.7220 | Val 정확도: 0.6718\n",
      "--- 에폭 18/20 ---\n",
      "에폭 [18/20] 결과 | Train 손실: 0.6423 | Train 정확도: 0.7104 | Val 손실: 0.6339 | Val 정확도: 0.7422\n",
      "--- 에폭 19/20 ---\n",
      "에폭 [19/20] 결과 | Train 손실: 0.4411 | Train 정확도: 0.8431 | Val 손실: 0.4042 | Val 정확도: 0.8661\n",
      "--- 에폭 20/20 ---\n",
      "에폭 [20/20] 결과 | Train 손실: 0.2903 | Train 정확도: 0.9105 | Val 손실: 0.3491 | Val 정확도: 0.8849\n",
      "===== LSTM_Scratch 학습 완료 =====\n",
      "LSTM_Scratch - 최종 검증 손실: 0.3491, 최종 검증 정확도: 0.8849, 총 소요 시간: 886.21s\n",
      "\n",
      "===== GRU_Scratch 학습 시작 =====\n",
      "--- 에폭 1/20 ---\n",
      "에폭 [1/20] 결과 | Train 손실: 1.3856 | Train 정확도: 0.2520 | Val 손실: 1.3851 | Val 정확도: 0.2500\n",
      "--- 에폭 2/20 ---\n",
      "에폭 [2/20] 결과 | Train 손실: 1.3852 | Train 정확도: 0.2523 | Val 손실: 1.3856 | Val 정확도: 0.2514\n",
      "--- 에폭 3/20 ---\n",
      "에폭 [3/20] 결과 | Train 손실: 1.3847 | Train 정확도: 0.2517 | Val 손실: 1.3854 | Val 정확도: 0.2513\n",
      "--- 에폭 4/20 ---\n",
      "에폭 [4/20] 결과 | Train 손실: 1.2669 | Train 정확도: 0.3711 | Val 손실: 0.9561 | Val 정확도: 0.5795\n",
      "--- 에폭 5/20 ---\n",
      "에폭 [5/20] 결과 | Train 손실: 0.6261 | Train 정확도: 0.7514 | Val 손실: 0.4285 | Val 정확도: 0.8536\n",
      "--- 에폭 6/20 ---\n",
      "에폭 [6/20] 결과 | Train 손실: 0.3368 | Train 정확도: 0.8897 | Val 손실: 0.3488 | Val 정확도: 0.8839\n",
      "--- 에폭 7/20 ---\n",
      "에폭 [7/20] 결과 | Train 손실: 0.2644 | Train 정확도: 0.9152 | Val 손실: 0.3161 | Val 정확도: 0.8921\n",
      "--- 에폭 8/20 ---\n",
      "에폭 [8/20] 결과 | Train 손실: 0.2208 | Train 정확도: 0.9298 | Val 손실: 0.3212 | Val 정확도: 0.8967\n",
      "--- 에폭 9/20 ---\n",
      "에폭 [9/20] 결과 | Train 손실: 0.1875 | Train 정확도: 0.9414 | Val 손실: 0.3170 | Val 정확도: 0.9003\n",
      "--- 에폭 10/20 ---\n",
      "에폭 [10/20] 결과 | Train 손실: 0.1587 | Train 정확도: 0.9517 | Val 손실: 0.3267 | Val 정확도: 0.8995\n",
      "--- 에폭 11/20 ---\n",
      "에폭 [11/20] 결과 | Train 손실: 0.1340 | Train 정확도: 0.9597 | Val 손실: 0.3438 | Val 정확도: 0.9011\n",
      "--- 에폭 12/20 ---\n",
      "에폭 [12/20] 결과 | Train 손실: 0.1130 | Train 정확도: 0.9669 | Val 손실: 0.3728 | Val 정확도: 0.8991\n",
      "--- 에폭 13/20 ---\n",
      "에폭 [13/20] 결과 | Train 손실: 0.0933 | Train 정확도: 0.9731 | Val 손실: 0.4071 | Val 정확도: 0.8983\n",
      "--- 에폭 14/20 ---\n",
      "에폭 [14/20] 결과 | Train 손실: 0.0812 | Train 정확도: 0.9770 | Val 손실: 0.4073 | Val 정확도: 0.9005\n",
      "--- 에폭 15/20 ---\n",
      "에폭 [15/20] 결과 | Train 손실: 0.0679 | Train 정확도: 0.9813 | Val 손실: 0.4319 | Val 정확도: 0.8963\n",
      "--- 에폭 16/20 ---\n",
      "에폭 [16/20] 결과 | Train 손실: 0.0573 | Train 정확도: 0.9843 | Val 손실: 0.4571 | Val 정확도: 0.8964\n",
      "--- 에폭 17/20 ---\n",
      "에폭 [17/20] 결과 | Train 손실: 0.0506 | Train 정확도: 0.9863 | Val 손실: 0.4810 | Val 정확도: 0.8983\n",
      "--- 에폭 18/20 ---\n",
      "에폭 [18/20] 결과 | Train 손실: 0.0434 | Train 정확도: 0.9882 | Val 손실: 0.4905 | Val 정확도: 0.8958\n",
      "--- 에폭 19/20 ---\n",
      "에폭 [19/20] 결과 | Train 손실: 0.0369 | Train 정확도: 0.9903 | Val 손실: 0.5069 | Val 정확도: 0.8939\n",
      "--- 에폭 20/20 ---\n",
      "에폭 [20/20] 결과 | Train 손실: 0.0315 | Train 정확도: 0.9918 | Val 손실: 0.5287 | Val 정확도: 0.8947\n",
      "===== GRU_Scratch 학습 완료 =====\n",
      "GRU_Scratch - 최종 검증 손실: 0.5287, 최종 검증 정확도: 0.8947, 총 소요 시간: 799.58s\n",
      "\n",
      "===== minLSTM 학습 시작 =====\n",
      "--- 에폭 1/20 ---\n",
      "에폭 [1/20] 결과 | Train 손실: 1.3870 | Train 정확도: 0.2502 | Val 손실: 1.3858 | Val 정확도: 0.2512\n",
      "--- 에폭 2/20 ---\n",
      "에폭 [2/20] 결과 | Train 손실: 1.3862 | Train 정확도: 0.2499 | Val 손실: 1.3855 | Val 정확도: 0.2513\n",
      "--- 에폭 3/20 ---\n",
      "에폭 [3/20] 결과 | Train 손실: 1.3860 | Train 정확도: 0.2510 | Val 손실: 1.3857 | Val 정확도: 0.2513\n",
      "--- 에폭 4/20 ---\n",
      "에폭 [4/20] 결과 | Train 손실: 1.3862 | Train 정확도: 0.2502 | Val 손실: 1.3861 | Val 정확도: 0.2514\n",
      "--- 에폭 5/20 ---\n",
      "에폭 [5/20] 결과 | Train 손실: 1.3859 | Train 정확도: 0.2498 | Val 손실: 1.3861 | Val 정확도: 0.2513\n",
      "--- 에폭 6/20 ---\n",
      "에폭 [6/20] 결과 | Train 손실: 1.3856 | Train 정확도: 0.2509 | Val 손실: 1.3867 | Val 정확도: 0.2500\n",
      "--- 에폭 7/20 ---\n",
      "에폭 [7/20] 결과 | Train 손실: 1.3856 | Train 정확도: 0.2531 | Val 손실: 1.3870 | Val 정확도: 0.2513\n",
      "--- 에폭 8/20 ---\n",
      "에폭 [8/20] 결과 | Train 손실: 1.3856 | Train 정확도: 0.2519 | Val 손실: 1.3866 | Val 정확도: 0.2516\n",
      "--- 에폭 9/20 ---\n",
      "에폭 [9/20] 결과 | Train 손실: 1.3853 | Train 정확도: 0.2508 | Val 손실: 1.3869 | Val 정확도: 0.2516\n",
      "--- 에폭 10/20 ---\n",
      "에폭 [10/20] 결과 | Train 손실: 1.3857 | Train 정확도: 0.2520 | Val 손실: 1.3861 | Val 정확도: 0.2514\n",
      "--- 에폭 11/20 ---\n",
      "에폭 [11/20] 결과 | Train 손실: 1.3853 | Train 정확도: 0.2507 | Val 손실: 1.3867 | Val 정확도: 0.2514\n",
      "--- 에폭 12/20 ---\n",
      "에폭 [12/20] 결과 | Train 손실: 1.3851 | Train 정확도: 0.2520 | Val 손실: 1.3862 | Val 정확도: 0.2517\n",
      "--- 에폭 13/20 ---\n",
      "에폭 [13/20] 결과 | Train 손실: 1.3849 | Train 정확도: 0.2524 | Val 손실: 1.3853 | Val 정확도: 0.2528\n",
      "--- 에폭 14/20 ---\n",
      "에폭 [14/20] 결과 | Train 손실: 1.3847 | Train 정확도: 0.2529 | Val 손실: 1.3856 | Val 정확도: 0.2516\n",
      "--- 에폭 15/20 ---\n",
      "에폭 [15/20] 결과 | Train 손실: 1.3843 | Train 정확도: 0.2530 | Val 손실: 1.3858 | Val 정확도: 0.2558\n",
      "--- 에폭 16/20 ---\n",
      "에폭 [16/20] 결과 | Train 손실: 1.3782 | Train 정확도: 0.2715 | Val 손실: 1.3633 | Val 정확도: 0.3125\n",
      "--- 에폭 17/20 ---\n",
      "에폭 [17/20] 결과 | Train 손실: 1.2060 | Train 정확도: 0.4310 | Val 손실: 1.0918 | Val 정확도: 0.5157\n",
      "--- 에폭 18/20 ---\n",
      "에폭 [18/20] 결과 | Train 손실: 0.9952 | Train 정확도: 0.5700 | Val 손실: 0.9681 | Val 정확도: 0.6099\n",
      "--- 에폭 19/20 ---\n",
      "에폭 [19/20] 결과 | Train 손실: 0.8698 | Train 정확도: 0.6618 | Val 손실: 0.8711 | Val 정확도: 0.6697\n",
      "--- 에폭 20/20 ---\n",
      "에폭 [20/20] 결과 | Train 손실: 0.7601 | Train 정확도: 0.7408 | Val 손실: 0.7785 | Val 정확도: 0.7517\n",
      "===== minLSTM 학습 완료 =====\n",
      "minLSTM - 최종 검증 손실: 0.7785, 최종 검증 정확도: 0.7517, 총 소요 시간: 136.47s\n",
      "\n",
      "===== minGRU 학습 시작 =====\n",
      "--- 에폭 1/20 ---\n",
      "에폭 [1/20] 결과 | Train 손실: 1.3870 | Train 정확도: 0.2509 | Val 손실: 1.3860 | Val 정확도: 0.2513\n",
      "--- 에폭 2/20 ---\n",
      "에폭 [2/20] 결과 | Train 손실: 1.3863 | Train 정확도: 0.2497 | Val 손실: 1.3866 | Val 정확도: 0.2500\n",
      "--- 에폭 3/20 ---\n",
      "에폭 [3/20] 결과 | Train 손실: 1.3860 | Train 정확도: 0.2510 | Val 손실: 1.3857 | Val 정확도: 0.2499\n",
      "--- 에폭 4/20 ---\n",
      "에폭 [4/20] 결과 | Train 손실: 1.3861 | Train 정확도: 0.2506 | Val 손실: 1.3873 | Val 정확도: 0.2512\n",
      "--- 에폭 5/20 ---\n",
      "에폭 [5/20] 결과 | Train 손실: 1.3858 | Train 정확도: 0.2510 | Val 손실: 1.3881 | Val 정확도: 0.2514\n",
      "--- 에폭 6/20 ---\n",
      "에폭 [6/20] 결과 | Train 손실: 1.3856 | Train 정확도: 0.2518 | Val 손실: 1.3857 | Val 정확도: 0.2514\n",
      "--- 에폭 7/20 ---\n",
      "에폭 [7/20] 결과 | Train 손실: 1.3860 | Train 정확도: 0.2507 | Val 손실: 1.3858 | Val 정확도: 0.2516\n",
      "--- 에폭 8/20 ---\n",
      "에폭 [8/20] 결과 | Train 손실: 1.3853 | Train 정확도: 0.2506 | Val 손실: 1.3856 | Val 정확도: 0.2514\n",
      "--- 에폭 9/20 ---\n",
      "에폭 [9/20] 결과 | Train 손실: 1.3850 | Train 정확도: 0.2509 | Val 손실: 1.3856 | Val 정확도: 0.2513\n",
      "--- 에폭 10/20 ---\n",
      "에폭 [10/20] 결과 | Train 손실: 1.3851 | Train 정확도: 0.2507 | Val 손실: 1.3857 | Val 정확도: 0.2499\n",
      "--- 에폭 11/20 ---\n",
      "에폭 [11/20] 결과 | Train 손실: 1.3850 | Train 정확도: 0.2531 | Val 손실: 1.3862 | Val 정확도: 0.2513\n",
      "--- 에폭 12/20 ---\n",
      "에폭 [12/20] 결과 | Train 손실: 1.3848 | Train 정확도: 0.2513 | Val 손실: 1.3859 | Val 정확도: 0.2518\n",
      "--- 에폭 13/20 ---\n",
      "에폭 [13/20] 결과 | Train 손실: 1.3844 | Train 정확도: 0.2541 | Val 손실: 1.3867 | Val 정확도: 0.2496\n",
      "--- 에폭 14/20 ---\n",
      "에폭 [14/20] 결과 | Train 손실: 1.3834 | Train 정확도: 0.2550 | Val 손실: 1.3869 | Val 정확도: 0.2525\n",
      "--- 에폭 15/20 ---\n",
      "에폭 [15/20] 결과 | Train 손실: 1.3743 | Train 정확도: 0.2842 | Val 손실: 1.3599 | Val 정확도: 0.3324\n",
      "--- 에폭 16/20 ---\n",
      "에폭 [16/20] 결과 | Train 손실: 1.2327 | Train 정확도: 0.4142 | Val 손실: 1.1393 | Val 정확도: 0.4800\n",
      "--- 에폭 17/20 ---\n",
      "에폭 [17/20] 결과 | Train 손실: 1.0427 | Train 정확도: 0.5432 | Val 손실: 1.0063 | Val 정확도: 0.6086\n",
      "--- 에폭 18/20 ---\n",
      "에폭 [18/20] 결과 | Train 손실: 0.9124 | Train 정확도: 0.6373 | Val 손실: 0.9289 | Val 정확도: 0.6379\n",
      "--- 에폭 19/20 ---\n",
      "에폭 [19/20] 결과 | Train 손실: 0.8339 | Train 정확도: 0.6799 | Val 손실: 0.8710 | Val 정확도: 0.6941\n",
      "--- 에폭 20/20 ---\n",
      "에폭 [20/20] 결과 | Train 손실: 0.7805 | Train 정확도: 0.7131 | Val 손실: 0.8615 | Val 정확도: 0.6936\n",
      "===== minGRU 학습 완료 =====\n",
      "minGRU - 최종 검증 손실: 0.8615, 최종 검증 정확도: 0.6936, 총 소요 시간: 128.49s\n",
      "\n",
      "--- 학습 결과 요약 (AG_NEWS) ---\n",
      "모델명           | 최종 검증 손실 | 최종 검증 정확도 | 시간(s)\n",
      "-----------------|----------------|-----------------|--------\n",
      "LSTM_Scratch    | 0.3491         | 0.8849          | 886.21 \n",
      "GRU_Scratch     | 0.5287         | 0.8947          | 799.58 \n",
      "minLSTM         | 0.7785         | 0.7517          | 136.47 \n",
      "minGRU          | 0.8615         | 0.6936          | 128.49 \n"
     ]
    }
   ],
   "source": [
    "# --- 필수 라이브러리 임포트 ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, Identity, Module\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence # 패딩을 위해 필요\n",
    "\n",
    "# *** 데이터셋 라이브러리 ***\n",
    "from datasets import load_dataset # Hugging Face datasets 라이브러리 임포트\n",
    "# *** 어휘 구축용 ***\n",
    "from collections import Counter, OrderedDict # OrderedDict는 안정적인 매핑 순서 보장\n",
    "\n",
    "import torchinfo # 모델 구조 시각화\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --- 헬퍼 함수들 (이전과 동일) ---\n",
    "def exists(v): return v is not None\n",
    "def default(v, d): return v if exists(v) else d\n",
    "def logcumsumexp(x, dim: int):\n",
    "    if hasattr(torch, \"logcumsumexp\"): return torch.logcumsumexp(x, dim=dim)\n",
    "    else:\n",
    "        max_val = torch.max(x, dim=dim, keepdim=True).values\n",
    "        max_val = torch.where(torch.isinf(max_val), torch.zeros_like(max_val), max_val)\n",
    "        x_adjusted = x - max_val\n",
    "        cumulative_exp = torch.cumsum(x_adjusted.exp(), dim=dim)\n",
    "        log_cumulative_exp = torch.log(cumulative_exp.clamp(min=1e-38))\n",
    "        return max_val + log_cumulative_exp\n",
    "def heinsen_associative_scan_log(log_coeffs, log_values):\n",
    "    log_coeffs = log_coeffs.contiguous(); log_values = log_values.contiguous()\n",
    "    a_star = log_coeffs.cumsum(dim = 1)\n",
    "    log_h0_plus_b_star = logcumsumexp(log_values - a_star, dim = 1)\n",
    "    log_h = a_star + log_h0_plus_b_star\n",
    "    return log_h.exp()\n",
    "def g(x): return torch.where(x >= 0, x + 0.5, x.sigmoid())\n",
    "def log_g(x):\n",
    "    relu_x_plus_0_5 = F.relu(x) + 0.5\n",
    "    return torch.where(x >= 0, relu_x_plus_0_5.log(), -F.softplus(-x))\n",
    "\n",
    "# --- RNN Cell 및 Layer 구현 (이전과 동일) ---\n",
    "# LSTMCellScratch, GRUCellScratch, SimpleRNNLayer, minLSTM, minGRU 클래스 정의\n",
    "# ... (이전 답변의 클래스 정의 코드를 여기에 붙여넣으세요) ...\n",
    "class LSTMCellScratch(nn.Module):\n",
    "    \"\"\"직접 구현한 LSTM Cell (PyTorch nn.Module 상속)\"\"\"\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        concat_size = input_size + hidden_size\n",
    "        # 가중치 및 편향 파라미터 정의\n",
    "        self.Wf = nn.Parameter(torch.Tensor(concat_size, hidden_size))\n",
    "        self.bf = nn.Parameter(torch.Tensor(hidden_size))\n",
    "        self.Wi = nn.Parameter(torch.Tensor(concat_size, hidden_size))\n",
    "        self.bi = nn.Parameter(torch.Tensor(hidden_size))\n",
    "        self.Wc = nn.Parameter(torch.Tensor(concat_size, hidden_size))\n",
    "        self.bc = nn.Parameter(torch.Tensor(hidden_size))\n",
    "        self.Wo = nn.Parameter(torch.Tensor(concat_size, hidden_size))\n",
    "        self.bo = nn.Parameter(torch.Tensor(hidden_size))\n",
    "        self.reset_parameters() # 파라미터 초기화 호출\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"파라미터 초기화\"\"\"\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "             nn.init.uniform_(weight, -stdv, stdv) # 균등 분포 초기화\n",
    "\n",
    "    def forward(self, xt, prev_state):\n",
    "        \"\"\"LSTM Cell 단일 스텝 순전파\"\"\"\n",
    "        prev_h, prev_c = prev_state # 이전 상태 분리\n",
    "        combined = torch.cat((prev_h, xt), dim=1) # 입력과 이전 은닉 상태 결합\n",
    "        # 게이트 및 상태 계산 (수식대로)\n",
    "        ft = torch.sigmoid(torch.matmul(combined, self.Wf) + self.bf)\n",
    "        it = torch.sigmoid(torch.matmul(combined, self.Wi) + self.bi)\n",
    "        c_tilde_t = torch.tanh(torch.matmul(combined, self.Wc) + self.bc)\n",
    "        ct = (ft * prev_c) + (it * c_tilde_t) # 셀 상태 업데이트\n",
    "        ot = torch.sigmoid(torch.matmul(combined, self.Wo) + self.bo)\n",
    "        ht = ot * torch.tanh(ct) # 은닉 상태 업데이트\n",
    "        return ht, ct # 현재 상태 반환\n",
    "\n",
    "class GRUCellScratch(nn.Module):\n",
    "    \"\"\"직접 구현한 GRU Cell (PyTorch nn.Module 상속)\"\"\"\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        concat_size = input_size + hidden_size\n",
    "        # 가중치 및 편향 파라미터 정의\n",
    "        self.Wr = nn.Parameter(torch.Tensor(concat_size, hidden_size))\n",
    "        self.br = nn.Parameter(torch.Tensor(hidden_size))\n",
    "        self.Wz = nn.Parameter(torch.Tensor(concat_size, hidden_size))\n",
    "        self.bz = nn.Parameter(torch.Tensor(hidden_size))\n",
    "        self.Wh = nn.Parameter(torch.Tensor(concat_size, hidden_size)) # 후보 상태 계산용\n",
    "        self.bh = nn.Parameter(torch.Tensor(hidden_size))\n",
    "        self.reset_parameters() # 파라미터 초기화 호출\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"파라미터 초기화\"\"\"\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "             nn.init.uniform_(weight, -stdv, stdv) # 균등 분포 초기화\n",
    "\n",
    "    def forward(self, xt, prev_h):\n",
    "        \"\"\"GRU Cell 단일 스텝 순전파\"\"\"\n",
    "        combined = torch.cat((prev_h, xt), dim=1) # 입력과 이전 은닉 상태 결합\n",
    "        # 게이트 및 상태 계산 (수식대로)\n",
    "        rt = torch.sigmoid(torch.matmul(combined, self.Wr) + self.br) # 리셋 게이트\n",
    "        zt = torch.sigmoid(torch.matmul(combined, self.Wz) + self.bz) # 업데이트 게이트\n",
    "        # 후보 상태 계산 시 리셋 게이트 적용\n",
    "        combined_reset = torch.cat((rt * prev_h, xt), dim=1)\n",
    "        h_tilde_t = torch.tanh(torch.matmul(combined_reset, self.Wh) + self.bh)\n",
    "        ht = (1 - zt) * prev_h + zt * h_tilde_t # 은닉 상태 업데이트\n",
    "        return ht # 현재 은닉 상태 반환\n",
    "\n",
    "class SimpleRNNLayer(nn.Module):\n",
    "    \"\"\"직접 구현한 Cell을 사용하여 시퀀스를 처리하는 레이어\"\"\"\n",
    "    def __init__(self, cell, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        # 주어진 Cell 클래스로 인스턴스 생성\n",
    "        self.cell = cell(input_size, hidden_size)\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        # Cell 타입이 LSTM인지 확인 (상태 처리 방식 구분용)\n",
    "        self.is_lstm = isinstance(self.cell, LSTMCellScratch)\n",
    "\n",
    "    def forward(self, x, initial_state=None):\n",
    "        \"\"\"시퀀스 데이터 순전파\"\"\"\n",
    "        batch_size, seq_len, _ = x.shape # 입력 shape 가져오기\n",
    "        device = x.device # 입력 데이터의 디바이스 가져오기\n",
    "\n",
    "        # 초기 상태 설정 (없으면 0으로 초기화)\n",
    "        if initial_state is None:\n",
    "            h = torch.zeros(batch_size, self.hidden_size, device=device)\n",
    "            c = torch.zeros(batch_size, self.hidden_size, device=device) if self.is_lstm else None\n",
    "        else: # 초기 상태가 주어지면 사용\n",
    "            if self.is_lstm:\n",
    "                h, c = initial_state\n",
    "            else:\n",
    "                h = initial_state\n",
    "                c = None\n",
    "\n",
    "        outputs = [] # 각 타임 스텝의 출력을 저장할 리스트\n",
    "        # 시퀀스 길이에 대해 반복하며 Cell 실행\n",
    "        for t in range(seq_len):\n",
    "            xt = x[:, t, :] # 현재 타임 스텝의 입력\n",
    "            if self.is_lstm: # LSTM Cell 처리\n",
    "                h, c = self.cell(xt, (h, c))\n",
    "                outputs.append(h.unsqueeze(1)) # 시간 차원 추가하여 저장\n",
    "            else: # GRU Cell 처리\n",
    "                h = self.cell(xt, h)\n",
    "                outputs.append(h.unsqueeze(1)) # 시간 차원 추가하여 저장\n",
    "\n",
    "        # 모든 타임 스텝의 출력을 하나의 텐서로 결합\n",
    "        outputs = torch.cat(outputs, dim=1) # shape: (batch, seq_len, hidden)\n",
    "        # 최종 상태 반환\n",
    "        final_state = (h, c) if self.is_lstm else h\n",
    "        return outputs, final_state\n",
    "\n",
    "# --- 업데이트된 minLSTM 구현 ---\n",
    "class minLSTM(Module):\n",
    "    \"\"\"Minimal LSTM (초기화 및 편향 추가)\"\"\"\n",
    "    def __init__(self, dim, expansion_factor = 1., proj_out = None):\n",
    "        super().__init__()\n",
    "        dim_inner = int(dim * expansion_factor)\n",
    "        self.proj_out_active = default(proj_out, expansion_factor != 1.)\n",
    "\n",
    "        # *** 수정: bias=True 로 변경 ***\n",
    "        self.to_hidden_and_f_i_gate = Linear(dim, dim_inner * 3, bias = True)\n",
    "        self.to_out = Linear(dim_inner, dim, bias = True) if self.proj_out_active else Identity()\n",
    "\n",
    "        # *** 추가: 파라미터 초기화 호출 ***\n",
    "        self.reset_parameters()\n",
    "\n",
    "    # *** 추가: 파라미터 초기화 메소드 ***\n",
    "    def reset_parameters(self):\n",
    "        # 예시: Xavier 초기화 (다른 방식도 가능)\n",
    "        for name, param in self.named_parameters():\n",
    "            if param.dim() > 1 : # 가중치 행렬\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif \"bias\" in name: # 편향 벡터\n",
    "                 nn.init.zeros_(param)\n",
    "\n",
    "\n",
    "    def forward(self, x, prev_hidden = None, return_next_prev_hidden = False):\n",
    "        # forward 메소드 내용은 이전과 동일\n",
    "        seq_len = x.shape[1]; device = x.device\n",
    "        hidden, f_gate_raw, i_gate_raw = self.to_hidden_and_f_i_gate(x).chunk(3, dim = -1)\n",
    "        if seq_len == 1:\n",
    "            hidden_candidate = hidden\n",
    "            f_gate = f_gate_raw.sigmoid(); i_gate = i_gate_raw.sigmoid()\n",
    "            f_gate_prime = f_gate / (f_gate + i_gate + 1e-8); i_gate_prime = i_gate / (f_gate + i_gate + 1e-8)\n",
    "            if exists(prev_hidden): out = (prev_hidden * f_gate_prime) + (hidden_candidate * i_gate_prime)\n",
    "            else: out = hidden_candidate * i_gate_prime\n",
    "        else:\n",
    "            diff = F.softplus(-f_gate_raw) - F.softplus(-i_gate_raw)\n",
    "            log_f = -F.softplus(diff); log_i = -F.softplus(-diff)\n",
    "            log_tilde_h = log_g(hidden) # 병렬 경로는 log_g 유지\n",
    "            log_values = log_i + log_tilde_h\n",
    "            if exists(prev_hidden):\n",
    "                try: log_h_0 = prev_hidden.log()\n",
    "                except Exception as e: log_h_0 = torch.zeros((x.shape[0], 1, hidden.shape[-1]), device=device)\n",
    "                log_values = torch.cat((log_h_0, log_values), dim = 1); log_f = F.pad(log_f, (0, 0, 1, 0), value=0.)\n",
    "            out = heinsen_associative_scan_log(log_f, log_values)\n",
    "            if exists(prev_hidden): out = out[:, 1:]\n",
    "        next_prev_hidden = out[:, -1:]\n",
    "        out = self.to_out(out)\n",
    "        if not return_next_prev_hidden: return out\n",
    "        return out, next_prev_hidden\n",
    "\n",
    "# --- 업데이트된 minGRU 구현 ---\n",
    "class minGRU(Module):\n",
    "    \"\"\"Minimal GRU (초기화 및 편향 추가)\"\"\"\n",
    "    def __init__(self, dim, expansion_factor = 1., proj_out = None):\n",
    "        super().__init__()\n",
    "        dim_inner = int(dim * expansion_factor)\n",
    "        self.proj_out_active = default(proj_out, expansion_factor != 1.)\n",
    "\n",
    "        # *** 수정: bias=True 로 변경 ***\n",
    "        self.to_hidden_and_gate = Linear(dim, dim_inner * 2, bias = True)\n",
    "        self.to_out = Linear(dim_inner, dim, bias = True) if self.proj_out_active else Identity()\n",
    "\n",
    "        # *** 추가: 파라미터 초기화 호출 ***\n",
    "        self.reset_parameters()\n",
    "\n",
    "    # *** 추가: 파라미터 초기화 메소드 ***\n",
    "    def reset_parameters(self):\n",
    "        # 예시: Xavier 초기화\n",
    "        for name, param in self.named_parameters():\n",
    "            if param.dim() > 1 : # 가중치 행렬\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif \"bias\" in name: # 편향 벡터\n",
    "                 nn.init.zeros_(param)\n",
    "\n",
    "    def forward(self, x, prev_hidden = None, return_next_prev_hidden = False):\n",
    "        # forward 메소드 내용은 이전과 동일\n",
    "        seq_len = x.shape[1]; device = x.device\n",
    "        hidden, gate_raw = self.to_hidden_and_gate(x).chunk(2, dim = -1)\n",
    "        if seq_len == 1:\n",
    "            hidden_candidate = hidden\n",
    "            gate = gate_raw.sigmoid()\n",
    "            if exists(prev_hidden): out = torch.lerp(prev_hidden, hidden_candidate, gate)\n",
    "            else: out = hidden_candidate * gate\n",
    "        else:\n",
    "            log_coeffs = -F.softplus(gate_raw); log_z = -F.softplus(-gate_raw)\n",
    "            log_tilde_h = log_g(hidden) # 병렬 경로는 log_g 유지\n",
    "            log_values = log_z + log_tilde_h\n",
    "            if exists(prev_hidden):\n",
    "                try: log_h_0 = prev_hidden.log()\n",
    "                except Exception as e: log_h_0 = torch.zeros((x.shape[0], 1, hidden.shape[-1]), device=device)\n",
    "                log_values = torch.cat((log_h_0, log_values), dim = 1); log_coeffs = F.pad(log_coeffs, (0, 0, 1, 0), value=0.)\n",
    "            out = heinsen_associative_scan_log(log_coeffs, log_values)\n",
    "            if exists(prev_hidden): out = out[:, 1:]\n",
    "        next_prev_hidden = out[:, -1:]\n",
    "        out = self.to_out(out)\n",
    "        if not return_next_prev_hidden: return out\n",
    "        return out, next_prev_hidden\n",
    "\n",
    "\n",
    "# === 데이터 로딩 및 전처리 (Hugging Face datasets + 수동 전처리) ===\n",
    "print(\"\\n--- AG_NEWS 데이터 로딩 및 전처리 (수동 토크나이저/단어집합) ---\")\n",
    "\n",
    "# 디바이스 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"사용 디바이스: {device}\")\n",
    "\n",
    "# 데이터셋 로드\n",
    "try:\n",
    "    ag_news_dataset = load_dataset(\"ag_news\")\n",
    "    print(\"AG_NEWS 데이터셋 로드 완료.\")\n",
    "except Exception as e:\n",
    "    print(f\"Hugging Face datasets 로드 중 오류: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 토크나이저 (기본 split 사용) ---\n",
    "tokenizer = lambda x: x.lower().split()\n",
    "print(\"기본 토크나이저 (str.lower().split()) 사용\")\n",
    "\n",
    "# --- 단어 집합(Vocabulary) 수동 구축 ---\n",
    "print(\"단어 집합 구축 시작...\")\n",
    "counter = Counter()\n",
    "# 학습 데이터셋 텍스트를 순회하며 단어 빈도 계산\n",
    "for example in ag_news_dataset['train']:\n",
    "    counter.update(tokenizer(example['text']))\n",
    "\n",
    "# 최소 빈도수 적용 및 정렬\n",
    "min_freq = 5\n",
    "specials = [\"<unk>\", \"<pad>\"]\n",
    "# OrderedDict를 사용하여 인덱스<->단어 매핑 순서 고정\n",
    "itos = OrderedDict()\n",
    "# 특수 토큰 먼저 추가\n",
    "for i, s in enumerate(specials):\n",
    "    itos[i] = s\n",
    "# 단어 추가 (빈도수 높은 순)\n",
    "curr_idx = len(specials)\n",
    "word_freq = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "for word, freq in word_freq:\n",
    "    if freq >= min_freq:\n",
    "        itos[curr_idx] = word\n",
    "        curr_idx += 1\n",
    "\n",
    "# 단어 -> 인덱스 매핑 생성\n",
    "stoi = {s: i for i, s in itos.items()}\n",
    "vocab_size = len(itos)\n",
    "unk_idx = stoi[\"<unk>\"]\n",
    "pad_idx = stoi[\"<pad>\"]\n",
    "print(f\"단어 집합 크기 (min_freq={min_freq}): {vocab_size}\")\n",
    "print(f\"Unk 인덱스: {unk_idx}, Pad 인덱스: {pad_idx}\")\n",
    "print(\"단어 집합 구축 완료.\")\n",
    "\n",
    "# 텍스트/레이블 파이프라인 정의 (수동 vocab 사용)\n",
    "text_pipeline = lambda x: [stoi.get(token, unk_idx) for token in tokenizer(x)]\n",
    "label_pipeline = lambda x: int(x) # AG_NEWS 레이블 (0-3)\n",
    "\n",
    "# 전처리 함수 (datasets.map 용)\n",
    "def preprocess_function(examples):\n",
    "    tokenized_texts = [text_pipeline(text) for text in examples['text']]\n",
    "    labels = [label_pipeline(label) for label in examples['label']]\n",
    "    return {'input_ids': tokenized_texts, 'label': labels}\n",
    "\n",
    "# 데이터셋에 전처리 함수 적용\n",
    "print(\"데이터셋 전처리 적용 중...\")\n",
    "# num_proc > 1 로 설정하여 병렬 처리 가능 (환경에 따라)\n",
    "tokenized_datasets = ag_news_dataset.map(preprocess_function, batched=True, remove_columns=['text'])\n",
    "print(\"데이터셋 전처리 완료.\")\n",
    "\n",
    "# DataLoader를 위한 Collate 함수\n",
    "def collate_batch_manual(batch):\n",
    "    label_list, text_list, lengths = [], [], []\n",
    "    for item in batch: # batch는 dict 리스트 {'input_ids': [...], 'label': ...}\n",
    "        label_list.append(item['label'])\n",
    "        processed_text = torch.tensor(item['input_ids'], dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        lengths.append(len(processed_text))\n",
    "\n",
    "    labels = torch.tensor(label_list, dtype=torch.int64)\n",
    "    texts_padded = pad_sequence(text_list, batch_first=True, padding_value=pad_idx)\n",
    "    lengths_tensor = torch.tensor(lengths, dtype=torch.int64) # 길이는 현재 사용 안 함\n",
    "    return labels.to(device), texts_padded.to(device), lengths_tensor.to(device)\n",
    "\n",
    "# DataLoader 생성\n",
    "batch_size = 512\n",
    "train_dataloader = DataLoader(tokenized_datasets['train'], batch_size=batch_size, shuffle=True, collate_fn=collate_batch_manual)\n",
    "test_dataloader = DataLoader(tokenized_datasets['test'], batch_size=batch_size, shuffle=False, collate_fn=collate_batch_manual)\n",
    "print(\"데이터 로더 생성 완료.\")\n",
    "\n",
    "# === 텍스트 분류 모델 정의 (TextClassifier - 이전과 동일, 단 pad_idx 전달) ===\n",
    "class TextClassifier(nn.Module):\n",
    "    \"\"\"텍스트 분류 모델\"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_layer, hidden_dim, num_classes, pad_idx): # pad_idx 추가\n",
    "        super().__init__()\n",
    "        # 임베딩 레이어에 padding_idx 지정\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.rnn = rnn_layer\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.rnn_is_custom_layer = isinstance(self.rnn, SimpleRNNLayer)\n",
    "        if not self.rnn_is_custom_layer:\n",
    "             self.rnn_is_min_model = isinstance(self.rnn, (minLSTM, minGRU))\n",
    "        else:\n",
    "             self.rnn_is_min_model = False\n",
    "\n",
    "    def forward(self, text, lengths=None): # lengths 인자 추가 (현재 사용 안 함)\n",
    "        embedded = self.embedding(text)\n",
    "\n",
    "        if self.rnn_is_custom_layer:\n",
    "             outputs, final_state = self.rnn(embedded)\n",
    "             final_hidden = final_state[0] if self.rnn.is_lstm else final_state\n",
    "        elif self.rnn_is_min_model:\n",
    "             outputs = self.rnn(embedded, prev_hidden=None)\n",
    "             final_hidden = outputs[:, -1, :]\n",
    "        else: # 다른 RNN 타입 대비\n",
    "             outputs, (final_hidden, _) = self.rnn(embedded)\n",
    "             final_hidden = final_hidden[-1]\n",
    "\n",
    "        return self.fc(final_hidden)\n",
    "\n",
    "# === 모델 구조 시각화 (torchinfo 사용 - 이전과 동일) ===\n",
    "print(\"\\n--- 모델 구조 시각화 (minLSTM 예시) ---\")\n",
    "# 파라미터 설정\n",
    "embedding_dim = 64\n",
    "hidden_dim = 128\n",
    "num_classes = 4\n",
    "\n",
    "# 시각화용 모델 인스턴스 생성 (minLSTM 사용, proj_out=False)\n",
    "temp_rnn = minLSTM(embedding_dim, expansion_factor=hidden_dim/embedding_dim, proj_out=False).to(device)\n",
    "# TextClassifier에 pad_idx 전달\n",
    "model_for_info = TextClassifier(vocab_size, embedding_dim, temp_rnn, hidden_dim, num_classes, pad_idx).to(device)\n",
    "\n",
    "# 더미 입력 데이터 생성\n",
    "dummy_batch_size = 4\n",
    "dummy_seq_len = 50\n",
    "dummy_input_ids = torch.randint(0, vocab_size, (dummy_batch_size, dummy_seq_len), dtype=torch.long).to(device)\n",
    "\n",
    "# torchinfo.summary 실행\n",
    "try:\n",
    "    torchinfo.summary(model_for_info, input_data=(dummy_input_ids,),\n",
    "                      col_names=[\"input_size\", \"output_size\", \"num_params\", \"mult_adds\"],\n",
    "                      row_settings=[\"var_names\"])\n",
    "except Exception as e:\n",
    "    print(f\"torchinfo.summary 실행 중 오류: {e}\")\n",
    "\n",
    "# === 학습 및 평가 (이전과 동일) ===\n",
    "print(\"\\n--- 텍스트 분류 학습 및 평가 ---\")\n",
    "\n",
    "# 학습 파라미터\n",
    "learning_rate = 0.001\n",
    "num_epochs = 20\n",
    "\n",
    "# 모델 인스턴스 생성 (TextClassifier 사용, pad_idx 전달)\n",
    "lstm_clf = TextClassifier(vocab_size, embedding_dim,\n",
    "                         SimpleRNNLayer(LSTMCellScratch, embedding_dim, hidden_dim),\n",
    "                         hidden_dim, num_classes, pad_idx).to(device)\n",
    "gru_clf = TextClassifier(vocab_size, embedding_dim,\n",
    "                        SimpleRNNLayer(GRUCellScratch, embedding_dim, hidden_dim),\n",
    "                        hidden_dim, num_classes, pad_idx).to(device)\n",
    "min_lstm_clf = TextClassifier(vocab_size, embedding_dim,\n",
    "                             minLSTM(embedding_dim, expansion_factor=hidden_dim/embedding_dim, proj_out=False),\n",
    "                             hidden_dim, num_classes, pad_idx).to(device)\n",
    "min_gru_clf = TextClassifier(vocab_size, embedding_dim,\n",
    "                            minGRU(embedding_dim, expansion_factor=hidden_dim/embedding_dim, proj_out=False),\n",
    "                            hidden_dim, num_classes, pad_idx).to(device)\n",
    "\n",
    "models_clf = {\n",
    "    \"LSTM_Scratch\": lstm_clf,\n",
    "    \"GRU_Scratch\": gru_clf,\n",
    "    \"minLSTM\": min_lstm_clf,\n",
    "    \"minGRU\": min_gru_clf\n",
    "}\n",
    "\n",
    "# 손실 함수\n",
    "criterion = nn.CrossEntropyLoss() # 패딩은 임베딩 레이어에서 처리되므로 무시 불필요\n",
    "\n",
    "results_clf = {}\n",
    "\n",
    "# 정확도 계산 함수 (이전과 동일)\n",
    "def calculate_accuracy(outputs, labels):\n",
    "    preds = outputs.argmax(dim=1)\n",
    "    correct = (preds == labels).sum().item()\n",
    "    return correct / len(labels) if len(labels) > 0 else 0.0\n",
    "\n",
    "# 학습 및 평가 함수 (이전과 동일)\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0; total_acc = 0; total_samples = 0\n",
    "    for i, (labels, text, lengths) in enumerate(dataloader):\n",
    "        labels, text = labels.to(device), text.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(text) # lengths 인자 제거\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_samples = labels.size(0)\n",
    "        total_loss += loss.item() * batch_samples\n",
    "        total_acc += calculate_accuracy(outputs, labels) * batch_samples\n",
    "        total_samples += batch_samples\n",
    "        # if (i + 1) % 100 == 0: print(f\"  스텝 [{i+1}/{len(dataloader)}], 손실: {total_loss/total_samples:.4f}, 정확도: {total_acc/total_samples:.4f}\") # 너무 자주 출력될 수 있음\n",
    "        del labels, text, lengths, outputs, loss\n",
    "        if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "    return total_loss / total_samples, total_acc / total_samples\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0; total_acc = 0; total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for labels, text, lengths in dataloader:\n",
    "            labels, text = labels.to(device), text.to(device)\n",
    "            outputs = model(text) # lengths 인자 제거\n",
    "            loss = criterion(outputs, labels)\n",
    "            batch_samples = labels.size(0)\n",
    "            total_loss += loss.item() * batch_samples\n",
    "            total_acc += calculate_accuracy(outputs, labels) * batch_samples\n",
    "            total_samples += batch_samples\n",
    "            del labels, text, lengths, outputs, loss\n",
    "            if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "    return total_loss / total_samples, total_acc / total_samples\n",
    "\n",
    "# 학습 루프 실행 (이전과 동일)\n",
    "for name, model in models_clf.items():\n",
    "    print(f\"\\n===== {name} 학습 시작 =====\")\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    start_time = time.time()\n",
    "    best_val_acc = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"--- 에폭 {epoch+1}/{num_epochs} ---\")\n",
    "        train_loss, train_acc = train_epoch(model, train_dataloader, optimizer, criterion, device)\n",
    "        val_loss, val_acc = evaluate(model, test_dataloader, criterion, device)\n",
    "        if val_acc > best_val_acc: best_val_acc = val_acc\n",
    "        print(f\"에폭 [{epoch+1}/{num_epochs}] 결과 | Train 손실: {train_loss:.4f} | Train 정확도: {train_acc:.4f} | Val 손실: {val_loss:.4f} | Val 정확도: {val_acc:.4f}\")\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    final_val_loss, final_val_acc = evaluate(model, test_dataloader, criterion, device)\n",
    "    results_clf[name] = {\"loss\": final_val_loss, \"accuracy\": final_val_acc, \"time\": duration}\n",
    "    print(f\"===== {name} 학습 완료 =====\")\n",
    "    print(f\"{name} - 최종 검증 손실: {final_val_loss:.4f}, 최종 검증 정확도: {final_val_acc:.4f}, 총 소요 시간: {duration:.2f}s\")\n",
    "\n",
    "# --- 최종 결과 비교 ---\n",
    "print(\"\\n--- 학습 결과 요약 (AG_NEWS) ---\")\n",
    "print(\"모델명           | 최종 검증 손실 | 최종 검증 정확도 | 시간(s)\")\n",
    "print(\"-----------------|----------------|-----------------|--------\")\n",
    "for name, res in results_clf.items():\n",
    "    print(f\"{name:<15} | {res['loss']:.4f}         | {res['accuracy']:.4f}          | {res['time']:<7.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318b3ef4",
   "metadata": {},
   "source": [
    "`모델의 구조를 보다 뚜렷하게 명시(레이어 수 등)하고, 전체적인 레이어 구조를 맞추는 경우와 파라미터 수를 맞추는 경우에 대해서도 제대로 된 데이터로 비교 필요`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
