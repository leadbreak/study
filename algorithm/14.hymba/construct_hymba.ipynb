{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06078178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== 1_Mamba_Only_Baseline ====================\n",
      "--- KV Cache 크기: 0.00 MB\n",
      "--- 처리량: 344,487.54 tokens/sec\n",
      "\n",
      "==================== 2_+_Attention (Hymba) ====================\n",
      "--- KV Cache 크기: 4.00 MB\n",
      "--- 처리량: 260,545.80 tokens/sec\n",
      "\n",
      "==================== 3_+_Meta_Tokens ====================\n",
      "--- KV Cache 크기: 4.03 MB\n",
      "--- 처리량: 247,209.44 tokens/sec\n",
      "\n",
      "==================== 4_+_Shared_KV_Cache ====================\n",
      "--- KV Cache 크기: 2.02 MB\n",
      "--- 처리량: 250,966.61 tokens/sec\n",
      "\n",
      "==================== 5_+_SWA ====================\n",
      "--- KV Cache 크기: 2.02 MB\n",
      "--- 처리량: 245,972.06 tokens/sec\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import math\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "# mamba-ssm과 causal-conv1d는 저수준 CUDA 커널을 사용하기 위해 필요합니다.\n",
    "# pip install mamba-ssm causal-conv1d\n",
    "try:\n",
    "    from mamba_ssm.ops.selective_scan_interface import selective_scan_fn\n",
    "    from causal_conv1d import causal_conv1d_fn\n",
    "except ImportError:\n",
    "    print(\"Warning: mamba-ssm or causal-conv1d not found. MambaBranch will not work.\")\n",
    "    selective_scan_fn = None\n",
    "    causal_conv1d_fn = None\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. 아키텍처의 기본 구성 요소 (Llama 및 Mamba)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
    "        return self.weight * hidden_states.to(input_dtype)\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim: int, max_seq_len: int, base: int = 10000, device: Optional[str] = None):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2, device=device, dtype=torch.float32) / self.dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "        self._set_cos_sin_cache(seq_len=max_seq_len, device=device)\n",
    "\n",
    "    def _set_cos_sin_cache(self, seq_len: int, device: Optional[str], dtype: torch.dtype = torch.float32):\n",
    "        self.max_seq_len_cached = seq_len\n",
    "        t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        self.register_buffer(\"cos_cached\", emb.cos().to(dtype), persistent=False)\n",
    "        self.register_buffer(\"sin_cached\", emb.sin().to(dtype), persistent=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, seq_len: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        if seq_len > self.max_seq_len_cached:\n",
    "            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)\n",
    "        return self.cos_cached[:seq_len, ...].to(dtype=x.dtype), self.sin_cached[:seq_len, ...].to(dtype=x.dtype)\n",
    "\n",
    "def rotate_half(x: torch.Tensor) -> torch.Tensor:\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "def apply_rotary_pos_emb(q: torch.Tensor, k: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    cos = cos.unsqueeze(0).unsqueeze(2)\n",
    "    sin = sin.unsqueeze(0).unsqueeze(2)\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n",
    "\n",
    "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
    "    if n_rep == 1:\n",
    "        return hidden_states\n",
    "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
    "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
    "\n",
    "class AttentionBranch(nn.Module):\n",
    "    def __init__(self, d_inner: int, n_heads: int, n_kv_heads: int, max_seq_len: int, window_size: int = -1):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.n_kv_heads = n_kv_heads\n",
    "        self.n_key_value_groups = n_heads // n_kv_heads\n",
    "        self.head_dim = d_inner // n_heads\n",
    "        self.rotary_emb = RotaryEmbedding(self.head_dim, max_seq_len)\n",
    "\n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, past_kv: Optional[Tuple[torch.Tensor, torch.Tensor]] = None, attn_mask: Optional[torch.Tensor] = None, use_cache: bool = False):\n",
    "        batch_size, q_len, _ = q.shape\n",
    "        \n",
    "        q = q.view(batch_size, q_len, self.n_heads, self.head_dim)\n",
    "        k = k.view(batch_size, q_len, self.n_kv_heads, self.head_dim)\n",
    "        v = v.view(batch_size, q_len, self.n_kv_heads, self.head_dim)\n",
    "\n",
    "        kv_seq_len = q_len\n",
    "        if past_kv is not None:\n",
    "            kv_seq_len += past_kv[0].shape[1]\n",
    "            \n",
    "        cos, sin = self.rotary_emb(v, seq_len=kv_seq_len)\n",
    "\n",
    "        if past_kv is not None:\n",
    "            cos = cos[past_kv[0].shape[1]:]\n",
    "            sin = sin[past_kv[0].shape[1]:]\n",
    "        \n",
    "        q, k = apply_rotary_pos_emb(q, k, cos, sin)\n",
    "        \n",
    "        if past_kv is not None:\n",
    "            past_key, past_value = past_kv\n",
    "            k = torch.cat([past_key, k], dim=1)\n",
    "            v = torch.cat([past_value, v], dim=1)\n",
    "            \n",
    "        present_kv = (k, v) if use_cache else None\n",
    "        \n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "        \n",
    "        k = repeat_kv(k, self.n_key_value_groups)\n",
    "        v = repeat_kv(v, self.n_key_value_groups)\n",
    "        \n",
    "        attn_output = F.scaled_dot_product_attention(q, k, v, attn_mask=attn_mask)\n",
    "        return attn_output.transpose(1, 2).contiguous().view(batch_size, q_len, -1), present_kv\n",
    "\n",
    "class MambaBranch(nn.Module):\n",
    "    def __init__(self, d_inner, d_state, d_conv, dt_rank):\n",
    "        super().__init__()\n",
    "        if causal_conv1d_fn is None or selective_scan_fn is None:\n",
    "            raise ImportError(\"Mamba packages not found. Please install them.\")\n",
    "        \n",
    "        self.d_inner, self.d_state, self.d_conv, self.dt_rank = d_inner, d_state, d_conv, dt_rank\n",
    "        \n",
    "        self.conv1d = nn.Conv1d(in_channels=self.d_inner, out_channels=self.d_inner, kernel_size=d_conv, bias=True, groups=self.d_inner, padding=d_conv - 1)\n",
    "        self.x_proj = nn.Linear(self.d_inner, self.dt_rank + 2 * self.d_state, bias=False)\n",
    "        self.dt_proj = nn.Linear(self.dt_rank, self.d_inner, bias=True)\n",
    "        \n",
    "        A = torch.arange(1, self.d_state + 1, dtype=torch.float32).repeat(self.d_inner, 1)\n",
    "        self.A_log = nn.Parameter(torch.log(A))\n",
    "        self.D = nn.Parameter(torch.ones(self.d_inner))\n",
    "\n",
    "    def forward(self, x, z):\n",
    "        x_transposed = x.transpose(1, 2).contiguous()\n",
    "        x_conv = causal_conv1d_fn(x_transposed, self.conv1d.weight.squeeze(1), self.conv1d.bias, activation=\"silu\")\n",
    "        \n",
    "        x_dbl = self.x_proj(x_conv.transpose(1, 2))\n",
    "        dt_pre, B, C = torch.split(x_dbl, [self.dt_rank, self.d_state, self.d_state], dim=-1)\n",
    "        \n",
    "        dt = self.dt_proj(dt_pre).transpose(1, 2)\n",
    "        A = -torch.exp(self.A_log.float())\n",
    "        \n",
    "        y = selective_scan_fn(\n",
    "            x_conv, dt, A, B.transpose(1, 2), C.transpose(1, 2), self.D.float(), \n",
    "            z=z.transpose(1,2), delta_bias=self.dt_proj.bias.float(), delta_softplus=True\n",
    "        )\n",
    "        return y.transpose(1,2)\n",
    "        \n",
    "# -----------------------------------------------------------------------------\n",
    "# 2. Ablation을 위한 각 단계별 블록 정의\n",
    "# -----------------------------------------------------------------------------\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model: int, hidden_dim: int):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(d_model, hidden_dim, bias=False)\n",
    "        self.w3 = nn.Linear(d_model, hidden_dim, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_dim, d_model, bias=False)\n",
    "        self.act_fn = F.silu\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.w2(self.act_fn(self.w1(x)) * self.w3(x))\n",
    "\n",
    "class MambaOnlyBlock(nn.Module):\n",
    "    \"\"\" 1단계: Mamba-only 베이스라인을 위한 블록 \"\"\"\n",
    "    def __init__(self, d_model: int, ffn_hidden_dim: int, mamba_params: dict):\n",
    "        super().__init__()\n",
    "        self.d_inner = mamba_params['expand'] * d_model\n",
    "        \n",
    "        self.norm = RMSNorm(d_model)\n",
    "        self.in_proj = nn.Linear(d_model, 2 * self.d_inner, bias=False)\n",
    "        self.mamba_branch = MambaBranch(d_inner=self.d_inner, **{k:v for k,v in mamba_params.items() if k != 'expand'})\n",
    "        self.out_proj = nn.Linear(self.d_inner, d_model, bias=False)\n",
    "        self.ffn = FeedForward(d_model, ffn_hidden_dim)\n",
    "        self.ffn_norm = RMSNorm(d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, **kwargs):\n",
    "        residual = x\n",
    "        x_norm = self.norm(x)\n",
    "        \n",
    "        xz = self.in_proj(x_norm)\n",
    "        x_mamba, z_mamba = xz.chunk(2, dim=-1)\n",
    "        \n",
    "        mamba_out = self.mamba_branch(x_mamba, z_mamba)\n",
    "        \n",
    "        h = residual + self.out_proj(mamba_out)\n",
    "        out = h + self.ffn(self.ffn_norm(h))\n",
    "        return out, None\n",
    "\n",
    "class HymbaBlock(nn.Module):\n",
    "    \"\"\" 2-5단계: 공식 코드 로직을 따르는 HymbaBlock \"\"\"\n",
    "    def __init__(self, d_model: int, n_heads: int, n_kv_heads: int, max_seq_len: int, window_size: int, mamba_params: dict):\n",
    "        super().__init__()\n",
    "        self.d_inner = mamba_params['expand'] * d_model\n",
    "        attn_head_dim = self.d_inner // n_heads\n",
    "        \n",
    "        latent_dim = self.d_inner + self.d_inner + (attn_head_dim * n_kv_heads * 2)\n",
    "        self.in_proj = nn.Linear(d_model, latent_dim + self.d_inner, bias=True)\n",
    "        self.out_proj = nn.Linear(self.d_inner, d_model, bias=True)\n",
    "\n",
    "        self.attn_branch = AttentionBranch(d_inner=self.d_inner, n_heads=n_heads, n_kv_heads=n_kv_heads, max_seq_len=max_seq_len, window_size=window_size)\n",
    "        self.mamba_branch = MambaBranch(d_inner=self.d_inner, d_state=mamba_params['d_state'], d_conv=mamba_params['d_conv'], dt_rank=mamba_params['dt_rank'])\n",
    "\n",
    "        self.norm = RMSNorm(d_model)\n",
    "        self.norm1 = RMSNorm(self.d_inner)\n",
    "        self.norm2 = RMSNorm(self.d_inner)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, past_kv: Optional[Tuple[torch.Tensor, torch.Tensor]] = None, attn_mask: Optional[torch.Tensor] = None, use_cache: bool = False):\n",
    "        projected = self.in_proj(self.norm(x))\n",
    "        \n",
    "        latent, gate = projected.tensor_split((projected.shape[-1] - self.d_inner,), dim=-1)\n",
    "        \n",
    "        attn_q_dim = self.d_inner\n",
    "        attn_k_dim = self.attn_branch.head_dim * self.attn_branch.n_kv_heads\n",
    "        \n",
    "        q, k, v, mamba_x = latent.tensor_split((attn_q_dim, attn_q_dim + attn_k_dim, attn_q_dim + 2 * attn_k_dim), dim=-1)\n",
    "\n",
    "        attn_out, present_kv = self.attn_branch(q, k, v, past_kv, attn_mask, use_cache)\n",
    "        mamba_out = self.mamba_branch(mamba_x, gate)\n",
    "\n",
    "        combined = (self.norm1(attn_out) + self.norm2(mamba_out)) / 2\n",
    "        return self.out_proj(combined), present_kv\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3. 전체 모델 및 마스크 생성\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class HymbaModel(nn.Module):\n",
    "    def __init__(self, config: dict):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embedding = nn.Embedding(config['vocab_size'], config['d_model'])\n",
    "        if config['use_meta_tokens']:\n",
    "            self.meta_tokens = nn.Parameter(torch.randn(1, config['n_meta_tokens'], config['d_model']))\n",
    "\n",
    "        layers = []\n",
    "        for _ in range(config['n_layers']):\n",
    "            if config['use_mamba_only']:\n",
    "                 layers.append(MambaOnlyBlock(\n",
    "                    d_model=config['d_model'],\n",
    "                    ffn_hidden_dim=config['ffn_hidden_dim'],\n",
    "                    mamba_params=config['mamba_params']\n",
    "                 ))\n",
    "            else: # Hymba Block for stages 2-5\n",
    "                layers.append(HymbaBlock(\n",
    "                    d_model=config['d_model'], n_heads=config['n_heads'], \n",
    "                    n_kv_heads=config['n_kv_heads'], max_seq_len=config['max_seq_len'],\n",
    "                    window_size=config['window_size'] if config['use_swa'] else -1,\n",
    "                    mamba_params=config['mamba_params']\n",
    "                 ))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.norm = RMSNorm(config['d_model'])\n",
    "        self.lm_head = nn.Linear(config['d_model'], config['vocab_size'], bias=False)\n",
    "\n",
    "    def _create_attention_mask(self, q_len: int, kv_len: int, window_size: int, n_meta_tokens: int, device: str) -> Optional[torch.Tensor]:\n",
    "        if q_len > 1:\n",
    "             mask = torch.full((1, 1, q_len, kv_len), float(\"-inf\"), device=device)\n",
    "             mask = torch.triu(mask, diagonal=1)\n",
    "             if window_size > 0:\n",
    "                 sliding_mask = torch.ones(q_len, kv_len, device=device).bool()\n",
    "                 sliding_mask.tril_(-1).triu_(-window_size)\n",
    "                 mask.masked_fill_(~sliding_mask[None, None, ...], float(\"-inf\"))\n",
    "             if n_meta_tokens > 0:\n",
    "                 mask[..., n_meta_tokens:, :n_meta_tokens] = 0\n",
    "             return mask.to(torch.float32)\n",
    "        return None\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor, use_cache: bool = False, return_kv_cache: bool = False):\n",
    "        batch_size, seq_len = tokens.shape\n",
    "        is_decoding = use_cache and tokens.shape[1] == 1\n",
    "        \n",
    "        h = self.embedding(tokens)\n",
    "        \n",
    "        current_seq_len = seq_len\n",
    "        if self.config['use_meta_tokens'] and not is_decoding:\n",
    "            meta_embeds = self.meta_tokens.expand(batch_size, -1, -1)\n",
    "            h = torch.cat([meta_embeds, h], dim=1)\n",
    "            current_seq_len += self.config['n_meta_tokens']\n",
    "            \n",
    "        kv_cache_list = [None] * self.config['n_layers']\n",
    "        \n",
    "        attn_mask = None\n",
    "        if not self.config['use_mamba_only'] and not is_decoding:\n",
    "            attn_mask = self._create_attention_mask(\n",
    "                q_len=current_seq_len, kv_len=current_seq_len,\n",
    "                window_size=self.config['window_size'] if self.config['use_swa'] else -1,\n",
    "                n_meta_tokens=self.config['n_meta_tokens'] if self.config['use_meta_tokens'] else 0,\n",
    "                device=h.device\n",
    "            )\n",
    "        \n",
    "        residual = h\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            past_kv = kv_cache_list[i] if use_cache and is_decoding else None\n",
    "            if use_cache and is_decoding and i > 0 and self.config['use_shared_kv_cache']:\n",
    "                 past_kv = kv_cache_list[i-1]\n",
    "            \n",
    "            output, present_kv = layer(h, past_kv=past_kv, attn_mask=attn_mask, use_cache=use_cache)\n",
    "            h = residual + output\n",
    "            residual = h\n",
    "\n",
    "            if use_cache:\n",
    "                kv_cache_list[i] = present_kv\n",
    "\n",
    "        h = self.norm(h)\n",
    "        \n",
    "        if self.config['use_meta_tokens'] and not is_decoding:\n",
    "            h = h[:, self.config['n_meta_tokens']:]\n",
    "            \n",
    "        logits = self.lm_head(h)\n",
    "        \n",
    "        if return_kv_cache:\n",
    "            return logits, kv_cache_list\n",
    "        return logits\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4. 단계별 모델링 및 성능 측정\n",
    "# -----------------------------------------------------------------------------\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if not torch.cuda.is_available() and selective_scan_fn is not None:\n",
    "        print(\"경고: 이 코드는 mamba-ssm의 CUDA 커널에 의존하므로 GPU 환경에서 실행해야 합니다.\")\n",
    "\n",
    "    base_config = {\n",
    "        'vocab_size': 32000, 'd_model': 256, 'n_layers': 4,\n",
    "        'n_heads': 8, 'n_kv_heads': 2, 'max_seq_len': 2048,\n",
    "        'ffn_hidden_dim': 256 * 4, 'window_size': 256, 'n_meta_tokens': 4,\n",
    "        'mamba_params': {'d_state': 16, 'd_conv': 4, 'expand': 2, 'dt_rank': math.ceil((256 * 2) / 16)},\n",
    "    }\n",
    "\n",
    "    ablation_stages = [\n",
    "        (\"1_Mamba_Only_Baseline\",     {'use_mamba_only': True, 'use_ssm_head': False, 'use_meta_tokens': False, 'use_shared_kv_cache': False, 'use_swa': False}),\n",
    "        (\"2_+_Attention (Hymba)\",     {'use_mamba_only': False, 'use_ssm_head': True, 'use_meta_tokens': False, 'use_shared_kv_cache': False, 'use_swa': False}),\n",
    "        (\"3_+_Meta_Tokens\",           {'use_mamba_only': False, 'use_ssm_head': True, 'use_meta_tokens': True,  'use_shared_kv_cache': False, 'use_swa': False}),\n",
    "        (\"4_+_Shared_KV_Cache\",       {'use_mamba_only': False, 'use_ssm_head': True, 'use_meta_tokens': True,  'use_shared_kv_cache': True,  'use_swa': False}),\n",
    "        (\"5_+_SWA\",                   {'use_mamba_only': False, 'use_ssm_head': True, 'use_meta_tokens': True,  'use_shared_kv_cache': True,  'use_swa': True}),\n",
    "    ]\n",
    "    \n",
    "    batch_size = 2\n",
    "    seq_len = 512\n",
    "    \n",
    "    for name, flags in ablation_stages:\n",
    "        print(f\"\\n{'='*20} {name} {'='*20}\")\n",
    "        config = base_config.copy()\n",
    "        config.update(flags)\n",
    "        \n",
    "        try:\n",
    "            model = HymbaModel(config).to(device)\n",
    "            model.eval()\n",
    "            \n",
    "            dummy_input = torch.randint(0, config['vocab_size'], (batch_size, seq_len)).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits, kv_cache_list = model(dummy_input, use_cache=True, return_kv_cache=True)\n",
    "            \n",
    "            total_cache_bytes = 0\n",
    "            if not config['use_mamba_only']:\n",
    "                if config['use_shared_kv_cache']:\n",
    "                    for i in range(1, config['n_layers'], 2):\n",
    "                         cache = kv_cache_list[i]\n",
    "                         if cache is not None:\n",
    "                            k, v = cache\n",
    "                            total_cache_bytes += k.numel() * k.element_size() + v.numel() * v.element_size()\n",
    "                else:\n",
    "                    for cache in kv_cache_list:\n",
    "                        if cache is not None:\n",
    "                            k, v = cache\n",
    "                            total_cache_bytes += k.numel() * k.element_size() + v.numel() * v.element_size()\n",
    "\n",
    "            total_cache_mb = total_cache_bytes / (1024 * 1024)\n",
    "            print(f\"--- KV Cache 크기: {total_cache_mb:.2f} MB\")\n",
    "\n",
    "            warmup_iterations = 5\n",
    "            measurement_iterations = 10\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for _ in range(warmup_iterations):\n",
    "                    _ = model(dummy_input, use_cache=False)\n",
    "                if torch.cuda.is_available(): torch.cuda.synchronize()\n",
    "\n",
    "                start_time = time.time()\n",
    "                for _ in range(measurement_iterations):\n",
    "                    _ = model(dummy_input, use_cache=False)\n",
    "                if torch.cuda.is_available(): torch.cuda.synchronize()\n",
    "                end_time = time.time()\n",
    "\n",
    "            total_time = end_time - start_time\n",
    "            total_tokens = batch_size * seq_len * measurement_iterations\n",
    "            tokens_per_second = total_tokens / total_time\n",
    "            print(f\"--- 처리량: {tokens_per_second:,.2f} tokens/sec\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"오류 발생: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
