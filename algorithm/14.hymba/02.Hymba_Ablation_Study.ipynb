{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Hymba Ablation Study\n\n## 내 구현 vs 공식 구현 스타일 비교 (arXiv:2411.13676)\n\n이 노트북은 **내 구현**과 **공식 구현 스타일**의 Hymba 모델을 비교하여 논문의 핵심 주장을 검증합니다.\n\n### 구현 비교\n\n| 구성요소 | 내 구현 | 공식 구현 스타일 |\n|----------|---------|------------------|\n| **Projection** | 분리된 Q, K, V projection | 단일 in_proj (모든 것 포함) |\n| **Fusion** | β_attn * norm(A) + β_mamba * norm(M) | **(norm(A) + norm(M)) / 2** |\n| **KV Sharing** | Producer-Consumer 패턴 | Producer-Consumer 패턴 |\n| **Attention** | FlexAttention / Standard | Standard attention |\n| **SWA Mask** | causal AND (window OR meta) | causal AND (window OR meta) |\n\n### Sliding Window Attention (SWA)\n\n```\nSWA 마스크 조건 (Local 레이어):\n1. Causal: k <= q (과거만 attend)\n2. Window: k >= q - window + 1 (윈도우 내)\n3. Meta: k < num_meta (meta 토큰은 항상 attend 가능)\n\n최종: causal AND (in_window OR is_meta)\n```\n\n### KV Sharing (Producer-Consumer 패턴)\n\n```\n공식 config (Hymba-1.5B-Base):\n\"kv_reuse_group\": [[1,2], [3,4], [5,6], ..., [29,30]]\n```\n\n- **Producer 레이어** (1, 3, 5, ...): K, V projection 수행\n- **Consumer 레이어** (2, 4, 6, ...): **Q만 계산**, K/V는 producer에서 재사용\n- Global 레이어 (0, 15, 31)는 KV 공유에서 제외\n\n### 실험 목표\n\n1. **아키텍처 비교**: Mamba-only vs Transformer-only vs Hybrid\n2. **구현 비교**: 내 구현 vs 공식 구현 스타일\n3. **컴포넌트별 기여도**: Meta Tokens, KV Sharing, SWA\n4. **학습 안정성**: Loss 수렴 비교"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport os\nimport warnings\nsys.path.append('./backbone')\n\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\nwarnings.filterwarnings('ignore')\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport time\nfrom tqdm.auto import tqdm\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Optional\n\n# 모듈 리로드\nimport importlib\nif 'hymba' in sys.modules:\n    importlib.reload(sys.modules['hymba'])\nif 'hymba_official' in sys.modules:\n    importlib.reload(sys.modules['hymba_official'])\n\n# 내 구현\nfrom hymba import Hymba, HymbaConfig, ArchType, AttentionType\n\n# 공식 구현 스타일\nfrom hymba_official import HymbaOfficialModel, HymbaOfficialConfig\n\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader, TensorDataset, random_split\n\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import BPE\nfrom tokenizers.trainers import BpeTrainer\nfrom tokenizers.pre_tokenizers import ByteLevel\nfrom tokenizers.processors import ByteLevel as ByteLevelProcessor\n\nRESULTS_DIR = './results'\nos.makedirs(RESULTS_DIR, exist_ok=True)\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f'Device: {device}')\nif device == 'cuda':\n    print(f'GPU: {torch.cuda.get_device_name()}')\n\nplt.rcParams['figure.figsize'] = (14, 8)\nplt.rcParams['figure.dpi'] = 150\nsns.set_style('whitegrid')\n\ntorch.manual_seed(42)\nnp.random.seed(42)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "데이터셋 로드: Tiny Shakespeare\n",
      "============================================================\n",
      "텍스트 길이: 1,003,854 문자\n",
      "샘플:\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us...\n"
     ]
    }
   ],
   "source": [
    "print('=' * 60)\n",
    "print('데이터셋 로드: Tiny Shakespeare')\n",
    "print('=' * 60)\n",
    "\n",
    "ds = load_dataset('karpathy/tiny_shakespeare')\n",
    "text = '\\n\\n'.join(ds['train']['text'])\n",
    "\n",
    "print(f'텍스트 길이: {len(text):,} 문자')\n",
    "print(f'샘플:\\n{text[:300]}...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPE 토크나이저 학습 중...\n",
      "\n",
      "\n",
      "\n",
      "어휘 크기: 4,096\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 4096\n",
    "\n",
    "print('BPE 토크나이저 학습 중...')\n",
    "tokenizer_bpe = Tokenizer(BPE(unk_token='<unk>'))\n",
    "tokenizer_bpe.pre_tokenizer = ByteLevel(add_prefix_space=False)\n",
    "tokenizer_bpe.post_processor = ByteLevelProcessor(trim_offsets=False)\n",
    "\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    special_tokens=['<unk>', '<pad>', '<bos>', '<eos>'],\n",
    "    min_frequency=2,\n",
    ")\n",
    "tokenizer_bpe.train_from_iterator([text], trainer=trainer)\n",
    "\n",
    "class TokenizerWrapper:\n",
    "    def __init__(self, tk):\n",
    "        self._tk = tk\n",
    "        self._vocab_size = tk.get_vocab_size()\n",
    "    \n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        return self._tk.encode(text).ids\n",
    "    \n",
    "    def decode(self, ids: List[int]) -> str:\n",
    "        return self._tk.decode(ids)\n",
    "    \n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        return self._vocab_size\n",
    "\n",
    "tokenizer = TokenizerWrapper(tokenizer_bpe)\n",
    "print(f'어휘 크기: {tokenizer.vocab_size:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습: 1,074 샘플, 67 배치\n",
      "검증: 120 샘플\n"
     ]
    }
   ],
   "source": [
    "SEQ_LEN = 256\n",
    "BATCH_SIZE = 16\n",
    "TRAIN_RATIO = 0.9\n",
    "\n",
    "def create_dataset(tokenizer, text: str, seq_len: int) -> TensorDataset:\n",
    "    ids = np.array(tokenizer.encode(text), dtype=np.int64)\n",
    "    x, y = ids[:-1], ids[1:]\n",
    "    n_samples = len(y) // seq_len\n",
    "    n = n_samples * seq_len\n",
    "    X = torch.tensor(x[:n].reshape(n_samples, seq_len))\n",
    "    Y = torch.tensor(y[:n].reshape(n_samples, seq_len))\n",
    "    return TensorDataset(X, Y)\n",
    "\n",
    "full_dataset = create_dataset(tokenizer, text, SEQ_LEN)\n",
    "train_size = int(TRAIN_RATIO * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(\n",
    "    full_dataset, [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f'학습: {len(train_dataset):,} 샘플, {len(train_loader):,} 배치')\n",
    "print(f'검증: {len(val_dataset):,} 샘플')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. 실험 설정\n",
    "\n",
    "### 핵심 제약 조건 (공식 구현 기반)\n",
    "\n",
    "1. **Local attention 레이어는 2의 배수**: KV sharing을 위해 연속 2개가 쌍(producer-consumer)을 이룸\n",
    "2. **Global attention**: 첫/중간/마지막 레이어만\n",
    "3. **Mamba**: 레이어당 1개만 (num_mamba=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": "@dataclass\nclass ExperimentConfig:\n    name: str\n    config: any  # HymbaConfig or HymbaOfficialConfig\n    description: str\n    is_official: bool = False  # 공식 구현 여부\n\nCOMMON = {\n    'vocab_size': tokenizer.vocab_size,\n    'swa_window': 128,\n    'dropout': 0.1,\n}\n\n# 실험 설정\n# Local 레이어 수가 짝수가 되도록 n_layers와 global_attn_idx 조정\nexperiments: Dict[str, ExperimentConfig] = {\n    # ===================== 내 구현 =====================\n    # 1. Mamba-only baseline\n    'Mamba-only': ExperimentConfig(\n        name='Mamba-only',\n        config=HymbaConfig(\n            **COMMON,\n            d_model=384,\n            n_layers=8,\n            n_heads=6,\n            n_kv_heads=2,\n            arch_type=ArchType.MAMBA_ONLY,\n            use_meta_tokens=False,\n        ),\n        description='순수 Mamba SSM',\n        is_official=False,\n    ),\n    \n    # 2. Transformer-only baseline\n    'Transformer-only': ExperimentConfig(\n        name='Transformer-only',\n        config=HymbaConfig(\n            **COMMON,\n            d_model=384,\n            n_layers=8,\n            n_heads=6,\n            n_kv_heads=2,\n            arch_type=ArchType.TRANSFORMER_ONLY,\n            global_attn_idx=list(range(8)),  # 모두 Global\n            use_meta_tokens=False,\n        ),\n        description='순수 Transformer',\n        is_official=False,\n    ),\n    \n    # 3. Hybrid (내 구현)\n    # 11 layers: global=[0,5,10] -> local=[1,2,3,4,6,7,8,9] = 8개 (짝수) OK!\n    'Hybrid-Mine': ExperimentConfig(\n        name='Hybrid-Mine',\n        config=HymbaConfig(\n            **COMMON,\n            d_model=384,\n            n_layers=11,\n            n_heads=6,\n            n_kv_heads=2,\n            arch_type=ArchType.HYBRID,\n            global_attn_idx=[0, 5, 10],  # 첫/중간/마지막 -> Local: 8개\n            use_meta_tokens=True,\n            num_meta_tokens=64,\n        ),\n        description='Hybrid (내 구현: Attn+Mamba, KV sharing, Meta tokens)',\n        is_official=False,\n    ),\n    \n    # 4. Hybrid without Meta Tokens (ablation)\n    'Hybrid-NoMeta': ExperimentConfig(\n        name='Hybrid-NoMeta',\n        config=HymbaConfig(\n            **COMMON,\n            d_model=384,\n            n_layers=11,\n            n_heads=6,\n            n_kv_heads=2,\n            arch_type=ArchType.HYBRID,\n            global_attn_idx=[0, 5, 10],\n            use_meta_tokens=False,\n        ),\n        description='Hybrid without Meta Tokens',\n        is_official=False,\n    ),\n    \n    # 5. Hybrid without KV Sharing (모두 Global -> KV 공유 없음)\n    'Hybrid-NoKVShare': ExperimentConfig(\n        name='Hybrid-NoKVShare',\n        config=HymbaConfig(\n            **COMMON,\n            d_model=384,\n            n_layers=8,\n            n_heads=6,\n            n_kv_heads=2,\n            arch_type=ArchType.HYBRID,\n            global_attn_idx=list(range(8)),  # 모두 Global -> KV 공유 없음\n            use_meta_tokens=True,\n            num_meta_tokens=64,\n            kv_reuse_groups=[],  # 명시적 비활성화\n        ),\n        description='Hybrid without KV Sharing',\n        is_official=False,\n    ),\n    \n    # ===================== 공식 구현 스타일 =====================\n    # 6. Hybrid (공식 구현 스타일)\n    'Hybrid-Official': ExperimentConfig(\n        name='Hybrid-Official',\n        config=HymbaOfficialConfig(\n            vocab_size=tokenizer.vocab_size,\n            hidden_size=384,\n            num_hidden_layers=11,\n            num_attention_heads=6,\n            num_key_value_heads=2,\n            attn_hidden_size=384,\n            global_attn_idx=[0, 5, 10],  # 첫/중간/마지막\n            num_memory_tokens=64,\n            attn_window_size=128,  # SWA window\n            mamba_expand=2,\n            mamba_d_state=16,\n            mamba_d_conv=4,\n            intermediate_size=384 * 3,  # FFN\n            attention_dropout=0.1,\n        ),\n        description='Hybrid (공식 스타일: 단일 in_proj, avg fusion)',\n        is_official=True,\n    ),\n    \n    # 7. 공식 스타일 - Meta Tokens 없이\n    'Official-NoMeta': ExperimentConfig(\n        name='Official-NoMeta',\n        config=HymbaOfficialConfig(\n            vocab_size=tokenizer.vocab_size,\n            hidden_size=384,\n            num_hidden_layers=11,\n            num_attention_heads=6,\n            num_key_value_heads=2,\n            attn_hidden_size=384,\n            global_attn_idx=[0, 5, 10],\n            num_memory_tokens=0,  # Meta tokens 없음\n            attn_window_size=128,\n            mamba_expand=2,\n            mamba_d_state=16,\n            mamba_d_conv=4,\n            intermediate_size=384 * 3,\n            attention_dropout=0.1,\n        ),\n        description='공식 스타일 without Meta Tokens',\n        is_official=True,\n    ),\n}\n\n# 실험 정보 출력\nprint('=' * 100)\nprint('실험 설정 (내 구현 + 공식 구현 스타일)')\nprint('=' * 100)\nprint(f'{\"Name\":<20} {\"Impl\":>8} {\"Params\":>10} {\"Layers\":>7} {\"Meta\":>6} {\"SWA\":>6}')\nprint('-' * 100)\n\nfor name, exp in experiments.items():\n    cfg = exp.config\n    impl = 'Official' if exp.is_official else 'Mine'\n    \n    if exp.is_official:\n        model = HymbaOfficialModel(cfg)\n        params = model.count_parameters()['total']\n        meta = cfg.num_memory_tokens\n        swa = cfg.attn_window_size\n        layers = cfg.num_hidden_layers\n    else:\n        model = Hymba(cfg)\n        params = model.count_parameters()['total']\n        meta = cfg.num_meta_tokens if cfg.use_meta_tokens else 0\n        swa = cfg.swa_window\n        layers = cfg.n_layers\n    \n    print(f'{name:<20} {impl:>8} {params/1e6:>9.2f}M {layers:>7} {meta:>6} {swa:>6}')\n    del model\n    torch.cuda.empty_cache()\n\nprint('=' * 100)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainConfig:\n",
    "    epochs: int = 25\n",
    "    lr: float = 3e-4\n",
    "    warmup_steps: int = 100\n",
    "    weight_decay: float = 0.1\n",
    "    grad_clip: float = 1.0\n",
    "    label_smoothing: float = 0.1\n",
    "    eval_interval: int = 50\n",
    "\n",
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    config: TrainConfig,\n",
    "    model_name: str = '',\n",
    "):\n",
    "    model = model.to(device).train()\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config.lr,\n",
    "        betas=(0.9, 0.99),\n",
    "        weight_decay=config.weight_decay,\n",
    "    )\n",
    "    \n",
    "    total_steps = config.epochs * len(train_loader)\n",
    "    \n",
    "    def lr_lambda(step):\n",
    "        if step < config.warmup_steps:\n",
    "            return step / config.warmup_steps\n",
    "        progress = (step - config.warmup_steps) / max(1, total_steps - config.warmup_steps)\n",
    "        return 0.1 + 0.9 * 0.5 * (1 + np.cos(np.pi * progress))\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "    \n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_ppl': [], 'step': []}\n",
    "    step = 0\n",
    "    best_val_loss = float('inf')\n",
    "    t0 = time.time()\n",
    "    \n",
    "    for epoch in range(config.epochs):\n",
    "        model.train()\n",
    "        pbar = tqdm(train_loader, desc=f'[{model_name}] Epoch {epoch+1}/{config.epochs}', leave=False)\n",
    "        \n",
    "        for xb, yb in pbar:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            \n",
    "            out = model(xb, targets=yb)\n",
    "            logits = out['logits']\n",
    "            loss = F.cross_entropy(\n",
    "                logits.reshape(-1, logits.size(-1)),\n",
    "                yb.reshape(-1),\n",
    "                label_smoothing=config.label_smoothing\n",
    "            )\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scheduler.step()\n",
    "            step += 1\n",
    "            \n",
    "            if step % config.eval_interval == 0:\n",
    "                model.eval()\n",
    "                val_loss = 0.0\n",
    "                val_tokens = 0\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for vxb, vyb in val_loader:\n",
    "                        vxb, vyb = vxb.to(device), vyb.to(device)\n",
    "                        vout = model(vxb, targets=vyb)\n",
    "                        val_loss += F.cross_entropy(\n",
    "                            vout['logits'].reshape(-1, vout['logits'].size(-1)),\n",
    "                            vyb.reshape(-1)\n",
    "                        ).item() * vxb.numel()\n",
    "                        val_tokens += vxb.numel()\n",
    "                \n",
    "                val_loss /= val_tokens\n",
    "                history['train_loss'].append(loss.item())\n",
    "                history['val_loss'].append(val_loss)\n",
    "                history['val_ppl'].append(np.exp(val_loss))\n",
    "                history['step'].append(step)\n",
    "                \n",
    "                best_val_loss = min(best_val_loss, val_loss)\n",
    "                pbar.set_postfix({'loss': f'{loss.item():.3f}', 'val': f'{val_loss:.3f}', 'ppl': f'{np.exp(val_loss):.1f}'})\n",
    "                model.train()\n",
    "    \n",
    "    elapsed = time.time() - t0\n",
    "    return {\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'best_ppl': np.exp(best_val_loss),\n",
    "        'final_ppl': history['val_ppl'][-1] if history['val_ppl'] else np.exp(best_val_loss),\n",
    "        'time_min': elapsed / 60,\n",
    "        'history': history,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": "train_config = TrainConfig(epochs=25, lr=3e-4, warmup_steps=100, eval_interval=50)\nresults: Dict[str, Dict] = {}\n\nprint('\\n' + '=' * 80)\nprint('실험 시작 (내 구현 + 공식 구현 스타일)')\nprint('=' * 80)\n\nfor exp_name, exp_config in experiments.items():\n    print(f\"\\n{'='*70}\")\n    print(f\"실험: {exp_name}\")\n    print(f\"설명: {exp_config.description}\")\n    print(f\"구현: {'공식 스타일' if exp_config.is_official else '내 구현'}\")\n    print(f\"{'='*70}\")\n    \n    cfg = exp_config.config\n    \n    # 모델 생성 (공식 vs 내 구현)\n    if exp_config.is_official:\n        model = HymbaOfficialModel(cfg)\n        params = model.count_parameters()\n        arch_type = 'hybrid'\n    else:\n        model = Hymba(cfg)\n        params = model.count_parameters()\n        arch_type = cfg.arch_type.value\n    \n    print(f'Parameters: {params[\"total\"]/1e6:.2f}M')\n    \n    # KV sharing 정보 (내 구현만)\n    if not exp_config.is_official and arch_type != 'mamba':\n        kv_info = model.get_kv_sharing_info()\n        print(f'KV Reduction: {kv_info[\"reduction\"]:.2f}x')\n        print(f'Producer layers: {kv_info[\"producer_layers\"]}')\n        print(f'Consumer layers: {kv_info[\"consumer_layers\"]}')\n    \n    train_result = train_model(model, train_loader, val_loader, train_config, exp_name)\n    \n    results[exp_name] = {\n        'config': cfg,\n        'params': params['total'],\n        'best_ppl': train_result['best_ppl'],\n        'final_ppl': train_result['final_ppl'],\n        'time_min': train_result['time_min'],\n        'history': train_result['history'],\n        'is_official': exp_config.is_official,\n    }\n    \n    print(f'Best PPL: {train_result[\"best_ppl\"]:.2f}')\n    print(f'Time: {train_result[\"time_min\"]:.1f} min')\n    \n    del model\n    torch.cuda.empty_cache()\n\nprint('\\n' + '=' * 80)\nprint('모든 실험 완료!')\nprint('=' * 80)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. 결과 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": "df = pd.DataFrame([{\n    'Model': name,\n    'Implementation': 'Official' if r.get('is_official', False) else 'Mine',\n    'Params (M)': r['params'] / 1e6,\n    'Best PPL': r['best_ppl'],\n    'Final PPL': r['final_ppl'],\n    'Time (min)': r['time_min'],\n} for name, r in results.items()])\n\ndf_sorted = df.sort_values('Best PPL')\n\nprint('\\n' + '=' * 90)\nprint('결과 요약 (Best PPL 기준 정렬)')\nprint('=' * 90)\nprint(df_sorted.to_string(index=False))\nprint('=' * 90)\n\n# 내 구현 vs 공식 구현 비교\nprint('\\n' + '=' * 90)\nprint('구현 비교: 내 구현 vs 공식 구현')\nprint('=' * 90)\nmine_df = df_sorted[df_sorted['Implementation'] == 'Mine']\nofficial_df = df_sorted[df_sorted['Implementation'] == 'Official']\nprint(f\"\\n내 구현 평균 PPL: {mine_df['Best PPL'].mean():.2f}\")\nprint(f\"공식 구현 평균 PPL: {official_df['Best PPL'].mean():.2f}\")\n\nbest = df_sorted.iloc[0]\nprint(f'\\n최고 성능: {best[\"Model\"]} ({best[\"Implementation\"]}) - PPL: {best[\"Best PPL\"]:.2f}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n\n# 색상 맵 (내 구현: 따뜻한 색, 공식 구현: 차가운 색)\ncolors = {\n    # 내 구현\n    'Mamba-only': '#FF6B6B',\n    'Transformer-only': '#4ECDC4',\n    'Hybrid-Mine': '#45B7D1',\n    'Hybrid-NoMeta': '#DDA0DD',\n    'Hybrid-NoKVShare': '#98D8C8',\n    # 공식 구현\n    'Hybrid-Official': '#6B5B95',\n    'Official-NoMeta': '#88B04B',\n}\n\n# 1. PPL 비교 (구현별 구분)\nax = axes[0, 0]\nbars = ax.barh(df_sorted['Model'], df_sorted['Best PPL'], \n               color=[colors.get(m, 'gray') for m in df_sorted['Model']],\n               edgecolor=['black' if df_sorted[df_sorted['Model']==m]['Implementation'].values[0] == 'Official' else 'none' \n                         for m in df_sorted['Model']],\n               linewidth=2)\nax.set_xlabel('Best Validation PPL (lower is better)')\nax.set_title('Model Performance (Black border = Official)', fontweight='bold')\nax.invert_yaxis()\nfor bar, val in zip(bars, df_sorted['Best PPL']):\n    ax.text(val + 0.3, bar.get_y() + bar.get_height()/2, f'{val:.1f}', va='center')\n\n# 2. 학습 곡선 (구현별 선 스타일)\nax = axes[0, 1]\nfor name, r in results.items():\n    linestyle = '--' if r.get('is_official', False) else '-'\n    linewidth = 2.5 if r.get('is_official', False) else 2\n    ax.plot(r['history']['step'], r['history']['val_ppl'], \n            label=name, linewidth=linewidth, linestyle=linestyle,\n            color=colors.get(name, 'gray'))\nax.set_xlabel('Steps')\nax.set_ylabel('Validation PPL')\nax.set_title('Training Curves (Dashed = Official)', fontweight='bold')\nax.legend(fontsize=7, loc='upper right')\nax.grid(True, alpha=0.3)\nax.set_ylim(0, min(500, ax.get_ylim()[1]))\n\n# 3. 내 구현 vs 공식 구현 직접 비교\nax = axes[1, 0]\ncomparison_pairs = [\n    ('Hybrid-Mine', 'Hybrid-Official', 'Full Hybrid'),\n    ('Hybrid-NoMeta', 'Official-NoMeta', 'No Meta'),\n]\nx_pos = np.arange(len(comparison_pairs))\nwidth = 0.35\n\nmine_ppls = []\nofficial_ppls = []\nlabels = []\n\nfor mine_name, official_name, label in comparison_pairs:\n    mine_ppl = results.get(mine_name, {}).get('best_ppl', 0)\n    official_ppl = results.get(official_name, {}).get('best_ppl', 0)\n    mine_ppls.append(mine_ppl)\n    official_ppls.append(official_ppl)\n    labels.append(label)\n\nbars1 = ax.bar(x_pos - width/2, mine_ppls, width, label='내 구현', color='#45B7D1')\nbars2 = ax.bar(x_pos + width/2, official_ppls, width, label='공식 스타일', color='#6B5B95')\n\nax.set_ylabel('Best PPL')\nax.set_title('내 구현 vs 공식 구현 비교', fontweight='bold')\nax.set_xticks(x_pos)\nax.set_xticklabels(labels)\nax.legend()\n\n# 값 표시\nfor bar, val in zip(bars1, mine_ppls):\n    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, f'{val:.1f}', ha='center', fontsize=9)\nfor bar, val in zip(bars2, official_ppls):\n    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, f'{val:.1f}', ha='center', fontsize=9)\n\n# 4. Parameters vs PPL (구현별 마커)\nax = axes[1, 1]\nfor name in df['Model']:\n    row = df[df['Model'] == name].iloc[0]\n    marker = 's' if row['Implementation'] == 'Official' else 'o'\n    ax.scatter(row['Params (M)'], row['Best PPL'], \n               s=150, c=colors.get(name, 'gray'), label=name, \n               alpha=0.8, edgecolors='black', marker=marker, linewidths=1.5)\nax.set_xlabel('Parameters (M)')\nax.set_ylabel('Best PPL')\nax.set_title('Parameters vs Performance (Square = Official)', fontweight='bold')\nax.legend(fontsize=7, loc='upper right')\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig(f'{RESULTS_DIR}/ablation_results.png', dpi=300, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. KV Sharing 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": "print('=' * 70)\nprint('KV Sharing Structure 비교')\nprint('=' * 70)\n\n# 내 구현\nprint('\\n[내 구현: Hybrid-Mine]')\ncfg_mine = experiments['Hybrid-Mine'].config\nmodel_mine = Hymba(cfg_mine)\nkv_info = model_mine.get_kv_sharing_info()\nattn_info = model_mine.get_attention_pattern_info()\n\nprint(f\"Total layers: {kv_info['total_layers']}\")\nprint(f\"Global layers: {attn_info['global_layers']}\")\nprint(f\"Local layers: {attn_info['local_layers']}\")\nprint(f\"KV reuse groups: {kv_info['kv_reuse_groups']}\")\nprint(f\"Producer layers: {kv_info['producer_layers']}\")\nprint(f\"Consumer layers: {kv_info['consumer_layers']}\")\nprint(f\"KV cache reduction: {kv_info['reduction']:.2f}x\")\n\ndel model_mine\n\n# 공식 구현\nprint('\\n[공식 스타일: Hybrid-Official]')\ncfg_official = experiments['Hybrid-Official'].config\nmodel_official = HymbaOfficialModel(cfg_official)\n\nprint(f\"Total layers: {cfg_official.num_hidden_layers}\")\nprint(f\"Global layers: {cfg_official.global_attn_idx}\")\nlocal_layers = [i for i in range(cfg_official.num_hidden_layers) if i not in cfg_official.global_attn_idx]\nprint(f\"Local layers: {local_layers}\")\nprint(f\"KV reuse groups: {cfg_official.kv_reuse_group}\")\n\n# 레이어별 Global/Local 확인\nprint(f\"\\n레이어별 Attention 타입:\")\nfor i, layer in enumerate(model_official.layers):\n    is_global = layer.mamba.self_attn.is_global\n    attn_type = 'Global' if is_global else 'Local (SWA)'\n    print(f\"  Layer {i:2d}: {attn_type}\")\n\ndel model_official\ntorch.cuda.empty_cache()\n\nprint('\\n' + '=' * 70)\nprint('구현 차이점 요약')\nprint('=' * 70)\nprint(\"\"\"\n| 구성요소 | 내 구현 | 공식 구현 스타일 |\n|----------|---------|------------------|\n| Projection | 분리된 Q, K, V projection | 단일 in_proj (모든 것 포함) |\n| Fusion | 학습 가능한 β 가중치 | (norm(A) + norm(M)) / 2 |\n| Out projection | 분리된 경로 | 단일 out_proj |\n| Attention | FlexAttention 지원 | Standard attention |\n\"\"\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. 텍스트 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": "print('=' * 70)\nprint('텍스트 생성 비교 (학습되지 않은 초기화 모델)')\nprint('=' * 70)\n\nprompts = ['ROMEO:', 'First Citizen:']\n\n# 내 구현\nprint('\\n' + '='*50)\nprint('[내 구현]')\nprint('='*50)\nfor exp_name in ['Mamba-only', 'Transformer-only', 'Hybrid-Mine']:\n    cfg = experiments[exp_name].config\n    model = Hymba(cfg).to(device).eval()\n    \n    print(f\"\\n{exp_name}:\")\n    for prompt in prompts:\n        prompt_ids = torch.tensor([tokenizer.encode(prompt)]).to(device)\n        with torch.no_grad():\n            output = model.generate(prompt_ids, max_new_tokens=30, temperature=0.8, top_k=40)\n        text = tokenizer.decode(output[0].cpu().tolist())\n        print(f\"  {prompt} {text[len(prompt):][:60]}...\")\n    \n    del model\n    torch.cuda.empty_cache()\n\n# 공식 구현\nprint('\\n' + '='*50)\nprint('[공식 구현 스타일]')\nprint('='*50)\nfor exp_name in ['Hybrid-Official', 'Official-NoMeta']:\n    cfg = experiments[exp_name].config\n    model = HymbaOfficialModel(cfg).to(device).eval()\n    \n    print(f\"\\n{exp_name}:\")\n    for prompt in prompts:\n        prompt_ids = torch.tensor([tokenizer.encode(prompt)]).to(device)\n        with torch.no_grad():\n            output = model.generate(prompt_ids, max_new_tokens=30, temperature=0.8, top_k=40)\n        text = tokenizer.decode(output[0].cpu().tolist())\n        print(f\"  {prompt} {text[len(prompt):][:60]}...\")\n    \n    del model\n    torch.cuda.empty_cache()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. 결론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": "print('=' * 70)\nprint('실험 결론')\nprint('=' * 70)\n\nbest_model = df.loc[df['Best PPL'].idxmin()]\nprint(f\"\\n1. 최고 성능: {best_model['Model']} ({best_model['Implementation']})\")\nprint(f\"   PPL: {best_model['Best PPL']:.2f}\")\n\n# 아키텍처 비교\nprint(f\"\\n2. 아키텍처 비교:\")\nfor name in ['Mamba-only', 'Transformer-only', 'Hybrid-Mine', 'Hybrid-Official']:\n    ppl = results.get(name, {}).get('best_ppl', 0)\n    impl = '공식' if results.get(name, {}).get('is_official', False) else '내구현'\n    if ppl > 0:\n        print(f\"   - {name}: {ppl:.2f} ({impl})\")\n\n# 내 구현 vs 공식 구현\nprint(f\"\\n3. 내 구현 vs 공식 구현 비교:\")\nhybrid_mine = results.get('Hybrid-Mine', {}).get('best_ppl', 0)\nhybrid_official = results.get('Hybrid-Official', {}).get('best_ppl', 0)\nif hybrid_mine and hybrid_official:\n    diff = hybrid_official - hybrid_mine\n    better = '내 구현' if diff > 0 else '공식 구현'\n    print(f\"   - Hybrid-Mine: {hybrid_mine:.2f}\")\n    print(f\"   - Hybrid-Official: {hybrid_official:.2f}\")\n    print(f\"   - 차이: {abs(diff):.2f} ({better}이 더 좋음)\")\n\n# Ablation\nprint(f\"\\n4. Ablation Study (내 구현 기준):\")\nhybrid_nometa = results.get('Hybrid-NoMeta', {}).get('best_ppl', 0)\nhybrid_nokv = results.get('Hybrid-NoKVShare', {}).get('best_ppl', 0)\nif hybrid_mine and hybrid_nometa:\n    delta_meta = hybrid_nometa - hybrid_mine\n    print(f\"   - Meta Tokens 제거: {delta_meta:+.2f} PPL\")\nif hybrid_mine and hybrid_nokv:\n    delta_kv = hybrid_nokv - hybrid_mine\n    print(f\"   - KV Sharing 제거: {delta_kv:+.2f} PPL\")\n\n# 공식 구현 Ablation\nprint(f\"\\n5. Ablation Study (공식 구현 기준):\")\nofficial_nometa = results.get('Official-NoMeta', {}).get('best_ppl', 0)\nif hybrid_official and official_nometa:\n    delta_meta_off = official_nometa - hybrid_official\n    print(f\"   - Meta Tokens 제거: {delta_meta_off:+.2f} PPL\")\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"핵심 발견\")\nprint(\"=\" * 70)\nprint(\"\"\"\n1. 구현 비교:\n   - 내 구현: 분리된 projection, 학습 가능한 fusion weights\n   - 공식 스타일: 단일 in_proj, 단순 평균 fusion\n   - 두 구현 모두 SWA + Meta tokens + KV sharing 지원\n\n2. Sliding Window Attention (SWA):\n   - Global 레이어: 전체 causal attention\n   - Local 레이어: window + meta token access\n   - 마스크: causal AND (in_window OR is_meta)\n\n3. Cross-layer KV Sharing:\n   - Producer: K, V projection 수행\n   - Consumer: Q만 계산, K/V는 재사용\n   - 파라미터 및 메모리 절감\n\n4. Meta Tokens:\n   - Attention sink 문제 해결\n   - 모든 content 토큰이 meta에 attend 가능\n\"\"\")"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장: ./results/ablation_results.csv\n"
     ]
    }
   ],
   "source": [
    "df.to_csv(f'{RESULTS_DIR}/ablation_results.csv', index=False)\n",
    "print(f'저장: {RESULTS_DIR}/ablation_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "1. **Hymba Paper**: Dong, X., et al. \"Hymba: A Hybrid-head Architecture for Small Language Models.\" arXiv:2411.13676 (2024). [ICLR 2025]\n",
    "\n",
    "2. **Official Implementation**: https://github.com/NVlabs/hymba\n",
    "\n",
    "3. **HuggingFace Model**: https://huggingface.co/nvidia/Hymba-1.5B-Base\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}