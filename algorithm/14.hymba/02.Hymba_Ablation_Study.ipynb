{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hymba Ablation Study - Tiny Shakespeare\n",
    "\n",
    "## 목표\n",
    "다양한 아키텍처 구성의 성능을 비교하여 각 컴포넌트의 기여도를 분석합니다.\n",
    "\n",
    "### 테스트 구성\n",
    "1. **Mamba-only**: SSM 기반 시퀀스 모델링\n",
    "2. **Transformer-only**: 표준 어텐션 기반\n",
    "3. **Hybrid (Hymba)**: Attention + Mamba 혼합\n",
    "   - Global Attention: 첫/중간/마지막 레이어\n",
    "   - Local Attention (SWA): 나머지 레이어\n",
    "   - Meta Tokens: 64개\n",
    "   - KV-Cache 공유\n",
    "\n",
    "### 평가 메트릭\n",
    "- 학습 Loss & Perplexity\n",
    "- 검증 Loss & Perplexity\n",
    "- 학습 속도 (tokens/sec)\n",
    "- 추론 속도 (tokens/sec)\n",
    "- KV-Cache 메모리 절감\n",
    "- 생성 품질"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "sys.path.append('./backbone')\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import importlib\n",
    "if 'hymba' in sys.modules:\n",
    "    importlib.reload(sys.modules['hymba'])\n",
    "\n",
    "from hymba import Hymba, HymbaConfig, ArchType, AttentionType\n",
    "\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import Unigram\n",
    "from tokenizers.trainers import UnigramTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.normalizers import Sequence as NormSeq, NFKC, Lowercase\n",
    "\n",
    "RESULTS_DIR = './results'\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Device: {device}')\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 로딩 중...\n",
      "전체 텍스트 길이: 1,003,854 문자\n"
     ]
    }
   ],
   "source": [
    "print('데이터 로딩 중...')\n",
    "ds = load_dataset('karpathy/tiny_shakespeare')\n",
    "text = '\\n\\n'.join(ds['train']['text'])\n",
    "print(f'전체 텍스트 길이: {len(text):,} 문자')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토크나이저 학습 중...\n",
      "\n",
      "\n",
      "어휘 크기: 4000\n"
     ]
    }
   ],
   "source": [
    "print('토크나이저 학습 중...')\n",
    "vocab_size = 4000\n",
    "\n",
    "tk = Tokenizer(Unigram())\n",
    "tk.normalizer = NormSeq([NFKC(), Lowercase()])\n",
    "tk.pre_tokenizer = Whitespace()\n",
    "trainer = UnigramTrainer(vocab_size=vocab_size, special_tokens=['<|unk|>'], unk_token='<|unk|>')\n",
    "tk.train_from_iterator([text], trainer=trainer)\n",
    "\n",
    "class TokenizerWrap:\n",
    "    def __init__(self, tk):\n",
    "        self.tk = tk\n",
    "    def encode(self, s):\n",
    "        return self.tk.encode(s).ids\n",
    "    def decode(self, ids):\n",
    "        return self.tk.decode(ids)\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return self.tk.get_vocab_size()\n",
    "\n",
    "tokenizer = TokenizerWrap(tk)\n",
    "print(f'어휘 크기: {tokenizer.vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 배치: 64\n",
      "검증 배치: 8\n"
     ]
    }
   ],
   "source": [
    "def make_dataset(tok, text, seq_len=256):\n",
    "    ids = np.array(tok.encode(text), dtype=np.int64)\n",
    "    x, y = ids[:-1], ids[1:]\n",
    "    n = (len(y) // seq_len) * seq_len\n",
    "    X = torch.tensor(x[:n].reshape(-1, seq_len))\n",
    "    Y = torch.tensor(y[:n].reshape(-1, seq_len))\n",
    "    return TensorDataset(X, Y)\n",
    "\n",
    "seq_len = 256\n",
    "batch_size = 16\n",
    "\n",
    "ds_full = make_dataset(tokenizer, text, seq_len)\n",
    "tr_len = int(0.9 * len(ds_full))\n",
    "va_len = len(ds_full) - tr_len\n",
    "train_ds, val_ds = random_split(ds_full, [tr_len, va_len])\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f'학습 배치: {len(train_dl)}')\n",
    "print(f'검증 배치: {len(val_dl)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 모델 구성 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 파라미터 수 검증 (목표: ~50M) ===\n",
      "Mamba-only          : 46.35M params | d=448 | Attn=  0 | Mamba= 12\n",
      "Transformer-only    : 49.25M params | d=512 | Attn= 96 | Mamba=  0\n",
      "Hybrid (1:1)        : 39.69M params | d=384 | Attn= 96 | Mamba= 12\n",
      "Hybrid (2:1)        : 46.24M params | d=352 | Attn= 96 | Mamba= 24\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# 공정한 비교를 위해 파라미터 수를 ~50M으로 맞춤\n",
    "# 각 아키텍처의 특성에 맞게 d_model 조정\n",
    "\n",
    "common_params = {\n",
    "    'vocab_size': vocab_size,\n",
    "    'swa_window': 128,\n",
    "    'dropout': 0.1,\n",
    "    'seq_len': seq_len,\n",
    "}\n",
    "\n",
    "# 파라미터 수 목표: ~50M\n",
    "configs = {\n",
    "    # Mamba-only: d_model을 조정 (Mamba가 expand=2로 파라미터가 많음)\n",
    "    'Mamba-only': HymbaConfig(\n",
    "        **common_params,\n",
    "        d_model=448,\n",
    "        n_layers=12,\n",
    "        n_heads=8,\n",
    "        n_kv_heads=4,\n",
    "        arch_type=ArchType.MAMBA_ONLY,\n",
    "        mamba_heads_per_layer=1,\n",
    "        use_meta_tokens=False,\n",
    "        use_kv_sharing=False,\n",
    "    ),\n",
    "    # Transformer-only: 기준 모델 (49M)\n",
    "    'Transformer-only': HymbaConfig(\n",
    "        **common_params,\n",
    "        d_model=512,\n",
    "        n_layers=12,\n",
    "        n_heads=8,\n",
    "        n_kv_heads=4,\n",
    "        arch_type=ArchType.TRANSFORMER_ONLY,\n",
    "        global_attn_indices=list(range(12)),  # 모든 레이어 Global\n",
    "        use_meta_tokens=False,\n",
    "        use_kv_sharing=False,\n",
    "    ),\n",
    "    # Hybrid (1:1): Attn + Mamba 각 1개씩\n",
    "    'Hybrid (1:1)': HymbaConfig(\n",
    "        **common_params,\n",
    "        d_model=384,\n",
    "        n_layers=12,\n",
    "        n_heads=8,\n",
    "        n_kv_heads=4,\n",
    "        arch_type=ArchType.HYBRID,\n",
    "        mamba_heads_per_layer=1,\n",
    "        global_attn_indices=[0, 5, 11],  # 첫/중간/마지막 Global\n",
    "        use_meta_tokens=True,\n",
    "        num_meta_tokens=64,\n",
    "        use_kv_sharing=True,\n",
    "    ),\n",
    "    # Hybrid (2:1): Mamba 비중 높임\n",
    "    'Hybrid (2:1)': HymbaConfig(\n",
    "        **common_params,\n",
    "        d_model=352,\n",
    "        n_layers=12,\n",
    "        n_heads=8,\n",
    "        n_kv_heads=4,\n",
    "        arch_type=ArchType.HYBRID,\n",
    "        mamba_heads_per_layer=2,\n",
    "        global_attn_indices=[0, 5, 11],\n",
    "        use_meta_tokens=True,\n",
    "        num_meta_tokens=64,\n",
    "        use_kv_sharing=True,\n",
    "    ),\n",
    "}\n",
    "\n",
    "# 파라미터 수 검증\n",
    "print('=== 파라미터 수 검증 (목표: ~50M) ===')\n",
    "for name, cfg in configs.items():\n",
    "    model = Hymba(cfg)\n",
    "    params = model.count_parameters()['total']\n",
    "    layer_configs = cfg.get_layer_configs()\n",
    "    total_attn = sum(c[0] for c in layer_configs)\n",
    "    total_mamba = sum(c[1] for c in layer_configs)\n",
    "    print(f'{name:20s}: {params/1e6:5.2f}M params | d={cfg.d_model:3d} | Attn={total_attn:3d} | Mamba={total_mamba:3d}')\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "print('='*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 학습 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dl, val_dl, epochs=30, lr=3e-4, warmup_steps=200, eval_interval=32, device='cuda'):\n",
    "    \"\"\"모델 학습 함수\"\"\"\n",
    "    model = model.to(device).train()\n",
    "    \n",
    "    # Optimizer: AdamW with weight decay\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, betas=(0.9, 0.95), weight_decay=0.1)\n",
    "    total_steps = epochs * len(train_dl)\n",
    "    \n",
    "    # Learning rate schedule: warmup + cosine decay\n",
    "    def lr_schedule(step):\n",
    "        if step < warmup_steps:\n",
    "            return step / warmup_steps\n",
    "        progress = (step - warmup_steps) / max(1, total_steps - warmup_steps)\n",
    "        return 0.5 * (1 + np.cos(np.pi * progress))\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_schedule)\n",
    "    \n",
    "    # Mixed precision training\n",
    "    use_amp = device == 'cuda' and torch.cuda.is_bf16_supported()\n",
    "    scaler = torch.amp.GradScaler('cuda') if use_amp else None\n",
    "    \n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_ppl': [], 'lr': [], 'step': []}\n",
    "    step, t0, best_val_loss = 0, time.time(), float('inf')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss, epoch_tokens = 0.0, 0\n",
    "        pbar = tqdm(train_dl, desc=f'Epoch {epoch+1}/{epochs}', leave=False)\n",
    "        \n",
    "        for xb, yb in pbar:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            \n",
    "            if use_amp:\n",
    "                with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "                    out = model(xb, targets=yb)\n",
    "                    loss = out['loss']\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                out = model(xb, targets=yb)\n",
    "                loss = out['loss']\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "            \n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scheduler.step()\n",
    "            epoch_loss += loss.item() * xb.numel()\n",
    "            epoch_tokens += xb.numel()\n",
    "            step += 1\n",
    "            \n",
    "            # Evaluation\n",
    "            if step % eval_interval == 0:\n",
    "                model.eval()\n",
    "                val_loss, val_tokens = 0.0, 0\n",
    "                with torch.no_grad():\n",
    "                    for vxb, vyb in val_dl:\n",
    "                        vxb, vyb = vxb.to(device), vyb.to(device)\n",
    "                        if use_amp:\n",
    "                            with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "                                vout = model(vxb, targets=vyb)\n",
    "                        else:\n",
    "                            vout = model(vxb, targets=vyb)\n",
    "                        val_loss += vout['loss'].item() * vxb.numel()\n",
    "                        val_tokens += vxb.numel()\n",
    "                \n",
    "                val_loss /= val_tokens\n",
    "                train_loss = epoch_loss / epoch_tokens\n",
    "                history['train_loss'].append(train_loss)\n",
    "                history['val_loss'].append(val_loss)\n",
    "                history['val_ppl'].append(np.exp(val_loss))\n",
    "                history['lr'].append(scheduler.get_last_lr()[0])\n",
    "                history['step'].append(step)\n",
    "                best_val_loss = min(best_val_loss, val_loss)\n",
    "                pbar.set_postfix({'loss': f'{train_loss:.3f}', 'val': f'{val_loss:.3f}', 'ppl': f'{np.exp(val_loss):.1f}'})\n",
    "                model.train()\n",
    "    \n",
    "    elapsed = time.time() - t0\n",
    "    tps = int(epochs * len(train_dl) * batch_size * seq_len / elapsed)\n",
    "    \n",
    "    return {\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'best_val_ppl': np.exp(best_val_loss),\n",
    "        'final_val_ppl': history['val_ppl'][-1] if history['val_ppl'] else np.exp(best_val_loss),\n",
    "        'train_tps': tps,\n",
    "        'time_min': elapsed / 60,\n",
    "        'history': history,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_inference(model, tokenizer, prompt='ROMEO:', max_tokens=100, n_runs=3, device='cuda'):\n",
    "    model = model.to(device).eval()\n",
    "    prompt_tensor = torch.tensor([tokenizer.encode(prompt)]).to(device)\n",
    "    times = []\n",
    "    for _ in range(n_runs):\n",
    "        torch.cuda.synchronize() if device == 'cuda' else None\n",
    "        t0 = time.time()\n",
    "        with torch.no_grad():\n",
    "            _ = model.generate(prompt_tensor, max_new_tokens=max_tokens, temperature=1.0)\n",
    "        torch.cuda.synchronize() if device == 'cuda' else None\n",
    "        times.append(time.time() - t0)\n",
    "    return {'tokens_per_sec': max_tokens / np.mean(times)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Model: Mamba-only\n",
      "============================================================\n",
      "Parameters: 46.35M | d_model: 448\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d7717c1c337423cbfb54601f5ffaa4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4adb5ef9571e4eaf8c77be13c39b2528",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1be528d0a80d4b6b9263fcebe08a3f88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8bae980c5784e89a7b9d67e999cef1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98e2d88f4a1d43fb97a60a98ad0cd6ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e97be5c35a5499882835672c0ea1007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca1528c967b84fe38f0ac30fd49f2c83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3369b9258fbe418cac75af339c76acac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f01578a28f44cbbb1c006ea5f3db08c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46aa94a73c0e483586c12e5cad108432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "932d0e897439477e9614e241cf320d3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f45dd5d1047e46d69126a4c4bd33bfd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c51963fc3c5d411fbfc5760e4725940f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bc5284022784798af79845cb79f43b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d584bd97d7294359b1af6f2fda7bb35d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "638c80419f1f40a28529e8c7ef1ee1aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 16/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4b5982e8ed54b5bbd376bbe72e5c545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 17/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf042fd50160497c8f783e067ab4f49c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 18/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5485758ca764c7a98f3bdc64da927fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 19/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "542d28d328db4fcdb29db5279e8f12b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 20/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14402d764dc54139aa99e1bf7a6729e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 21/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db8b0b971c7e4e6b9c6a9caa110239f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 22/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89732c510ecb4f3fa38e64cfbe49f229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 23/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30a53558240c4ecb9917b3187db38bad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 24/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "046f90f5d0b2428684f053505eac0e48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 25/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6687225dc9ad4c56bf1e63d176dbdaca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 26/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d00e7af4e55c4b6b8a5f2ab474e332ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 27/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8855caf2f704d2c8c7e48ff49885789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 28/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7adeff11a7c4081b691d27cbdb9ab5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 29/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8995dec6d3204801b8389dc838b5b315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 30/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best PPL: 74.19\n",
      "Inference: 108.5 tok/s\n",
      "\n",
      "============================================================\n",
      "Model: Transformer-only\n",
      "============================================================\n",
      "Parameters: 49.25M | d_model: 512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96af3231db3b45b3a3f52581a650846b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5e20c166d514a9a8c5cb9fbe8a4dcda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f65879448ea34466990fa4cd67fe2c71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "143b367ee4d24093b5f4542793baceb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5ab0be5256d46df8ca35d2bf16a4f38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "817ba1f3094b4a7d8fc8d4e644f9a727",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e857d9a950944e9b007ef8d2839be06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b640e3331b274986b91c6e580899e9d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b19fdbd311084feaa7d28b3ba6829a60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af0787c18aa3433ba4d697147dfcfe10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac4a56153e6a4c71a5b15f2f860f3f7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c390977a9aa43a49f1b717a2c2a3bad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "747d125844bf4b38ac92b1377541f3e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37fbbc7eb2524ef0bb099e10c27392ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f64e4df6336b4ed09406ceebfa0c5b92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d137ed0f753e4cac96aa0afcee1b4b3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 16/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "877856464f84424bbab858673f9471b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 17/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a1a75a8550a486caaee0f914722d370",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 18/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18869aab6ad54744bb24c399ca735029",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 19/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56437585da5a4b4fbd7beb8049aef87b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 20/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9e5e2e915134f4891db6b9ff94abd1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 21/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2eac8f65e7e4652a3e8734ebb18e101",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 22/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "154dd6443d844beb89643d9ed6146dac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 23/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f695847cbfc4f29ae53e3d20ddc0802",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 24/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "498cd62befc647109b879400963189fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 25/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fef9b7aab9a43599ae6971a49d9b091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 26/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9446822f7b934b4e81d6f635bb9117d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 27/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9296ddb5e5c4977817c11cec59d6575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 28/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da74ce70f9d245278e08ebb7d4aca2e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 29/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "830608e3a34848a69f02de3696f223b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 30/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best PPL: 78.61\n",
      "Inference: 71.8 tok/s\n",
      "\n",
      "============================================================\n",
      "Model: Hybrid (1:1)\n",
      "============================================================\n",
      "Parameters: 39.69M | d_model: 384\n",
      "KV Reduction: 1.50x\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7af7d7d2a579420cb6f6d649e58c6fea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1209 20:56:49.421000 125859 torch/fx/experimental/symbolic_shapes.py:6823] [0/2] _maybe_guard_rel() was called on non-relation expression Eq(s4, 1) | Eq(s5, s4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bf7c2c2bed44b06947aa37d6ad99980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbc90afbd1654dfda5e3ffdc67a85e31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16b3ba8cb4e24739a1897cdd53a5ba3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb30128e9a7841a79964736735c21a74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "822da447c077476ea3ab48277150922a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08efe3ac91774fb2be23b4e098615d9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fc42251506f40eb8a90c7bf33585a0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0201d97ee3d04e69a69e810dfdd3dc24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78361a22903c4b128514600be6478959",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "470f0c7ea02a4dcbaa832102c6a08e29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11afa1d8be4b46cdb1b913b838b514cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "128d438e93a4415e88e46b323152761d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85957a3603fa4e7eaaea3ce635f3997a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e632cf7fdb4a42fca191cfb0fd875e5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61334cb011c642d8867e4f0fd56d36b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 16/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efa62649eea34da8a7cec2fb3b16a540",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 17/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30748a0ead2240c1a36f4a89fd8eee44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 18/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b49c50f0ef454cecb3fab47ce61180df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 19/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97b3b2fd314649dfbea0bb422cc4c3b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 20/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "927a3e8f66124c659ed4f2d788128e88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 21/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1639044d57b45118466c30a92be2cc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 22/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a826444c65754f348409c3d0553c5773",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 23/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d1d3730cd71443daeabd95200082cc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 24/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61afb722d239478a9c12d35214c064c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 25/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88c072dcf9494267b7125bb423dca143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 26/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "281b7ff1ad974d2982b97545b5cfb7e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 27/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4728472d522a497bb6fa8f9cd4a9dfbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 28/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c4b95d5baa14fcd8b666b42f2b735be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 29/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54fc84d824864019b405e22412da2254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 30/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "InductorError",
     "evalue": "SubprocException: An exception occurred in a subprocess:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/triton/language/core.py\", line 76, in wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/triton/language/core.py\", line 42, in wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/triton/language/core.py\", line 1935, in reshape\n    shape = _shape_check_impl(_unwrap_iterable(shape))\n  File \"/usr/local/lib/python3.10/dist-packages/triton/language/core.py\", line 1677, in _shape_check_impl\n    validate_block_shape(shape)\n  File \"/usr/local/lib/python3.10/dist-packages/triton/_utils.py\", line 52, in validate_block_shape\n    raise ValueError(f\"Shape element {i} must be a power of 2\")\nValueError: Shape element 2 must be a power of 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_worker/subproc_pool.py\", line 340, in do_job\n    result = job()\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/compile_tasks.py\", line 62, in _worker_compile_triton\n    kernel.precompile(warm_cache_only=True)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 399, in precompile\n    self._precompile_worker()\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 430, in _precompile_worker\n    compile_results.append(self._precompile_config(c))\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 713, in _precompile_config\n    binary = triton.compile(*compile_args, **compile_kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/triton/compiler/compiler.py\", line 339, in compile\n    module = src.make_ir(options, codegen_fns, module_map, context)\n  File \"/usr/local/lib/python3.10/dist-packages/triton/compiler/compiler.py\", line 83, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 283:10:\n        tl.store(M_block_ptr, m_i, boundary_check=(1,))\n        tl.store(L_block_ptr, l_i, boundary_check=(1,))\n\n    # -- store output\n    idx_z = off_z\n    idx_t = off_t\n    idx_hq = off_hkv*G + off_g[:, None, None]\n    idx_m = off_m[None, :, None]\n    idx_d = offs_vd[None, None, :]\n\n    mask = (idx_m < Q_LEN) & (idx_d < V_HEAD_DIM)\n    acc = acc.reshape(G, BLOCK_M_PER_HQ, V_HEAD_DIM)\n          ^\n\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInductorError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m train_results \u001b[38;5;241m=\u001b[39m train_model(model, train_dl, val_dl, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3e-4\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# 추론 벤치마크\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m infer_results \u001b[38;5;241m=\u001b[39m \u001b[43mbenchmark_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# 샘플 생성\u001b[39;00m\n\u001b[1;32m     27\u001b[0m samples \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m, in \u001b[0;36mbenchmark_inference\u001b[0;34m(model, tokenizer, prompt, max_tokens, n_runs, device)\u001b[0m\n\u001b[1;32m      7\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 9\u001b[0m     _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize() \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     11\u001b[0m times\u001b[38;5;241m.\u001b[39mappend(time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/qscar/research/algorithm/14.hymba/./backbone/hymba.py:650\u001b[0m, in \u001b[0;36mHymba.generate\u001b[0;34m(self, idx, max_new_tokens, temperature, top_k)\u001b[0m\n\u001b[1;32m    648\u001b[0m owner \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mowner[i]\n\u001b[1;32m    649\u001b[0m cache \u001b[38;5;241m=\u001b[39m kv_caches\u001b[38;5;241m.\u001b[39mget(owner)\n\u001b[0;32m--> 650\u001b[0m h, new_cache, _ \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_attn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m owner \u001b[38;5;129;01mand\u001b[39;00m new_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    652\u001b[0m     kv_caches[owner] \u001b[38;5;241m=\u001b[39m new_cache\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/workspace/qscar/research/algorithm/14.hymba/./backbone/hymba.py:396\u001b[0m, in \u001b[0;36mHymbaBlock.forward\u001b[0;34m(self, x, kv_cache, return_attn)\u001b[0m\n\u001b[1;32m    393\u001b[0m         y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmamba_proj(torch\u001b[38;5;241m.\u001b[39mcat(mamba_outs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Hybrid\u001b[39;00m\n\u001b[0;32m--> 396\u001b[0m     attn_out, new_cache, attn_w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_attn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_attn:\n\u001b[1;32m    398\u001b[0m         attn_weights \u001b[38;5;241m=\u001b[39m attn_w\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/workspace/qscar/research/algorithm/14.hymba/./backbone/hymba.py:243\u001b[0m, in \u001b[0;36mTransformerAttention.forward\u001b[0;34m(self, x, kv_cache, return_attn)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_flex \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_attn:\n\u001b[1;32m    242\u001b[0m     block_mask \u001b[38;5;241m=\u001b[39m create_block_mask(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflex_mask, B\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, H\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, Q_LEN\u001b[38;5;241m=\u001b[39mT, KV_LEN\u001b[38;5;241m=\u001b[39mTk)\n\u001b[0;32m--> 243\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflex_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_full\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv_full\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(B, T, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mH \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mDh)\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m HAS_FLASH_ATTN \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_attn \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_meta \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_type \u001b[38;5;241m==\u001b[39m AttentionType\u001b[38;5;241m.\u001b[39mGLOBAL:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:749\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>.compile_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    745\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__cause__\u001b[39;00m  \u001b[38;5;66;03m# User compiler error\u001b[39;00m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ShortenTraceback \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;66;03m# Failures in the backend likely don't have useful\u001b[39;00m\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;66;03m# data in the TorchDynamo frames, so we strip them out.\u001b[39;00m\n\u001b[0;32m--> 749\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mremove_dynamo_frames() \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# see TORCHDYNAMO_VERBOSE=1\u001b[39;00m\n\u001b[1;32m    750\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    751\u001b[0m     \u001b[38;5;66;03m# Restore the dynamic layer stack depth if necessary.\u001b[39;00m\n\u001b[1;32m    752\u001b[0m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py:923\u001b[0m, in \u001b[0;36m_compile_fx_inner\u001b[0;34m(gm, example_inputs, **graph_kwargs)\u001b[0m\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 923\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InductorError(e, currentframe())\u001b[38;5;241m.\u001b[39mwith_traceback(\n\u001b[1;32m    924\u001b[0m         e\u001b[38;5;241m.\u001b[39m__traceback__\n\u001b[1;32m    925\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    927\u001b[0m     TritonBundler\u001b[38;5;241m.\u001b[39mend_compile()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py:907\u001b[0m, in \u001b[0;36m_compile_fx_inner\u001b[0;34m(gm, example_inputs, **graph_kwargs)\u001b[0m\n\u001b[1;32m    905\u001b[0m TritonBundler\u001b[38;5;241m.\u001b[39mbegin_compile()\n\u001b[1;32m    906\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 907\u001b[0m     mb_compiled_graph \u001b[38;5;241m=\u001b[39m \u001b[43mfx_codegen_and_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_to_check\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgraph_kwargs\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    910\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m mb_compiled_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    911\u001b[0m     mb_compiled_graph\u001b[38;5;241m.\u001b[39m_time_taken_ns \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime_ns() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py:1578\u001b[0m, in \u001b[0;36mfx_codegen_and_compile\u001b[0;34m(gm, example_inputs, inputs_to_check, **graph_kwargs)\u001b[0m\n\u001b[1;32m   1573\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(scheme, _OutOfProcessFxCompile), (\n\u001b[1;32m   1574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masync is only valid with an out-of-process compile mode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1575\u001b[0m     )\n\u001b[1;32m   1576\u001b[0m     scheme \u001b[38;5;241m=\u001b[39m _AsyncFxCompile(scheme)\n\u001b[0;32m-> 1578\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscheme\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcodegen_and_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_to_check\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py:1456\u001b[0m, in \u001b[0;36m_InProcessFxCompile.codegen_and_compile\u001b[0;34m(self, gm, example_inputs, inputs_to_check, graph_kwargs)\u001b[0m\n\u001b[1;32m   1438\u001b[0m         compiled_fn \u001b[38;5;241m=\u001b[39m AotCodeCompiler\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m   1439\u001b[0m             graph,\n\u001b[1;32m   1440\u001b[0m             wrapper_code\u001b[38;5;241m.\u001b[39mvalue,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1453\u001b[0m             ],\n\u001b[1;32m   1454\u001b[0m         )\n\u001b[1;32m   1455\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1456\u001b[0m     compiled_module \u001b[38;5;241m=\u001b[39m \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_to_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1457\u001b[0m     compiled_fn \u001b[38;5;241m=\u001b[39m compiled_module\u001b[38;5;241m.\u001b[39mcall\n\u001b[1;32m   1458\u001b[0m     compiled_fn_runner \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[1;32m   1459\u001b[0m         compiled_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrunner\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1460\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py:2293\u001b[0m, in \u001b[0;36mGraphLowering.compile_to_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompile_to_module\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CompiledModule:\n\u001b[1;32m   2287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\n\u001b[1;32m   2288\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGraphLowering.compile_to_module\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2289\u001b[0m         phase_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_gen\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2290\u001b[0m         log_pt2_compile_event\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   2291\u001b[0m         dynamo_compile_column_us\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minductor_code_gen_cumulative_compile_time_us\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2292\u001b[0m     ):\n\u001b[0;32m-> 2293\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compile_to_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py:2303\u001b[0m, in \u001b[0;36mGraphLowering._compile_to_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2298\u001b[0m wrapper_code, _ \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcodegen_with_cpp_wrapper() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcpp_wrapper \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcodegen()\n\u001b[1;32m   2300\u001b[0m )\n\u001b[1;32m   2302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(wrapper_code, ValueWithLineMap):\n\u001b[0;32m-> 2303\u001b[0m     mod \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compile_to_module_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapper_code\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2304\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(wrapper_code, FileBackedGraphModule):\n\u001b[1;32m   2305\u001b[0m     mod \u001b[38;5;241m=\u001b[39m wrapper_code\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py:2371\u001b[0m, in \u001b[0;36mGraphLowering._compile_to_module_lines\u001b[0;34m(self, wrapper_code)\u001b[0m\n\u001b[1;32m   2365\u001b[0m     trace_structured(\n\u001b[1;32m   2366\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minductor_output_code\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2367\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m\"\u001b[39m: path},\n\u001b[1;32m   2368\u001b[0m         payload_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m: wrapper_code\u001b[38;5;241m.\u001b[39mvalue,\n\u001b[1;32m   2369\u001b[0m     )\n\u001b[1;32m   2370\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPyCodeCache.load_by_key_path\u001b[39m\u001b[38;5;124m\"\u001b[39m, log_pt2_compile_event\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m-> 2371\u001b[0m     mod \u001b[38;5;241m=\u001b[39m \u001b[43mPyCodeCache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_by_key_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2372\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2373\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2374\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlinemap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlinemap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   2375\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstants\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtorchbind_constants\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2376\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_key \u001b[38;5;241m=\u001b[39m key\n\u001b[1;32m   2378\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_path \u001b[38;5;241m=\u001b[39m path\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py:3296\u001b[0m, in \u001b[0;36mPyCodeCache.load_by_key_path\u001b[0;34m(cls, key, path, linemap, attrs)\u001b[0m\n\u001b[1;32m   3293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmodules_no_attr[path]\n\u001b[1;32m   3295\u001b[0m in_toplevel \u001b[38;5;241m=\u001b[39m in_toplevel_process()\n\u001b[0;32m-> 3296\u001b[0m mod \u001b[38;5;241m=\u001b[39m \u001b[43m_reload_python_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_sys_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_toplevel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3298\u001b[0m \u001b[38;5;66;03m# unzip into separate lines/nodes lists\u001b[39;00m\n\u001b[1;32m   3299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m in_toplevel:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/compile_tasks.py:31\u001b[0m, in \u001b[0;36m_reload_python_module\u001b[0;34m(key, path, set_sys_modules)\u001b[0m\n\u001b[1;32m     29\u001b[0m mod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__file__\u001b[39m \u001b[38;5;241m=\u001b[39m path\n\u001b[1;32m     30\u001b[0m mod\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;241m=\u001b[39m key  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[43mexec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__dict__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__dict__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m set_sys_modules:\n\u001b[1;32m     33\u001b[0m     sys\u001b[38;5;241m.\u001b[39mmodules[mod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m] \u001b[38;5;241m=\u001b[39m mod\n",
      "File \u001b[0;32m/tmp/torchinductor_root/li/clis5b7wqd4z2xzypa7sgwq7vsm6yjxecvnzim6nqd4k3m2c7xbq.py:743\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;66;03m# kernel path: /tmp/torchinductor_root/ul/culg4fhpcekz32kvn2xxcyqu3drye7z2ikx7brtsg3zv7hg4f7n6.py\u001b[39;00m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;66;03m# Topologically Sorted Source Nodes: [flex_attention], Original ATen: []\u001b[39;00m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;66;03m# Source node to ATen node mapping:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;66;03m#   %flex_attention : [num_users=1] = call_function[target=torch.ops.higher_order.flex_attention](args = (%arg1_1, %arg3_1, %arg5_1, %sdpa_score0, (%arg7_1, %arg8_1, %arg9_1, %arg6_1, %arg10_1, %arg11_1, %arg12_1, %arg13_1, %arg14_1, %arg15_1, 128, 128, %sdpa_mask0), 0.14433756729740646, {PRESCALE_QK: False, ROWS_GUARANTEED_SAFE: False, BLOCKS_ARE_CONTIGUOUS: False, WRITE_DQ: True, OUTPUT_LOGSUMEXP: False}, (), ()), kwargs = {})\u001b[39;00m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;66;03m#   %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%flex_attention, 0), kwargs = {})\u001b[39;00m\n\u001b[1;32m    688\u001b[0m triton_per_fused_2 \u001b[38;5;241m=\u001b[39m async_compile\u001b[38;5;241m.\u001b[39mtriton(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtriton_per_fused_2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;124mimport triton\u001b[39m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;124mimport triton.language as tl\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;124m    tl.store(in_out_ptr0 + (x3), tmp17, xmask)\u001b[39m\n\u001b[1;32m    740\u001b[0m \u001b[38;5;124m'''\u001b[39m, device_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 743\u001b[0m \u001b[43masync_compile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    744\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m async_compile\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(args):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/async_compile.py:491\u001b[0m, in \u001b[0;36mAsyncCompile.wait\u001b[0;34m(self, scope)\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m get_compile_threads() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\n\u001b[1;32m    485\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masync_compile.wait\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    486\u001b[0m         log_pt2_compile_event\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    489\u001b[0m         waitcounter_name_override\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompile_triton\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    490\u001b[0m     ):\n\u001b[0;32m--> 491\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_futures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    493\u001b[0m _compile_end()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/async_compile.py:511\u001b[0m, in \u001b[0;36mAsyncCompile._wait_futures\u001b[0;34m(self, scope)\u001b[0m\n\u001b[1;32m    509\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mset_postfix_str(key)\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m     kernel \u001b[38;5;241m=\u001b[39m \u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    512\u001b[0m     scope[key] \u001b[38;5;241m=\u001b[39m kernel\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BrokenProcessPool \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py:4014\u001b[0m, in \u001b[0;36mLambdaFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   4013\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresult\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, Any]:\n\u001b[0;32m-> 4014\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/async_compile.py:370\u001b[0m, in \u001b[0;36mAsyncCompile.triton.<locals>.get_result\u001b[0;34m()\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_result\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CachingAutotuner:\n\u001b[0;32m--> 370\u001b[0m     kernel, elapsed_us \u001b[38;5;241m=\u001b[39m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;66;03m# Now that we've compiled, we should clear the future\u001b[39;00m\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;66;03m# so it can't be used again\u001b[39;00m\n\u001b[1;32m    373\u001b[0m     kernel\u001b[38;5;241m.\u001b[39mset_compile_info(compile_id, is_backward)\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:458\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mInductorError\u001b[0m: SubprocException: An exception occurred in a subprocess:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/triton/language/core.py\", line 76, in wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/triton/language/core.py\", line 42, in wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/triton/language/core.py\", line 1935, in reshape\n    shape = _shape_check_impl(_unwrap_iterable(shape))\n  File \"/usr/local/lib/python3.10/dist-packages/triton/language/core.py\", line 1677, in _shape_check_impl\n    validate_block_shape(shape)\n  File \"/usr/local/lib/python3.10/dist-packages/triton/_utils.py\", line 52, in validate_block_shape\n    raise ValueError(f\"Shape element {i} must be a power of 2\")\nValueError: Shape element 2 must be a power of 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_worker/subproc_pool.py\", line 340, in do_job\n    result = job()\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/compile_tasks.py\", line 62, in _worker_compile_triton\n    kernel.precompile(warm_cache_only=True)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 399, in precompile\n    self._precompile_worker()\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 430, in _precompile_worker\n    compile_results.append(self._precompile_config(c))\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 713, in _precompile_config\n    binary = triton.compile(*compile_args, **compile_kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/triton/compiler/compiler.py\", line 339, in compile\n    module = src.make_ir(options, codegen_fns, module_map, context)\n  File \"/usr/local/lib/python3.10/dist-packages/triton/compiler/compiler.py\", line 83, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 283:10:\n        tl.store(M_block_ptr, m_i, boundary_check=(1,))\n        tl.store(L_block_ptr, l_i, boundary_check=(1,))\n\n    # -- store output\n    idx_z = off_z\n    idx_t = off_t\n    idx_hq = off_hkv*G + off_g[:, None, None]\n    idx_m = off_m[None, :, None]\n    idx_d = offs_vd[None, None, :]\n\n    mask = (idx_m < Q_LEN) & (idx_d < V_HEAD_DIM)\n    acc = acc.reshape(G, BLOCK_M_PER_HQ, V_HEAD_DIM)\n          ^\n\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for name, cfg in configs.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Model: {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    model = Hymba(cfg)\n",
    "    params = model.count_parameters()\n",
    "    print(f'Parameters: {params[\"total\"]/1e6:.2f}M | d_model: {cfg.d_model}')\n",
    "    \n",
    "    # KV cache reduction (Hybrid에서만 의미 있음)\n",
    "    kv_reduction = 1.0\n",
    "    if cfg.arch_type != ArchType.MAMBA_ONLY:\n",
    "        kv_info = model.get_kv_sharing_info()\n",
    "        kv_reduction = kv_info['reduction']\n",
    "        if cfg.use_kv_sharing:\n",
    "            print(f'KV Reduction: {kv_reduction:.2f}x')\n",
    "    \n",
    "    # 학습\n",
    "    train_results = train_model(model, train_dl, val_dl, epochs=30, lr=3e-4, device=device)\n",
    "    \n",
    "    # 추론 벤치마크\n",
    "    infer_results = benchmark_inference(model, tokenizer, device=device)\n",
    "    \n",
    "    # 샘플 생성\n",
    "    samples = []\n",
    "    for prompt in ['ROMEO:', 'First Citizen:', 'KING:']:\n",
    "        prompt_tensor = torch.tensor([tokenizer.encode(prompt)]).to(device)\n",
    "        with torch.no_grad():\n",
    "            gen = model.generate(prompt_tensor, max_new_tokens=80, temperature=0.8, top_k=40)\n",
    "        samples.append(tokenizer.decode(gen[0].cpu().tolist()))\n",
    "    \n",
    "    results[name] = {\n",
    "        **train_results,\n",
    "        **infer_results,\n",
    "        'params': params['total'],\n",
    "        'd_model': cfg.d_model,\n",
    "        'kv_reduction': kv_reduction,\n",
    "        'samples': samples,\n",
    "    }\n",
    "    \n",
    "    print(f'Best PPL: {train_results[\"best_val_ppl\"]:.2f}')\n",
    "    print(f'Inference: {infer_results[\"tokens_per_sec\"]:.1f} tok/s')\n",
    "    \n",
    "    # 메모리 정리\n",
    "    del model\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 결과 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 DataFrame 생성\n",
    "df = pd.DataFrame([{\n",
    "    'Model': name,\n",
    "    'd_model': r['d_model'],\n",
    "    'Params (M)': r['params'] / 1e6,\n",
    "    'Best PPL': r['best_val_ppl'],\n",
    "    'Infer TPS': r['tokens_per_sec'],\n",
    "    'KV Red.': r['kv_reduction'],\n",
    "    'Time (min)': r['time_min'],\n",
    "} for name, r in results.items()])\n",
    "\n",
    "print('\\n' + '='*90)\n",
    "print('Results Summary (Fair Comparison with Similar Parameter Counts)')\n",
    "print('='*90)\n",
    "print(df.to_string(index=False))\n",
    "print('='*90)\n",
    "\n",
    "# 가장 좋은 모델 하이라이트\n",
    "best_ppl_model = df.loc[df['Best PPL'].idxmin(), 'Model']\n",
    "best_ppl_val = df['Best PPL'].min()\n",
    "print(f'\\n*** Best PPL: {best_ppl_model} ({best_ppl_val:.2f}) ***')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "\n",
    "# 1. Performance (PPL) - 낮을수록 좋음\n",
    "ax = axes[0, 0]\n",
    "colors = ['coral' if 'Hybrid' in m else 'skyblue' for m in df['Model']]\n",
    "bars = ax.barh(df['Model'], df['Best PPL'], color=colors)\n",
    "ax.set_xlabel('Best Val PPL (lower is better)')\n",
    "ax.set_title('Model Performance')\n",
    "ax.invert_yaxis()\n",
    "# 최고 성능 표시\n",
    "best_idx = df['Best PPL'].idxmin()\n",
    "bars[best_idx].set_color('green')\n",
    "bars[best_idx].set_edgecolor('darkgreen')\n",
    "bars[best_idx].set_linewidth(2)\n",
    "\n",
    "# 2. Parameter Count - 비슷해야 공정한 비교\n",
    "ax = axes[0, 1]\n",
    "ax.barh(df['Model'], df['Params (M)'], color='lightyellow', edgecolor='orange')\n",
    "ax.set_xlabel('Parameters (M)')\n",
    "ax.set_title('Model Size (Target: ~50M)')\n",
    "ax.axvline(x=50, color='red', linestyle='--', alpha=0.7, label='Target')\n",
    "ax.invert_yaxis()\n",
    "ax.legend()\n",
    "\n",
    "# 3. Inference Speed\n",
    "ax = axes[0, 2]\n",
    "ax.barh(df['Model'], df['Infer TPS'], color='lightgreen')\n",
    "ax.set_xlabel('Inference Speed (tok/s)')\n",
    "ax.set_title('Inference Speed')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# 4. Training Curves\n",
    "ax = axes[1, 0]\n",
    "for name, r in results.items():\n",
    "    ax.plot(r['history']['step'], r['history']['val_ppl'], label=name, linewidth=2)\n",
    "ax.set_xlabel('Steps')\n",
    "ax.set_ylabel('Val PPL')\n",
    "ax.set_title('Training Curves')\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. KV Cache Reduction\n",
    "ax = axes[1, 1]\n",
    "kv_colors = ['gray' if x == 1.0 else 'orange' for x in df['KV Red.']]\n",
    "ax.barh(df['Model'], df['KV Red.'], color=kv_colors)\n",
    "ax.set_xlabel('KV Reduction')\n",
    "ax.set_title('Memory Efficiency (higher is better)')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# 6. Training Time\n",
    "ax = axes[1, 2]\n",
    "ax.barh(df['Model'], df['Time (min)'], color='plum')\n",
    "ax.set_xlabel('Training Time (min)')\n",
    "ax.set_title('Training Duration')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_DIR}/ablation_results.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 생성 품질"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = ['ROMEO:', 'First Citizen:', 'KING:']\n",
    "for i, prompt in enumerate(prompts):\n",
    "    print(f\"\\n{'='*60}\\nPrompt: {prompt}\\n{'='*60}\")\n",
    "    for name, r in results.items():\n",
    "        print(f'\\n[{name}]')\n",
    "        print(r['samples'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 결론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*70)\n",
    "print('Key Findings (Fair Comparison with ~50M Parameters)')\n",
    "print('='*70)\n",
    "\n",
    "# 1. Best Performance\n",
    "best_ppl = df.loc[df['Best PPL'].idxmin()]\n",
    "print(f'1. Best Performance: {best_ppl[\"Model\"]} (PPL: {best_ppl[\"Best PPL\"]:.2f})')\n",
    "\n",
    "# 2. Fastest Inference\n",
    "best_infer = df.loc[df['Infer TPS'].idxmax()]\n",
    "print(f'2. Fastest Inference: {best_infer[\"Model\"]} ({best_infer[\"Infer TPS\"]:.1f} tok/s)')\n",
    "\n",
    "# 3. Best Memory Efficiency\n",
    "best_kv = df.loc[df['KV Red.'].idxmax()]\n",
    "if best_kv['KV Red.'] > 1.0:\n",
    "    print(f'3. Best Memory Efficiency: {best_kv[\"Model\"]} ({best_kv[\"KV Red.\"]:.2f}x KV reduction)')\n",
    "else:\n",
    "    print(f'3. Memory Efficiency: Hybrid models achieve KV cache reduction via sharing')\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('Conclusions')\n",
    "print('='*70)\n",
    "print(\"\"\"\n",
    "- Mamba-only: Fast inference due to O(n) complexity, no KV cache needed\n",
    "- Transformer-only: Strong performance with global attention\n",
    "- Hybrid (Hymba): \n",
    "  * Combines Mamba's efficiency with Attention's expressiveness\n",
    "  * KV cache sharing reduces memory footprint\n",
    "  * Meta tokens help with attention sink problem\n",
    "  * Global attention at key layers (first/middle/last)\n",
    "  * Local (SWA) attention elsewhere for efficiency\n",
    "\"\"\")\n",
    "\n",
    "# Hybrid vs Single-arch 비교\n",
    "hybrid_models = df[df['Model'].str.contains('Hybrid')]\n",
    "single_models = df[~df['Model'].str.contains('Hybrid')]\n",
    "\n",
    "if not hybrid_models.empty and not single_models.empty:\n",
    "    best_hybrid_ppl = hybrid_models['Best PPL'].min()\n",
    "    best_single_ppl = single_models['Best PPL'].min()\n",
    "    \n",
    "    if best_hybrid_ppl < best_single_ppl:\n",
    "        improvement = (best_single_ppl - best_hybrid_ppl) / best_single_ppl * 100\n",
    "        print(f'Hybrid achieves {improvement:.1f}% better PPL than best single-architecture model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 결과 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'{RESULTS_DIR}/ablation_results.csv', index=False)\n",
    "print(f'Saved: {RESULTS_DIR}/ablation_results.csv')\n",
    "\n",
    "with open(f'{RESULTS_DIR}/generation_samples.txt', 'w') as f:\n",
    "    for i, prompt in enumerate(['ROMEO:', 'First Citizen:', 'KING:']):\n",
    "        f.write(f'Prompt: {prompt}\\n')\n",
    "        for name, r in results.items():\n",
    "            f.write(f'[{name}]\\n{r[\"samples\"][i]}\\n\\n')\n",
    "print(f'Saved: {RESULTS_DIR}/generation_samples.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
