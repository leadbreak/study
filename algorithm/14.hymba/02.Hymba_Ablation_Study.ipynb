{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hymba Ablation Study - Tiny Shakespeare\n",
    "\n",
    "## 목표\n",
    "다양한 아키텍처 구성의 성능을 비교하여 각 컴포넌트의 기여도를 분석합니다.\n",
    "\n",
    "### 테스트 구성\n",
    "1. **Mamba-only**: SSM 기반 시퀀스 모델링\n",
    "2. **Transformer-only**: 표준 어텐션 기반\n",
    "3. **Hybrid (Hymba)**: Attention + Mamba 혼합\n",
    "   - Global Attention: 첫/중간/마지막 레이어\n",
    "   - Local Attention (SWA): 나머지 레이어\n",
    "   - Meta Tokens: 64개\n",
    "   - KV-Cache 공유\n",
    "\n",
    "### 평가 메트릭\n",
    "- 학습 Loss & Perplexity\n",
    "- 검증 Loss & Perplexity\n",
    "- 학습 속도 (tokens/sec)\n",
    "- 추론 속도 (tokens/sec)\n",
    "- KV-Cache 메모리 절감\n",
    "- 생성 품질"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "sys.path.append('./backbone')\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import importlib\n",
    "if 'hymba' in sys.modules:\n",
    "    importlib.reload(sys.modules['hymba'])\n",
    "\n",
    "from hymba import Hymba, HymbaConfig, ArchType, AttentionType\n",
    "\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import Unigram\n",
    "from tokenizers.trainers import UnigramTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.normalizers import Sequence as NormSeq, NFKC, Lowercase\n",
    "\n",
    "RESULTS_DIR = './results'\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Device: {device}')\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('데이터 로딩 중...')\n",
    "ds = load_dataset('karpathy/tiny_shakespeare')\n",
    "text = '\\n\\n'.join(ds['train']['text'])\n",
    "print(f'전체 텍스트 길이: {len(text):,} 문자')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('토크나이저 학습 중...')\n",
    "vocab_size = 4000\n",
    "\n",
    "tk = Tokenizer(Unigram())\n",
    "tk.normalizer = NormSeq([NFKC(), Lowercase()])\n",
    "tk.pre_tokenizer = Whitespace()\n",
    "trainer = UnigramTrainer(vocab_size=vocab_size, special_tokens=['<|unk|>'], unk_token='<|unk|>')\n",
    "tk.train_from_iterator([text], trainer=trainer)\n",
    "\n",
    "class TokenizerWrap:\n",
    "    def __init__(self, tk):\n",
    "        self.tk = tk\n",
    "    def encode(self, s):\n",
    "        return self.tk.encode(s).ids\n",
    "    def decode(self, ids):\n",
    "        return self.tk.decode(ids)\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return self.tk.get_vocab_size()\n",
    "\n",
    "tokenizer = TokenizerWrap(tk)\n",
    "print(f'어휘 크기: {tokenizer.vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(tok, text, seq_len=256):\n",
    "    ids = np.array(tok.encode(text), dtype=np.int64)\n",
    "    x, y = ids[:-1], ids[1:]\n",
    "    n = (len(y) // seq_len) * seq_len\n",
    "    X = torch.tensor(x[:n].reshape(-1, seq_len))\n",
    "    Y = torch.tensor(y[:n].reshape(-1, seq_len))\n",
    "    return TensorDataset(X, Y)\n",
    "\n",
    "seq_len = 256\n",
    "batch_size = 16\n",
    "\n",
    "ds_full = make_dataset(tokenizer, text, seq_len)\n",
    "tr_len = int(0.9 * len(ds_full))\n",
    "va_len = len(ds_full) - tr_len\n",
    "train_ds, val_ds = random_split(ds_full, [tr_len, va_len])\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f'학습 배치: {len(train_dl)}')\n",
    "print(f'검증 배치: {len(val_dl)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 모델 구성 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공정한 비교를 위해 파라미터 수를 ~50M으로 맞춤\n",
    "# 각 아키텍처의 특성에 맞게 d_model 조정\n",
    "\n",
    "common_params = {\n",
    "    'vocab_size': vocab_size,\n",
    "    'swa_window': 128,\n",
    "    'dropout': 0.1,\n",
    "    'seq_len': seq_len,\n",
    "}\n",
    "\n",
    "# 파라미터 수 목표: ~50M\n",
    "configs = {\n",
    "    # Mamba-only: d_model을 조정 (Mamba가 expand=2로 파라미터가 많음)\n",
    "    'Mamba-only': HymbaConfig(\n",
    "        **common_params,\n",
    "        d_model=448,\n",
    "        n_layers=12,\n",
    "        n_heads=8,\n",
    "        n_kv_heads=4,\n",
    "        arch_type=ArchType.MAMBA_ONLY,\n",
    "        mamba_heads_per_layer=1,\n",
    "        use_meta_tokens=False,\n",
    "        use_kv_sharing=False,\n",
    "    ),\n",
    "    # Transformer-only: 기준 모델 (49M)\n",
    "    'Transformer-only': HymbaConfig(\n",
    "        **common_params,\n",
    "        d_model=512,\n",
    "        n_layers=12,\n",
    "        n_heads=8,\n",
    "        n_kv_heads=4,\n",
    "        arch_type=ArchType.TRANSFORMER_ONLY,\n",
    "        global_attn_indices=list(range(12)),  # 모든 레이어 Global\n",
    "        use_meta_tokens=False,\n",
    "        use_kv_sharing=False,\n",
    "    ),\n",
    "    # Hybrid (1:1): Attn + Mamba 각 1개씩\n",
    "    'Hybrid (1:1)': HymbaConfig(\n",
    "        **common_params,\n",
    "        d_model=384,\n",
    "        n_layers=12,\n",
    "        n_heads=8,\n",
    "        n_kv_heads=4,\n",
    "        arch_type=ArchType.HYBRID,\n",
    "        mamba_heads_per_layer=1,\n",
    "        global_attn_indices=[0, 5, 11],  # 첫/중간/마지막 Global\n",
    "        use_meta_tokens=True,\n",
    "        num_meta_tokens=64,\n",
    "        use_kv_sharing=True,\n",
    "    ),\n",
    "    # Hybrid (2:1): Mamba 비중 높임\n",
    "    'Hybrid (2:1)': HymbaConfig(\n",
    "        **common_params,\n",
    "        d_model=352,\n",
    "        n_layers=12,\n",
    "        n_heads=8,\n",
    "        n_kv_heads=4,\n",
    "        arch_type=ArchType.HYBRID,\n",
    "        mamba_heads_per_layer=2,\n",
    "        global_attn_indices=[0, 5, 11],\n",
    "        use_meta_tokens=True,\n",
    "        num_meta_tokens=64,\n",
    "        use_kv_sharing=True,\n",
    "    ),\n",
    "}\n",
    "\n",
    "# 파라미터 수 검증\n",
    "print('=== 파라미터 수 검증 (목표: ~50M) ===')\n",
    "for name, cfg in configs.items():\n",
    "    model = Hymba(cfg)\n",
    "    params = model.count_parameters()['total']\n",
    "    layer_configs = cfg.get_layer_configs()\n",
    "    total_attn = sum(c[0] for c in layer_configs)\n",
    "    total_mamba = sum(c[1] for c in layer_configs)\n",
    "    print(f'{name:20s}: {params/1e6:5.2f}M params | d={cfg.d_model:3d} | Attn={total_attn:3d} | Mamba={total_mamba:3d}')\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "print('='*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 학습 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dl, val_dl, epochs=30, lr=3e-4, warmup_steps=200, eval_interval=32, device='cuda'):\n",
    "    \"\"\"모델 학습 함수\"\"\"\n",
    "    model = model.to(device).train()\n",
    "    \n",
    "    # Optimizer: AdamW with weight decay\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, betas=(0.9, 0.95), weight_decay=0.1)\n",
    "    total_steps = epochs * len(train_dl)\n",
    "    \n",
    "    # Learning rate schedule: warmup + cosine decay\n",
    "    def lr_schedule(step):\n",
    "        if step < warmup_steps:\n",
    "            return step / warmup_steps\n",
    "        progress = (step - warmup_steps) / max(1, total_steps - warmup_steps)\n",
    "        return 0.5 * (1 + np.cos(np.pi * progress))\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_schedule)\n",
    "    \n",
    "    # Mixed precision training\n",
    "    use_amp = device == 'cuda' and torch.cuda.is_bf16_supported()\n",
    "    scaler = torch.amp.GradScaler('cuda') if use_amp else None\n",
    "    \n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_ppl': [], 'lr': [], 'step': []}\n",
    "    step, t0, best_val_loss = 0, time.time(), float('inf')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss, epoch_tokens = 0.0, 0\n",
    "        pbar = tqdm(train_dl, desc=f'Epoch {epoch+1}/{epochs}', leave=False)\n",
    "        \n",
    "        for xb, yb in pbar:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            \n",
    "            if use_amp:\n",
    "                with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "                    out = model(xb, targets=yb)\n",
    "                    loss = out['loss']\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                out = model(xb, targets=yb)\n",
    "                loss = out['loss']\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "            \n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scheduler.step()\n",
    "            epoch_loss += loss.item() * xb.numel()\n",
    "            epoch_tokens += xb.numel()\n",
    "            step += 1\n",
    "            \n",
    "            # Evaluation\n",
    "            if step % eval_interval == 0:\n",
    "                model.eval()\n",
    "                val_loss, val_tokens = 0.0, 0\n",
    "                with torch.no_grad():\n",
    "                    for vxb, vyb in val_dl:\n",
    "                        vxb, vyb = vxb.to(device), vyb.to(device)\n",
    "                        if use_amp:\n",
    "                            with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "                                vout = model(vxb, targets=vyb)\n",
    "                        else:\n",
    "                            vout = model(vxb, targets=vyb)\n",
    "                        val_loss += vout['loss'].item() * vxb.numel()\n",
    "                        val_tokens += vxb.numel()\n",
    "                \n",
    "                val_loss /= val_tokens\n",
    "                train_loss = epoch_loss / epoch_tokens\n",
    "                history['train_loss'].append(train_loss)\n",
    "                history['val_loss'].append(val_loss)\n",
    "                history['val_ppl'].append(np.exp(val_loss))\n",
    "                history['lr'].append(scheduler.get_last_lr()[0])\n",
    "                history['step'].append(step)\n",
    "                best_val_loss = min(best_val_loss, val_loss)\n",
    "                pbar.set_postfix({'loss': f'{train_loss:.3f}', 'val': f'{val_loss:.3f}', 'ppl': f'{np.exp(val_loss):.1f}'})\n",
    "                model.train()\n",
    "    \n",
    "    elapsed = time.time() - t0\n",
    "    tps = int(epochs * len(train_dl) * batch_size * seq_len / elapsed)\n",
    "    \n",
    "    return {\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'best_val_ppl': np.exp(best_val_loss),\n",
    "        'final_val_ppl': history['val_ppl'][-1] if history['val_ppl'] else np.exp(best_val_loss),\n",
    "        'train_tps': tps,\n",
    "        'time_min': elapsed / 60,\n",
    "        'history': history,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_inference(model, tokenizer, prompt='ROMEO:', max_tokens=100, n_runs=3, device='cuda'):\n",
    "    model = model.to(device).eval()\n",
    "    prompt_tensor = torch.tensor([tokenizer.encode(prompt)]).to(device)\n",
    "    times = []\n",
    "    for _ in range(n_runs):\n",
    "        torch.cuda.synchronize() if device == 'cuda' else None\n",
    "        t0 = time.time()\n",
    "        with torch.no_grad():\n",
    "            _ = model.generate(prompt_tensor, max_new_tokens=max_tokens, temperature=1.0)\n",
    "        torch.cuda.synchronize() if device == 'cuda' else None\n",
    "        times.append(time.time() - t0)\n",
    "    return {'tokens_per_sec': max_tokens / np.mean(times)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for name, cfg in configs.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Model: {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    model = Hymba(cfg)\n",
    "    params = model.count_parameters()\n",
    "    print(f'Parameters: {params[\"total\"]/1e6:.2f}M | d_model: {cfg.d_model}')\n",
    "    \n",
    "    # KV cache reduction (Hybrid에서만 의미 있음)\n",
    "    kv_reduction = 1.0\n",
    "    if cfg.arch_type != ArchType.MAMBA_ONLY:\n",
    "        kv_info = model.get_kv_sharing_info()\n",
    "        kv_reduction = kv_info['reduction']\n",
    "        if cfg.use_kv_sharing:\n",
    "            print(f'KV Reduction: {kv_reduction:.2f}x')\n",
    "    \n",
    "    # 학습\n",
    "    train_results = train_model(model, train_dl, val_dl, epochs=30, lr=3e-4, device=device)\n",
    "    \n",
    "    # 추론 벤치마크\n",
    "    infer_results = benchmark_inference(model, tokenizer, device=device)\n",
    "    \n",
    "    # 샘플 생성\n",
    "    samples = []\n",
    "    for prompt in ['ROMEO:', 'First Citizen:', 'KING:']:\n",
    "        prompt_tensor = torch.tensor([tokenizer.encode(prompt)]).to(device)\n",
    "        with torch.no_grad():\n",
    "            gen = model.generate(prompt_tensor, max_new_tokens=80, temperature=0.8, top_k=40)\n",
    "        samples.append(tokenizer.decode(gen[0].cpu().tolist()))\n",
    "    \n",
    "    results[name] = {\n",
    "        **train_results,\n",
    "        **infer_results,\n",
    "        'params': params['total'],\n",
    "        'd_model': cfg.d_model,\n",
    "        'kv_reduction': kv_reduction,\n",
    "        'samples': samples,\n",
    "    }\n",
    "    \n",
    "    print(f'Best PPL: {train_results[\"best_val_ppl\"]:.2f}')\n",
    "    print(f'Inference: {infer_results[\"tokens_per_sec\"]:.1f} tok/s')\n",
    "    \n",
    "    # 메모리 정리\n",
    "    del model\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 결과 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 DataFrame 생성\n",
    "df = pd.DataFrame([{\n",
    "    'Model': name,\n",
    "    'd_model': r['d_model'],\n",
    "    'Params (M)': r['params'] / 1e6,\n",
    "    'Best PPL': r['best_val_ppl'],\n",
    "    'Infer TPS': r['tokens_per_sec'],\n",
    "    'KV Red.': r['kv_reduction'],\n",
    "    'Time (min)': r['time_min'],\n",
    "} for name, r in results.items()])\n",
    "\n",
    "print('\\n' + '='*90)\n",
    "print('Results Summary (Fair Comparison with Similar Parameter Counts)')\n",
    "print('='*90)\n",
    "print(df.to_string(index=False))\n",
    "print('='*90)\n",
    "\n",
    "# 가장 좋은 모델 하이라이트\n",
    "best_ppl_model = df.loc[df['Best PPL'].idxmin(), 'Model']\n",
    "best_ppl_val = df['Best PPL'].min()\n",
    "print(f'\\n*** Best PPL: {best_ppl_model} ({best_ppl_val:.2f}) ***')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "\n",
    "# 1. Performance (PPL) - 낮을수록 좋음\n",
    "ax = axes[0, 0]\n",
    "colors = ['coral' if 'Hybrid' in m else 'skyblue' for m in df['Model']]\n",
    "bars = ax.barh(df['Model'], df['Best PPL'], color=colors)\n",
    "ax.set_xlabel('Best Val PPL (lower is better)')\n",
    "ax.set_title('Model Performance')\n",
    "ax.invert_yaxis()\n",
    "# 최고 성능 표시\n",
    "best_idx = df['Best PPL'].idxmin()\n",
    "bars[best_idx].set_color('green')\n",
    "bars[best_idx].set_edgecolor('darkgreen')\n",
    "bars[best_idx].set_linewidth(2)\n",
    "\n",
    "# 2. Parameter Count - 비슷해야 공정한 비교\n",
    "ax = axes[0, 1]\n",
    "ax.barh(df['Model'], df['Params (M)'], color='lightyellow', edgecolor='orange')\n",
    "ax.set_xlabel('Parameters (M)')\n",
    "ax.set_title('Model Size (Target: ~50M)')\n",
    "ax.axvline(x=50, color='red', linestyle='--', alpha=0.7, label='Target')\n",
    "ax.invert_yaxis()\n",
    "ax.legend()\n",
    "\n",
    "# 3. Inference Speed\n",
    "ax = axes[0, 2]\n",
    "ax.barh(df['Model'], df['Infer TPS'], color='lightgreen')\n",
    "ax.set_xlabel('Inference Speed (tok/s)')\n",
    "ax.set_title('Inference Speed')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# 4. Training Curves\n",
    "ax = axes[1, 0]\n",
    "for name, r in results.items():\n",
    "    ax.plot(r['history']['step'], r['history']['val_ppl'], label=name, linewidth=2)\n",
    "ax.set_xlabel('Steps')\n",
    "ax.set_ylabel('Val PPL')\n",
    "ax.set_title('Training Curves')\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. KV Cache Reduction\n",
    "ax = axes[1, 1]\n",
    "kv_colors = ['gray' if x == 1.0 else 'orange' for x in df['KV Red.']]\n",
    "ax.barh(df['Model'], df['KV Red.'], color=kv_colors)\n",
    "ax.set_xlabel('KV Reduction')\n",
    "ax.set_title('Memory Efficiency (higher is better)')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# 6. Training Time\n",
    "ax = axes[1, 2]\n",
    "ax.barh(df['Model'], df['Time (min)'], color='plum')\n",
    "ax.set_xlabel('Training Time (min)')\n",
    "ax.set_title('Training Duration')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_DIR}/ablation_results.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 생성 품질"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = ['ROMEO:', 'First Citizen:', 'KING:']\n",
    "for i, prompt in enumerate(prompts):\n",
    "    print(f\"\\n{'='*60}\\nPrompt: {prompt}\\n{'='*60}\")\n",
    "    for name, r in results.items():\n",
    "        print(f'\\n[{name}]')\n",
    "        print(r['samples'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 결론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*70)\n",
    "print('Key Findings (Fair Comparison with ~50M Parameters)')\n",
    "print('='*70)\n",
    "\n",
    "# 1. Best Performance\n",
    "best_ppl = df.loc[df['Best PPL'].idxmin()]\n",
    "print(f'1. Best Performance: {best_ppl[\"Model\"]} (PPL: {best_ppl[\"Best PPL\"]:.2f})')\n",
    "\n",
    "# 2. Fastest Inference\n",
    "best_infer = df.loc[df['Infer TPS'].idxmax()]\n",
    "print(f'2. Fastest Inference: {best_infer[\"Model\"]} ({best_infer[\"Infer TPS\"]:.1f} tok/s)')\n",
    "\n",
    "# 3. Best Memory Efficiency\n",
    "best_kv = df.loc[df['KV Red.'].idxmax()]\n",
    "if best_kv['KV Red.'] > 1.0:\n",
    "    print(f'3. Best Memory Efficiency: {best_kv[\"Model\"]} ({best_kv[\"KV Red.\"]:.2f}x KV reduction)')\n",
    "else:\n",
    "    print(f'3. Memory Efficiency: Hybrid models achieve KV cache reduction via sharing')\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('Conclusions')\n",
    "print('='*70)\n",
    "print(\"\"\"\n",
    "- Mamba-only: Fast inference due to O(n) complexity, no KV cache needed\n",
    "- Transformer-only: Strong performance with global attention\n",
    "- Hybrid (Hymba): \n",
    "  * Combines Mamba's efficiency with Attention's expressiveness\n",
    "  * KV cache sharing reduces memory footprint\n",
    "  * Meta tokens help with attention sink problem\n",
    "  * Global attention at key layers (first/middle/last)\n",
    "  * Local (SWA) attention elsewhere for efficiency\n",
    "\"\"\")\n",
    "\n",
    "# Hybrid vs Single-arch 비교\n",
    "hybrid_models = df[df['Model'].str.contains('Hybrid')]\n",
    "single_models = df[~df['Model'].str.contains('Hybrid')]\n",
    "\n",
    "if not hybrid_models.empty and not single_models.empty:\n",
    "    best_hybrid_ppl = hybrid_models['Best PPL'].min()\n",
    "    best_single_ppl = single_models['Best PPL'].min()\n",
    "    \n",
    "    if best_hybrid_ppl < best_single_ppl:\n",
    "        improvement = (best_single_ppl - best_hybrid_ppl) / best_single_ppl * 100\n",
    "        print(f'Hybrid achieves {improvement:.1f}% better PPL than best single-architecture model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 결과 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'{RESULTS_DIR}/ablation_results.csv', index=False)\n",
    "print(f'Saved: {RESULTS_DIR}/ablation_results.csv')\n",
    "\n",
    "with open(f'{RESULTS_DIR}/generation_samples.txt', 'w') as f:\n",
    "    for i, prompt in enumerate(['ROMEO:', 'First Citizen:', 'KING:']):\n",
    "        f.write(f'Prompt: {prompt}\\n')\n",
    "        for name, r in results.items():\n",
    "            f.write(f'[{name}]\\n{r[\"samples\"][i]}\\n\\n')\n",
    "print(f'Saved: {RESULTS_DIR}/generation_samples.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
