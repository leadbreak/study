{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06078178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== 1_Mamba_Only_Baseline ====================\n",
      "--- KV Cache 크기: 0.00 MB\n",
      "--- 처리량: 333,978.79 tokens/sec\n",
      "\n",
      "==================== 2_+_Attention (Hymba) ====================\n",
      "--- KV Cache 크기: 4.00 MB\n",
      "--- 처리량: 258,544.51 tokens/sec\n",
      "\n",
      "==================== 3_+_Meta_Tokens ====================\n",
      "--- KV Cache 크기: 4.03 MB\n",
      "--- 처리량: 249,739.34 tokens/sec\n",
      "\n",
      "==================== 4_+_Shared_KV_Cache ====================\n",
      "--- KV Cache 크기: 2.02 MB\n",
      "--- 처리량: 244,119.62 tokens/sec\n",
      "\n",
      "==================== 5_+_SWA ====================\n",
      "--- KV Cache 크기: 2.02 MB\n",
      "--- 처리량: 249,162.72 tokens/sec\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import math\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "# mamba-ssm과 causal-conv1d는 저수준 CUDA 커널을 사용하기 위해 필요합니다.\n",
    "# pip install mamba-ssm causal-conv1d\n",
    "try:\n",
    "    from mamba_ssm.ops.selective_scan_interface import selective_scan_fn\n",
    "    from causal_conv1d import causal_conv1d_fn\n",
    "except ImportError:\n",
    "    print(\"Warning: mamba-ssm or causal-conv1d not found. MambaBranch will not work.\")\n",
    "    selective_scan_fn = None\n",
    "    causal_conv1d_fn = None\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. 아키텍처의 기본 구성 요소 (Llama 및 Mamba)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
    "        return self.weight * hidden_states.to(input_dtype)\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim: int, max_seq_len: int, base: int = 10000, device: Optional[str] = None):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2, device=device, dtype=torch.float32) / self.dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "        self._set_cos_sin_cache(seq_len=max_seq_len, device=device)\n",
    "\n",
    "    def _set_cos_sin_cache(self, seq_len: int, device: Optional[str], dtype: torch.dtype = torch.float32):\n",
    "        self.max_seq_len_cached = seq_len\n",
    "        t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        self.register_buffer(\"cos_cached\", emb.cos().to(dtype), persistent=False)\n",
    "        self.register_buffer(\"sin_cached\", emb.sin().to(dtype), persistent=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, seq_len: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        if seq_len > self.max_seq_len_cached:\n",
    "            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)\n",
    "        return self.cos_cached[:seq_len, ...].to(dtype=x.dtype), self.sin_cached[:seq_len, ...].to(dtype=x.dtype)\n",
    "\n",
    "def rotate_half(x: torch.Tensor) -> torch.Tensor:\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "def apply_rotary_pos_emb(q: torch.Tensor, k: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    cos = cos.unsqueeze(0).unsqueeze(2)\n",
    "    sin = sin.unsqueeze(0).unsqueeze(2)\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n",
    "\n",
    "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
    "    if n_rep == 1:\n",
    "        return hidden_states\n",
    "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
    "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
    "\n",
    "class AttentionBranch(nn.Module):\n",
    "    def __init__(self, d_inner: int, n_heads: int, n_kv_heads: int, max_seq_len: int, window_size: int = -1):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.n_kv_heads = n_kv_heads\n",
    "        self.n_key_value_groups = n_heads // n_kv_heads\n",
    "        self.head_dim = d_inner // n_heads\n",
    "        self.rotary_emb = RotaryEmbedding(self.head_dim, max_seq_len)\n",
    "\n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, past_kv: Optional[Tuple[torch.Tensor, torch.Tensor]] = None, attn_mask: Optional[torch.Tensor] = None, use_cache: bool = False):\n",
    "        batch_size, q_len, _ = q.shape\n",
    "        \n",
    "        q = q.view(batch_size, q_len, self.n_heads, self.head_dim)\n",
    "        k = k.view(batch_size, q_len, self.n_kv_heads, self.head_dim)\n",
    "        v = v.view(batch_size, q_len, self.n_kv_heads, self.head_dim)\n",
    "\n",
    "        kv_seq_len = q_len\n",
    "        if past_kv is not None:\n",
    "            kv_seq_len += past_kv[0].shape[1]\n",
    "            \n",
    "        cos, sin = self.rotary_emb(v, seq_len=kv_seq_len)\n",
    "\n",
    "        if past_kv is not None:\n",
    "            cos = cos[past_kv[0].shape[1]:]\n",
    "            sin = sin[past_kv[0].shape[1]:]\n",
    "        \n",
    "        q, k = apply_rotary_pos_emb(q, k, cos, sin)\n",
    "        \n",
    "        if past_kv is not None:\n",
    "            past_key, past_value = past_kv\n",
    "            k = torch.cat([past_key, k], dim=1)\n",
    "            v = torch.cat([past_value, v], dim=1)\n",
    "            \n",
    "        present_kv = (k, v) if use_cache else None\n",
    "        \n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "        \n",
    "        k = repeat_kv(k, self.n_key_value_groups)\n",
    "        v = repeat_kv(v, self.n_key_value_groups)\n",
    "        \n",
    "        attn_output = F.scaled_dot_product_attention(q, k, v, attn_mask=attn_mask)\n",
    "        return attn_output.transpose(1, 2).contiguous().view(batch_size, q_len, -1), present_kv\n",
    "\n",
    "class MambaBranch(nn.Module):\n",
    "    def __init__(self, d_inner, d_state, d_conv, dt_rank):\n",
    "        super().__init__()\n",
    "        if causal_conv1d_fn is None or selective_scan_fn is None:\n",
    "            raise ImportError(\"Mamba packages not found. Please install them.\")\n",
    "        \n",
    "        self.d_inner, self.d_state, self.d_conv, self.dt_rank = d_inner, d_state, d_conv, dt_rank\n",
    "        \n",
    "        self.conv1d = nn.Conv1d(in_channels=self.d_inner, out_channels=self.d_inner, kernel_size=d_conv, bias=True, groups=self.d_inner, padding=d_conv - 1)\n",
    "        self.x_proj = nn.Linear(self.d_inner, self.dt_rank + 2 * self.d_state, bias=False)\n",
    "        self.dt_proj = nn.Linear(self.dt_rank, self.d_inner, bias=True)\n",
    "        \n",
    "        A = torch.arange(1, self.d_state + 1, dtype=torch.float32).repeat(self.d_inner, 1)\n",
    "        self.A_log = nn.Parameter(torch.log(A))\n",
    "        self.D = nn.Parameter(torch.ones(self.d_inner))\n",
    "\n",
    "    def forward(self, x, z):\n",
    "        x_transposed = x.transpose(1, 2).contiguous()\n",
    "        x_conv = causal_conv1d_fn(x_transposed, self.conv1d.weight.squeeze(1), self.conv1d.bias, activation=\"silu\")\n",
    "        \n",
    "        x_dbl = self.x_proj(x_conv.transpose(1, 2))\n",
    "        dt_pre, B, C = torch.split(x_dbl, [self.dt_rank, self.d_state, self.d_state], dim=-1)\n",
    "        \n",
    "        dt = self.dt_proj(dt_pre).transpose(1, 2)\n",
    "        A = -torch.exp(self.A_log.float())\n",
    "        \n",
    "        y = selective_scan_fn(\n",
    "            x_conv, dt, A, B.transpose(1, 2), C.transpose(1, 2), self.D.float(), \n",
    "            z=z.transpose(1,2), delta_bias=self.dt_proj.bias.float(), delta_softplus=True\n",
    "        )\n",
    "        return y.transpose(1,2)\n",
    "        \n",
    "# -----------------------------------------------------------------------------\n",
    "# 2. Ablation을 위한 각 단계별 블록 정의\n",
    "# -----------------------------------------------------------------------------\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model: int, hidden_dim: int):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(d_model, hidden_dim, bias=False)\n",
    "        self.w3 = nn.Linear(d_model, hidden_dim, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_dim, d_model, bias=False)\n",
    "        self.act_fn = F.silu\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.w2(self.act_fn(self.w1(x)) * self.w3(x))\n",
    "\n",
    "class MambaOnlyBlock(nn.Module):\n",
    "    \"\"\" 1단계: Mamba-only 베이스라인을 위한 블록 \"\"\"\n",
    "    def __init__(self, d_model: int, ffn_hidden_dim: int, mamba_params: dict):\n",
    "        super().__init__()\n",
    "        self.d_inner = mamba_params['expand'] * d_model\n",
    "        \n",
    "        self.norm = RMSNorm(d_model)\n",
    "        self.in_proj = nn.Linear(d_model, 2 * self.d_inner, bias=False)\n",
    "        self.mamba_branch = MambaBranch(d_inner=self.d_inner, **{k:v for k,v in mamba_params.items() if k != 'expand'})\n",
    "        self.out_proj = nn.Linear(self.d_inner, d_model, bias=False)\n",
    "        self.ffn = FeedForward(d_model, ffn_hidden_dim)\n",
    "        self.ffn_norm = RMSNorm(d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, **kwargs):\n",
    "        residual = x\n",
    "        x_norm = self.norm(x)\n",
    "        \n",
    "        xz = self.in_proj(x_norm)\n",
    "        x_mamba, z_mamba = xz.chunk(2, dim=-1)\n",
    "        \n",
    "        mamba_out = self.mamba_branch(x_mamba, z_mamba)\n",
    "        \n",
    "        h = residual + self.out_proj(mamba_out)\n",
    "        out = h + self.ffn(self.ffn_norm(h))\n",
    "        return out, None\n",
    "\n",
    "class HymbaBlock(nn.Module):\n",
    "    \"\"\" 2-5단계: 공식 코드 로직을 따르는 HymbaBlock \"\"\"\n",
    "    def __init__(self, d_model: int, n_heads: int, n_kv_heads: int, max_seq_len: int, window_size: int, mamba_params: dict):\n",
    "        super().__init__()\n",
    "        self.d_inner = mamba_params['expand'] * d_model\n",
    "        attn_head_dim = self.d_inner // n_heads\n",
    "        \n",
    "        latent_dim = self.d_inner + self.d_inner + (attn_head_dim * n_kv_heads * 2)\n",
    "        self.in_proj = nn.Linear(d_model, latent_dim + self.d_inner, bias=True)\n",
    "        self.out_proj = nn.Linear(self.d_inner, d_model, bias=True)\n",
    "\n",
    "        self.attn_branch = AttentionBranch(d_inner=self.d_inner, n_heads=n_heads, n_kv_heads=n_kv_heads, max_seq_len=max_seq_len, window_size=window_size)\n",
    "        self.mamba_branch = MambaBranch(d_inner=self.d_inner, d_state=mamba_params['d_state'], d_conv=mamba_params['d_conv'], dt_rank=mamba_params['dt_rank'])\n",
    "\n",
    "        self.norm = RMSNorm(d_model)\n",
    "        self.norm1 = RMSNorm(self.d_inner)\n",
    "        self.norm2 = RMSNorm(self.d_inner)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, past_kv: Optional[Tuple[torch.Tensor, torch.Tensor]] = None, attn_mask: Optional[torch.Tensor] = None, use_cache: bool = False):\n",
    "        projected = self.in_proj(self.norm(x))\n",
    "        \n",
    "        latent, gate = projected.tensor_split((projected.shape[-1] - self.d_inner,), dim=-1)\n",
    "        \n",
    "        attn_q_dim = self.d_inner\n",
    "        attn_k_dim = self.attn_branch.head_dim * self.attn_branch.n_kv_heads\n",
    "        \n",
    "        q, k, v, mamba_x = latent.tensor_split((attn_q_dim, attn_q_dim + attn_k_dim, attn_q_dim + 2 * attn_k_dim), dim=-1)\n",
    "\n",
    "        attn_out, present_kv = self.attn_branch(q, k, v, past_kv, attn_mask, use_cache)\n",
    "        mamba_out = self.mamba_branch(mamba_x, gate)\n",
    "\n",
    "        combined = (self.norm1(attn_out) + self.norm2(mamba_out)) / 2\n",
    "        return self.out_proj(combined), present_kv\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3. 전체 모델 및 마스크 생성\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class HymbaModel(nn.Module):\n",
    "    def __init__(self, config: dict):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embedding = nn.Embedding(config['vocab_size'], config['d_model'])\n",
    "        if config['use_meta_tokens']:\n",
    "            self.meta_tokens = nn.Parameter(torch.randn(1, config['n_meta_tokens'], config['d_model']))\n",
    "\n",
    "        layers = []\n",
    "        for _ in range(config['n_layers']):\n",
    "            if config['use_mamba_only']:\n",
    "                 layers.append(MambaOnlyBlock(\n",
    "                    d_model=config['d_model'],\n",
    "                    ffn_hidden_dim=config['ffn_hidden_dim'],\n",
    "                    mamba_params=config['mamba_params']\n",
    "                 ))\n",
    "            else: # Hymba Block for stages 2-5\n",
    "                layers.append(HymbaBlock(\n",
    "                    d_model=config['d_model'], n_heads=config['n_heads'], \n",
    "                    n_kv_heads=config['n_kv_heads'], max_seq_len=config['max_seq_len'],\n",
    "                    window_size=config['window_size'] if config['use_swa'] else -1,\n",
    "                    mamba_params=config['mamba_params']\n",
    "                 ))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.norm = RMSNorm(config['d_model'])\n",
    "        self.lm_head = nn.Linear(config['d_model'], config['vocab_size'], bias=False)\n",
    "\n",
    "    def _create_attention_mask(self, q_len: int, kv_len: int, window_size: int, n_meta_tokens: int, device: str) -> Optional[torch.Tensor]:\n",
    "        if q_len > 1:\n",
    "             mask = torch.full((1, 1, q_len, kv_len), float(\"-inf\"), device=device)\n",
    "             mask = torch.triu(mask, diagonal=1)\n",
    "             if window_size > 0:\n",
    "                 sliding_mask = torch.ones(q_len, kv_len, device=device).bool()\n",
    "                 sliding_mask.tril_(-1).triu_(-window_size)\n",
    "                 mask.masked_fill_(~sliding_mask[None, None, ...], float(\"-inf\"))\n",
    "             if n_meta_tokens > 0:\n",
    "                 mask[..., n_meta_tokens:, :n_meta_tokens] = 0\n",
    "             return mask.to(torch.float32)\n",
    "        return None\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor, use_cache: bool = False, return_kv_cache: bool = False):\n",
    "        batch_size, seq_len = tokens.shape\n",
    "        is_decoding = use_cache and tokens.shape[1] == 1\n",
    "        \n",
    "        h = self.embedding(tokens)\n",
    "        \n",
    "        current_seq_len = seq_len\n",
    "        if self.config['use_meta_tokens'] and not is_decoding:\n",
    "            meta_embeds = self.meta_tokens.expand(batch_size, -1, -1)\n",
    "            h = torch.cat([meta_embeds, h], dim=1)\n",
    "            current_seq_len += self.config['n_meta_tokens']\n",
    "            \n",
    "        kv_cache_list = [None] * self.config['n_layers']\n",
    "        \n",
    "        attn_mask = None\n",
    "        if not self.config['use_mamba_only'] and not is_decoding:\n",
    "            attn_mask = self._create_attention_mask(\n",
    "                q_len=current_seq_len, kv_len=current_seq_len,\n",
    "                window_size=self.config['window_size'] if self.config['use_swa'] else -1,\n",
    "                n_meta_tokens=self.config['n_meta_tokens'] if self.config['use_meta_tokens'] else 0,\n",
    "                device=h.device\n",
    "            )\n",
    "        \n",
    "        residual = h\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            past_kv = kv_cache_list[i] if use_cache and is_decoding else None\n",
    "            if use_cache and is_decoding and i > 0 and self.config['use_shared_kv_cache']:\n",
    "                 past_kv = kv_cache_list[i-1]\n",
    "            \n",
    "            output, present_kv = layer(h, past_kv=past_kv, attn_mask=attn_mask, use_cache=use_cache)\n",
    "            h = residual + output\n",
    "            residual = h\n",
    "\n",
    "            if use_cache:\n",
    "                kv_cache_list[i] = present_kv\n",
    "\n",
    "        h = self.norm(h)\n",
    "        \n",
    "        if self.config['use_meta_tokens'] and not is_decoding:\n",
    "            h = h[:, self.config['n_meta_tokens']:]\n",
    "            \n",
    "        logits = self.lm_head(h)\n",
    "        \n",
    "        if return_kv_cache:\n",
    "            return logits, kv_cache_list\n",
    "        return logits\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4. 단계별 모델링 및 성능 측정\n",
    "# -----------------------------------------------------------------------------\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if not torch.cuda.is_available() and selective_scan_fn is not None:\n",
    "        print(\"경고: 이 코드는 mamba-ssm의 CUDA 커널에 의존하므로 GPU 환경에서 실행해야 합니다.\")\n",
    "\n",
    "    base_config = {\n",
    "        'vocab_size': 32000, 'd_model': 256, 'n_layers': 4,\n",
    "        'n_heads': 8, 'n_kv_heads': 2, 'max_seq_len': 2048,\n",
    "        'ffn_hidden_dim': 256 * 4, 'window_size': 256, 'n_meta_tokens': 4,\n",
    "        'mamba_params': {'d_state': 16, 'd_conv': 4, 'expand': 2, 'dt_rank': math.ceil((256 * 2) / 16)},\n",
    "    }\n",
    "\n",
    "    ablation_stages = [\n",
    "        (\"1_Mamba_Only_Baseline\",     {'use_mamba_only': True, 'use_ssm_head': False, 'use_meta_tokens': False, 'use_shared_kv_cache': False, 'use_swa': False}),\n",
    "        (\"2_+_Attention (Hymba)\",     {'use_mamba_only': False, 'use_ssm_head': True, 'use_meta_tokens': False, 'use_shared_kv_cache': False, 'use_swa': False}),\n",
    "        (\"3_+_Meta_Tokens\",           {'use_mamba_only': False, 'use_ssm_head': True, 'use_meta_tokens': True,  'use_shared_kv_cache': False, 'use_swa': False}),\n",
    "        (\"4_+_Shared_KV_Cache\",       {'use_mamba_only': False, 'use_ssm_head': True, 'use_meta_tokens': True,  'use_shared_kv_cache': True,  'use_swa': False}),\n",
    "        (\"5_+_SWA\",                   {'use_mamba_only': False, 'use_ssm_head': True, 'use_meta_tokens': True,  'use_shared_kv_cache': True,  'use_swa': True}),\n",
    "    ]\n",
    "    \n",
    "    batch_size = 2\n",
    "    seq_len = 512\n",
    "    \n",
    "    for name, flags in ablation_stages:\n",
    "        print(f\"\\n{'='*20} {name} {'='*20}\")\n",
    "        config = base_config.copy()\n",
    "        config.update(flags)\n",
    "        \n",
    "        try:\n",
    "            model = HymbaModel(config).to(device)\n",
    "            model.eval()\n",
    "            \n",
    "            dummy_input = torch.randint(0, config['vocab_size'], (batch_size, seq_len)).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits, kv_cache_list = model(dummy_input, use_cache=True, return_kv_cache=True)\n",
    "            \n",
    "            total_cache_bytes = 0\n",
    "            if not config['use_mamba_only']:\n",
    "                if config['use_shared_kv_cache']:\n",
    "                    for i in range(1, config['n_layers'], 2):\n",
    "                         cache = kv_cache_list[i]\n",
    "                         if cache is not None:\n",
    "                            k, v = cache\n",
    "                            total_cache_bytes += k.numel() * k.element_size() + v.numel() * v.element_size()\n",
    "                else:\n",
    "                    for cache in kv_cache_list:\n",
    "                        if cache is not None:\n",
    "                            k, v = cache\n",
    "                            total_cache_bytes += k.numel() * k.element_size() + v.numel() * v.element_size()\n",
    "\n",
    "            total_cache_mb = total_cache_bytes / (1024 * 1024)\n",
    "            print(f\"--- KV Cache 크기: {total_cache_mb:.2f} MB\")\n",
    "\n",
    "            warmup_iterations = 5\n",
    "            measurement_iterations = 10\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for _ in range(warmup_iterations):\n",
    "                    _ = model(dummy_input, use_cache=False)\n",
    "                if torch.cuda.is_available(): torch.cuda.synchronize()\n",
    "\n",
    "                start_time = time.time()\n",
    "                for _ in range(measurement_iterations):\n",
    "                    _ = model(dummy_input, use_cache=False)\n",
    "                if torch.cuda.is_available(): torch.cuda.synchronize()\n",
    "                end_time = time.time()\n",
    "\n",
    "            total_time = end_time - start_time\n",
    "            total_tokens = batch_size * seq_len * measurement_iterations\n",
    "            tokens_per_second = total_tokens / total_time\n",
    "            print(f\"--- 처리량: {tokens_per_second:,.2f} tokens/sec\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"오류 발생: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cc2db5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81d0af30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== 1_Mamba_Only_Baseline ====================\n",
      "--- KV Cache 크기: 0.00 MB\n",
      "\n",
      "--- 훈련(Training) 모드 벤치마크 (전체 시퀀스 처리)\n",
      "--- 처리량: 482,489.79 tokens/sec\n",
      "\n",
      "--- 추론(Inference) 모드 벤치마크 (토큰당 생성)\n",
      "--- 처리량: 743.54 tokens/sec\n",
      "\n",
      "==================== 2_Hybrid ====================\n",
      "--- KV Cache 크기: 16.00 MB\n",
      "\n",
      "--- 훈련(Training) 모드 벤치마크 (전체 시퀀스 처리)\n",
      "--- 처리량: 339,068.78 tokens/sec\n",
      "\n",
      "--- 추론(Inference) 모드 벤치마크 (토큰당 생성)\n",
      "--- 처리량: 519.74 tokens/sec\n",
      "\n",
      "==================== 3_SWA_plus_Full_Attn ====================\n",
      "--- KV Cache 크기: 16.00 MB\n",
      "\n",
      "--- 훈련(Training) 모드 벤치마크 (전체 시퀀스 처리)\n",
      "--- 처리량: 342,285.06 tokens/sec\n",
      "\n",
      "--- 추론(Inference) 모드 벤치마크 (토큰당 생성)\n",
      "--- 처리량: 544.49 tokens/sec\n",
      "\n",
      "==================== 4_SWA_plus_Full_Attn_plus_KV_Sharing ====================\n",
      "--- KV Cache 크기: 8.00 MB\n",
      "\n",
      "--- 훈련(Training) 모드 벤치마크 (전체 시퀀스 처리)\n",
      "--- 처리량: 342,659.18 tokens/sec\n",
      "\n",
      "--- 추론(Inference) 모드 벤치마크 (토큰당 생성)\n",
      "--- 처리량: 518.31 tokens/sec\n",
      "\n",
      "==================== 5_Replace_Mean_by_Concat ====================\n",
      "--- KV Cache 크기: 8.00 MB\n",
      "\n",
      "--- 훈련(Training) 모드 벤치마크 (전체 시퀀스 처리)\n",
      "--- 처리량: 336,800.08 tokens/sec\n",
      "\n",
      "--- 추론(Inference) 모드 벤치마크 (토큰당 생성)\n",
      "--- 처리량: 515.38 tokens/sec\n",
      "\n",
      "==================== 6_Full_Hymba_Model_with_Meta_Tokens ====================\n",
      "--- KV Cache 크기: 8.02 MB\n",
      "\n",
      "--- 훈련(Training) 모드 벤치마크 (전체 시퀀스 처리)\n",
      "--- 처리량: 325,661.26 tokens/sec\n",
      "\n",
      "--- 추론(Inference) 모드 벤치마크 (토큰당 생성)\n",
      "--- 처리량: 519.17 tokens/sec\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import math\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "# mamba-ssm과 causal-conv1d는 저수준 CUDA 커널을 사용하기 위해 필요합니다.\n",
    "# pip install mamba-ssm causal-conv1d\n",
    "try:\n",
    "    from mamba_ssm.ops.selective_scan_interface import selective_scan_fn\n",
    "    from causal_conv1d import causal_conv1d_fn\n",
    "except ImportError:\n",
    "    print(\"Warning: mamba-ssm or causal-conv1d not found. MambaBranch will not work.\")\n",
    "    selective_scan_fn = None\n",
    "    causal_conv1d_fn = None\n",
    "\n",
    "# Flash Attention 관련 API 가져오기\n",
    "try:\n",
    "    from flash_attn import flash_attn_func\n",
    "    _FLASH_ATTENTION_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Warning: Flash Attention not found. Falling back to native PyTorch.\")\n",
    "    flash_attn_func = None\n",
    "    _FLASH_ATTENTION_AVAILABLE = False\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. 아키텍처의 기본 구성 요소\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
    "        return self.weight * hidden_states.to(input_dtype)\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim: int, max_seq_len: int, base: int = 10000, device: Optional[str] = None):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2, device=device, dtype=torch.float32) / self.dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "        self._set_cos_sin_cache(seq_len=max_seq_len, device=device)\n",
    "\n",
    "    def _set_cos_sin_cache(self, seq_len: int, device: Optional[str], dtype: torch.dtype = torch.float32):\n",
    "        self.max_seq_len_cached = seq_len\n",
    "        t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        self.register_buffer(\"cos_cached\", emb.cos().to(dtype), persistent=False)\n",
    "        self.register_buffer(\"sin_cached\", emb.sin().to(dtype), persistent=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, seq_len: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        if seq_len > self.max_seq_len_cached:\n",
    "            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)\n",
    "        return self.cos_cached[:seq_len, ...].to(dtype=x.dtype), self.sin_cached[:seq_len, ...].to(dtype=x.dtype)\n",
    "\n",
    "def rotate_half(x: torch.Tensor) -> torch.Tensor:\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "def apply_rotary_pos_emb(q: torch.Tensor, k: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    cos = cos.unsqueeze(0).unsqueeze(2)\n",
    "    sin = sin.unsqueeze(0).unsqueeze(2)\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n",
    "\n",
    "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
    "    if n_rep == 1:\n",
    "        return hidden_states\n",
    "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
    "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
    "\n",
    "class AttentionBranch(nn.Module):\n",
    "    def __init__(self, d_inner: int, n_heads: int, n_kv_heads: int, max_seq_len: int, window_size: int = -1, n_meta_tokens: int = 0):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.n_kv_heads = n_kv_heads\n",
    "        self.n_key_value_groups = n_heads // n_kv_heads\n",
    "        self.head_dim = d_inner // n_heads\n",
    "        self.rotary_emb = RotaryEmbedding(self.head_dim, max_seq_len)\n",
    "        self.window_size = window_size\n",
    "        self.n_meta_tokens = n_meta_tokens\n",
    "\n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, past_kv: Optional[Tuple[torch.Tensor, torch.Tensor]] = None, attn_mask: Optional[torch.Tensor] = None, use_cache: bool = False):\n",
    "        batch_size, q_len, _ = q.shape\n",
    "\n",
    "        q = q.view(batch_size, q_len, self.n_heads, self.head_dim)\n",
    "        k = k.view(batch_size, q_len, self.n_kv_heads, self.head_dim)\n",
    "        v = v.view(batch_size, q_len, self.n_kv_heads, self.head_dim)\n",
    "\n",
    "        kv_seq_len = q_len\n",
    "        if past_kv is not None:\n",
    "            kv_seq_len += past_kv[0].shape[1]\n",
    "        \n",
    "        cos, sin = self.rotary_emb(v, seq_len=kv_seq_len)\n",
    "        \n",
    "        if past_kv is not None:\n",
    "            cos = cos[past_kv[0].shape[1]:]\n",
    "            sin = sin[past_kv[0].shape[1]:]\n",
    "        \n",
    "        q, k = apply_rotary_pos_emb(q, k, cos, sin)\n",
    "        \n",
    "        if past_kv is not None:\n",
    "            past_key, past_value = past_kv\n",
    "            k = torch.cat([past_key, k], dim=1)\n",
    "            v = torch.cat([past_value, v], dim=1)\n",
    "            \n",
    "        present_kv = (k, v) if use_cache else None\n",
    "        \n",
    "        if flash_attn_func is not None and not use_cache and attn_mask is None:\n",
    "            q = q.transpose(1, 2)\n",
    "            k = repeat_kv(k.transpose(1, 2), self.n_key_value_groups)\n",
    "            v = repeat_kv(v.transpose(1, 2), self.n_key_value_groups)\n",
    "\n",
    "            attn_output = flash_attn_func(q, k, v, causal=True)\n",
    "            attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, q_len, -1)\n",
    "            \n",
    "        else:\n",
    "            q = q.transpose(1, 2)\n",
    "            k = repeat_kv(k.transpose(1, 2), self.n_key_value_groups)\n",
    "            v = repeat_kv(v.transpose(1, 2), self.n_key_value_groups)\n",
    "            \n",
    "            if attn_mask is None and q_len > 1:\n",
    "                 pass\n",
    "\n",
    "            attn_output = F.scaled_dot_product_attention(q, k, v, attn_mask=attn_mask)\n",
    "            attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, q_len, -1)\n",
    "            \n",
    "        return attn_output, present_kv\n",
    "\n",
    "class MambaBranch(nn.Module):\n",
    "    def __init__(self, d_inner, d_state, d_conv, dt_rank):\n",
    "        super().__init__()\n",
    "        if causal_conv1d_fn is None or selective_scan_fn is None:\n",
    "            raise ImportError(\"Mamba packages not found. Please install them.\")\n",
    "        \n",
    "        self.d_inner, self.d_state, self.d_conv, self.dt_rank = d_inner, d_state, d_conv, dt_rank\n",
    "        \n",
    "        self.conv1d = nn.Conv1d(in_channels=self.d_inner, out_channels=self.d_inner, kernel_size=d_conv, bias=True, groups=self.d_inner, padding=d_conv - 1)\n",
    "        self.x_proj = nn.Linear(self.d_inner, self.dt_rank + 2 * self.d_state, bias=False)\n",
    "        self.dt_proj = nn.Linear(self.dt_rank, self.d_inner, bias=True)\n",
    "        \n",
    "        A = torch.arange(1, self.d_state + 1, dtype=torch.float32).repeat(self.d_inner, 1)\n",
    "        self.A_log = nn.Parameter(torch.log(A))\n",
    "        self.D = nn.Parameter(torch.ones(self.d_inner))\n",
    "\n",
    "    def forward(self, x, z):\n",
    "        x_transposed = x.transpose(1, 2).contiguous()\n",
    "        x_conv = causal_conv1d_fn(x_transposed, self.conv1d.weight.squeeze(1), self.conv1d.bias, activation=\"silu\")\n",
    "        \n",
    "        x_dbl = self.x_proj(x_conv.transpose(1, 2))\n",
    "        dt_pre, B, C = torch.split(x_dbl, [self.dt_rank, self.d_state, self.d_state], dim=-1)\n",
    "        \n",
    "        dt = self.dt_proj(dt_pre).transpose(1, 2)\n",
    "        A = -torch.exp(self.A_log.float())\n",
    "        \n",
    "        y = selective_scan_fn(\n",
    "            x_conv, dt, A, B.transpose(1, 2), C.transpose(1, 2), self.D.float(), \n",
    "            z=z.transpose(1,2), delta_bias=self.dt_proj.bias.float(), delta_softplus=True\n",
    "        )\n",
    "        return y.transpose(1,2)\n",
    "        \n",
    "# -----------------------------------------------------------------------------\n",
    "# 2. Ablation을 위한 각 단계별 블록 정의\n",
    "# -----------------------------------------------------------------------------\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model: int, hidden_dim: int):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(d_model, hidden_dim, bias=False)\n",
    "        self.w3 = nn.Linear(d_model, hidden_dim, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_dim, d_model, bias=False)\n",
    "        self.act_fn = F.silu\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.w2(self.act_fn(self.w1(x)) * self.w3(x))\n",
    "\n",
    "class MambaOnlyBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, ffn_hidden_dim: int, mamba_params: dict):\n",
    "        super().__init__()\n",
    "        self.d_inner = mamba_params['expand'] * d_model\n",
    "        \n",
    "        self.norm = RMSNorm(d_model)\n",
    "        self.in_proj = nn.Linear(d_model, 2 * self.d_inner, bias=False)\n",
    "        self.mamba_branch = MambaBranch(d_inner=self.d_inner, **{k:v for k,v in mamba_params.items() if k != 'expand'})\n",
    "        self.out_proj = nn.Linear(self.d_inner, d_model, bias=False)\n",
    "        self.ffn = FeedForward(d_model, ffn_hidden_dim)\n",
    "        self.ffn_norm = RMSNorm(d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, **kwargs):\n",
    "        residual = x\n",
    "        x_norm = self.norm(x)\n",
    "        \n",
    "        xz = self.in_proj(x_norm)\n",
    "        x_mamba, z_mamba = xz.chunk(2, dim=-1)\n",
    "        \n",
    "        mamba_out = self.mamba_branch(x_mamba, z_mamba)\n",
    "        \n",
    "        h = residual + self.out_proj(mamba_out)\n",
    "        out = h + self.ffn(self.ffn_norm(h))\n",
    "        return out, None\n",
    "\n",
    "class HymbaBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, n_kv_heads: int, max_seq_len: int, window_size: int, mamba_params: dict, n_meta_tokens: int, use_concat: bool = False):\n",
    "        super().__init__()\n",
    "        self.d_inner = mamba_params['expand'] * d_model\n",
    "        attn_head_dim = self.d_inner // n_heads\n",
    "        \n",
    "        self.use_concat = use_concat\n",
    "        \n",
    "        out_proj_input_dim = self.d_inner * 2 if self.use_concat else self.d_inner\n",
    "        \n",
    "        latent_dim = self.d_inner + self.d_inner + (attn_head_dim * n_kv_heads * 2)\n",
    "        self.in_proj = nn.Linear(d_model, latent_dim + self.d_inner, bias=True)\n",
    "        self.out_proj = nn.Linear(out_proj_input_dim, d_model, bias=True)\n",
    "\n",
    "        self.attn_branch = AttentionBranch(d_inner=self.d_inner, n_heads=n_heads, n_kv_heads=n_kv_heads, max_seq_len=max_seq_len, window_size=window_size, n_meta_tokens=n_meta_tokens)\n",
    "        self.mamba_branch = MambaBranch(d_inner=self.d_inner, d_state=mamba_params['d_state'], d_conv=mamba_params['d_conv'], dt_rank=mamba_params['dt_rank'])\n",
    "\n",
    "        self.norm = RMSNorm(d_model)\n",
    "        self.norm1 = RMSNorm(self.d_inner)\n",
    "        self.norm2 = RMSNorm(self.d_inner)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, past_kv: Optional[Tuple[torch.Tensor, torch.Tensor]] = None, attn_mask: Optional[torch.Tensor] = None, use_cache: bool = False):\n",
    "        projected = self.in_proj(self.norm(x))\n",
    "        \n",
    "        latent, gate = projected.tensor_split((projected.shape[-1] - self.d_inner,), dim=-1)\n",
    "        \n",
    "        attn_q_dim = self.d_inner\n",
    "        attn_k_dim = self.attn_branch.head_dim * self.attn_branch.n_kv_heads\n",
    "        \n",
    "        q, k, v, mamba_x = latent.tensor_split((attn_q_dim, attn_q_dim + attn_k_dim, attn_q_dim + 2 * attn_k_dim), dim=-1)\n",
    "\n",
    "        attn_out, present_kv = self.attn_branch(q, k, v, past_kv, attn_mask, use_cache)\n",
    "        mamba_out = self.mamba_branch(mamba_x, gate)\n",
    "\n",
    "        if self.use_concat:\n",
    "            combined = torch.cat([self.norm1(attn_out), self.norm2(mamba_out)], dim=-1)\n",
    "        else:\n",
    "            combined = (self.norm1(attn_out) + self.norm2(mamba_out)) / 2\n",
    "            \n",
    "        return self.out_proj(combined), present_kv\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3. 전체 모델 및 마스크 생성\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class HymbaModel(nn.Module):\n",
    "    def __init__(self, config: dict):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embedding = nn.Embedding(config['vocab_size'], config['d_model'])\n",
    "        if config['use_meta_tokens']:\n",
    "            self.meta_tokens = nn.Parameter(torch.randn(1, config['n_meta_tokens'], config['d_model']))\n",
    "\n",
    "        layers = []\n",
    "        for i in range(config['n_layers']):\n",
    "            is_full_attn_layer = False\n",
    "            is_mamba_only_config = config['use_mamba_only']\n",
    "\n",
    "            if not is_mamba_only_config and config['use_full_attention_layers']:\n",
    "                mid_layer = config['n_layers'] // 2\n",
    "                if config['n_layers'] % 2 == 0:\n",
    "                    mid_layer = mid_layer - 1\n",
    "\n",
    "                if i == 0 or i == mid_layer or i == config['n_layers'] - 1:\n",
    "                    is_full_attn_layer = True\n",
    "\n",
    "            use_swa = config['use_swa'] and not is_full_attn_layer and not is_mamba_only_config\n",
    "\n",
    "            if is_mamba_only_config:\n",
    "                 layers.append(MambaOnlyBlock(\n",
    "                     d_model=config['d_model'],\n",
    "                     ffn_hidden_dim=config['ffn_hidden_dim'],\n",
    "                     mamba_params=config['mamba_params']\n",
    "                   ))\n",
    "            else:\n",
    "                layers.append(HymbaBlock(\n",
    "                     d_model=config['d_model'], n_heads=config['n_heads'], \n",
    "                     n_kv_heads=config['n_kv_heads'], max_seq_len=config['max_seq_len'],\n",
    "                     window_size=config['window_size'] if use_swa else -1,\n",
    "                     mamba_params=config['mamba_params'],\n",
    "                     n_meta_tokens=config['n_meta_tokens'] if config['use_meta_tokens'] else 0,\n",
    "                     use_concat=config['use_concat']\n",
    "                   ))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.norm = RMSNorm(config['d_model'])\n",
    "        self.lm_head = nn.Linear(config['d_model'], config['vocab_size'], bias=False)\n",
    "        \n",
    "    def _create_attention_mask(self, q_len: int, kv_len: int, window_size: int, n_meta_tokens: int, device: str) -> Optional[torch.Tensor]:\n",
    "        if q_len > 1:\n",
    "            mask = torch.full((1, 1, q_len, kv_len), float(\"-inf\"), device=device)\n",
    "            mask = torch.triu(mask, diagonal=1)\n",
    "            \n",
    "            if window_size > 0:\n",
    "                sliding_mask = torch.ones(q_len, kv_len, device=device).bool()\n",
    "                sliding_mask.tril_(-1).triu_(-window_size)\n",
    "                mask.masked_fill_(~sliding_mask[None, None, ...], float(\"-inf\"))\n",
    "            \n",
    "            if n_meta_tokens > 0:\n",
    "                mask[..., n_meta_tokens:, :n_meta_tokens] = 0\n",
    "            \n",
    "            return mask.to(torch.float32)\n",
    "        return None\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor, use_cache: bool = False, return_kv_cache: bool = False):\n",
    "        batch_size, seq_len = tokens.shape\n",
    "        is_decoding = use_cache and tokens.shape[1] == 1\n",
    "        \n",
    "        h = self.embedding(tokens)\n",
    "        \n",
    "        current_seq_len = seq_len\n",
    "        if self.config['use_meta_tokens'] and not is_decoding:\n",
    "            meta_embeds = self.meta_tokens.expand(batch_size, -1, -1)\n",
    "            h = torch.cat([meta_embeds, h], dim=1)\n",
    "            current_seq_len += self.config['n_meta_tokens']\n",
    "            \n",
    "        kv_cache_list = [None] * self.config['n_layers']\n",
    "        \n",
    "        attn_mask = None\n",
    "        if not self.config['use_mamba_only'] and not is_decoding:\n",
    "            attn_mask = self._create_attention_mask(\n",
    "                q_len=current_seq_len, kv_len=current_seq_len,\n",
    "                window_size=self.config['window_size'] if self.config['use_swa'] else -1,\n",
    "                n_meta_tokens=self.config['n_meta_tokens'] if self.config['use_meta_tokens'] else 0,\n",
    "                device=h.device\n",
    "            )\n",
    "        \n",
    "        residual = h\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            past_kv = kv_cache_list[i] if use_cache and is_decoding else None\n",
    "            \n",
    "            if use_cache and is_decoding and i > 0 and self.config['use_shared_kv_cache']:\n",
    "                 past_kv = kv_cache_list[i-1] if i % 2 == 1 else past_kv\n",
    "            \n",
    "            output, present_kv = layer(h, past_kv=past_kv, attn_mask=attn_mask, use_cache=use_cache)\n",
    "            h = residual + output\n",
    "            residual = h\n",
    "\n",
    "            if use_cache:\n",
    "                kv_cache_list[i] = present_kv\n",
    "\n",
    "        h = self.norm(h)\n",
    "        \n",
    "        if self.config['use_meta_tokens'] and not is_decoding:\n",
    "            h = h[:, self.config['n_meta_tokens']:]\n",
    "            \n",
    "        logits = self.lm_head(h)\n",
    "        \n",
    "        if return_kv_cache:\n",
    "            return logits, kv_cache_list\n",
    "        return logits\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4. 단계별 모델링 및 성능 측정\n",
    "# -----------------------------------------------------------------------------\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if not torch.cuda.is_available() and selective_scan_fn is not None:\n",
    "        print(\"경고: 이 코드는 mamba-ssm의 CUDA 커널에 의존하므로 GPU 환경에서 실행해야 합니다.\")\n",
    "    \n",
    "    base_config = {\n",
    "        'vocab_size': 32000, 'd_model': 256, 'n_layers': 4,\n",
    "        'n_heads': 8, 'n_kv_heads': 2, 'max_seq_len': 2048,\n",
    "        'ffn_hidden_dim': 256 * 4, 'window_size': 256, 'n_meta_tokens': 4,\n",
    "        'mamba_params': {'d_state': 16, 'd_conv': 4, 'expand': 2, 'dt_rank': math.ceil((256 * 2) / 16)},\n",
    "    }\n",
    "    \n",
    "    # 통합된 기능들을 제어하기 위한 새로운 어블레이션 스테이지\n",
    "    ablation_stages_all_features = [\n",
    "        (\"1_Mamba_Only_Baseline\",          {'use_mamba_only': True,  'use_swa': False, 'use_shared_kv_cache': False, 'use_meta_tokens': False, 'use_full_attention_layers': False, 'use_concat': False}),\n",
    "        (\"2_Hybrid\",                       {'use_mamba_only': False, 'use_swa': False, 'use_shared_kv_cache': False, 'use_meta_tokens': False, 'use_full_attention_layers': False, 'use_concat': False}),\n",
    "        (\"3_SWA_plus_Full_Attn\",           {'use_mamba_only': False, 'use_swa': True,  'use_shared_kv_cache': False, 'use_meta_tokens': False, 'use_full_attention_layers': True, 'use_concat': False}),\n",
    "        (\"4_SWA_plus_Full_Attn_plus_KV_Sharing\",    {'use_mamba_only': False, 'use_swa': True,  'use_shared_kv_cache': True, 'use_meta_tokens': False, 'use_full_attention_layers': True, 'use_concat': False}),\n",
    "        (\"5_Replace_Mean_by_Concat\",       {'use_mamba_only': False, 'use_swa': True,  'use_shared_kv_cache': True, 'use_meta_tokens': False, 'use_full_attention_layers': True, 'use_concat': True}),\n",
    "        (\"6_Full_Hymba_Model_with_Meta_Tokens\", {'use_mamba_only': False, 'use_swa': True, 'use_shared_kv_cache': True, 'use_meta_tokens': True, 'use_full_attention_layers': True, 'use_concat': False}),\n",
    "    ]\n",
    "\n",
    "    batch_size = 2\n",
    "    seq_len = 2048\n",
    "    \n",
    "    for name, flags in ablation_stages_all_features:\n",
    "        print(f\"\\n{'='*20} {name} {'='*20}\")\n",
    "        config = base_config.copy()\n",
    "        config.update(flags)\n",
    "        \n",
    "        try:\n",
    "            model = HymbaModel(config).to(device)\n",
    "            model.eval()\n",
    "            \n",
    "            dummy_input = torch.randint(0, config['vocab_size'], (batch_size, seq_len)).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits, kv_cache_list = model(dummy_input, use_cache=True, return_kv_cache=True)\n",
    "            \n",
    "            total_cache_bytes = 0\n",
    "            if not config['use_mamba_only']:\n",
    "                if config['use_shared_kv_cache']:\n",
    "                    for i in range(1, config['n_layers'], 2):\n",
    "                        cache = kv_cache_list[i]\n",
    "                        if cache is not None:\n",
    "                            k, v = cache\n",
    "                            total_cache_bytes += k.numel() * k.element_size() + v.numel() * v.element_size()\n",
    "                else:\n",
    "                    for cache in kv_cache_list:\n",
    "                        if cache is not None:\n",
    "                            k, v = cache\n",
    "                            total_cache_bytes += k.numel() * k.element_size() + v.numel() * v.element_size()\n",
    "\n",
    "            total_cache_mb = total_cache_bytes / (1024 * 1024)\n",
    "            print(f\"--- KV Cache 크기: {total_cache_mb:.2f} MB\")\n",
    "\n",
    "            print(\"\\n--- 훈련(Training) 모드 벤치마크 (전체 시퀀스 처리)\")\n",
    "            warmup_iterations = 5\n",
    "            measurement_iterations = 10\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for _ in range(warmup_iterations):\n",
    "                    _ = model(dummy_input, use_cache=False)\n",
    "                if torch.cuda.is_available(): torch.cuda.synchronize()\n",
    "\n",
    "                start_time = time.time()\n",
    "                for _ in range(measurement_iterations):\n",
    "                    _ = model(dummy_input, use_cache=False)\n",
    "                if torch.cuda.is_available(): torch.cuda.synchronize()\n",
    "                end_time = time.time()\n",
    "\n",
    "            total_time = end_time - start_time\n",
    "            total_tokens = batch_size * seq_len * measurement_iterations\n",
    "            tokens_per_second = total_tokens / total_time\n",
    "            print(f\"--- 처리량: {tokens_per_second:,.2f} tokens/sec\")\n",
    "\n",
    "            print(\"\\n--- 추론(Inference) 모드 벤치마크 (토큰당 생성)\")\n",
    "            input_tokens = torch.randint(0, config['vocab_size'], (batch_size, 1)).to(device)\n",
    "            num_tokens_to_generate = 100\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for _ in range(warmup_iterations):\n",
    "                    _ = model(input_tokens, use_cache=True, return_kv_cache=True)\n",
    "                if torch.cuda.is_available(): torch.cuda.synchronize()\n",
    "\n",
    "                start_time = time.time()\n",
    "                current_kv_cache = None\n",
    "                for _ in range(num_tokens_to_generate):\n",
    "                    logits, current_kv_cache = model(input_tokens, use_cache=True, return_kv_cache=True)\n",
    "                    next_token = torch.argmax(logits[:, -1, :], dim=-1).unsqueeze(1)\n",
    "                    input_tokens = next_token\n",
    "                \n",
    "                if torch.cuda.is_available(): torch.cuda.synchronize()\n",
    "                end_time = time.time()\n",
    "\n",
    "            total_time = end_time - start_time\n",
    "            tokens_per_second = (batch_size * num_tokens_to_generate) / total_time\n",
    "            print(f\"--- 처리량: {tokens_per_second:,.2f} tokens/sec\")\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a35f183",
   "metadata": {},
   "source": [
    "### Flash&FlexAttention 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d617025",
   "metadata": {},
   "source": [
    "하이브리드 구조의 오버헤드:\n",
    "\n",
    "- Attention의 본질적 한계: Attention 연산은 토큰을 하나씩 생성하는 추론 단계에서, 이전에 생성된 모든 토큰과의 관계를 계산해야 합니다. 이는 시퀀스 길이가 길어질수록 연산량이 증가하며, Mamba의 선형적인 상태 업데이트 방식에 비해 비효율적입니다.\n",
    "\n",
    "- 병렬 연산의 충돌: HymbaBlock은 Attention과 Mamba 연산을 병렬로 수행한 후 결과를 합칩니다. 이 과정에서 두 연산의 데이터 접근 패턴과 메모리 사용 방식이 달라 GPU 리소스 활용에 비효율이 발생할 수 있습니다. Mamba는 특정 커널로 최적화되어 있지만, Attention이 추가되면서 전체 블록의 효율성이 떨어지는 것입니다.\n",
    "\n",
    "KV 캐시 관리의 비효율:\n",
    "\n",
    "- 메모리 복사: 추론 단계에서 KV 캐시를 사용하면, 매번 새로운 토큰의 키(K)와 값(V)을 기존 캐시에 추가해야 합니다. 이 과정에서 텐서를 재할당하고 메모리를 복사하는 오버헤드가 발생합니다. 특히, HymbaBlock이 Attention 연산을 위해 KV 캐시를 필요로 하기 때문에 이러한 오버헤드가 누적됩니다.\n",
    "\n",
    "- KV 캐시 공유의 한계: Cross-layer KV sharing은 메모리 사용량을 줄이지만, 추론 속도에 직접적인 이점을 주지 못할 수 있습니다. 공유된 캐시를 사용하는 복잡한 인덱싱 및 접근 로직이 추가 오버헤드를 발생시킬 수 있기 때문입니다.\n",
    "\n",
    "벤치마크 환경의 영향:\n",
    "\n",
    "- 단일 토큰 생성: 현재의 추론 벤치마크는 토큰을 하나씩 순차적으로 생성하는 방식입니다. 이 방식은 FlashAttention과 같이 전체 시퀀스를 한 번에 처리하는 데 최적화된 커널의 이점을 제대로 살리지 못합니다. Mamba는 이와 같은 순차적인 추론에 특화되어 있으므로, Mamba-only 모델이 더 빠른 결과를 내는 것이 자연스럽습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab4a9623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-03 10:16:55,526] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-03 10:16:57.068568: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-09-03 10:16:57.780339: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== 1_Mamba_Only_Baseline ====================\n",
      "--- KV Cache 크기: 0.00 MB\n",
      "\n",
      "--- 훈련(Training) 모드 벤치마크 (전체 시퀀스 처리)\n",
      "--- 처리량: 367,892.25 tokens/sec\n",
      "\n",
      "--- 추론(Inference) 모드 벤치마크 (토큰당 생성)\n",
      "--- 처리량: 240.78 tokens/sec\n",
      "\n",
      "==================== 2_Hybrid ====================\n",
      "--- KV Cache 크기: 48.00 MB\n",
      "\n",
      "--- 훈련(Training) 모드 벤치마크 (전체 시퀀스 처리)\n",
      "--- 처리량: 290,215.69 tokens/sec\n",
      "\n",
      "--- 추론(Inference) 모드 벤치마크 (토큰당 생성)\n",
      "--- 처리량: 177.73 tokens/sec\n",
      "\n",
      "==================== 3_SWA_plus_Full_Attn ====================\n",
      "--- KV Cache 크기: 48.00 MB\n",
      "\n",
      "--- 훈련(Training) 모드 벤치마크 (전체 시퀀스 처리)\n",
      "--- 처리량: 289,625.64 tokens/sec\n",
      "\n",
      "--- 추론(Inference) 모드 벤치마크 (토큰당 생성)\n",
      "--- 처리량: 174.12 tokens/sec\n",
      "\n",
      "==================== 4_SWA_plus_Full_Attn_plus_KV_Sharing ====================\n",
      "--- KV Cache 크기: 24.00 MB\n",
      "\n",
      "--- 훈련(Training) 모드 벤치마크 (전체 시퀀스 처리)\n",
      "--- 처리량: 289,438.76 tokens/sec\n",
      "\n",
      "--- 추론(Inference) 모드 벤치마크 (토큰당 생성)\n",
      "--- 처리량: 173.71 tokens/sec\n",
      "\n",
      "==================== 5_Replace_Mean_by_Concat ====================\n",
      "--- KV Cache 크기: 24.00 MB\n",
      "\n",
      "--- 훈련(Training) 모드 벤치마크 (전체 시퀀스 처리)\n",
      "--- 처리량: 284,322.89 tokens/sec\n",
      "\n",
      "--- 추론(Inference) 모드 벤치마크 (토큰당 생성)\n",
      "--- 처리량: 175.90 tokens/sec\n",
      "\n",
      "==================== 6_Full_Hymba_Model_with_Meta_Tokens ====================\n",
      "--- KV Cache 크기: 24.05 MB\n",
      "\n",
      "--- 훈련(Training) 모드 벤치마크 (전체 시퀀스 처리)\n",
      "--- 처리량: 30,343.56 tokens/sec\n",
      "\n",
      "--- 추론(Inference) 모드 벤치마크 (토큰당 생성)\n",
      "--- 처리량: 175.69 tokens/sec\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import math\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "# mamba-ssm과 causal-conv1d는 저수준 CUDA 커널을 사용하기 위해 필요합니다.\n",
    "# pip install mamba-ssm causal-conv1d\n",
    "try:\n",
    "    from mamba_ssm.ops.selective_scan_interface import selective_scan_fn\n",
    "    from causal_conv1d import causal_conv1d_fn\n",
    "except ImportError:\n",
    "    print(\"Warning: mamba-ssm or causal-conv1d not found. MambaBranch will not work.\")\n",
    "    selective_scan_fn = None\n",
    "    causal_conv1d_fn = None\n",
    "\n",
    "# Flash Attention 및 Flex Attention 관련 API 가져오기\n",
    "try:\n",
    "    from flash_attn import flash_attn_func\n",
    "    from torch.nn.attention.flex_attention import flex_attention, create_block_mask, and_masks, or_masks\n",
    "    _FLEX_ATTENTION_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Warning: Flex Attention not found. Falling back to native PyTorch.\")\n",
    "    flash_attn_func = None\n",
    "    _FLEX_ATTENTION_AVAILABLE = False\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. 아키텍처의 기본 구성 요소\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        # autocast를 제거하고, bfloat16에서도 동작하도록 수정\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
    "        return self.weight * hidden_states\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim: int, max_seq_len: int, base: int = 10000, device: Optional[str] = None):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2, device=device, dtype=torch.float32) / self.dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "        self._set_cos_sin_cache(seq_len=max_seq_len, device=device)\n",
    "\n",
    "    def _set_cos_sin_cache(self, seq_len: int, device: Optional[str], dtype: torch.dtype = torch.float32):\n",
    "        self.max_seq_len_cached = seq_len\n",
    "        t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        self.register_buffer(\"cos_cached\", emb.cos().to(dtype), persistent=False)\n",
    "        self.register_buffer(\"sin_cached\", emb.sin().to(dtype), persistent=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, seq_len: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        if seq_len > self.max_seq_len_cached:\n",
    "            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)\n",
    "        return self.cos_cached[:seq_len, ...].to(dtype=x.dtype), self.sin_cached[:seq_len, ...].to(dtype=x.dtype)\n",
    "\n",
    "def rotate_half(x: torch.Tensor) -> torch.Tensor:\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "def apply_rotary_pos_emb(q: torch.Tensor, k: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    cos = cos.unsqueeze(0).unsqueeze(2)\n",
    "    sin = sin.unsqueeze(0).unsqueeze(2)\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n",
    "\n",
    "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
    "    if n_rep == 1:\n",
    "        return hidden_states\n",
    "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
    "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
    "\n",
    "class AttentionBranch(nn.Module):\n",
    "    def __init__(self, d_inner: int, n_heads: int, n_kv_heads: int, max_seq_len: int, window_size: int = -1, n_meta_tokens: int = 0):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.n_kv_heads = n_kv_heads\n",
    "        self.n_key_value_groups = n_heads // n_kv_heads\n",
    "        self.head_dim = d_inner // n_heads\n",
    "        self.rotary_emb = RotaryEmbedding(self.head_dim, max_seq_len)\n",
    "        self.window_size = window_size\n",
    "        self.n_meta_tokens = n_meta_tokens\n",
    "        self.use_flex_attention = (window_size > 0 and n_meta_tokens > 0 and _FLEX_ATTENTION_AVAILABLE)\n",
    "\n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, past_kv: Optional[Tuple[torch.Tensor, torch.Tensor]] = None, attn_mask: Optional[torch.Tensor] = None, use_cache: bool = False):\n",
    "        batch_size, q_len, _ = q.shape\n",
    "\n",
    "        q = q.view(batch_size, q_len, self.n_heads, self.head_dim)\n",
    "        k = k.view(batch_size, q_len, self.n_kv_heads, self.head_dim)\n",
    "        v = v.view(batch_size, q_len, self.n_kv_heads, self.head_dim)\n",
    "\n",
    "        kv_seq_len = q_len\n",
    "        if past_kv is not None:\n",
    "            kv_seq_len += past_kv[0].shape[1]\n",
    "        \n",
    "        cos, sin = self.rotary_emb(v, seq_len=kv_seq_len)\n",
    "        \n",
    "        if past_kv is not None:\n",
    "            cos = cos[past_kv[0].shape[1]:]\n",
    "            sin = sin[past_kv[0].shape[1]:]\n",
    "        \n",
    "        q, k = apply_rotary_pos_emb(q, k, cos, sin)\n",
    "        \n",
    "        if past_kv is not None:\n",
    "            past_key, past_value = past_kv\n",
    "            k = torch.cat([past_key, k], dim=1)\n",
    "            v = torch.cat([past_value, v], dim=1)\n",
    "            \n",
    "        present_kv = (k, v) if use_cache else None\n",
    "        \n",
    "        if flash_attn_func is not None and not use_cache and not self.use_flex_attention:\n",
    "            q = q.transpose(1, 2)\n",
    "            k = repeat_kv(k.transpose(1, 2), self.n_key_value_groups)\n",
    "            v = repeat_kv(v.transpose(1, 2), self.n_key_value_groups)\n",
    "            attn_output = flash_attn_func(q, k, v, causal=True)\n",
    "            attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, q_len, -1)\n",
    "            \n",
    "        elif self.use_flex_attention:\n",
    "            if not use_cache:\n",
    "                q = q.transpose(1, 2)\n",
    "                k = repeat_kv(k.transpose(1, 2), self.n_key_value_groups)\n",
    "                v = repeat_kv(v.transpose(1, 2), self.n_key_value_groups)\n",
    "\n",
    "                def sliding_window_mask_fn(b, h, q_idx, kv_idx):\n",
    "                    return q_idx - kv_idx <= self.window_size\n",
    "\n",
    "                def causal_mask_fn(b, h, q_idx, kv_idx):\n",
    "                    return q_idx >= kv_idx\n",
    "                \n",
    "                def prefix_mask_fn(b, h, q_idx, kv_idx):\n",
    "                    return kv_idx < self.n_meta_tokens\n",
    "                \n",
    "                attn_mask_fn = and_masks(causal_mask_fn, sliding_window_mask_fn)\n",
    "                combined_mask_fn = or_masks(attn_mask_fn, prefix_mask_fn)\n",
    "                \n",
    "                block_mask = create_block_mask(\n",
    "                    combined_mask_fn, B=batch_size, H=self.n_heads, Q_LEN=q_len, KV_LEN=kv_seq_len\n",
    "                )\n",
    "\n",
    "                attn_output = flex_attention(q, k, v, block_mask=block_mask)\n",
    "                attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, q_len, -1)\n",
    "            else:\n",
    "                q = q.transpose(1, 2)\n",
    "                k = repeat_kv(k.transpose(1, 2), self.n_key_value_groups)\n",
    "                v = repeat_kv(v.transpose(1, 2), self.n_key_value_groups)\n",
    "                if attn_mask is None and q_len > 1:\n",
    "                     attn_mask = torch.full((q_len, kv_seq_len), float(\"-inf\"), device=q.device)\n",
    "                     attn_mask = torch.triu(attn_mask, diagonal=1)\n",
    "                attn_output = F.scaled_dot_product_attention(q, k, v, attn_mask=attn_mask)\n",
    "                attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, q_len, -1)\n",
    "        else:\n",
    "            q = q.transpose(1, 2)\n",
    "            k = repeat_kv(k.transpose(1, 2), self.n_key_value_groups)\n",
    "            v = repeat_kv(v.transpose(1, 2), self.n_key_value_groups)\n",
    "            \n",
    "            if attn_mask is None and q_len > 1:\n",
    "                 pass\n",
    "            attn_output = F.scaled_dot_product_attention(q, k, v, attn_mask=attn_mask)\n",
    "            attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, q_len, -1)\n",
    "            \n",
    "        return attn_output, present_kv\n",
    "\n",
    "class MambaBranch(nn.Module):\n",
    "    def __init__(self, d_inner, d_state, d_conv, dt_rank):\n",
    "        super().__init__()\n",
    "        if causal_conv1d_fn is None or selective_scan_fn is None:\n",
    "            raise ImportError(\"Mamba packages not found. Please install them.\")\n",
    "        \n",
    "        self.d_inner, self.d_state, self.d_conv, self.dt_rank = d_inner, d_state, d_conv, dt_rank\n",
    "        \n",
    "        self.conv1d = nn.Conv1d(in_channels=self.d_inner, out_channels=self.d_inner, kernel_size=d_conv, bias=True, groups=self.d_inner, padding=d_conv - 1)\n",
    "        self.x_proj = nn.Linear(self.d_inner, self.dt_rank + 2 * self.d_state, bias=False)\n",
    "        self.dt_proj = nn.Linear(self.dt_rank, self.d_inner, bias=True)\n",
    "        \n",
    "        A = torch.arange(1, self.d_state + 1, dtype=torch.float32).repeat(self.d_inner, 1)\n",
    "        self.A_log = nn.Parameter(torch.log(A))\n",
    "        self.D = nn.Parameter(torch.ones(self.d_inner))\n",
    "\n",
    "    def forward(self, x, z):\n",
    "        x_transposed = x.transpose(1, 2).contiguous()\n",
    "        x_conv = causal_conv1d_fn(x_transposed, self.conv1d.weight.squeeze(1), self.conv1d.bias, activation=\"silu\")\n",
    "        \n",
    "        x_dbl = self.x_proj(x_conv.transpose(1, 2))\n",
    "        dt_pre, B, C = torch.split(x_dbl, [self.dt_rank, self.d_state, self.d_state], dim=-1)\n",
    "        \n",
    "        dt = self.dt_proj(dt_pre).transpose(1, 2)\n",
    "        A = -torch.exp(self.A_log.float())\n",
    "        \n",
    "        y = selective_scan_fn(\n",
    "            x_conv, dt, A, B.transpose(1, 2), C.transpose(1, 2), self.D.float(), \n",
    "            z=z.transpose(1,2), delta_bias=self.dt_proj.bias.float(), delta_softplus=True\n",
    "        )\n",
    "        return y.transpose(1,2)\n",
    "        \n",
    "# -----------------------------------------------------------------------------\n",
    "# 2. Ablation을 위한 각 단계별 블록 정의\n",
    "# -----------------------------------------------------------------------------\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model: int, hidden_dim: int):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(d_model, hidden_dim, bias=False)\n",
    "        self.w3 = nn.Linear(d_model, hidden_dim, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_dim, d_model, bias=False)\n",
    "        self.act_fn = F.silu\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.w2(self.act_fn(self.w1(x)) * self.w3(x))\n",
    "\n",
    "class MambaOnlyBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, ffn_hidden_dim: int, mamba_params: dict):\n",
    "        super().__init__()\n",
    "        self.d_inner = mamba_params['expand'] * d_model\n",
    "        \n",
    "        self.norm = RMSNorm(d_model)\n",
    "        self.in_proj = nn.Linear(d_model, 2 * self.d_inner, bias=False)\n",
    "        self.mamba_branch = MambaBranch(d_inner=self.d_inner, **{k:v for k,v in mamba_params.items() if k != 'expand'})\n",
    "        self.out_proj = nn.Linear(self.d_inner, d_model, bias=False)\n",
    "        self.ffn = FeedForward(d_model, ffn_hidden_dim)\n",
    "        self.ffn_norm = RMSNorm(d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, **kwargs):\n",
    "        residual = x\n",
    "        x_norm = self.norm(x)\n",
    "        \n",
    "        xz = self.in_proj(x_norm)\n",
    "        x_mamba, z_mamba = xz.chunk(2, dim=-1)\n",
    "        \n",
    "        mamba_out = self.mamba_branch(x_mamba, z_mamba)\n",
    "        \n",
    "        h = residual + self.out_proj(mamba_out)\n",
    "        out = h + self.ffn(self.ffn_norm(h))\n",
    "        return out, None\n",
    "\n",
    "class HymbaBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, n_kv_heads: int, max_seq_len: int, window_size: int, mamba_params: dict, n_meta_tokens: int, use_concat: bool = False):\n",
    "        super().__init__()\n",
    "        self.d_inner = mamba_params['expand'] * d_model\n",
    "        attn_head_dim = self.d_inner // n_heads\n",
    "        \n",
    "        self.use_concat = use_concat\n",
    "        \n",
    "        out_proj_input_dim = self.d_inner * 2 if self.use_concat else self.d_inner\n",
    "        \n",
    "        latent_dim = self.d_inner + self.d_inner + (attn_head_dim * n_kv_heads * 2)\n",
    "        self.in_proj = nn.Linear(d_model, latent_dim + self.d_inner, bias=True)\n",
    "        self.out_proj = nn.Linear(out_proj_input_dim, d_model, bias=True)\n",
    "\n",
    "        self.attn_branch = AttentionBranch(d_inner=self.d_inner, n_heads=n_heads, n_kv_heads=n_kv_heads, max_seq_len=max_seq_len, window_size=window_size, n_meta_tokens=n_meta_tokens)\n",
    "        self.mamba_branch = MambaBranch(d_inner=self.d_inner, d_state=mamba_params['d_state'], d_conv=mamba_params['d_conv'], dt_rank=mamba_params['dt_rank'])\n",
    "\n",
    "        self.norm = RMSNorm(d_model)\n",
    "        self.norm1 = RMSNorm(self.d_inner)\n",
    "        self.norm2 = RMSNorm(self.d_inner)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, past_kv: Optional[Tuple[torch.Tensor, torch.Tensor]] = None, attn_mask: Optional[torch.Tensor] = None, use_cache: bool = False):\n",
    "        projected = self.in_proj(self.norm(x))\n",
    "        \n",
    "        latent, gate = projected.tensor_split((projected.shape[-1] - self.d_inner,), dim=-1)\n",
    "        \n",
    "        attn_q_dim = self.d_inner\n",
    "        attn_k_dim = self.attn_branch.head_dim * self.attn_branch.n_kv_heads\n",
    "        \n",
    "        q, k, v, mamba_x = latent.tensor_split((attn_q_dim, attn_q_dim + attn_k_dim, attn_q_dim + 2 * attn_k_dim), dim=-1)\n",
    "\n",
    "        attn_out, present_kv = self.attn_branch(q, k, v, past_kv, attn_mask, use_cache)\n",
    "        mamba_out = self.mamba_branch(mamba_x, gate)\n",
    "\n",
    "        if self.use_concat:\n",
    "            combined = torch.cat([self.norm1(attn_out), self.norm2(mamba_out)], dim=-1)\n",
    "        else:\n",
    "            combined = (self.norm1(attn_out) + self.norm2(mamba_out)) / 2\n",
    "            \n",
    "        return self.out_proj(combined), present_kv\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3. 전체 모델 및 마스크 생성\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class HymbaModel(nn.Module):\n",
    "    def __init__(self, config: dict):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embedding = nn.Embedding(config['vocab_size'], config['d_model'])\n",
    "        if config['use_meta_tokens']:\n",
    "            self.meta_tokens = nn.Parameter(torch.randn(1, config['n_meta_tokens'], config['d_model']))\n",
    "\n",
    "        layers = []\n",
    "        for i in range(config['n_layers']):\n",
    "            is_full_attn_layer = False\n",
    "            is_mamba_only_config = config['use_mamba_only']\n",
    "\n",
    "            if not is_mamba_only_config and config['use_full_attention_layers']:\n",
    "                mid_layer = config['n_layers'] // 2\n",
    "                if config['n_layers'] % 2 == 0:\n",
    "                    mid_layer = mid_layer - 1\n",
    "\n",
    "                if i == 0 or i == mid_layer or i == config['n_layers'] - 1:\n",
    "                    is_full_attn_layer = True\n",
    "\n",
    "            use_swa = config['use_swa'] and not is_full_attn_layer and not is_mamba_only_config\n",
    "\n",
    "            if is_mamba_only_config:\n",
    "                 layers.append(MambaOnlyBlock(\n",
    "                     d_model=config['d_model'],\n",
    "                     ffn_hidden_dim=config['ffn_hidden_dim'],\n",
    "                     mamba_params=config['mamba_params']\n",
    "                   ))\n",
    "            else:\n",
    "                layers.append(HymbaBlock(\n",
    "                     d_model=config['d_model'], n_heads=config['n_heads'], \n",
    "                     n_kv_heads=config['n_kv_heads'], max_seq_len=config['max_seq_len'],\n",
    "                     window_size=config['window_size'] if use_swa else -1,\n",
    "                     mamba_params=config['mamba_params'],\n",
    "                     n_meta_tokens=config['n_meta_tokens'] if config['use_meta_tokens'] else 0,\n",
    "                     use_concat=config['use_concat']\n",
    "                   ))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.norm = RMSNorm(config['d_model'])\n",
    "        self.lm_head = nn.Linear(config['d_model'], config['vocab_size'], bias=False)\n",
    "        \n",
    "        # 모델을 bfloat16으로 변환\n",
    "        self.to(torch.bfloat16)\n",
    "\n",
    "    def _create_attention_mask(self, q_len: int, kv_len: int, window_size: int, n_meta_tokens: int, device: str) -> Optional[torch.Tensor]:\n",
    "        if q_len > 1:\n",
    "            # bfloat16 마스크 텐서 생성\n",
    "            mask = torch.full((1, 1, q_len, kv_len), float(\"-inf\"), device=device, dtype=torch.bfloat16)\n",
    "            mask = torch.triu(mask, diagonal=1)\n",
    "            \n",
    "            if window_size > 0:\n",
    "                sliding_mask = torch.ones(q_len, kv_len, device=device, dtype=torch.bool)\n",
    "                sliding_mask.tril_(-1).triu_(-window_size)\n",
    "                mask.masked_fill_(~sliding_mask[None, None, ...], float(\"-inf\"))\n",
    "            \n",
    "            if n_meta_tokens > 0:\n",
    "                mask[..., n_meta_tokens:, :n_meta_tokens] = 0\n",
    "            \n",
    "            return mask\n",
    "        return None\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor, use_cache: bool = False, return_kv_cache: bool = False):\n",
    "        batch_size, seq_len = tokens.shape\n",
    "        is_decoding = use_cache and tokens.shape[1] == 1\n",
    "        \n",
    "        h = self.embedding(tokens)\n",
    "        \n",
    "        current_seq_len = seq_len\n",
    "        if self.config['use_meta_tokens'] and not is_decoding:\n",
    "            meta_embeds = self.meta_tokens.expand(batch_size, -1, -1)\n",
    "            h = torch.cat([meta_embeds, h], dim=1)\n",
    "            current_seq_len += self.config['n_meta_tokens']\n",
    "            \n",
    "        kv_cache_list = [None] * self.config['n_layers']\n",
    "        \n",
    "        attn_mask = None\n",
    "        if not self.config['use_mamba_only'] and not is_decoding:\n",
    "            attn_mask = self._create_attention_mask(\n",
    "                q_len=current_seq_len, kv_len=current_seq_len,\n",
    "                window_size=self.config['window_size'] if self.config['use_swa'] else -1,\n",
    "                n_meta_tokens=self.config['n_meta_tokens'] if self.config['use_meta_tokens'] else 0,\n",
    "                device=h.device\n",
    "            )\n",
    "        \n",
    "        residual = h\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            past_kv = kv_cache_list[i] if use_cache and is_decoding else None\n",
    "            \n",
    "            if use_cache and is_decoding and i > 0 and self.config['use_shared_kv_cache']:\n",
    "                 past_kv = kv_cache_list[i-1] if i % 2 == 1 else past_kv\n",
    "            \n",
    "            output, present_kv = layer(h, past_kv=past_kv, attn_mask=attn_mask, use_cache=use_cache)\n",
    "            h = residual + output\n",
    "            residual = h\n",
    "\n",
    "            if use_cache:\n",
    "                kv_cache_list[i] = present_kv\n",
    "\n",
    "        h = self.norm(h)\n",
    "        \n",
    "        if self.config['use_meta_tokens'] and not is_decoding:\n",
    "            h = h[:, self.config['n_meta_tokens']:]\n",
    "            \n",
    "        logits = self.lm_head(h)\n",
    "        \n",
    "        if return_kv_cache:\n",
    "            return logits, kv_cache_list\n",
    "        return logits\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4. 단계별 모델링 및 성능 측정\n",
    "# -----------------------------------------------------------------------------\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if not torch.cuda.is_available() and selective_scan_fn is not None:\n",
    "        print(\"경고: 이 코드는 mamba-ssm의 CUDA 커널에 의존하므로 GPU 환경에서 실행해야 합니다.\")\n",
    "    \n",
    "    base_config = {\n",
    "        'vocab_size': 32000, 'd_model': 512, 'n_layers': 12,\n",
    "        'n_heads': 16, 'n_kv_heads': 4, 'max_seq_len': 4096,\n",
    "        'ffn_hidden_dim': 512 * 4, 'window_size': 512, 'n_meta_tokens': 4,\n",
    "        'mamba_params': {'d_state': 16, 'd_conv': 4, 'expand': 2, 'dt_rank': math.ceil((256 * 2) / 16)},\n",
    "    }\n",
    "    \n",
    "    ablation_stages_all_features = [\n",
    "        (\"1_Mamba_Only_Baseline\",          {'use_mamba_only': True,  'use_swa': False, 'use_shared_kv_cache': False, 'use_meta_tokens': False, 'use_full_attention_layers': False, 'use_concat': False}),\n",
    "        (\"2_Hybrid\",                       {'use_mamba_only': False, 'use_swa': False, 'use_shared_kv_cache': False, 'use_meta_tokens': False, 'use_full_attention_layers': False, 'use_concat': False}),\n",
    "        (\"3_SWA_plus_Full_Attn\",           {'use_mamba_only': False, 'use_swa': True,  'use_shared_kv_cache': False, 'use_meta_tokens': False, 'use_full_attention_layers': True, 'use_concat': False}),\n",
    "        (\"4_SWA_plus_Full_Attn_plus_KV_Sharing\",    {'use_mamba_only': False, 'use_swa': True,  'use_shared_kv_cache': True, 'use_meta_tokens': False, 'use_full_attention_layers': True, 'use_concat': False}),\n",
    "        (\"5_Replace_Mean_by_Concat\",       {'use_mamba_only': False, 'use_swa': True,  'use_shared_kv_cache': True, 'use_meta_tokens': False, 'use_full_attention_layers': True, 'use_concat': True}),\n",
    "        (\"6_Full_Hymba_Model_with_Meta_Tokens\", {'use_mamba_only': False, 'use_swa': True, 'use_shared_kv_cache': True, 'use_meta_tokens': True, 'use_full_attention_layers': True, 'use_concat': False}),\n",
    "    ]\n",
    "    \n",
    "    batch_size = 2\n",
    "    seq_len = 2048\n",
    "    \n",
    "    for name, flags in ablation_stages_all_features:\n",
    "        print(f\"\\n{'='*20} {name} {'='*20}\")\n",
    "        config = base_config.copy()\n",
    "        config.update(flags)\n",
    "        \n",
    "        try:\n",
    "            model = HymbaModel(config).to(device)\n",
    "            model.eval()\n",
    "            \n",
    "            dummy_input = torch.randint(0, config['vocab_size'], (batch_size, seq_len)).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits, kv_cache_list = model(dummy_input, use_cache=True, return_kv_cache=True)\n",
    "            \n",
    "            total_cache_bytes = 0\n",
    "            if not config['use_mamba_only']:\n",
    "                if config['use_shared_kv_cache']:\n",
    "                    for i in range(1, config['n_layers'], 2):\n",
    "                        cache = kv_cache_list[i]\n",
    "                        if cache is not None:\n",
    "                            k, v = cache\n",
    "                            total_cache_bytes += k.numel() * k.element_size() + v.numel() * v.element_size()\n",
    "                else:\n",
    "                    for cache in kv_cache_list:\n",
    "                        if cache is not None:\n",
    "                            k, v = cache\n",
    "                            total_cache_bytes += k.numel() * k.element_size() + v.numel() * v.element_size()\n",
    "\n",
    "            total_cache_mb = total_cache_bytes / (1024 * 1024)\n",
    "            print(f\"--- KV Cache 크기: {total_cache_mb:.2f} MB\")\n",
    "\n",
    "            print(\"\\n--- 훈련(Training) 모드 벤치마크 (전체 시퀀스 처리)\")\n",
    "            warmup_iterations = 5\n",
    "            measurement_iterations = 10\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for _ in range(warmup_iterations):\n",
    "                    _ = model(dummy_input, use_cache=False)\n",
    "                if torch.cuda.is_available(): torch.cuda.synchronize()\n",
    "\n",
    "                start_time = time.time()\n",
    "                for _ in range(measurement_iterations):\n",
    "                    _ = model(dummy_input, use_cache=False)\n",
    "                if torch.cuda.is_available(): torch.cuda.synchronize()\n",
    "                end_time = time.time()\n",
    "\n",
    "            total_time = end_time - start_time\n",
    "            total_tokens = batch_size * seq_len * measurement_iterations\n",
    "            tokens_per_second = total_tokens / total_time\n",
    "            print(f\"--- 처리량: {tokens_per_second:,.2f} tokens/sec\")\n",
    "\n",
    "            print(\"\\n--- 추론(Inference) 모드 벤치마크 (토큰당 생성)\")\n",
    "            input_tokens = torch.randint(0, config['vocab_size'], (batch_size, 1)).to(device)\n",
    "            num_tokens_to_generate = 100\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for _ in range(warmup_iterations):\n",
    "                    _ = model(input_tokens, use_cache=True, return_kv_cache=True)\n",
    "                if torch.cuda.is_available(): torch.cuda.synchronize()\n",
    "\n",
    "                start_time = time.time()\n",
    "                current_kv_cache = None\n",
    "                for _ in range(num_tokens_to_generate):\n",
    "                    logits, current_kv_cache = model(input_tokens, use_cache=True, return_kv_cache=True)\n",
    "                    next_token = torch.argmax(logits[:, -1, :], dim=-1).unsqueeze(1)\n",
    "                    input_tokens = next_token\n",
    "                \n",
    "                if torch.cuda.is_available(): torch.cuda.synchronize()\n",
    "                end_time = time.time()\n",
    "\n",
    "            total_time = end_time - start_time\n",
    "            tokens_per_second = (batch_size * num_tokens_to_generate) / total_time\n",
    "            print(f\"--- 처리량: {tokens_per_second:,.2f} tokens/sec\")\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325a276d",
   "metadata": {},
   "source": [
    "- flash&flex attention 제거시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45a9dd13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== 1_Mamba_Only_Baseline ====================\n",
      "--- KV Cache 크기: 0.00 MB\n",
      "\n",
      "--- 훈련(Training) 모드 벤치마크 (전체 시퀀스 처리)\n",
      "--- 처리량: 367,977.36 tokens/sec\n",
      "\n",
      "--- 추론(Inference) 모드 벤치마크 (토큰당 생성)\n",
      "--- 처리량: 243.99 tokens/sec\n",
      "\n",
      "==================== 2_Hybrid ====================\n",
      "--- KV Cache 크기: 48.00 MB\n",
      "\n",
      "--- 훈련(Training) 모드 벤치마크 (전체 시퀀스 처리)\n",
      "--- 처리량: 203,278.62 tokens/sec\n",
      "\n",
      "--- 추론(Inference) 모드 벤치마크 (토큰당 생성)\n",
      "--- 처리량: 177.24 tokens/sec\n",
      "\n",
      "==================== 3_SWA_plus_Full_Attn ====================\n",
      "--- KV Cache 크기: 48.00 MB\n",
      "\n",
      "--- 훈련(Training) 모드 벤치마크 (전체 시퀀스 처리)\n",
      "--- 처리량: 203,164.44 tokens/sec\n",
      "\n",
      "--- 추론(Inference) 모드 벤치마크 (토큰당 생성)\n",
      "--- 처리량: 178.70 tokens/sec\n",
      "\n",
      "==================== 4_SWA_plus_Full_Attn_plus_KV_Sharing ====================\n",
      "--- KV Cache 크기: 24.00 MB\n",
      "\n",
      "--- 훈련(Training) 모드 벤치마크 (전체 시퀀스 처리)\n",
      "--- 처리량: 202,989.44 tokens/sec\n",
      "\n",
      "--- 추론(Inference) 모드 벤치마크 (토큰당 생성)\n",
      "--- 처리량: 175.89 tokens/sec\n",
      "\n",
      "==================== 5_Replace_Mean_by_Concat ====================\n",
      "--- KV Cache 크기: 24.00 MB\n",
      "\n",
      "--- 훈련(Training) 모드 벤치마크 (전체 시퀀스 처리)\n",
      "--- 처리량: 199,963.09 tokens/sec\n",
      "\n",
      "--- 추론(Inference) 모드 벤치마크 (토큰당 생성)\n",
      "--- 처리량: 174.38 tokens/sec\n",
      "\n",
      "==================== 6_Full_Hymba_Model_with_Meta_Tokens ====================\n",
      "--- KV Cache 크기: 24.05 MB\n",
      "\n",
      "--- 훈련(Training) 모드 벤치마크 (전체 시퀀스 처리)\n",
      "--- 처리량: 168,574.70 tokens/sec\n",
      "\n",
      "--- 추론(Inference) 모드 벤치마크 (토큰당 생성)\n",
      "--- 처리량: 174.42 tokens/sec\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import math\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "# mamba-ssm과 causal-conv1d는 저수준 CUDA 커널을 사용하기 위해 필요합니다.\n",
    "# pip install mamba-ssm causal-conv1d\n",
    "try:\n",
    "    from mamba_ssm.ops.selective_scan_interface import selective_scan_fn\n",
    "    from causal_conv1d import causal_conv1d_fn\n",
    "except ImportError:\n",
    "    print(\"Warning: mamba-ssm or causal-conv1d not found. MambaBranch will not work.\")\n",
    "    selective_scan_fn = None\n",
    "    causal_conv1d_fn = None\n",
    "\n",
    "# Flash Attention 및 Flex Attention 관련 라이브러리 가져오기 부분을 삭제합니다.\n",
    "# 이로 인해 _FLEX_ATTENTION_AVAILABLE 및 flash_attn_func 변수도 제거됩니다.\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. 아키텍처의 기본 구성 요소\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
    "        return self.weight * hidden_states\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim: int, max_seq_len: int, base: int = 10000, device: Optional[str] = None):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2, device=device, dtype=torch.float32) / self.dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "        self._set_cos_sin_cache(seq_len=max_seq_len, device=device)\n",
    "\n",
    "    def _set_cos_sin_cache(self, seq_len: int, device: Optional[str], dtype: torch.dtype = torch.float32):\n",
    "        self.max_seq_len_cached = seq_len\n",
    "        t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        self.register_buffer(\"cos_cached\", emb.cos().to(dtype), persistent=False)\n",
    "        self.register_buffer(\"sin_cached\", emb.sin().to(dtype), persistent=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, seq_len: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        if seq_len > self.max_seq_len_cached:\n",
    "            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)\n",
    "        return self.cos_cached[:seq_len, ...].to(dtype=x.dtype), self.sin_cached[:seq_len, ...].to(dtype=x.dtype)\n",
    "\n",
    "def rotate_half(x: torch.Tensor) -> torch.Tensor:\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "def apply_rotary_pos_emb(q: torch.Tensor, k: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    cos = cos.unsqueeze(0).unsqueeze(2)\n",
    "    sin = sin.unsqueeze(0).unsqueeze(2)\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n",
    "\n",
    "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
    "    if n_rep == 1:\n",
    "        return hidden_states\n",
    "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
    "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
    "\n",
    "class AttentionBranch(nn.Module):\n",
    "    def __init__(self, d_inner: int, n_heads: int, n_kv_heads: int, max_seq_len: int, window_size: int = -1, n_meta_tokens: int = 0):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.n_kv_heads = n_kv_heads\n",
    "        self.n_key_value_groups = n_heads // n_kv_heads\n",
    "        self.head_dim = d_inner // n_heads\n",
    "        self.rotary_emb = RotaryEmbedding(self.head_dim, max_seq_len)\n",
    "        self.window_size = window_size\n",
    "        self.n_meta_tokens = n_meta_tokens\n",
    "\n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, past_kv: Optional[Tuple[torch.Tensor, torch.Tensor]] = None, attn_mask: Optional[torch.Tensor] = None, use_cache: bool = False):\n",
    "        batch_size, q_len, _ = q.shape\n",
    "\n",
    "        q = q.view(batch_size, q_len, self.n_heads, self.head_dim)\n",
    "        k = k.view(batch_size, q_len, self.n_kv_heads, self.head_dim)\n",
    "        v = v.view(batch_size, q_len, self.n_kv_heads, self.head_dim)\n",
    "\n",
    "        kv_seq_len = q_len\n",
    "        if past_kv is not None:\n",
    "            kv_seq_len += past_kv[0].shape[1]\n",
    "        \n",
    "        cos, sin = self.rotary_emb(v, seq_len=kv_seq_len)\n",
    "        \n",
    "        if past_kv is not None:\n",
    "            cos = cos[past_kv[0].shape[1]:]\n",
    "            sin = sin[past_kv[0].shape[1]:]\n",
    "        \n",
    "        q, k = apply_rotary_pos_emb(q, k, cos, sin)\n",
    "        \n",
    "        if past_kv is not None:\n",
    "            past_key, past_value = past_kv\n",
    "            k = torch.cat([past_key, k], dim=1)\n",
    "            v = torch.cat([past_value, v], dim=1)\n",
    "            \n",
    "        present_kv = (k, v) if use_cache else None\n",
    "        \n",
    "        # Flash/Flex Attention 로직을 제거하고 F.scaled_dot_product_attention만 사용합니다.\n",
    "        q = q.transpose(1, 2)\n",
    "        k = repeat_kv(k.transpose(1, 2), self.n_key_value_groups)\n",
    "        v = repeat_kv(v.transpose(1, 2), self.n_key_value_groups)\n",
    "        \n",
    "        if attn_mask is None and q_len > 1:\n",
    "             pass\n",
    "        attn_output = F.scaled_dot_product_attention(q, k, v, attn_mask=attn_mask)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, q_len, -1)\n",
    "            \n",
    "        return attn_output, present_kv\n",
    "\n",
    "class MambaBranch(nn.Module):\n",
    "    def __init__(self, d_inner, d_state, d_conv, dt_rank):\n",
    "        super().__init__()\n",
    "        if causal_conv1d_fn is None or selective_scan_fn is None:\n",
    "            raise ImportError(\"Mamba packages not found. Please install them.\")\n",
    "        \n",
    "        self.d_inner, self.d_state, self.d_conv, self.dt_rank = d_inner, d_state, d_conv, dt_rank\n",
    "        \n",
    "        self.conv1d = nn.Conv1d(in_channels=self.d_inner, out_channels=self.d_inner, kernel_size=d_conv, bias=True, groups=self.d_inner, padding=d_conv - 1)\n",
    "        self.x_proj = nn.Linear(self.d_inner, self.dt_rank + 2 * self.d_state, bias=False)\n",
    "        self.dt_proj = nn.Linear(self.dt_rank, self.d_inner, bias=True)\n",
    "        \n",
    "        A = torch.arange(1, self.d_state + 1, dtype=torch.float32).repeat(self.d_inner, 1)\n",
    "        self.A_log = nn.Parameter(torch.log(A))\n",
    "        self.D = nn.Parameter(torch.ones(self.d_inner))\n",
    "\n",
    "    def forward(self, x, z):\n",
    "        x_transposed = x.transpose(1, 2).contiguous()\n",
    "        x_conv = causal_conv1d_fn(x_transposed, self.conv1d.weight.squeeze(1), self.conv1d.bias, activation=\"silu\")\n",
    "        \n",
    "        x_dbl = self.x_proj(x_conv.transpose(1, 2))\n",
    "        dt_pre, B, C = torch.split(x_dbl, [self.dt_rank, self.d_state, self.d_state], dim=-1)\n",
    "        \n",
    "        dt = self.dt_proj(dt_pre).transpose(1, 2)\n",
    "        A = -torch.exp(self.A_log.float())\n",
    "        \n",
    "        y = selective_scan_fn(\n",
    "            x_conv, dt, A, B.transpose(1, 2), C.transpose(1, 2), self.D.float(), \n",
    "            z=z.transpose(1,2), delta_bias=self.dt_proj.bias.float(), delta_softplus=True\n",
    "        )\n",
    "        return y.transpose(1,2)\n",
    "        \n",
    "# -----------------------------------------------------------------------------\n",
    "# 2. Ablation을 위한 각 단계별 블록 정의\n",
    "# -----------------------------------------------------------------------------\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model: int, hidden_dim: int):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(d_model, hidden_dim, bias=False)\n",
    "        self.w3 = nn.Linear(d_model, hidden_dim, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_dim, d_model, bias=False)\n",
    "        self.act_fn = F.silu\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.w2(self.act_fn(self.w1(x)) * self.w3(x))\n",
    "\n",
    "class MambaOnlyBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, ffn_hidden_dim: int, mamba_params: dict):\n",
    "        super().__init__()\n",
    "        self.d_inner = mamba_params['expand'] * d_model\n",
    "        \n",
    "        self.norm = RMSNorm(d_model)\n",
    "        self.in_proj = nn.Linear(d_model, 2 * self.d_inner, bias=False)\n",
    "        self.mamba_branch = MambaBranch(d_inner=self.d_inner, **{k:v for k,v in mamba_params.items() if k != 'expand'})\n",
    "        self.out_proj = nn.Linear(self.d_inner, d_model, bias=False)\n",
    "        self.ffn = FeedForward(d_model, ffn_hidden_dim)\n",
    "        self.ffn_norm = RMSNorm(d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, **kwargs):\n",
    "        residual = x\n",
    "        x_norm = self.norm(x)\n",
    "        \n",
    "        xz = self.in_proj(x_norm)\n",
    "        x_mamba, z_mamba = xz.chunk(2, dim=-1)\n",
    "        \n",
    "        mamba_out = self.mamba_branch(x_mamba, z_mamba)\n",
    "        \n",
    "        h = residual + self.out_proj(mamba_out)\n",
    "        out = h + self.ffn(self.ffn_norm(h))\n",
    "        return out, None\n",
    "\n",
    "class HymbaBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, n_kv_heads: int, max_seq_len: int, window_size: int, mamba_params: dict, n_meta_tokens: int, use_concat: bool = False):\n",
    "        super().__init__()\n",
    "        self.d_inner = mamba_params['expand'] * d_model\n",
    "        attn_head_dim = self.d_inner // n_heads\n",
    "        \n",
    "        self.use_concat = use_concat\n",
    "        \n",
    "        out_proj_input_dim = self.d_inner * 2 if self.use_concat else self.d_inner\n",
    "        \n",
    "        latent_dim = self.d_inner + self.d_inner + (attn_head_dim * n_kv_heads * 2)\n",
    "        self.in_proj = nn.Linear(d_model, latent_dim + self.d_inner, bias=True)\n",
    "        self.out_proj = nn.Linear(out_proj_input_dim, d_model, bias=True)\n",
    "\n",
    "        self.attn_branch = AttentionBranch(d_inner=self.d_inner, n_heads=n_heads, n_kv_heads=n_kv_heads, max_seq_len=max_seq_len, window_size=window_size, n_meta_tokens=n_meta_tokens)\n",
    "        self.mamba_branch = MambaBranch(d_inner=self.d_inner, d_state=mamba_params['d_state'], d_conv=mamba_params['d_conv'], dt_rank=mamba_params['dt_rank'])\n",
    "\n",
    "        self.norm = RMSNorm(d_model)\n",
    "        self.norm1 = RMSNorm(self.d_inner)\n",
    "        self.norm2 = RMSNorm(self.d_inner)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, past_kv: Optional[Tuple[torch.Tensor, torch.Tensor]] = None, attn_mask: Optional[torch.Tensor] = None, use_cache: bool = False):\n",
    "        projected = self.in_proj(self.norm(x))\n",
    "        \n",
    "        latent, gate = projected.tensor_split((projected.shape[-1] - self.d_inner,), dim=-1)\n",
    "        \n",
    "        attn_q_dim = self.d_inner\n",
    "        attn_k_dim = self.attn_branch.head_dim * self.attn_branch.n_kv_heads\n",
    "        \n",
    "        q, k, v, mamba_x = latent.tensor_split((attn_q_dim, attn_q_dim + attn_k_dim, attn_q_dim + 2 * attn_k_dim), dim=-1)\n",
    "\n",
    "        attn_out, present_kv = self.attn_branch(q, k, v, past_kv, attn_mask, use_cache)\n",
    "        mamba_out = self.mamba_branch(mamba_x, gate)\n",
    "\n",
    "        if self.use_concat:\n",
    "            combined = torch.cat([self.norm1(attn_out), self.norm2(mamba_out)], dim=-1)\n",
    "        else:\n",
    "            combined = (self.norm1(attn_out) + self.norm2(mamba_out)) / 2\n",
    "            \n",
    "        return self.out_proj(combined), present_kv\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3. 전체 모델 및 마스크 생성\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class HymbaModel(nn.Module):\n",
    "    def __init__(self, config: dict):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embedding = nn.Embedding(config['vocab_size'], config['d_model'])\n",
    "        if config['use_meta_tokens']:\n",
    "            self.meta_tokens = nn.Parameter(torch.randn(1, config['n_meta_tokens'], config['d_model']))\n",
    "\n",
    "        layers = []\n",
    "        for i in range(config['n_layers']):\n",
    "            is_full_attn_layer = False\n",
    "            is_mamba_only_config = config['use_mamba_only']\n",
    "\n",
    "            if not is_mamba_only_config and config['use_full_attention_layers']:\n",
    "                mid_layer = config['n_layers'] // 2\n",
    "                if config['n_layers'] % 2 == 0:\n",
    "                    mid_layer = mid_layer - 1\n",
    "\n",
    "                if i == 0 or i == mid_layer or i == config['n_layers'] - 1:\n",
    "                    is_full_attn_layer = True\n",
    "\n",
    "            use_swa = config['use_swa'] and not is_full_attn_layer and not is_mamba_only_config\n",
    "\n",
    "            if is_mamba_only_config:\n",
    "                 layers.append(MambaOnlyBlock(\n",
    "                     d_model=config['d_model'],\n",
    "                     ffn_hidden_dim=config['ffn_hidden_dim'],\n",
    "                     mamba_params=config['mamba_params']\n",
    "                   ))\n",
    "            else:\n",
    "                layers.append(HymbaBlock(\n",
    "                     d_model=config['d_model'], n_heads=config['n_heads'], \n",
    "                     n_kv_heads=config['n_kv_heads'], max_seq_len=config['max_seq_len'],\n",
    "                     window_size=config['window_size'] if use_swa else -1,\n",
    "                     mamba_params=config['mamba_params'],\n",
    "                     n_meta_tokens=config['n_meta_tokens'] if config['use_meta_tokens'] else 0,\n",
    "                     use_concat=config['use_concat']\n",
    "                   ))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.norm = RMSNorm(config['d_model'])\n",
    "        self.lm_head = nn.Linear(config['d_model'], config['vocab_size'], bias=False)\n",
    "        \n",
    "        self.to(torch.bfloat16)\n",
    "\n",
    "    def _create_attention_mask(self, q_len: int, kv_len: int, window_size: int, n_meta_tokens: int, device: str) -> Optional[torch.Tensor]:\n",
    "        if q_len > 1:\n",
    "            mask = torch.full((1, 1, q_len, kv_len), float(\"-inf\"), device=device, dtype=torch.bfloat16)\n",
    "            mask = torch.triu(mask, diagonal=1)\n",
    "            \n",
    "            if window_size > 0:\n",
    "                sliding_mask = torch.ones(q_len, kv_len, device=device, dtype=torch.bool)\n",
    "                sliding_mask.tril_(-1).triu_(-window_size)\n",
    "                mask.masked_fill_(~sliding_mask[None, None, ...], float(\"-inf\"))\n",
    "            \n",
    "            if n_meta_tokens > 0:\n",
    "                mask[..., n_meta_tokens:, :n_meta_tokens] = 0\n",
    "            \n",
    "            return mask\n",
    "        return None\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor, use_cache: bool = False, return_kv_cache: bool = False):\n",
    "        batch_size, seq_len = tokens.shape\n",
    "        is_decoding = use_cache and tokens.shape[1] == 1\n",
    "        \n",
    "        h = self.embedding(tokens)\n",
    "        \n",
    "        current_seq_len = seq_len\n",
    "        if self.config['use_meta_tokens'] and not is_decoding:\n",
    "            meta_embeds = self.meta_tokens.expand(batch_size, -1, -1)\n",
    "            h = torch.cat([meta_embeds, h], dim=1)\n",
    "            current_seq_len += self.config['n_meta_tokens']\n",
    "            \n",
    "        kv_cache_list = [None] * self.config['n_layers']\n",
    "        \n",
    "        attn_mask = None\n",
    "        if not self.config['use_mamba_only'] and not is_decoding:\n",
    "            attn_mask = self._create_attention_mask(\n",
    "                q_len=current_seq_len, kv_len=current_seq_len,\n",
    "                window_size=self.config['window_size'] if self.config['use_swa'] else -1,\n",
    "                n_meta_tokens=self.config['n_meta_tokens'] if self.config['use_meta_tokens'] else 0,\n",
    "                device=h.device\n",
    "            )\n",
    "        \n",
    "        residual = h\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            past_kv = kv_cache_list[i] if use_cache and is_decoding else None\n",
    "            \n",
    "            if use_cache and is_decoding and i > 0 and self.config['use_shared_kv_cache']:\n",
    "                 past_kv = kv_cache_list[i-1] if i % 2 == 1 else past_kv\n",
    "            \n",
    "            output, present_kv = layer(h, past_kv=past_kv, attn_mask=attn_mask, use_cache=use_cache)\n",
    "            h = residual + output\n",
    "            residual = h\n",
    "\n",
    "            if use_cache:\n",
    "                kv_cache_list[i] = present_kv\n",
    "\n",
    "        h = self.norm(h)\n",
    "        \n",
    "        if self.config['use_meta_tokens'] and not is_decoding:\n",
    "            h = h[:, self.config['n_meta_tokens']:]\n",
    "            \n",
    "        logits = self.lm_head(h)\n",
    "        \n",
    "        if return_kv_cache:\n",
    "            return logits, kv_cache_list\n",
    "        return logits\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4. 단계별 모델링 및 성능 측정\n",
    "# -----------------------------------------------------------------------------\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if not torch.cuda.is_available() and selective_scan_fn is not None:\n",
    "        print(\"경고: 이 코드는 mamba-ssm의 CUDA 커널에 의존하므로 GPU 환경에서 실행해야 합니다.\")\n",
    "    \n",
    "    base_config = {\n",
    "        'vocab_size': 32000, 'd_model': 512, 'n_layers': 12,\n",
    "        'n_heads': 16, 'n_kv_heads': 4, 'max_seq_len': 4096,\n",
    "        'ffn_hidden_dim': 512 * 4, 'window_size': 512, 'n_meta_tokens': 4,\n",
    "        'mamba_params': {'d_state': 16, 'd_conv': 4, 'expand': 2, 'dt_rank': math.ceil((256 * 2) / 16)},\n",
    "    }\n",
    "    \n",
    "    ablation_stages_all_features = [\n",
    "        (\"1_Mamba_Only_Baseline\",          {'use_mamba_only': True,  'use_swa': False, 'use_shared_kv_cache': False, 'use_meta_tokens': False, 'use_full_attention_layers': False, 'use_concat': False}),\n",
    "        (\"2_Hybrid\",                       {'use_mamba_only': False, 'use_swa': False, 'use_shared_kv_cache': False, 'use_meta_tokens': False, 'use_full_attention_layers': False, 'use_concat': False}),\n",
    "        (\"3_SWA_plus_Full_Attn\",           {'use_mamba_only': False, 'use_swa': True,  'use_shared_kv_cache': False, 'use_meta_tokens': False, 'use_full_attention_layers': True, 'use_concat': False}),\n",
    "        (\"4_SWA_plus_Full_Attn_plus_KV_Sharing\",    {'use_mamba_only': False, 'use_swa': True,  'use_shared_kv_cache': True, 'use_meta_tokens': False, 'use_full_attention_layers': True, 'use_concat': False}),\n",
    "        (\"5_Replace_Mean_by_Concat\",       {'use_mamba_only': False, 'use_swa': True,  'use_shared_kv_cache': True, 'use_meta_tokens': False, 'use_full_attention_layers': True, 'use_concat': True}),\n",
    "        (\"6_Full_Hymba_Model_with_Meta_Tokens\", {'use_mamba_only': False, 'use_swa': True, 'use_shared_kv_cache': True, 'use_meta_tokens': True, 'use_full_attention_layers': True, 'use_concat': False}),\n",
    "    ]\n",
    "    \n",
    "    batch_size = 2\n",
    "    seq_len = 2048\n",
    "    \n",
    "    for name, flags in ablation_stages_all_features:\n",
    "        print(f\"\\n{'='*20} {name} {'='*20}\")\n",
    "        config = base_config.copy()\n",
    "        config.update(flags)\n",
    "        \n",
    "        try:\n",
    "            model = HymbaModel(config).to(device)\n",
    "            model.eval()\n",
    "            \n",
    "            dummy_input = torch.randint(0, config['vocab_size'], (batch_size, seq_len)).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits, kv_cache_list = model(dummy_input, use_cache=True, return_kv_cache=True)\n",
    "            \n",
    "            total_cache_bytes = 0\n",
    "            if not config['use_mamba_only']:\n",
    "                if config['use_shared_kv_cache']:\n",
    "                    for i in range(1, config['n_layers'], 2):\n",
    "                        cache = kv_cache_list[i]\n",
    "                        if cache is not None:\n",
    "                            k, v = cache\n",
    "                            total_cache_bytes += k.numel() * k.element_size() + v.numel() * v.element_size()\n",
    "                else:\n",
    "                    for cache in kv_cache_list:\n",
    "                        if cache is not None:\n",
    "                            k, v = cache\n",
    "                            total_cache_bytes += k.numel() * k.element_size() + v.numel() * v.element_size()\n",
    "\n",
    "            total_cache_mb = total_cache_bytes / (1024 * 1024)\n",
    "            print(f\"--- KV Cache 크기: {total_cache_mb:.2f} MB\")\n",
    "\n",
    "            print(\"\\n--- 훈련(Training) 모드 벤치마크 (전체 시퀀스 처리)\")\n",
    "            warmup_iterations = 5\n",
    "            measurement_iterations = 10\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for _ in range(warmup_iterations):\n",
    "                    _ = model(dummy_input, use_cache=False)\n",
    "                if torch.cuda.is_available(): torch.cuda.synchronize()\n",
    "\n",
    "                start_time = time.time()\n",
    "                for _ in range(measurement_iterations):\n",
    "                    _ = model(dummy_input, use_cache=False)\n",
    "                if torch.cuda.is_available(): torch.cuda.synchronize()\n",
    "                end_time = time.time()\n",
    "\n",
    "            total_time = end_time - start_time\n",
    "            total_tokens = batch_size * seq_len * measurement_iterations\n",
    "            tokens_per_second = total_tokens / total_time\n",
    "            print(f\"--- 처리량: {tokens_per_second:,.2f} tokens/sec\")\n",
    "\n",
    "            print(\"\\n--- 추론(Inference) 모드 벤치마크 (토큰당 생성)\")\n",
    "            input_tokens = torch.randint(0, config['vocab_size'], (batch_size, 1)).to(device)\n",
    "            num_tokens_to_generate = 100\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for _ in range(warmup_iterations):\n",
    "                    _ = model(input_tokens, use_cache=True, return_kv_cache=True)\n",
    "                if torch.cuda.is_available(): torch.cuda.synchronize()\n",
    "\n",
    "                start_time = time.time()\n",
    "                current_kv_cache = None\n",
    "                for _ in range(num_tokens_to_generate):\n",
    "                    logits, current_kv_cache = model(input_tokens, use_cache=True, return_kv_cache=True)\n",
    "                    next_token = torch.argmax(logits[:, -1, :], dim=-1).unsqueeze(1)\n",
    "                    input_tokens = next_token\n",
    "                \n",
    "                if torch.cuda.is_available(): torch.cuda.synchronize()\n",
    "                end_time = time.time()\n",
    "\n",
    "            total_time = end_time - start_time\n",
    "            tokens_per_second = (batch_size * num_tokens_to_generate) / total_time\n",
    "            print(f\"--- 처리량: {tokens_per_second:,.2f} tokens/sec\")\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb229f6",
   "metadata": {},
   "source": [
    "**meta token 추가에 따른 성능 저하**\n",
    "- meta token이 추가됨에 따라 특정 sequence 길이에 최적화된 attention module이 어긋남에 따라 엄청난 속도 저하를 일으키게됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7081a3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Without_Meta_Tokens ====================\n",
      "--- KV Cache 크기: 48.00 MB\n",
      "\n",
      "--- 훈련(Training) 모드 벤치마크 (전체 시퀀스 처리)\n",
      "--- 처리량: 308,023.17 tokens/sec\n",
      "\n",
      "--- 추론(Inference) 모드 벤치마크 (토큰당 생성)\n",
      "--- 처리량: 171.44 tokens/sec\n",
      "\n",
      "==================== With_Meta_Tokens ====================\n",
      "--- KV Cache 크기: 48.00 MB\n",
      "\n",
      "--- 훈련(Training) 모드 벤치마크 (전체 시퀀스 처리)\n",
      "--- 처리량: 20,265.73 tokens/sec\n",
      "\n",
      "--- 추론(Inference) 모드 벤치마크 (토큰당 생성)\n",
      "--- 처리량: 176.24 tokens/sec\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import math\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "# mamba-ssm과 causal-conv1d는 저수준 CUDA 커널을 사용하기 위해 필요합니다.\n",
    "# pip install mamba-ssm causal-conv1d\n",
    "try:\n",
    "    from mamba_ssm.ops.selective_scan_interface import selective_scan_fn\n",
    "    from causal_conv1d import causal_conv1d_fn\n",
    "except ImportError:\n",
    "    print(\"Warning: mamba-ssm or causal-conv1d not found. MambaBranch will not work.\")\n",
    "    selective_scan_fn = None\n",
    "    causal_conv1d_fn = None\n",
    "\n",
    "# Flash Attention 및 Flex Attention 관련 API 가져오기\n",
    "try:\n",
    "    from flash_attn import flash_attn_func\n",
    "    from torch.nn.attention.flex_attention import flex_attention, create_block_mask, and_masks, or_masks\n",
    "    _FLEX_ATTENTION_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Warning: Flex Attention not found. Falling back to native PyTorch.\")\n",
    "    flash_attn_func = None\n",
    "    _FLEX_ATTENTION_AVAILABLE = False\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. 아키텍처의 기본 구성 요소\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
    "        return self.weight * hidden_states\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim: int, max_seq_len: int, base: int = 10000, device: Optional[str] = None):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2, device=device, dtype=torch.float32) / self.dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "        self._set_cos_sin_cache(seq_len=max_seq_len, device=device)\n",
    "\n",
    "    def _set_cos_sin_cache(self, seq_len: int, device: Optional[str], dtype: torch.dtype = torch.float32):\n",
    "        self.max_seq_len_cached = seq_len\n",
    "        t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        self.register_buffer(\"cos_cached\", emb.cos().to(dtype), persistent=False)\n",
    "        self.register_buffer(\"sin_cached\", emb.sin().to(dtype), persistent=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, seq_len: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        if seq_len > self.max_seq_len_cached:\n",
    "            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)\n",
    "        return self.cos_cached[:seq_len, ...].to(dtype=x.dtype), self.sin_cached[:seq_len, ...].to(dtype=x.dtype)\n",
    "\n",
    "def rotate_half(x: torch.Tensor) -> torch.Tensor:\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "def apply_rotary_pos_emb(q: torch.Tensor, k: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    cos = cos.unsqueeze(0).unsqueeze(2)\n",
    "    sin = sin.unsqueeze(0).unsqueeze(2)\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n",
    "\n",
    "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
    "    if n_rep == 1:\n",
    "        return hidden_states\n",
    "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
    "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
    "\n",
    "class AttentionBranch(nn.Module):\n",
    "    def __init__(self, d_inner: int, n_heads: int, n_kv_heads: int, max_seq_len: int, window_size: int = -1, n_meta_tokens: int = 0):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.n_kv_heads = n_kv_heads\n",
    "        self.n_key_value_groups = n_heads // n_kv_heads\n",
    "        self.head_dim = d_inner // n_heads\n",
    "        self.rotary_emb = RotaryEmbedding(self.head_dim, max_seq_len)\n",
    "        self.window_size = window_size\n",
    "        self.n_meta_tokens = n_meta_tokens\n",
    "        self.use_flex_attention = (window_size > 0 and n_meta_tokens > 0 and _FLEX_ATTENTION_AVAILABLE)\n",
    "\n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, past_kv: Optional[Tuple[torch.Tensor, torch.Tensor]] = None, attn_mask: Optional[torch.Tensor] = None, use_cache: bool = False):\n",
    "        batch_size, q_len, _ = q.shape\n",
    "\n",
    "        q = q.view(batch_size, q_len, self.n_heads, self.head_dim)\n",
    "        k = k.view(batch_size, q_len, self.n_kv_heads, self.head_dim)\n",
    "        v = v.view(batch_size, q_len, self.n_kv_heads, self.head_dim)\n",
    "\n",
    "        kv_seq_len = q_len\n",
    "        if past_kv is not None:\n",
    "            kv_seq_len += past_kv[0].shape[1]\n",
    "        \n",
    "        cos, sin = self.rotary_emb(v, seq_len=kv_seq_len)\n",
    "        \n",
    "        if past_kv is not None:\n",
    "            cos = cos[past_kv[0].shape[1]:]\n",
    "            sin = sin[past_kv[0].shape[1]:]\n",
    "        \n",
    "        q, k = apply_rotary_pos_emb(q, k, cos, sin)\n",
    "        \n",
    "        if past_kv is not None:\n",
    "            past_key, past_value = past_kv\n",
    "            k = torch.cat([past_key, k], dim=1)\n",
    "            v = torch.cat([past_value, v], dim=1)\n",
    "            \n",
    "        present_kv = (k, v) if use_cache else None\n",
    "        \n",
    "        if flash_attn_func is not None and not use_cache and not self.use_flex_attention:\n",
    "            q = q.transpose(1, 2)\n",
    "            k = repeat_kv(k.transpose(1, 2), self.n_key_value_groups)\n",
    "            v = repeat_kv(v.transpose(1, 2), self.n_key_value_groups)\n",
    "            attn_output = flash_attn_func(q, k, v, causal=True)\n",
    "            attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, q_len, -1)\n",
    "            \n",
    "        elif self.use_flex_attention:\n",
    "            if not use_cache:\n",
    "                q = q.transpose(1, 2)\n",
    "                k = repeat_kv(k.transpose(1, 2), self.n_key_value_groups)\n",
    "                v = repeat_kv(v.transpose(1, 2), self.n_key_value_groups)\n",
    "\n",
    "                def sliding_window_mask_fn(b, h, q_idx, kv_idx):\n",
    "                    return q_idx - kv_idx <= self.window_size\n",
    "\n",
    "                def causal_mask_fn(b, h, q_idx, kv_idx):\n",
    "                    return q_idx >= kv_idx\n",
    "                \n",
    "                def prefix_mask_fn(b, h, q_idx, kv_idx):\n",
    "                    return kv_idx < self.n_meta_tokens\n",
    "                \n",
    "                attn_mask_fn = and_masks(causal_mask_fn, sliding_window_mask_fn)\n",
    "                combined_mask_fn = or_masks(attn_mask_fn, prefix_mask_fn)\n",
    "                \n",
    "                block_mask = create_block_mask(\n",
    "                    combined_mask_fn, B=batch_size, H=self.n_heads, Q_LEN=q_len, KV_LEN=kv_seq_len\n",
    "                )\n",
    "\n",
    "                attn_output = flex_attention(q, k, v, block_mask=block_mask)\n",
    "                attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, q_len, -1)\n",
    "            else:\n",
    "                q = q.transpose(1, 2)\n",
    "                k = repeat_kv(k.transpose(1, 2), self.n_key_value_groups)\n",
    "                v = repeat_kv(v.transpose(1, 2), self.n_key_value_groups)\n",
    "                if attn_mask is None and q_len > 1:\n",
    "                     attn_mask = torch.full((q_len, kv_seq_len), float(\"-inf\"), device=q.device)\n",
    "                     attn_mask = torch.triu(attn_mask, diagonal=1)\n",
    "                attn_output = F.scaled_dot_product_attention(q, k, v, attn_mask=attn_mask)\n",
    "                attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, q_len, -1)\n",
    "        else:\n",
    "            q = q.transpose(1, 2)\n",
    "            k = repeat_kv(k.transpose(1, 2), self.n_key_value_groups)\n",
    "            v = repeat_kv(v.transpose(1, 2), self.n_key_value_groups)\n",
    "            \n",
    "            if attn_mask is None and q_len > 1:\n",
    "                 pass\n",
    "            attn_output = F.scaled_dot_product_attention(q, k, v, attn_mask=attn_mask)\n",
    "            attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, q_len, -1)\n",
    "            \n",
    "        return attn_output, present_kv\n",
    "\n",
    "class MambaBranch(nn.Module):\n",
    "    def __init__(self, d_inner, d_state, d_conv, dt_rank):\n",
    "        super().__init__()\n",
    "        if causal_conv1d_fn is None or selective_scan_fn is None:\n",
    "            raise ImportError(\"Mamba packages not found. Please install them.\")\n",
    "        \n",
    "        self.d_inner, self.d_state, self.d_conv, self.dt_rank = d_inner, d_state, d_conv, dt_rank\n",
    "        \n",
    "        self.conv1d = nn.Conv1d(in_channels=self.d_inner, out_channels=self.d_inner, kernel_size=d_conv, bias=True, groups=self.d_inner, padding=d_conv - 1)\n",
    "        self.x_proj = nn.Linear(self.d_inner, self.dt_rank + 2 * self.d_state, bias=False)\n",
    "        self.dt_proj = nn.Linear(self.dt_rank, self.d_inner, bias=True)\n",
    "        \n",
    "        A = torch.arange(1, self.d_state + 1, dtype=torch.float32).repeat(self.d_inner, 1)\n",
    "        self.A_log = nn.Parameter(torch.log(A))\n",
    "        self.D = nn.Parameter(torch.ones(self.d_inner))\n",
    "\n",
    "    def forward(self, x, z):\n",
    "        x_transposed = x.transpose(1, 2).contiguous()\n",
    "        x_conv = causal_conv1d_fn(x_transposed, self.conv1d.weight.squeeze(1), self.conv1d.bias, activation=\"silu\")\n",
    "        \n",
    "        x_dbl = self.x_proj(x_conv.transpose(1, 2))\n",
    "        dt_pre, B, C = torch.split(x_dbl, [self.dt_rank, self.d_state, self.d_state], dim=-1)\n",
    "        \n",
    "        dt = self.dt_proj(dt_pre).transpose(1, 2)\n",
    "        A = -torch.exp(self.A_log.float())\n",
    "        \n",
    "        y = selective_scan_fn(\n",
    "            x_conv, dt, A, B.transpose(1, 2), C.transpose(1, 2), self.D.float(), \n",
    "            z=z.transpose(1,2), delta_bias=self.dt_proj.bias.float(), delta_softplus=True\n",
    "        )\n",
    "        return y.transpose(1,2)\n",
    "        \n",
    "# -----------------------------------------------------------------------------\n",
    "# 2. Ablation을 위한 각 단계별 블록 정의\n",
    "# -----------------------------------------------------------------------------\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model: int, hidden_dim: int):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(d_model, hidden_dim, bias=False)\n",
    "        self.w3 = nn.Linear(d_model, hidden_dim, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_dim, d_model, bias=False)\n",
    "        self.act_fn = F.silu\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.w2(self.act_fn(self.w1(x)) * self.w3(x))\n",
    "\n",
    "class MambaOnlyBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, ffn_hidden_dim: int, mamba_params: dict):\n",
    "        super().__init__()\n",
    "        self.d_inner = mamba_params['expand'] * d_model\n",
    "        \n",
    "        self.norm = RMSNorm(d_model)\n",
    "        self.in_proj = nn.Linear(d_model, 2 * self.d_inner, bias=False)\n",
    "        self.mamba_branch = MambaBranch(d_inner=self.d_inner, **{k:v for k,v in mamba_params.items() if k != 'expand'})\n",
    "        self.out_proj = nn.Linear(self.d_inner, d_model, bias=False)\n",
    "        self.ffn = FeedForward(d_model, ffn_hidden_dim)\n",
    "        self.ffn_norm = RMSNorm(d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, **kwargs):\n",
    "        residual = x\n",
    "        x_norm = self.norm(x)\n",
    "        \n",
    "        xz = self.in_proj(x_norm)\n",
    "        x_mamba, z_mamba = xz.chunk(2, dim=-1)\n",
    "        \n",
    "        mamba_out = self.mamba_branch(x_mamba, z_mamba)\n",
    "        \n",
    "        h = residual + self.out_proj(mamba_out)\n",
    "        out = h + self.ffn(self.ffn_norm(h))\n",
    "        return out, None\n",
    "\n",
    "class HymbaBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, n_kv_heads: int, max_seq_len: int, window_size: int, mamba_params: dict, n_meta_tokens: int, use_concat: bool = False):\n",
    "        super().__init__()\n",
    "        self.d_inner = mamba_params['expand'] * d_model\n",
    "        attn_head_dim = self.d_inner // n_heads\n",
    "        \n",
    "        self.use_concat = use_concat\n",
    "        \n",
    "        out_proj_input_dim = self.d_inner * 2 if self.use_concat else self.d_inner\n",
    "        \n",
    "        latent_dim = self.d_inner + self.d_inner + (attn_head_dim * n_kv_heads * 2)\n",
    "        self.in_proj = nn.Linear(d_model, latent_dim + self.d_inner, bias=True)\n",
    "        self.out_proj = nn.Linear(out_proj_input_dim, d_model, bias=True)\n",
    "\n",
    "        self.attn_branch = AttentionBranch(d_inner=self.d_inner, n_heads=n_heads, n_kv_heads=n_kv_heads, max_seq_len=max_seq_len, window_size=window_size, n_meta_tokens=n_meta_tokens)\n",
    "        self.mamba_branch = MambaBranch(d_inner=self.d_inner, d_state=mamba_params['d_state'], d_conv=mamba_params['d_conv'], dt_rank=mamba_params['dt_rank'])\n",
    "\n",
    "        self.norm = RMSNorm(d_model)\n",
    "        self.norm1 = RMSNorm(self.d_inner)\n",
    "        self.norm2 = RMSNorm(self.d_inner)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, past_kv: Optional[Tuple[torch.Tensor, torch.Tensor]] = None, attn_mask: Optional[torch.Tensor] = None, use_cache: bool = False):\n",
    "        projected = self.in_proj(self.norm(x))\n",
    "        \n",
    "        latent, gate = projected.tensor_split((projected.shape[-1] - self.d_inner,), dim=-1)\n",
    "        \n",
    "        attn_q_dim = self.d_inner\n",
    "        attn_k_dim = self.attn_branch.head_dim * self.attn_branch.n_kv_heads\n",
    "        \n",
    "        q, k, v, mamba_x = latent.tensor_split((attn_q_dim, attn_q_dim + attn_k_dim, attn_q_dim + 2 * attn_k_dim), dim=-1)\n",
    "\n",
    "        attn_out, present_kv = self.attn_branch(q, k, v, past_kv, attn_mask, use_cache)\n",
    "        mamba_out = self.mamba_branch(mamba_x, gate)\n",
    "\n",
    "        if self.use_concat:\n",
    "            combined = torch.cat([self.norm1(attn_out), self.norm2(mamba_out)], dim=-1)\n",
    "        else:\n",
    "            combined = (self.norm1(attn_out) + self.norm2(mamba_out)) / 2\n",
    "            \n",
    "        return self.out_proj(combined), present_kv\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3. 전체 모델 및 마스크 생성\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class HymbaModel(nn.Module):\n",
    "    def __init__(self, config: dict):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.embedding = nn.Embedding(config['vocab_size'], config['d_model'])\n",
    "        if config['use_meta_tokens']:\n",
    "            self.meta_tokens = nn.Parameter(torch.randn(1, config['n_meta_tokens'], config['d_model']))\n",
    "\n",
    "        layers = []\n",
    "        for i in range(config['n_layers']):\n",
    "            is_full_attn_layer = False\n",
    "            is_mamba_only_config = config['use_mamba_only']\n",
    "\n",
    "            if not is_mamba_only_config and config['use_full_attention_layers']:\n",
    "                mid_layer = config['n_layers'] // 2\n",
    "                if config['n_layers'] % 2 == 0:\n",
    "                    mid_layer = mid_layer - 1\n",
    "\n",
    "                if i == 0 or i == mid_layer or i == config['n_layers'] - 1:\n",
    "                    is_full_attn_layer = True\n",
    "\n",
    "            use_swa = config['use_swa'] and not is_full_attn_layer and not is_mamba_only_config\n",
    "\n",
    "            if is_mamba_only_config:\n",
    "                 layers.append(MambaOnlyBlock(\n",
    "                     d_model=config['d_model'],\n",
    "                     ffn_hidden_dim=config['ffn_hidden_dim'],\n",
    "                     mamba_params=config['mamba_params']\n",
    "                   ))\n",
    "            else:\n",
    "                layers.append(HymbaBlock(\n",
    "                     d_model=config['d_model'], n_heads=config['n_heads'], \n",
    "                     n_kv_heads=config['n_kv_heads'], max_seq_len=config['max_seq_len'],\n",
    "                     window_size=config['window_size'] if use_swa else -1,\n",
    "                     mamba_params=config['mamba_params'],\n",
    "                     n_meta_tokens=config['n_meta_tokens'] if config['use_meta_tokens'] else 0,\n",
    "                     use_concat=config['use_concat']\n",
    "                   ))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.norm = RMSNorm(config['d_model'])\n",
    "        self.lm_head = nn.Linear(config['d_model'], config['vocab_size'], bias=False)\n",
    "        \n",
    "        self.to(torch.bfloat16)\n",
    "\n",
    "    def _create_attention_mask(self, q_len: int, kv_len: int, window_size: int, n_meta_tokens: int, device: str) -> Optional[torch.Tensor]:\n",
    "        if q_len > 1:\n",
    "            mask = torch.full((1, 1, q_len, kv_len), float(\"-inf\"), device=device, dtype=torch.bfloat16)\n",
    "            mask = torch.triu(mask, diagonal=1)\n",
    "            \n",
    "            if window_size > 0:\n",
    "                sliding_mask = torch.ones(q_len, kv_len, device=device, dtype=torch.bool)\n",
    "                sliding_mask.tril_(-1).triu_(-window_size)\n",
    "                mask.masked_fill_(~sliding_mask[None, None, ...], float(\"-inf\"))\n",
    "            \n",
    "            if n_meta_tokens > 0:\n",
    "                mask[..., n_meta_tokens:, :n_meta_tokens] = 0\n",
    "            \n",
    "            return mask\n",
    "        return None\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor, use_cache: bool = False, return_kv_cache: bool = False):\n",
    "        batch_size, seq_len = tokens.shape\n",
    "        is_decoding = use_cache and tokens.shape[1] == 1\n",
    "        \n",
    "        h = self.embedding(tokens)\n",
    "        \n",
    "        current_seq_len = seq_len\n",
    "        if self.config['use_meta_tokens'] and not is_decoding:\n",
    "            meta_embeds = self.meta_tokens.expand(batch_size, -1, -1)\n",
    "            h = torch.cat([meta_embeds, h], dim=1)\n",
    "            current_seq_len += self.config['n_meta_tokens']\n",
    "            \n",
    "        kv_cache_list = [None] * self.config['n_layers']\n",
    "        \n",
    "        attn_mask = None\n",
    "        if not self.config['use_mamba_only'] and not is_decoding:\n",
    "            attn_mask = self._create_attention_mask(\n",
    "                q_len=current_seq_len, kv_len=current_seq_len,\n",
    "                window_size=self.config['window_size'] if self.config['use_swa'] else -1,\n",
    "                n_meta_tokens=self.config['n_meta_tokens'] if self.config['use_meta_tokens'] else 0,\n",
    "                device=h.device\n",
    "            )\n",
    "        \n",
    "        residual = h\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            past_kv = kv_cache_list[i] if use_cache and is_decoding else None\n",
    "            \n",
    "            if use_cache and is_decoding and i > 0 and self.config['use_shared_kv_cache']:\n",
    "                 past_kv = kv_cache_list[i-1] if i % 2 == 1 else past_kv\n",
    "            \n",
    "            output, present_kv = layer(h, past_kv=past_kv, attn_mask=attn_mask, use_cache=use_cache)\n",
    "            h = residual + output\n",
    "            residual = h\n",
    "\n",
    "            if use_cache:\n",
    "                kv_cache_list[i] = present_kv\n",
    "\n",
    "        h = self.norm(h)\n",
    "        \n",
    "        if self.config['use_meta_tokens'] and not is_decoding:\n",
    "            h = h[:, self.config['n_meta_tokens']:]\n",
    "            \n",
    "        logits = self.lm_head(h)\n",
    "        \n",
    "        if return_kv_cache:\n",
    "            return logits, kv_cache_list\n",
    "        return logits\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4. 단계별 모델링 및 성능 측정\n",
    "# -----------------------------------------------------------------------------\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if not torch.cuda.is_available() and selective_scan_fn is not None:\n",
    "        print(\"경고: 이 코드는 mamba-ssm의 CUDA 커널에 의존하므로 GPU 환경에서 실행해야 합니다.\")\n",
    "    \n",
    "    # 모델 크기와 시퀀스 길이를 확장합니다.\n",
    "    base_config = {\n",
    "        'vocab_size': 32000,\n",
    "        'd_model': 512,\n",
    "        'n_layers': 12,\n",
    "        'n_heads': 16,\n",
    "        'n_kv_heads': 4,\n",
    "        'max_seq_len': 4096,\n",
    "        'ffn_hidden_dim': 512 * 4,\n",
    "        'window_size': 1024,\n",
    "        'n_meta_tokens': 4,\n",
    "        'mamba_params': {'d_state': 16, 'd_conv': 4, 'expand': 2, 'dt_rank': math.ceil((512 * 2) / 16)},\n",
    "    }\n",
    "    \n",
    "    configs_to_run = [\n",
    "        (\"Without_Meta_Tokens\", {'use_mamba_only': False, 'use_swa': True, 'use_shared_kv_cache': True, 'use_meta_tokens': False, 'use_full_attention_layers': True, 'use_concat': False}),\n",
    "        (\"With_Meta_Tokens\", {'use_mamba_only': False, 'use_swa': True, 'use_shared_kv_cache': True, 'use_meta_tokens': True, 'use_full_attention_layers': True, 'use_concat': False}),\n",
    "    ]\n",
    "    \n",
    "    batch_size = 2\n",
    "    \n",
    "    for name, flags in configs_to_run:\n",
    "        print(f\"\\n{'='*20} {name} {'='*20}\")\n",
    "        \n",
    "        config = base_config.copy()\n",
    "        config.update(flags)\n",
    "\n",
    "        try:\n",
    "            model = HymbaModel(config).to(device)\n",
    "            model.eval()\n",
    "\n",
    "            # 훈련 시퀀스 길이를 동적으로 조정하는 로직\n",
    "            training_seq_len = config['max_seq_len']\n",
    "            if config['use_meta_tokens']:\n",
    "                # 메타 토큰의 길이를 제외한 실제 입력 시퀀스 길이\n",
    "                training_seq_len -= config['n_meta_tokens']\n",
    "\n",
    "            dummy_input = torch.randint(0, config['vocab_size'], (batch_size, training_seq_len)).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                logits, kv_cache_list = model(dummy_input, use_cache=True, return_kv_cache=True)\n",
    "            \n",
    "            total_cache_bytes = 0\n",
    "            if not config['use_mamba_only']:\n",
    "                if config['use_shared_kv_cache']:\n",
    "                    for i in range(1, config['n_layers'], 2):\n",
    "                        cache = kv_cache_list[i]\n",
    "                        if cache is not None:\n",
    "                            k, v = cache\n",
    "                            total_cache_bytes += k.numel() * k.element_size() + v.numel() * v.element_size()\n",
    "                else:\n",
    "                    for cache in kv_cache_list:\n",
    "                        if cache is not None:\n",
    "                            k, v = cache\n",
    "                            total_cache_bytes += k.numel() * k.element_size() + v.numel() * v.element_size()\n",
    "\n",
    "            total_cache_mb = total_cache_bytes / (1024 * 1024)\n",
    "            print(f\"--- KV Cache 크기: {total_cache_mb:.2f} MB\")\n",
    "\n",
    "            print(\"\\n--- 훈련(Training) 모드 벤치마크 (전체 시퀀스 처리)\")\n",
    "            warmup_iterations = 5\n",
    "            measurement_iterations = 10\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for _ in range(warmup_iterations):\n",
    "                    _ = model(dummy_input, use_cache=False)\n",
    "                if torch.cuda.is_available(): torch.cuda.synchronize()\n",
    "\n",
    "                start_time = time.time()\n",
    "                for _ in range(measurement_iterations):\n",
    "                    _ = model(dummy_input, use_cache=False)\n",
    "                if torch.cuda.is_available(): torch.cuda.synchronize()\n",
    "                end_time = time.time()\n",
    "\n",
    "            total_time = end_time - start_time\n",
    "            total_tokens = batch_size * training_seq_len * measurement_iterations\n",
    "            tokens_per_second = total_tokens / total_time\n",
    "            print(f\"--- 처리량: {tokens_per_second:,.2f} tokens/sec\")\n",
    "\n",
    "            print(\"\\n--- 추론(Inference) 모드 벤치마크 (토큰당 생성)\")\n",
    "            input_tokens = torch.randint(0, config['vocab_size'], (batch_size, 1)).to(device)\n",
    "            num_tokens_to_generate = 100\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for _ in range(warmup_iterations):\n",
    "                    _ = model(input_tokens, use_cache=True, return_kv_cache=True)\n",
    "                if torch.cuda.is_available(): torch.cuda.synchronize()\n",
    "\n",
    "                start_time = time.time()\n",
    "                current_kv_cache = None\n",
    "                for _ in range(num_tokens_to_generate):\n",
    "                    logits, current_kv_cache = model(input_tokens, use_cache=True, return_kv_cache=True)\n",
    "                    next_token = torch.argmax(logits[:, -1, :], dim=-1).unsqueeze(1)\n",
    "                    input_tokens = next_token\n",
    "                \n",
    "                if torch.cuda.is_available(): torch.cuda.synchronize()\n",
    "                end_time = time.time()\n",
    "\n",
    "            total_time = end_time - start_time\n",
    "            tokens_per_second = (batch_size * num_tokens_to_generate) / total_time\n",
    "            print(f\"--- 처리량: {tokens_per_second:,.2f} tokens/sec\")\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"오류 발생: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
