{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Hymba 수정된 구현 평가\n",
    "\n",
    "## 수정 사항\n",
    "\n",
    "### 1. KV Sharing RoPE 버그 수정\n",
    "- **문제**: Producer가 RoPE 적용 전 K를 공유 → Consumer에서 RoPE 중복 적용\n",
    "- **해결**: Producer가 RoPE 적용 후 K를 공유, Consumer는 RoPE 재적용 안함\n",
    "\n",
    "### 2. SWA Window Size 최적화\n",
    "- **변경**: `window=256` → `window=128` (seq_len=1024의 1/8)\n",
    "- **이유**: 더 명확한 local attention 효과\n",
    "\n",
    "## 예상 결과\n",
    "- Hybrid-Mine PPL: 40.70 → ~37-38 (Official 수준으로 개선)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA A100 80GB PCIe\n",
      "Memory: 85.1 GB\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "import gc\n",
    "sys.path.append('./backbone')\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from collections import defaultdict\n",
    "\n",
    "# 모듈 리로드 (수정된 코드 반영)\n",
    "import importlib\n",
    "if 'hymba' in sys.modules:\n",
    "    importlib.reload(sys.modules['hymba'])\n",
    "if 'hymba_official' in sys.modules:\n",
    "    importlib.reload(sys.modules['hymba_official'])\n",
    "\n",
    "# 수정된 내 구현\n",
    "from hymba import Hymba, HymbaConfig, ArchType, AttentionType\n",
    "\n",
    "# 공식 구현 스타일\n",
    "from hymba_official import HymbaOfficialModel, HymbaOfficialConfig\n",
    "\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "RESULTS_DIR = './results'\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Device: {device}')\n",
    "if device == 'cuda':\n",
    "    print(f'GPU: {torch.cuda.get_device_name()}')\n",
    "    print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (16, 10)\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. 데이터셋 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "WikiText-103 데이터셋 로드\n",
      "======================================================================\n",
      "Tokenizer: GPT-2 (vocab_size=50257)\n",
      "\n",
      "Train samples: 1,801,350\n",
      "Validation samples: 3,760\n"
     ]
    }
   ],
   "source": [
    "print('=' * 70)\n",
    "print('WikiText-103 데이터셋 로드')\n",
    "print('=' * 70)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "\n",
    "print(f'Tokenizer: GPT-2 (vocab_size={VOCAB_SIZE})')\n",
    "\n",
    "dataset = load_dataset('wikitext', 'wikitext-103-raw-v1')\n",
    "\n",
    "print(f\"\\nTrain samples: {len(dataset['train']):,}\")\n",
    "print(f\"Validation samples: {len(dataset['validation']):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "데이터셋 생성 (seq_len=1024)\n",
      "======================================================================\n",
      "텍스트 토큰화 중...\n"
     ]
    }
   ],
   "source": [
    "class WikiTextDataset(Dataset):\n",
    "    def __init__(self, texts: List[str], tokenizer, seq_len: int, max_tokens: int = None):\n",
    "        self.seq_len = seq_len\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        print(f'텍스트 토큰화 중...')\n",
    "        all_text = ' '.join([t for t in texts if t.strip()])\n",
    "        tokens = tokenizer.encode(all_text, add_special_tokens=False)\n",
    "        \n",
    "        if max_tokens:\n",
    "            tokens = tokens[:max_tokens]\n",
    "        \n",
    "        self.tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "        self.n_chunks = (len(self.tokens) - 1) // seq_len\n",
    "        \n",
    "        print(f'총 토큰: {len(self.tokens):,}')\n",
    "        print(f'청크 수 (seq_len={seq_len}): {self.n_chunks:,}')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_chunks\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        start = idx * self.seq_len\n",
    "        end = start + self.seq_len + 1\n",
    "        chunk = self.tokens[start:end]\n",
    "        return chunk[:-1], chunk[1:]\n",
    "\n",
    "\n",
    "SEQ_LEN = 1024\n",
    "BATCH_SIZE = 8\n",
    "MAX_TRAIN_TOKENS = 50_000_000\n",
    "MAX_VAL_TOKENS = 500_000\n",
    "\n",
    "print('\\n' + '=' * 70)\n",
    "print(f'데이터셋 생성 (seq_len={SEQ_LEN})')\n",
    "print('=' * 70)\n",
    "\n",
    "train_dataset = WikiTextDataset(\n",
    "    dataset['train']['text'], tokenizer, SEQ_LEN, max_tokens=MAX_TRAIN_TOKENS\n",
    ")\n",
    "val_dataset = WikiTextDataset(\n",
    "    dataset['validation']['text'], tokenizer, SEQ_LEN, max_tokens=MAX_VAL_TOKENS\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f'\\n학습: {len(train_dataset):,} 청크, {len(train_loader):,} 배치')\n",
    "print(f'검증: {len(val_dataset):,} 청크')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. 모델 설정 (수정된 버전)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    name: str\n",
    "    config: any\n",
    "    description: str\n",
    "    is_official: bool = False\n",
    "\n",
    "\n",
    "# 수정된 설정:\n",
    "# - SWA window: 256 → 128 (더 명확한 local attention)\n",
    "SWA_WINDOW = 128  # 수정: seq_len의 1/8\n",
    "NUM_META_TOKENS = 64\n",
    "\n",
    "experiments: Dict[str, ExperimentConfig] = {\n",
    "    # Hybrid - 수정된 내 구현 (RoPE 버그 수정 + window 최적화)\n",
    "    'Hybrid-Mine-Fixed': ExperimentConfig(\n",
    "        name='Hybrid-Mine-Fixed',\n",
    "        config=HymbaConfig(\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "            d_model=320,\n",
    "            n_layers=11,\n",
    "            n_heads=5,\n",
    "            n_kv_heads=1,\n",
    "            arch_type=ArchType.HYBRID,\n",
    "            global_attn_idx=[0, 5, 10],\n",
    "            use_meta_tokens=True,\n",
    "            num_meta_tokens=NUM_META_TOKENS,\n",
    "            swa_window=SWA_WINDOW,\n",
    "            dropout=0.1,\n",
    "        ),\n",
    "        description='Hybrid (수정됨: RoPE 버그 수정 + window=128)',\n",
    "    ),\n",
    "    \n",
    "    # Hybrid - 공식 스타일\n",
    "    'Hybrid-Official': ExperimentConfig(\n",
    "        name='Hybrid-Official',\n",
    "        config=HymbaOfficialConfig(\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "            hidden_size=320,\n",
    "            num_hidden_layers=11,\n",
    "            num_attention_heads=5,\n",
    "            num_key_value_heads=1,\n",
    "            attn_hidden_size=320,\n",
    "            global_attn_idx=[0, 5, 10],\n",
    "            num_memory_tokens=NUM_META_TOKENS,\n",
    "            attn_window_size=SWA_WINDOW,\n",
    "            mamba_expand=2,\n",
    "            mamba_d_state=16,\n",
    "            mamba_d_conv=4,\n",
    "            intermediate_size=320 * 3,\n",
    "            attention_dropout=0.1,\n",
    "        ),\n",
    "        description='Hybrid (공식 스타일: 단일 in_proj, avg fusion)',\n",
    "        is_official=True,\n",
    "    ),\n",
    "}\n",
    "\n",
    "print('=' * 90)\n",
    "print('실험 모델 설정')\n",
    "print('=' * 90)\n",
    "print(f'{\"Name\":<20} {\"Params\":>10} {\"Layers\":>7} {\"d_model\":>8} {\"Meta\":>6} {\"SWA\":>6}')\n",
    "print('-' * 90)\n",
    "\n",
    "for name, exp in experiments.items():\n",
    "    cfg = exp.config\n",
    "    \n",
    "    if exp.is_official:\n",
    "        model = HymbaOfficialModel(cfg)\n",
    "        d_model = cfg.hidden_size\n",
    "        layers = cfg.num_hidden_layers\n",
    "        meta = cfg.num_memory_tokens\n",
    "        swa = cfg.attn_window_size\n",
    "    else:\n",
    "        model = Hymba(cfg)\n",
    "        d_model = cfg.d_model\n",
    "        layers = cfg.n_layers\n",
    "        meta = cfg.num_meta_tokens if cfg.use_meta_tokens else 0\n",
    "        swa = cfg.swa_window\n",
    "    \n",
    "    params = model.count_parameters()['total']\n",
    "    print(f'{name:<20} {params/1e6:>9.2f}M {layers:>7} {d_model:>8} {meta:>6} {swa:>6}')\n",
    "    del model\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print('=' * 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainConfig:\n",
    "    epochs: int = 3\n",
    "    lr: float = 3e-4\n",
    "    min_lr: float = 3e-5\n",
    "    warmup_ratio: float = 0.05\n",
    "    weight_decay: float = 0.1\n",
    "    grad_clip: float = 1.0\n",
    "    eval_interval: int = 500\n",
    "    log_interval: int = 100\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    config: TrainConfig,\n",
    "    model_name: str = '',\n",
    ") -> Dict:\n",
    "    model = model.to(device).train()\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config.lr,\n",
    "        betas=(0.9, 0.95),\n",
    "        weight_decay=config.weight_decay,\n",
    "    )\n",
    "    \n",
    "    total_steps = config.epochs * len(train_loader)\n",
    "    warmup_steps = int(total_steps * config.warmup_ratio)\n",
    "    \n",
    "    def lr_lambda(step):\n",
    "        if step < warmup_steps:\n",
    "            return step / warmup_steps\n",
    "        progress = (step - warmup_steps) / max(1, total_steps - warmup_steps)\n",
    "        cosine_decay = 0.5 * (1 + np.cos(np.pi * progress))\n",
    "        return config.min_lr / config.lr + (1 - config.min_lr / config.lr) * cosine_decay\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "    \n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_ppl': [], 'step': [], 'lr': []}\n",
    "    step = 0\n",
    "    best_val_loss = float('inf')\n",
    "    t0 = time.time()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for epoch in range(config.epochs):\n",
    "        model.train()\n",
    "        pbar = tqdm(train_loader, desc=f'[{model_name}] Epoch {epoch+1}/{config.epochs}', leave=False)\n",
    "        \n",
    "        for xb, yb in pbar:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            \n",
    "            out = model(xb, targets=yb)\n",
    "            loss = out['loss']\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scheduler.step()\n",
    "            step += 1\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            if step % config.log_interval == 0:\n",
    "                avg_loss = running_loss / config.log_interval\n",
    "                running_loss = 0.0\n",
    "                pbar.set_postfix({'loss': f'{avg_loss:.3f}', 'lr': f'{scheduler.get_last_lr()[0]:.2e}'})\n",
    "            \n",
    "            if step % config.eval_interval == 0:\n",
    "                model.eval()\n",
    "                val_loss = 0.0\n",
    "                val_count = 0\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for vxb, vyb in val_loader:\n",
    "                        vxb, vyb = vxb.to(device), vyb.to(device)\n",
    "                        vout = model(vxb, targets=vyb)\n",
    "                        val_loss += vout['loss'].item() * vxb.size(0)\n",
    "                        val_count += vxb.size(0)\n",
    "                \n",
    "                val_loss /= val_count\n",
    "                val_ppl = np.exp(val_loss)\n",
    "                \n",
    "                history['train_loss'].append(loss.item())\n",
    "                history['val_loss'].append(val_loss)\n",
    "                history['val_ppl'].append(val_ppl)\n",
    "                history['step'].append(step)\n",
    "                history['lr'].append(scheduler.get_last_lr()[0])\n",
    "                \n",
    "                best_val_loss = min(best_val_loss, val_loss)\n",
    "                model.train()\n",
    "    \n",
    "    elapsed = time.time() - t0\n",
    "    tokens_processed = step * BATCH_SIZE * SEQ_LEN\n",
    "    \n",
    "    return {\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'best_ppl': np.exp(best_val_loss),\n",
    "        'final_ppl': history['val_ppl'][-1] if history['val_ppl'] else np.exp(best_val_loss),\n",
    "        'time_min': elapsed / 60,\n",
    "        'tokens_per_sec': tokens_processed / elapsed,\n",
    "        'history': history,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = TrainConfig(epochs=3, lr=3e-4, eval_interval=500, log_interval=100)\n",
    "results: Dict[str, Dict] = {}\n",
    "trained_models: Dict[str, nn.Module] = {}\n",
    "\n",
    "print('\\n' + '=' * 80)\n",
    "print('학습 시작 (수정된 버전)')\n",
    "print(f'총 에폭: {train_config.epochs}')\n",
    "print(f'배치당 토큰: {BATCH_SIZE * SEQ_LEN:,}')\n",
    "print(f'SWA Window: {SWA_WINDOW} (seq_len의 {SWA_WINDOW/SEQ_LEN*100:.1f}%)')\n",
    "print('=' * 80)\n",
    "\n",
    "for exp_name, exp_config in experiments.items():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"실험: {exp_name}\")\n",
    "    print(f\"설명: {exp_config.description}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    cfg = exp_config.config\n",
    "    \n",
    "    if exp_config.is_official:\n",
    "        model = HymbaOfficialModel(cfg)\n",
    "    else:\n",
    "        model = Hymba(cfg)\n",
    "    \n",
    "    params = model.count_parameters()\n",
    "    print(f'Parameters: {params[\"total\"]/1e6:.2f}M')\n",
    "    \n",
    "    train_result = train_model(model, train_loader, val_loader, train_config, exp_name)\n",
    "    \n",
    "    results[exp_name] = {\n",
    "        'config': cfg,\n",
    "        'params': params['total'],\n",
    "        'best_ppl': train_result['best_ppl'],\n",
    "        'final_ppl': train_result['final_ppl'],\n",
    "        'time_min': train_result['time_min'],\n",
    "        'tokens_per_sec': train_result['tokens_per_sec'],\n",
    "        'history': train_result['history'],\n",
    "        'is_official': exp_config.is_official,\n",
    "    }\n",
    "    \n",
    "    trained_models[exp_name] = model.eval()\n",
    "    \n",
    "    print(f'Best PPL: {train_result[\"best_ppl\"]:.2f}')\n",
    "    print(f'Throughput: {train_result[\"tokens_per_sec\"]/1000:.1f}K tokens/sec')\n",
    "    print(f'Time: {train_result[\"time_min\"]:.1f} min')\n",
    "\n",
    "print('\\n' + '=' * 80)\n",
    "print('모든 학습 완료!')\n",
    "print('=' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. 결과 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([{\n",
    "    'Model': name,\n",
    "    'Type': 'Official' if r.get('is_official', False) else 'Mine',\n",
    "    'Params (M)': r['params'] / 1e6,\n",
    "    'Best PPL': r['best_ppl'],\n",
    "    'Final PPL': r['final_ppl'],\n",
    "    'Throughput (K tok/s)': r['tokens_per_sec'] / 1000,\n",
    "    'Time (min)': r['time_min'],\n",
    "} for name, r in results.items()])\n",
    "\n",
    "df_sorted = df.sort_values('Best PPL')\n",
    "\n",
    "print('\\n' + '=' * 100)\n",
    "print('결과 요약 (Best PPL 기준 정렬)')\n",
    "print('=' * 100)\n",
    "print(df_sorted.to_string(index=False))\n",
    "print('=' * 100)\n",
    "\n",
    "# 이전 결과와 비교\n",
    "print('\\n' + '=' * 80)\n",
    "print('이전 결과와 비교')\n",
    "print('=' * 80)\n",
    "print(f'이전 Hybrid-Mine PPL: 40.70')\n",
    "print(f'이전 Hybrid-Official PPL: 37.14')\n",
    "print(f'이전 차이: 3.56')\n",
    "print()\n",
    "\n",
    "mine_ppl = results.get('Hybrid-Mine-Fixed', {}).get('best_ppl', 0)\n",
    "official_ppl = results.get('Hybrid-Official', {}).get('best_ppl', 0)\n",
    "if mine_ppl and official_ppl:\n",
    "    diff = mine_ppl - official_ppl\n",
    "    print(f'수정 후 Hybrid-Mine-Fixed PPL: {mine_ppl:.2f}')\n",
    "    print(f'수정 후 Hybrid-Official PPL: {official_ppl:.2f}')\n",
    "    print(f'수정 후 차이: {diff:.2f}')\n",
    "    \n",
    "    improvement = 40.70 - mine_ppl\n",
    "    print(f'\\nHybrid-Mine 개선: {improvement:.2f} PPL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 곡선 시각화\n",
    "colors = {\n",
    "    'Hybrid-Mine-Fixed': '#45B7D1',\n",
    "    'Hybrid-Official': '#6B5B95',\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# PPL 비교\n",
    "ax = axes[0]\n",
    "bars = ax.barh(df_sorted['Model'], df_sorted['Best PPL'],\n",
    "               color=[colors.get(m, 'gray') for m in df_sorted['Model']])\n",
    "ax.set_xlabel('Best Validation PPL (lower is better)')\n",
    "ax.set_title('Model Performance Comparison', fontweight='bold', fontsize=12)\n",
    "ax.invert_yaxis()\n",
    "for bar, val in zip(bars, df_sorted['Best PPL']):\n",
    "    ax.text(val + 0.3, bar.get_y() + bar.get_height()/2, f'{val:.2f}', va='center')\n",
    "\n",
    "# 학습 곡선\n",
    "ax = axes[1]\n",
    "for name, r in results.items():\n",
    "    linestyle = '--' if r.get('is_official', False) else '-'\n",
    "    ax.plot(r['history']['step'], r['history']['val_ppl'],\n",
    "            label=name, linewidth=2, linestyle=linestyle,\n",
    "            color=colors.get(name, 'gray'))\n",
    "ax.set_xlabel('Training Steps')\n",
    "ax.set_ylabel('Validation PPL')\n",
    "ax.set_title('Training Curves', fontweight='bold', fontsize=12)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_DIR}/training_comparison_fixed.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. 결론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 80)\n",
    "print('실험 결론')\n",
    "print('=' * 80)\n",
    "\n",
    "print('''\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│                              수정 사항                                        │\n",
    "├─────────────────────────────────────────────────────────────────────────────┤\n",
    "│ 1. KV Sharing RoPE 버그 수정                                                 │\n",
    "│    - 이전: RoPE 적용 전 K 공유 → Consumer에서 RoPE 중복 적용                   │\n",
    "│    - 수정: RoPE 적용 후 K 공유 → Consumer는 RoPE 재적용 안함                   │\n",
    "│                                                                               │\n",
    "│ 2. SWA Window Size 최적화                                                    │\n",
    "│    - 이전: window=256 (seq_len의 25%)                                         │\n",
    "│    - 수정: window=128 (seq_len의 12.5%)                                       │\n",
    "│    - 효과: 더 명확한 local attention 패턴                                     │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "''')\n",
    "\n",
    "mine_ppl = results.get('Hybrid-Mine-Fixed', {}).get('best_ppl', 0)\n",
    "official_ppl = results.get('Hybrid-Official', {}).get('best_ppl', 0)\n",
    "\n",
    "print('\\n결과 비교:')\n",
    "print(f'  이전 Hybrid-Mine PPL: 40.70')\n",
    "print(f'  수정 후 Hybrid-Mine-Fixed PPL: {mine_ppl:.2f}')\n",
    "print(f'  개선량: {40.70 - mine_ppl:.2f} PPL')\n",
    "print(f'\\n  Hybrid-Official PPL: {official_ppl:.2f}')\n",
    "print(f'  차이: {mine_ppl - official_ppl:.2f} (이전: 3.56)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 저장\n",
    "df.to_csv(f'{RESULTS_DIR}/fixed_comparison_results.csv', index=False)\n",
    "print(f'결과 저장 완료: {RESULTS_DIR}/fixed_comparison_results.csv')\n",
    "\n",
    "# 메모리 정리\n",
    "del trained_models\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print('메모리 정리 완료')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
