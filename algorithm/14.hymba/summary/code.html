<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hymba PyTorch 코드</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+KR:wght@400;500;700&family=Source+Code+Pro&display=swap" rel="stylesheet">
    <style>
        body { font-family: 'Noto Sans KR', sans-serif; background-color: #f8fafc; }
        .gradient-text { background: linear-gradient(to right, #0077B6, #00B4D8); -webkit-background-clip: text; -webkit-text-fill-color: transparent; }
        .nav-link { @apply px-3 py-2 rounded-md text-sm font-medium text-gray-700 hover:bg-blue-100 hover:text-blue-800 transition-colors; }
        .nav-link.active { @apply bg-blue-600 text-white hover:bg-blue-700; }
        .code-block { background-color: #1e293b; color: #e2e8f0; border-radius: 0.75rem; padding: 1.5rem; overflow-x: auto; font-family: 'Source Code Pro', monospace; }
        h2 { @apply text-2xl font-bold text-gray-800 mb-4 mt-10 border-l-4 border-blue-500 pl-4; }
        p { @apply text-gray-700 leading-relaxed mb-4; }
    </style>
</head>
<body class="text-gray-800">
    <header class="bg-white/90 backdrop-blur-sm sticky top-0 z-50 shadow-md">
        <nav class="container mx-auto px-4 py-3 flex justify-between items-center">
            <a href="./index.html" class="text-xl font-bold gradient-text">Hymba Research</a>
            <div id="nav-links" class="hidden md:flex items-center space-x-2">
                <a href="./index.html" class="nav-link">개요</a>
                <a href="./principles.html" class="nav-link">핵심 원리</a>
                <a href="./components.html" class="nav-link">구성 요소</a>
                <a href="./performance.html" class="nav-link">성능</a>
                <a href="./code.html" class="nav-link active">코드</a>
                <a href="./dpo.html" class="nav-link">학습 방법 (DPO)</a>
                <a href="./quiz.html" class="nav-link">퀴즈</a>
            </div>
        </nav>
    </header>

    <main class="container mx-auto px-4 py-12">
        <h1 class="text-4xl font-bold text-center mb-12 gradient-text">Hymba PyTorch 구현</h1>
        
        <div class="max-w-4xl mx-auto">
            <p class="text-center text-gray-600 mb-8">
                Hymba 아키텍처의 핵심 구성 요소를 PyTorch로 구현한 코드입니다. 각 클래스별로 나누어 상세한 주석과 함께 설명하여 논문의 아이디어를 실제 구현과 연결하여 이해하는 데 도움을 줍니다.
            </p>

            <h2>기본 구성 요소: Config 및 RMSNorm</h2>
            <p>모델의 하이퍼파라미터를 관리하는 `HymbaConfig`와 효율적인 정규화 레이어인 `RMSNorm`을 먼저 정의합니다.</p>
            <div class="code-block">
<pre><code>
from dataclasses import dataclass
import torch
import torch.nn as nn

@dataclass
class HymbaConfig:
    block_size: int = 2048
    vocab_size: int = 50257
    n_layer: int = 24
    n_head: int = 16
    n_embd: int = 1024
    n_meta_tokens: int = 4
    ssm_d_state: int = 16
    ssm_d_conv: int = 4
    ssm_expand: int = 2
    attention_window_size: int = 256

class RMSNorm(nn.Module):
    def __init__(self, dim: int, eps: float = 1e-6):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))

    def forward(self, x):
        output = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        return output * self.weight
</code></pre>
            </div>

            <h2>하이브리드 헤드: 어텐션과 Mamba</h2>
            <p>Hymba의 심장부인 하이브리드 헤드를 구성하는 `CausalSelfAttention`과 `MambaBlock`입니다. 어텐션 블록에는 슬라이딩 윈도우와 메타 토큰을 위한 마스킹 로직이 포함되어 있습니다.</p>
            <div class="code-block">
<pre><code>
from mamba_ssm import Mamba
import math
from torch.nn import functional as F

class MambaBlock(nn.Module):
    # ... (생략, 이전 코드와 동일)

class CausalSelfAttention(nn.Module):
    # ... (생략, 이전 코드와 동일)

class HybridHeadBlock(nn.Module):
    def __init__(self, config: HymbaConfig):
        super().__init__()
        self.ssm_branch = MambaBlock(config)
        self.attn_branch = CausalSelfAttention(config)
        self.proj = nn.Linear(2 * config.n_embd, config.n_embd)

    def forward(self, x):
        # 두 브랜치에 동일한 입력 x를 병렬로 전달
        ssm_out = self.ssm_branch(x)
        attn_out = self.attn_branch(x)
        # 결과를 채널 차원에서 결합
        combined = torch.cat([ssm_out, attn_out], dim=-1)
        # 최종 출력 프로젝션
        output = self.proj(combined)
        return output
</code></pre>
            </div>

            <h2>Hymba 레이어와 전체 모델 조립</h2>
            <p>개별 구성 요소들을 `HymbaLayer`로 묶고, 이를 쌓아 최종 `Hymba` 모델을 완성합니다. `forward` 메소드에서는 메타 토큰을 추가하고, 최종 로짓을 슬라이싱하여 예측을 정렬하는 과정을 볼 수 있습니다.</p>
            <div class="code-block">
<pre><code>
class FFN(nn.Module):
    # ... (생략, 이전 코드와 동일)

class HymbaLayer(nn.Module):
    def __init__(self, config: HymbaConfig):
        super().__init__()
        self.norm1 = RMSNorm(config.n_embd)
        self.hybrid_block = HybridHeadBlock(config)
        self.norm2 = RMSNorm(config.n_embd)
        self.ffn = FFN(config)

    def forward(self, x):
        # 잔차 연결과 함께 하이브리드 블록과 FFN을 순차적으로 적용
        x = x + self.hybrid_block(self.norm1(x))
        x = x + self.ffn(self.norm2(x))
        return x

class Hymba(nn.Module):
    def __init__(self, config: HymbaConfig):
        super().__init__()
        self.config = config
        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(config.vocab_size, config.n_embd),
            h = nn.ModuleList([HymbaLayer(config) for _ in range(config.n_layer)]),
            ln_f = RMSNorm(config.n_embd),
        ))
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
        self.transformer.wte.weight = self.lm_head.weight # 가중치 공유
        self.meta_tokens = nn.Parameter(torch.randn(1, config.n_meta_tokens, config.n_embd))

    def forward(self, idx):
        B, T = idx.size()
        tok_emb = self.transformer.wte(idx)
        # 입력 임베딩 앞에 메타 토큰 추가
        meta_emb = self.meta_tokens.expand(B, -1, -1)
        x = torch.cat((meta_emb, tok_emb), dim=1)
        
        for block in self.transformer.h:
            x = block(x)
        
        x = self.transformer.ln_f(x)
        
        # 메타 토큰에 해당하는 출력을 제외하고, 실제 입력 토큰에 대한 예측만 반환
        logits = self.lm_head(x[:, self.config.n_meta_tokens-1:-1, :])
        return logits
</code></pre>
            </div>
        </div>
    </main>
<script>
document.addEventListener('DOMContentLoaded', () => {
    // Navigation active state
    const navLinks = document.querySelectorAll('#nav-links .nav-link');
    const currentPage = window.location.pathname.split('/').pop() || 'index.html';
    navLinks.forEach(link => {
        if (link.getAttribute('href').includes(currentPage)) {
            link.classList.add('active');
        }
    });
});
</script>
</body>
</html>
