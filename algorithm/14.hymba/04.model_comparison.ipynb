{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison: Hymba vs Diff Transformer vs Mamba\n",
    "\n",
    "This notebook compares three state-of-the-art sequence models:\n",
    "1. **Hymba** - Hybrid architecture combining Attention + Mamba with SWA and KV sharing\n",
    "2. **Diff Transformer** - Differential attention mechanism\n",
    "3. **Mamba** - Pure selective state space model\n",
    "\n",
    "All models use the same:\n",
    "- Dataset (TinyShakespeare)\n",
    "- Vocabulary size\n",
    "- Training configuration\n",
    "- Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import math\n",
    "from contextlib import nullcontext\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Models and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all three models\n",
    "from backbone.hymba_v2 import HymbaV2, ModelCfg as HymbaCfg, TrainCfg, build_everything as build_hymba\n",
    "from backbone.diff_transformer import DiffTransformer, ModelCfg as DiffCfg, build_everything as build_diff\n",
    "from backbone.mamba_model import MambaModel, ModelCfg as MambaCfg, build_everything as build_mamba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shared configuration\n",
    "SEQ_LEN = 512\n",
    "BATCH_SIZE = 32\n",
    "VOCAB_SIZE = 6000\n",
    "D_MODEL = 384\n",
    "N_LAYERS = 12\n",
    "N_HEADS = 6\n",
    "N_KV_HEADS = 2\n",
    "\n",
    "# Training configuration\n",
    "STEPS = 500\n",
    "LR = 6e-4\n",
    "WARMUP = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Hymba\n",
    "print(\"Building Hymba...\")\n",
    "hymba_model, tok, train_dl, val_dl = build_hymba(seq_len=SEQ_LEN, bs=BATCH_SIZE, vocab_size=VOCAB_SIZE)\n",
    "hymba_model.to(device)\n",
    "\n",
    "print(f\"\\nHymba parameters: {sum(p.numel() for p in hymba_model.parameters() if p.requires_grad):,}\")\n",
    "display(hymba_model.layer_table())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Diff Transformer\n",
    "print(\"Building Diff Transformer...\")\n",
    "diff_cfg = DiffCfg(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    d_model=D_MODEL,\n",
    "    n_layers=N_LAYERS,\n",
    "    n_heads=N_HEADS,\n",
    "    n_kv_heads=N_KV_HEADS,\n",
    "    seq_len=SEQ_LEN\n",
    ")\n",
    "diff_model = DiffTransformer(diff_cfg).to(device)\n",
    "\n",
    "print(f\"\\nDiff Transformer parameters: {sum(p.numel() for p in diff_model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Mamba\n",
    "print(\"Building Mamba...\")\n",
    "mamba_cfg = MambaCfg(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    d_model=D_MODEL,\n",
    "    n_layers=N_LAYERS,\n",
    "    state_size=16,\n",
    "    seq_len=SEQ_LEN\n",
    ")\n",
    "mamba_model = MambaModel(mamba_cfg).to(device)\n",
    "\n",
    "print(f\"\\nMamba parameters: {sum(p.numel() for p in mamba_model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dl, val_dl, model_name, steps=STEPS):\n",
    "    \"\"\"Train a model and return metrics\"\"\"\n",
    "    from backbone.hymba_v2 import train_loop\n",
    "    \n",
    "    tcfg = TrainCfg(\n",
    "        seq_len=SEQ_LEN,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        steps=steps,\n",
    "        lr=LR,\n",
    "        warmup=WARMUP,\n",
    "        amp=True,\n",
    "        grad_clip=1.0\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    stats = train_loop(model, train_dl, val_dl, tcfg, device=device)\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    stats['model'] = model_name\n",
    "    stats['time_s'] = elapsed\n",
    "    stats['params'] = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_ppl(model, val_dl, amp=True):\n",
    "    \"\"\"Evaluate perplexity on validation set\"\"\"\n",
    "    model.eval()\n",
    "    nll = 0.0\n",
    "    tok = 0\n",
    "    ctx = (torch.amp.autocast(\"cuda\") if (amp and device==\"cuda\") else nullcontext())\n",
    "    \n",
    "    with ctx:\n",
    "        for xb, yb in val_dl:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            out = model(xb, targets=yb)\n",
    "            nll += out[\"loss\"].item() * xb.numel()\n",
    "            tok += xb.numel()\n",
    "    \n",
    "    return math.exp(nll / max(1, tok))\n",
    "\n",
    "@torch.no_grad()\n",
    "def bench_generate(model, prompt_len=512, gen_len=256, warmup=1, repeat=2):\n",
    "    \"\"\"Benchmark generation speed\"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # Use vocab_size from model config\n",
    "    if hasattr(model, 'cfg'):\n",
    "        vocab = model.cfg.vocab_size\n",
    "    else:\n",
    "        vocab = VOCAB_SIZE\n",
    "    \n",
    "    torch.manual_seed(0)\n",
    "    prompt = torch.randint(0, vocab, (1, prompt_len), device=device)\n",
    "\n",
    "    # Warmup\n",
    "    for _ in range(warmup):\n",
    "        _ = model.generate(prompt, max_new_tokens=16)\n",
    "\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    times = []\n",
    "    for _ in range(repeat):\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "        t0 = time.time()\n",
    "        _ = model.generate(prompt, max_new_tokens=gen_len)\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "        times.append(time.time() - t0)\n",
    "\n",
    "    sec = sum(times) / len(times)\n",
    "    tps = int((prompt_len + gen_len) / sec)\n",
    "    mem = 0.0\n",
    "    if device.type == \"cuda\":\n",
    "        mem = torch.cuda.max_memory_allocated() / (1024**2)\n",
    "\n",
    "    return {\n",
    "        \"gen_latency_s\": round(sec, 3),\n",
    "        \"gen_tps\": tps,\n",
    "        \"gen_peak_mb\": round(mem, 2),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "# Train Hymba\n",
    "hymba_stats = train_model(hymba_model, train_dl, val_dl, \"Hymba\")\n",
    "results.append(hymba_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Diff Transformer\n",
    "diff_stats = train_model(diff_model, train_dl, val_dl, \"Diff Transformer\")\n",
    "results.append(diff_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Mamba\n",
    "mamba_stats = train_model(mamba_model, train_dl, val_dl, \"Mamba\")\n",
    "results.append(mamba_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Benchmark Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_results = []\n",
    "\n",
    "for model, name in [(hymba_model, \"Hymba\"), (diff_model, \"Diff Transformer\"), (mamba_model, \"Mamba\")]:\n",
    "    print(f\"\\nBenchmarking {name} generation...\")\n",
    "    bench = bench_generate(model, prompt_len=512, gen_len=256)\n",
    "    bench['model'] = name\n",
    "    gen_results.append(bench)\n",
    "    print(f\"{name}: {bench['gen_tps']} tokens/s, {bench['gen_latency_s']}s latency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training results\n",
    "df_train = pd.DataFrame(results)\n",
    "df_train = df_train[['model', 'train_loss', 'val_loss', 'ppl', 'tps', 'time_s', 'params']]\n",
    "display(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation results\n",
    "df_gen = pd.DataFrame(gen_results)\n",
    "df_gen = df_gen[['model', 'gen_tps', 'gen_latency_s', 'gen_peak_mb']]\n",
    "display(df_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Plot 1: Validation Loss\n",
    "axes[0, 0].bar(df_train['model'], df_train['val_loss'])\n",
    "axes[0, 0].set_title('Validation Loss (Lower is Better)')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].tick_params(axis='x', rotation=15)\n",
    "\n",
    "# Plot 2: Perplexity\n",
    "axes[0, 1].bar(df_train['model'], df_train['ppl'])\n",
    "axes[0, 1].set_title('Perplexity (Lower is Better)')\n",
    "axes[0, 1].set_ylabel('PPL')\n",
    "axes[0, 1].tick_params(axis='x', rotation=15)\n",
    "\n",
    "# Plot 3: Training Throughput\n",
    "axes[0, 2].bar(df_train['model'], df_train['tps'])\n",
    "axes[0, 2].set_title('Training Throughput (Higher is Better)')\n",
    "axes[0, 2].set_ylabel('Tokens/s')\n",
    "axes[0, 2].tick_params(axis='x', rotation=15)\n",
    "\n",
    "# Plot 4: Generation Speed\n",
    "axes[1, 0].bar(df_gen['model'], df_gen['gen_tps'])\n",
    "axes[1, 0].set_title('Generation Speed (Higher is Better)')\n",
    "axes[1, 0].set_ylabel('Tokens/s')\n",
    "axes[1, 0].tick_params(axis='x', rotation=15)\n",
    "\n",
    "# Plot 5: Generation Latency\n",
    "axes[1, 1].bar(df_gen['model'], df_gen['gen_latency_s'])\n",
    "axes[1, 1].set_title('Generation Latency (Lower is Better)')\n",
    "axes[1, 1].set_ylabel('Seconds')\n",
    "axes[1, 1].tick_params(axis='x', rotation=15)\n",
    "\n",
    "# Plot 6: Memory Usage\n",
    "axes[1, 2].bar(df_gen['model'], df_gen['gen_peak_mb'])\n",
    "axes[1, 2].set_title('Peak Memory Usage (Lower is Better)')\n",
    "axes[1, 2].set_ylabel('MB')\n",
    "axes[1, 2].tick_params(axis='x', rotation=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Text Generation Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample text from each model\n",
    "prompt = \"To be or not to be\"\n",
    "prompt_ids = torch.tensor([tok.encode(prompt)], device=device)\n",
    "\n",
    "print(\"Prompt:\", prompt)\n",
    "print(\"=\"*60)\n",
    "\n",
    "for model, name in [(hymba_model, \"Hymba\"), (diff_model, \"Diff Transformer\"), (mamba_model, \"Mamba\")]:\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(\"-\"*60)\n",
    "    generated = model.generate(prompt_ids, max_new_tokens=100, temperature=0.8)\n",
    "    text = tok.decode(generated[0].tolist())\n",
    "    print(text[:200], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Key Findings\n",
    "\n",
    "This comparison evaluates:\n",
    "\n",
    "1. **Training Efficiency**: Tokens/second during training\n",
    "2. **Model Quality**: Validation loss and perplexity\n",
    "3. **Inference Speed**: Generation throughput and latency\n",
    "4. **Memory Efficiency**: Peak memory usage during generation\n",
    "\n",
    "### Expected Results:\n",
    "\n",
    "- **Hymba**: Best balance of quality and efficiency with hybrid architecture\n",
    "- **Diff Transformer**: Strong quality from differential attention but higher compute\n",
    "- **Mamba**: Fast inference with linear complexity but may need more tuning\n",
    "\n",
    "### Architecture Highlights:\n",
    "\n",
    "- **Hymba**: Combines attention (global context) + Mamba (efficiency) with SWA and KV sharing\n",
    "- **Diff Transformer**: Differential attention reduces noise by subtracting two attention patterns\n",
    "- **Mamba**: Selective SSM with input-dependent parameters for efficient long-range modeling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
