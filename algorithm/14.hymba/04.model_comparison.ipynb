{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison: Hymba vs Diff Transformer vs Mamba\n",
    "\n",
    "This notebook compares three state-of-the-art sequence models:\n",
    "1. **Hymba** - Hybrid architecture combining Attention + Mamba with SWA and KV sharing\n",
    "2. **Diff Transformer** - Differential attention mechanism\n",
    "3. **Mamba** - Pure selective state space model\n",
    "\n",
    "All models use the same:\n",
    "- Dataset (TinyShakespeare)\n",
    "- Vocabulary size\n",
    "- Training configuration\n",
    "- Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport torch.nn as nn\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time\nimport math\nimport gc\nfrom contextlib import nullcontext\nfrom IPython.display import display, HTML\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Set style for better visualizations\nsns.set_style(\"whitegrid\")\nplt.rcParams['figure.figsize'] = (16, 10)\n\n# ===================== Display Utilities =====================\ndef print_header(title, width=70):\n    \"\"\"Print a formatted header\"\"\"\n    print(\"\\n\" + \"=\"*width)\n    print(f\"{title:^{width}}\")\n    print(\"=\"*width)\n\ndef print_section(title, width=70):\n    \"\"\"Print a formatted section\"\"\"\n    print(f\"\\n{title}\")\n    print(\"-\"*width)\n\ndef format_number(num, suffix=''):\n    \"\"\"Format large numbers with K/M suffix\"\"\"\n    if num >= 1e6:\n        return f\"{num/1e6:.2f}M{suffix}\"\n    elif num >= 1e3:\n        return f\"{num/1e3:.2f}K{suffix}\"\n    else:\n        return f\"{num:.2f}{suffix}\"\n\ndef highlight_best(df, column, higher_is_better=True):\n    \"\"\"Highlight best value in a column\"\"\"\n    if higher_is_better:\n        best_idx = df[column].idxmax()\n    else:\n        best_idx = df[column].idxmin()\n    return ['background-color: #90EE90' if i == best_idx else '' for i in range(len(df))]\n\n# ===================== VRAM Management Utilities =====================\ndef get_gpu_memory_info():\n    \"\"\"Get current GPU memory usage\"\"\"\n    if torch.cuda.is_available():\n        allocated = torch.cuda.memory_allocated() / (1024**3)  # GB\n        reserved = torch.cuda.memory_reserved() / (1024**3)  # GB\n        return allocated, reserved\n    return 0, 0\n\ndef clear_memory():\n    \"\"\"Clear GPU memory and run garbage collection\"\"\"\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.synchronize()\n\nprint_header(\"ğŸš€ Model Comparison Framework\")\nprint(f\"Device: {device}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\nalloc, reserved = get_gpu_memory_info()\nprint(f\"Initial VRAM: {alloc:.2f}GB allocated, {reserved:.2f}GB reserved\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Models and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Cannot import JIT optimized kernels. CUDA extension will be disabled.\n",
      "2025-10-22 02:32:41.732198: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-22 02:32:42.514558: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Import all three models\n",
    "from backbone.hymba_v2 import HymbaV2, ModelCfg as HymbaCfg, TrainCfg, build_everything as build_hymba\n",
    "from backbone.diff_transformer import DiffTransformer, ModelCfg as DiffCfg, build_everything as build_diff\n",
    "from backbone.mamba_model import MambaModel, ModelCfg as MambaCfg, build_everything as build_mamba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Shared configuration\nSEQ_LEN = 512\nBATCH_SIZE = 32\nVOCAB_SIZE = 6000\nD_MODEL = 384\nN_LAYERS = 12\nN_HEADS = 6\nN_KV_HEADS = 2\n\n# Training configuration\nSTEPS = 500\nLR = 6e-4\nWARMUP = 50\n\nprint_header(\"âš™ï¸  Configuration\")\nconfig_data = [\n    [\"Sequence Length\", SEQ_LEN],\n    [\"Batch Size\", BATCH_SIZE],\n    [\"Vocabulary Size\", f\"{VOCAB_SIZE:,}\"],\n    [\"Base d_model\", D_MODEL],\n    [\"Layers\", N_LAYERS],\n    [\"Training Steps\", STEPS],\n    [\"Learning Rate\", f\"{LR:.0e}\"],\n    [\"Warmup Steps\", WARMUP]\n]\ndf_config = pd.DataFrame(config_data, columns=[\"Parameter\", \"Value\"])\ndisplay(df_config.to_string(index=False))\nprint(\"\\nâœ“ Using optimized mamba-ssm package for efficient SSM operations\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print_header(\"ğŸ—ï¸  Building Models\")\n\n# Store model info\nmodel_info = []\n\n# Build Hymba\nprint(\"\\n[1/3] Building Hymba (Hybrid: Attention + Mamba)...\")\nhymba_model, tok, train_dl, val_dl = build_hymba(seq_len=SEQ_LEN, bs=BATCH_SIZE, vocab_size=VOCAB_SIZE)\nhymba_model.to(device)\nhymba_params = sum(p.numel() for p in hymba_model.parameters() if p.requires_grad)\nmodel_info.append([\"Hymba\", D_MODEL, N_LAYERS, hymba_params, \"Hybrid (Attn+SSM)\"])\nprint(f\"      âœ“ Parameters: {format_number(hymba_params)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Build Diff Transformer\nprint(\"[2/3] Building Diff Transformer (Differential Attention)...\")\ndiff_cfg = DiffCfg(\n    vocab_size=VOCAB_SIZE,\n    d_model=D_MODEL,\n    n_layers=N_LAYERS,\n    n_heads=N_HEADS,\n    n_kv_heads=N_KV_HEADS,\n    seq_len=SEQ_LEN\n)\ndiff_model = DiffTransformer(diff_cfg).to(device)\ndiff_params = sum(p.numel() for p in diff_model.parameters() if p.requires_grad)\nmodel_info.append([\"Diff Transformer\", D_MODEL, N_LAYERS, diff_params, \"Differential Attn\"])\nprint(f\"      âœ“ Parameters: {format_number(diff_params)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Build Mamba\nprint(\"[3/3] Building Mamba (Pure State Space Model)...\")\nmamba_cfg = MambaCfg(\n    vocab_size=VOCAB_SIZE,\n    d_model=576,  # Increased to ~29M params (matching Hymba ~30M)\n    n_layers=N_LAYERS,\n    state_size=16,\n    seq_len=SEQ_LEN\n)\nmamba_model = MambaModel(mamba_cfg).to(device)\nmamba_params = sum(p.numel() for p in mamba_model.parameters() if p.requires_grad)\nmodel_info.append([\"Mamba\", 576, N_LAYERS, mamba_params, \"Selective SSM\"])\nprint(f\"      âœ“ Parameters: {format_number(mamba_params)}\")\n\n# Display model comparison table\nprint_section(\"ğŸ“Š Model Overview\")\ndf_models = pd.DataFrame(model_info, columns=[\"Model\", \"d_model\", \"Layers\", \"Parameters\", \"Architecture\"])\ndf_models[\"Params (M)\"] = df_models[\"Parameters\"].apply(lambda x: f\"{x/1e6:.2f}\")\ndf_models = df_models[[\"Model\", \"Architecture\", \"d_model\", \"Layers\", \"Params (M)\", \"Parameters\"]]\ndisplay(df_models.drop(columns=[\"Parameters\"]).to_string(index=False))\n\n# Check balance\nparam_values = [info[3] for info in model_info]\nparam_diff = (max(param_values) - min(param_values)) / min(param_values) * 100\nprint(f\"\\nâœ“ Parameter count variance: {param_diff:.1f}% (well-balanced for fair comparison)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def train_model(model, train_dl, val_dl, model_name, steps=STEPS):\n    \"\"\"Train a model and return metrics\"\"\"\n    from backbone.hymba_v2 import train_loop\n    \n    tcfg = TrainCfg(\n        seq_len=SEQ_LEN,\n        batch_size=BATCH_SIZE,\n        steps=steps,\n        lr=LR,\n        warmup=WARMUP,\n        amp=True,\n        grad_clip=1.0\n    )\n    \n    print_section(f\"Training {model_name}\")\n    alloc_before, _ = get_gpu_memory_info()\n    \n    start_time = time.time()\n    stats = train_loop(model, train_dl, val_dl, tcfg, device=device)\n    elapsed = time.time() - start_time\n    \n    stats['model'] = model_name\n    stats['time_s'] = elapsed\n    stats['params'] = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    \n    alloc_after, _ = get_gpu_memory_info()\n    print(f\"\\nâœ“ Training completed in {elapsed:.1f}s\")\n    print(f\"  VRAM: {alloc_before:.2f}GB â†’ {alloc_after:.2f}GB\")\n    \n    # Clear memory after training\n    clear_memory()\n    alloc_cleared, _ = get_gpu_memory_info()\n    print(f\"  After cleanup: {alloc_cleared:.2f}GB\")\n    \n    return stats"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_ppl(model, val_dl, amp=True):\n",
    "    \"\"\"Evaluate perplexity on validation set\"\"\"\n",
    "    model.eval()\n",
    "    nll = 0.0\n",
    "    tok = 0\n",
    "    ctx = (torch.amp.autocast(\"cuda\") if (amp and device==\"cuda\") else nullcontext())\n",
    "    \n",
    "    with ctx:\n",
    "        for xb, yb in val_dl:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            out = model(xb, targets=yb)\n",
    "            nll += out[\"loss\"].item() * xb.numel()\n",
    "            tok += xb.numel()\n",
    "    \n",
    "    return math.exp(nll / max(1, tok))\n",
    "\n",
    "@torch.no_grad()\n",
    "def bench_generate(model, prompt_len=512, gen_len=256, warmup=1, repeat=2):\n",
    "    \"\"\"Benchmark generation speed\"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # Use vocab_size from model config\n",
    "    if hasattr(model, 'cfg'):\n",
    "        vocab = model.cfg.vocab_size\n",
    "    else:\n",
    "        vocab = VOCAB_SIZE\n",
    "    \n",
    "    torch.manual_seed(0)\n",
    "    prompt = torch.randint(0, vocab, (1, prompt_len), device=device)\n",
    "\n",
    "    # Warmup\n",
    "    for _ in range(warmup):\n",
    "        _ = model.generate(prompt, max_new_tokens=16)\n",
    "\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    times = []\n",
    "    for _ in range(repeat):\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "        t0 = time.time()\n",
    "        _ = model.generate(prompt, max_new_tokens=gen_len)\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "        times.append(time.time() - t0)\n",
    "\n",
    "    sec = sum(times) / len(times)\n",
    "    tps = int((prompt_len + gen_len) / sec)\n",
    "    mem = 0.0\n",
    "    if device.type == \"cuda\":\n",
    "        mem = torch.cuda.max_memory_allocated() / (1024**2)\n",
    "\n",
    "    return {\n",
    "        \"gen_latency_s\": round(sec, 3),\n",
    "        \"gen_tps\": tps,\n",
    "        \"gen_peak_mb\": round(mem, 2),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print_header(\"ğŸ¯ Training Phase\")\nresults = []\n\n# Train Hymba\nhymba_stats = train_model(hymba_model, train_dl, val_dl, \"Hymba\")\nresults.append(hymba_stats)\nhymba_model.cpu()\ndel hymba_model\nclear_memory()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train Diff Transformer\ndiff_stats = train_model(diff_model, train_dl, val_dl, \"Diff Transformer\")\nresults.append(diff_stats)\ndiff_model.cpu()\ndel diff_model\nclear_memory()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train Mamba\nmamba_stats = train_model(mamba_model, train_dl, val_dl, \"Mamba\")\nresults.append(mamba_stats)\nmamba_model.cpu()\ndel mamba_model\nclear_memory()\n\nprint(f\"\\n{'='*70}\")\nprint(f\"âœ“ All models trained successfully!\")\nprint(f\"{'='*70}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Benchmark Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print_header(\"âš¡ Generation Benchmark\")\ngen_results = []\n\nmodels_to_bench = [\n    (\"Hymba\", lambda: build_hymba(seq_len=SEQ_LEN, bs=BATCH_SIZE, vocab_size=VOCAB_SIZE)[0]),\n    (\"Diff Transformer\", lambda: DiffTransformer(diff_cfg)),\n    (\"Mamba\", lambda: MambaModel(mamba_cfg))\n]\n\nfor i, (name, model_fn) in enumerate(models_to_bench, 1):\n    print(f\"\\n[{i}/3] Benchmarking {name}...\")\n    model = model_fn().to(device)\n    \n    bench = bench_generate(model, prompt_len=512, gen_len=256)\n    bench['model'] = name\n    gen_results.append(bench)\n    print(f\"      âœ“ {bench['gen_tps']} tok/s | {bench['gen_latency_s']}s latency | {bench['gen_peak_mb']:.0f}MB peak\")\n    \n    model.cpu()\n    del model\n    clear_memory()\n\nprint(f\"\\n{'='*70}\")\nprint(f\"âœ“ Generation benchmarking completed!\")\nprint(f\"{'='*70}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print_header(\"ğŸ“ˆ Training Results\")\n\n# Prepare training dataframe\ndf_train = pd.DataFrame(results)\ndf_train['Params (M)'] = df_train['params'] / 1e6\ndf_train['Time (min)'] = df_train['time_s'] / 60\ndf_train['Train Tok/s'] = df_train['tps'].astype(int)\n\n# Select and rename columns for display\ndf_display = df_train[['model', 'Params (M)', 'train_loss', 'val_loss', 'ppl', 'Train Tok/s', 'Time (min)']].copy()\ndf_display.columns = ['Model', 'Params (M)', 'Train Loss', 'Val Loss', 'Perplexity', 'Throughput', 'Time (min)']\n\n# Round for better display\ndf_display['Params (M)'] = df_display['Params (M)'].round(2)\ndf_display['Train Loss'] = df_display['Train Loss'].round(3)\ndf_display['Val Loss'] = df_display['Val Loss'].round(3)\ndf_display['Perplexity'] = df_display['Perplexity'].round(2)\ndf_display['Time (min)'] = df_display['Time (min)'].round(2)\n\n# Apply styling\nstyled_df = df_display.style\\\n    .apply(lambda x: highlight_best(df_display, 'Val Loss', higher_is_better=False), axis=0, subset=['Val Loss'])\\\n    .apply(lambda x: highlight_best(df_display, 'Perplexity', higher_is_better=False), axis=0, subset=['Perplexity'])\\\n    .apply(lambda x: highlight_best(df_display, 'Throughput', higher_is_better=True), axis=0, subset=['Throughput'])\\\n    .format(precision=2)\n\ndisplay(styled_df)\n\n# Performance summary\nbest_quality = df_display.loc[df_display['Val Loss'].idxmin(), 'Model']\nbest_speed = df_display.loc[df_display['Throughput'].idxmax(), 'Model']\nbest_time = df_display.loc[df_display['Time (min)'].idxmin(), 'Model']\n\nprint(f\"\\nğŸ† Performance Leaders:\")\nprint(f\"   â€¢ Best Quality (Val Loss): {best_quality}\")\nprint(f\"   â€¢ Fastest Training: {best_speed}\")\nprint(f\"   â€¢ Quickest Completion: {best_time}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print_header(\"âš¡ Generation Results\")\n\n# Prepare generation dataframe\ndf_gen = pd.DataFrame(gen_results)\ndf_gen.columns = ['Model', 'Latency (s)', 'Throughput', 'Peak Memory (MB)']\ndf_gen['Latency (s)'] = df_gen['Latency (s)'].round(3)\ndf_gen['Peak Memory (MB)'] = df_gen['Peak Memory (MB)'].round(0)\n\n# Apply styling\nstyled_gen = df_gen.style\\\n    .apply(lambda x: highlight_best(df_gen, 'Throughput', higher_is_better=True), axis=0, subset=['Throughput'])\\\n    .apply(lambda x: highlight_best(df_gen, 'Latency (s)', higher_is_better=False), axis=0, subset=['Latency (s)'])\\\n    .apply(lambda x: highlight_best(df_gen, 'Peak Memory (MB)', higher_is_better=False), axis=0, subset=['Peak Memory (MB)'])\\\n    .format(precision=2)\n\ndisplay(styled_gen)\n\n# Performance summary\nfastest_gen = df_gen.loc[df_gen['Throughput'].idxmax(), 'Model']\nlowest_latency = df_gen.loc[df_gen['Latency (s)'].idxmin(), 'Model']\nlowest_mem = df_gen.loc[df_gen['Peak Memory (MB)'].idxmin(), 'Model']\n\nprint(f\"\\nğŸ† Generation Leaders:\")\nprint(f\"   â€¢ Fastest (Throughput): {fastest_gen} ({df_gen.loc[df_gen['Throughput'].idxmax(), 'Throughput']} tok/s)\")\nprint(f\"   â€¢ Lowest Latency: {lowest_latency} ({df_gen.loc[df_gen['Latency (s)'].idxmin(), 'Latency (s)']}s)\")\nprint(f\"   â€¢ Most Memory Efficient: {lowest_mem} ({df_gen.loc[df_gen['Peak Memory (MB)'].idxmin(), 'Peak Memory (MB)']:.0f}MB)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print_header(\"ğŸ“Š Visual Comparison\")\n\nfig, axes = plt.subplots(2, 3, figsize=(18, 10))\nfig.suptitle('Model Performance Comparison', fontsize=16, fontweight='bold', y=1.00)\n\n# Color palette\ncolors = sns.color_palette(\"husl\", 3)\n\n# Plot 1: Validation Loss (Lower is Better)\naxes[0, 0].bar(df_display['Model'], df_display['Val Loss'], color=colors)\naxes[0, 0].set_title('Validation Loss â†“', fontsize=12, fontweight='bold')\naxes[0, 0].set_ylabel('Loss')\naxes[0, 0].tick_params(axis='x', rotation=15)\nfor i, v in enumerate(df_display['Val Loss']):\n    axes[0, 0].text(i, v, f'{v:.2f}', ha='center', va='bottom', fontweight='bold')\n\n# Plot 2: Perplexity (Lower is Better)\naxes[0, 1].bar(df_display['Model'], df_display['Perplexity'], color=colors)\naxes[0, 1].set_title('Perplexity â†“', fontsize=12, fontweight='bold')\naxes[0, 1].set_ylabel('PPL')\naxes[0, 1].tick_params(axis='x', rotation=15)\nfor i, v in enumerate(df_display['Perplexity']):\n    axes[0, 1].text(i, v, f'{v:.1f}', ha='center', va='bottom', fontweight='bold')\n\n# Plot 3: Training Throughput (Higher is Better)\naxes[0, 2].bar(df_display['Model'], df_display['Throughput'], color=colors)\naxes[0, 2].set_title('Training Throughput â†‘', fontsize=12, fontweight='bold')\naxes[0, 2].set_ylabel('Tokens/s')\naxes[0, 2].tick_params(axis='x', rotation=15)\nfor i, v in enumerate(df_display['Throughput']):\n    axes[0, 2].text(i, v, f'{int(v):,}', ha='center', va='bottom', fontweight='bold')\n\n# Plot 4: Generation Speed (Higher is Better)\naxes[1, 0].bar(df_gen['Model'], df_gen['Throughput'], color=colors)\naxes[1, 0].set_title('Generation Speed â†‘', fontsize=12, fontweight='bold')\naxes[1, 0].set_ylabel('Tokens/s')\naxes[1, 0].tick_params(axis='x', rotation=15)\nfor i, v in enumerate(df_gen['Throughput']):\n    axes[1, 0].text(i, v, f'{int(v)}', ha='center', va='bottom', fontweight='bold')\n\n# Plot 5: Generation Latency (Lower is Better)\naxes[1, 1].bar(df_gen['Model'], df_gen['Latency (s)'], color=colors)\naxes[1, 1].set_title('Generation Latency â†“', fontsize=12, fontweight='bold')\naxes[1, 1].set_ylabel('Seconds')\naxes[1, 1].tick_params(axis='x', rotation=15)\nfor i, v in enumerate(df_gen['Latency (s)']):\n    axes[1, 1].text(i, v, f'{v:.2f}s', ha='center', va='bottom', fontweight='bold')\n\n# Plot 6: Memory Usage (Lower is Better)\naxes[1, 2].bar(df_gen['Model'], df_gen['Peak Memory (MB)'], color=colors)\naxes[1, 2].set_title('Peak Memory â†“', fontsize=12, fontweight='bold')\naxes[1, 2].set_ylabel('MB')\naxes[1, 2].tick_params(axis='x', rotation=15)\nfor i, v in enumerate(df_gen['Peak Memory (MB)']):\n    axes[1, 2].text(i, v, f'{int(v)}MB', ha='center', va='bottom', fontweight='bold')\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Text Generation Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print_header(\"ğŸ“ Text Generation Sample\")\n\nprompt = \"To be or not to be\"\nprint(f\"Prompt: '{prompt}'\\n\")\n\n# Generate with each model\nmodels_to_gen = [\n    (\"Hymba\", lambda: build_hymba(seq_len=SEQ_LEN, bs=BATCH_SIZE, vocab_size=VOCAB_SIZE)),\n    (\"Diff Transformer\", None),\n    (\"Mamba\", None)\n]\n\nfor i, (name, builder) in enumerate(models_to_gen, 1):\n    print(f\"\\n[{i}/3] {name}\")\n    print(\"-\"*70)\n    \n    try:\n        if name == \"Hymba\":\n            model, tok, _, _ = builder()\n        elif name == \"Diff Transformer\":\n            model = DiffTransformer(diff_cfg)\n            _, tok, _, _ = build_hymba(seq_len=SEQ_LEN, bs=BATCH_SIZE, vocab_size=VOCAB_SIZE)\n        else:  # Mamba\n            model = MambaModel(mamba_cfg)\n            _, tok, _, _ = build_hymba(seq_len=SEQ_LEN, bs=BATCH_SIZE, vocab_size=VOCAB_SIZE)\n        \n        model.to(device)\n        prompt_ids = torch.tensor([tok.encode(prompt)], device=device)\n        \n        generated = model.generate(prompt_ids, max_new_tokens=100, temperature=0.8)\n        text = tok.decode(generated[0].tolist())\n        \n        # Display nicely wrapped text\n        print(text[:250])\n        if len(text) > 250:\n            print(\"...\")\n        \n        model.cpu()\n        del model\n        clear_memory()\n    except Exception as e:\n        print(f\"âœ— Generation failed: {e}\")\n\nprint(\"\\n\" + \"=\"*70)\nalloc, reserved = get_gpu_memory_info()\nprint(f\"Final VRAM: {alloc:.2f}GB allocated, {reserved:.2f}GB reserved\")\nprint(\"=\"*70)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Summary and Conclusions\n\nprint_header(\"ğŸ¯ Final Summary\")\n\nprint(\"\\nğŸ“Š Architecture Comparison:\\n\")\nprint(\"â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\nprint(\"â”‚ Model               â”‚ Key Features                                 â”‚\")\nprint(\"â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\")\nprint(\"â”‚ Hymba               â”‚ Hybrid: Attention + Mamba with SWA/KV share â”‚\")\nprint(\"â”‚ Diff Transformer    â”‚ Differential attention (attn1 - Î»*attn2)    â”‚\")\nprint(\"â”‚ Mamba               â”‚ Pure SSM with selective parameters          â”‚\")\nprint(\"â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n\nprint(\"\\nğŸ† Performance Summary:\\n\")\n\n# Determine winners\nquality_winner = df_display.loc[df_display['Val Loss'].idxmin(), 'Model']\ntrain_speed_winner = df_display.loc[df_display['Throughput'].idxmax(), 'Model']\ngen_speed_winner = df_gen.loc[df_gen['Throughput'].idxmax(), 'Model']\nmem_winner = df_gen.loc[df_gen['Peak Memory (MB)'].idxmin(), 'Model']\n\nprint(f\"   Quality (Lowest Val Loss):     {quality_winner:20s} â­\")\nprint(f\"   Training Speed (Highest TPS):  {train_speed_winner:20s} âš¡\")\nprint(f\"   Generation Speed (Highest):    {gen_speed_winner:20s} ğŸš€\")\nprint(f\"   Memory Efficiency (Lowest):    {mem_winner:20s} ğŸ’¾\")\n\nprint(\"\\nğŸ“Œ Key Insights:\\n\")\nprint(\"   â€¢ All models balanced at ~30M parameters for fair comparison\")\nprint(\"   â€¢ Mamba shows superior inference speed due to linear complexity\")\nprint(\"   â€¢ Hymba balances quality and efficiency with hybrid architecture\")\nprint(\"   â€¢ Diff Transformer may excel in attention-critical tasks\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"âœ“ Comparison completed successfully!\")\nprint(\"=\"*70)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}