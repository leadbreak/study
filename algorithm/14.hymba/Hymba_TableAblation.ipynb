{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {},
  "cells": [
    {
      "id": "eec48f5d",
      "cell_type": "markdown",
      "source": "\n# Hymba Ablation Notebook (Table-Style Replication)\nÏ≤®Î∂Ä ÌëúÏùò **ÎÖºÎ¶¨ ÌùêÎ¶Ñ(1‚Üí13)**Ïóê ÎßûÏ∂∞, ÌïòÎÇòÏùò **ÏôÑÏÑ±Îêú Hymba Î™®Îç∏**ÏùÑ Ï†ïÏùòÌïòÍ≥† ÏÑ§Ï†ïÎßå Î∞îÍøîÍ∞ÄÎ©∞\nÏûëÏùÄ Îç∞Ïù¥ÌÑ∞ÏÖãÏóê **ÏßÅÏ†ë ÌïôÏäµ¬∑ÌèâÍ∞Ä**Ìï† Ïàò ÏûàÎèÑÎ°ù Íµ¨ÏÑ±ÌñàÏäµÎãàÎã§.\n\n**ÌïµÏã¨ ÌäπÏßï**\n- ÌòÑÎåÄÏ†Å **Unigram(SentencePiece Í≥ÑÏó¥)** ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†ÄÎ•º **ÏßÅÏ†ë ÌïôÏäµ**\n- Îç∞Ïù¥ÌÑ∞ÏÖã Í≤ΩÎ°úÍ∞Ä ÏóÜÏúºÎ©¥ ÏûêÎèôÏúºÎ°ú ü§ó `datasets`ÏóêÏÑú Îã§Ïö¥Î°úÎìú (torchtext Î∂àÏÇ¨Ïö©)\n- Î™®Îç∏: ÌïòÏù¥Î∏åÎ¶¨Îìú(Attention+Mamba), **SWA**, **Cross-layer KV sharing**, **Meta Tokens**, **GQA on/off**, **Fusion(Mean/Concat)**\n- ÌõàÎ†®: **AdamW + Cosine with Warmup**, **AMP**, **Grad Clip**\n- Í≤∞Í≥º: **pandas DataFrame**ÏúºÎ°ú **Throughput(tokens/s)**, **PPL**, **ÌÜ†ÌÅ∞ Ï†ïÌôïÎèÑ**, **KV‚Äëcache(MB)**, **Attn:Mamba ÎπÑÏú®**ÏùÑ Î≥¥Í∏∞ Ï¢ãÍ≤å ÏöîÏïΩ\n",
      "metadata": {}
    },
    {
      "id": "1104ba02",
      "cell_type": "markdown",
      "source": "## 0) ÏùòÏ°¥ÏÑ± ÏÑ§Ïπò (ÌååÏù¥Ïç¨ ÏΩîÎìúÎ°ú Ïã§Ìñâ)",
      "metadata": {}
    },
    {
      "id": "97d3e389",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "import sys, subprocess, pkg_resources\ndef pip_install(pkgs):\n    for p in pkgs:\n        try:\n            pkg_resources.get_distribution(p.split('==')[0].split('>=')[0])\n        except Exception:\n            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", p])\npip_install([\n    \"torch>=2.2.0\",\n    \"datasets>=2.18.0\",\n    \"tokenizers>=0.15.2\",\n    \"accelerate>=0.32.0\",\n    \"mamba-ssm>=2.2.2\",\n    \"pandas>=2.1.0\",\n    \"matplotlib>=3.8.0\"\n])\nimport os, math, time, dataclasses, typing as T, numpy as np, pandas as pd\nimport torch, torch.nn as nn, torch.nn.functional as F\nfrom torch.utils.data import DataLoader, TensorDataset, random_split\nfrom datasets import load_dataset\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice\n",
      "outputs": []
    },
    {
      "id": "3898187e",
      "cell_type": "markdown",
      "source": "\n## 1) Ïã§Ìóò ÏÑ§Ï†ï\n- A100 80GB Í∏∞Ï§Ä Í∏∞Î≥∏Í∞í. Îç∞Î™® Î™©Ï†ÅÏúºÎ°ú `max_steps`Îäî ÏûëÍ≤å ÎëêÏóàÏäµÎãàÎã§(ÌïÑÏöî Ïãú ÌÇ§Ïö∞ÏÑ∏Ïöî).\n",
      "metadata": {}
    },
    {
      "id": "45a8710d",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "@dataclasses.dataclass\nclass CFG:\n    dataset: str = \"karpathy/tiny_shakespeare\"  # Î°úÏª¨ Í≤ΩÎ°ú or HF hub Ïù¥Î¶Ñ\n    seq_len: int = 512\n    batch_size: int = 16\n    epochs: int = 1\n    max_steps: int = 150\n    lr: float = 3e-4\n    weight_decay: float = 0.1\n    warmup_steps: int = 40\n    grad_accum: int = 1\n    grad_clip: float = 1.0\n    amp: bool = True\n    # tokenizer\n    vocab_size: int = 32000\n    bos_token: str = \"<|bos|>\"\n    eos_token: str = \"<|eos|>\"\n    pad_token: str = \"<|pad|>\"\n    # model base\n    d_model: int = 512\n    n_layers: int = 12\n    n_heads: int = 8\n    n_kv_heads: int = 4\n    # SWA\n    swa_window: int = 256\n    # train/val split\n    val_ratio: float = 0.05\n\ncfg = CFG(); cfg\n",
      "outputs": []
    },
    {
      "id": "018023e6",
      "cell_type": "markdown",
      "source": "\n## 2) Îç∞Ïù¥ÌÑ∞ & ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä\n- Í≤ΩÎ°úÍ∞Ä ÏóÜÏúºÎ©¥ ü§ó `datasets`ÏóêÏÑú ÏûêÎèô Îã§Ïö¥Î°úÎìú\n- ÏµúÏã† **Unigram** ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†ÄÎ•º **ÏßÅÏ†ë ÌïôÏäµ**(BOS/EOS/PAD Ìè¨Ìï®)\n",
      "metadata": {}
    },
    {
      "id": "d7b0a65f",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "from tokenizers import Tokenizer\nfrom tokenizers.models import Unigram\nfrom tokenizers.trainers import UnigramTrainer\nfrom tokenizers.pre_tokenizers import Whitespace\nfrom tokenizers.normalizers import NFKC, Lowercase, Sequence as NormSeq\nfrom tokenizers.processors import TemplateProcessing\n\ndef read_local_text(path: str):\n    if not os.path.exists(path):\n        return None\n    if os.path.isdir(path):\n        texts=[]\n        for root,_,files in os.walk(path):\n            for f in files:\n                if f.endswith(\".txt\"):\n                    with open(os.path.join(root,f),\"r\",encoding=\"utf-8\",errors=\"ignore\") as fh:\n                        texts.append(fh.read())\n        return \"\\n\\n\".join(texts) if texts else None\n    else:\n        with open(path,\"r\",encoding=\"utf-8\",errors=\"ignore\") as fh:\n            return fh.read()\n\ndef get_corpus(spec:str)->str:\n    text = read_local_text(spec)\n    if text is not None:\n        return text\n    ds = load_dataset(spec)\n    # heuristic column choice\n    for k in [\"text\",\"content\",\"document\",\"raw\",\"data\"]:\n        if k in ds[\"train\"].column_names:\n            return \"\\n\\n\".join(ds[\"train\"][k])\n    return \"\\n\\n\".join(map(str, ds[\"train\"][:1000]))\n\ndef train_unigram_tokenizer(corpus_text:str, vocab_size:int, bos:str, eos:str, pad:str):\n    def batch_iter(text, bs=1000000):\n        for i in range(0, len(text), bs):\n            yield text[i:i+bs]\n    tok = Tokenizer(Unigram())\n    tok.normalizer = NormSeq([NFKC(), Lowercase()])\n    tok.pre_tokenizer = Whitespace()\n    trainer = UnigramTrainer(vocab_size=vocab_size, special_tokens=[pad,bos,eos])\n    tok.train_from_iterator(batch_iter(corpus_text), trainer=trainer)\n    pad_id = tok.token_to_id(pad); bos_id = tok.token_to_id(bos); eos_id = tok.token_to_id(eos)\n    tok.post_processor = TemplateProcessing(\n        single=f\"{bos} $A {eos}\",\n        pair=f\"{bos} $A {eos} $B:1 {eos}:1\",\n        special_tokens=[(bos, bos_id), (eos, eos_id)]\n    )\n    class ModernTok:\n        def __init__(self, tk, bos_id, eos_id, pad_id):\n            self.tk=tk; self.bos_token_id=bos_id; self.eos_token_id=eos_id; self.pad_token_id=pad_id\n        def encode(self, s): return self.tk.encode(s).ids\n        def decode(self, ids): return self.tk.decode(ids)\n        @property\n        def vocab_size(self): return self.tk.get_vocab_size()\n    return ModernTok(tok, bos_id, eos_id, pad_id)\n\ncorpus = get_corpus(cfg.dataset)\ntok = train_unigram_tokenizer(corpus, cfg.vocab_size, cfg.bos_token, cfg.eos_token, cfg.pad_token)\nVOCAB = tok.vocab_size; EOS_ID = tok.eos_token_id\nVOCAB, EOS_ID\n",
      "outputs": []
    },
    {
      "id": "7ddc2dde",
      "cell_type": "markdown",
      "source": "\n### 2-1) ÌÜ†ÌÅ∞Ìôî & Ï≤≠ÌÅ¨ ‚Üí Train/Val Split\n",
      "metadata": {}
    },
    {
      "id": "aa6cffb4",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "def chunk_tokens(text:str, seq_len:int, tok=tok, eos_id=EOS_ID):\n    ids = tok.encode(text)\n    n = (len(ids)//seq_len)*seq_len; ids = ids[:n]\n    arr = np.array(ids, dtype=np.int64).reshape(-1, seq_len)\n    y = np.copy(arr); y[:, :-1] = arr[:, 1:]; y[:, -1] = eos_id\n    X = torch.tensor(arr); Y = torch.tensor(y)\n    return TensorDataset(X, Y)\n\nfull_ds = chunk_tokens(corpus, cfg.seq_len, tok, EOS_ID)\nval_len = max(1, int(len(full_ds)*cfg.val_ratio))\ntrain_len = len(full_ds) - val_len\ntrain_ds, val_ds = random_split(full_ds, [train_len, val_len])\ntrain_dl = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True, drop_last=True)\nval_dl   = DataLoader(val_ds, batch_size=cfg.batch_size, shuffle=False, drop_last=False)\nlen(train_ds), len(val_ds)\n",
      "outputs": []
    },
    {
      "id": "e01be458",
      "cell_type": "markdown",
      "source": "\n## 3) Î™®Îç∏: ÌïòÎÇòÏùò Íµ¨ÌòÑÏúºÎ°ú Î™®Îì† ÏÑ§Ï†ïÏùÑ ÌÜ†Í∏Ä\n- **Attn:Mamba Ï∞®Ïõê ÎπÑÏú®**(Param Ratio)Î°ú Í≤ΩÎ°ú Î∂ÑÌï†\n- **GQA on/off** (`n_kv_heads < n_heads`Î©¥ GQA)\n- **SWA Î†àÏù¥Ïñ¥ ÏßëÌï©**(Ï†ÑÎ∂Ä/ÏùºÎ∂Ä/ÏóÜÏùå), **Cross-layer KV sharing**\n- **Fusion**: `mean`(Í∏∞Î≥∏) vs `concat`\n- **Meta Tokens**: on/off\n",
      "metadata": {}
    },
    {
      "id": "ea02c606",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "class RotaryEmbedding(nn.Module):\n    def __init__(self, dim, max_position=131072, base=10000.0):\n        super().__init__()\n        inv = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n        t = torch.arange(max_position, dtype=torch.float32)\n        freqs = torch.einsum(\"i,j->ij\", t, inv)\n        self.register_buffer(\"cos_cached\", torch.cos(freqs), persistent=False)\n        self.register_buffer(\"sin_cached\", torch.sin(freqs), persistent=False)\n    def forward(self, x, positions):\n        cos = self.cos_cached[positions][:, None, None, :]\n        sin = self.sin_cached[positions][:, None, None, :]\n        x1 = x[..., ::2]; x2 = x[..., 1::2]\n        return torch.stack([x1*cos - x2*sin, x1*sin + x2*cos], dim=-1).flatten(-2)\n\nclass RMSNorm(nn.Module):\n    def __init__(self, d, eps=1e-6): super().__init__(); self.eps=eps; self.w=nn.Parameter(torch.ones(d))\n    def forward(self, x): return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) * self.w\n\nclass SwiGLU_FFN(nn.Module):\n    def __init__(self, d, mult=4.0, dropout=0.0):\n        super().__init__()\n        h=int(d*mult); self.w1=nn.Linear(d,h, bias=False); self.w2=nn.Linear(d,h, bias=False)\n        self.w3=nn.Linear(h,d, bias=False); self.drop=nn.Dropout(dropout)\n    def forward(self, x): return self.w3(self.drop(F.silu(self.w1(x)) * self.w2(x)))\n\ndef band_mask(T, w, device, dtype):\n    i = torch.arange(T, device=device); j = torch.arange(T, device=device)\n    m = (j[None,:] <= i[:,None]) & (j[None,:] >= (i[:,None]-w+1))\n    M = torch.zeros((T,T), device=device, dtype=dtype)\n    return M.masked_fill(~m, float(\"-inf\"))\n\nclass GQA(nn.Module):\n    def __init__(self, d_model, H, KV, rope=None, use_swa=False, swa_window=4096, dropout=0.0):\n        super().__init__(); assert (H==0) or (d_model%max(1,H)==0)\n        self.enabled = (d_model>0 and H>0)\n        self.H=H; self.KV=KV if KV>0 else H\n        self.rep=max(1,self.H//self.KV); self.Dh = (d_model//max(1,self.H)) if self.enabled else 1\n        self.q=nn.Linear(d_model, self.H*self.Dh, bias=False) if self.enabled else None\n        self.k=nn.Linear(d_model, self.KV*self.Dh, bias=False) if self.enabled else None\n        self.v=nn.Linear(d_model, self.KV*self.Dh, bias=False) if self.enabled else None\n        self.o=nn.Linear(self.H*self.Dh, d_model, bias=False) if self.enabled else None\n        self.drop=nn.Dropout(dropout); self.rope=rope; self.use_swa=use_swa; self.swa_window=swa_window\n    def forward(self, x, kv_cache=None, global_mask=None):\n        if not self.enabled: \n            return torch.zeros_like(x), None\n        B,T,C=x.shape\n        q=self.q(x).view(B,T,self.H,self.Dh).transpose(1,2)\n        k=self.k(x).view(B,T,self.KV,self.Dh).transpose(1,2)\n        v=self.v(x).view(B,T,self.KV,self.Dh).transpose(1,2)\n        if kv_cache is not None and kv_cache[0] is not None and kv_cache[0].numel()>0:\n            pk,pv=kv_cache; k=torch.cat([pk,k],2); v=torch.cat([pv,v],2)\n        if self.rope is not None:\n            pos_q=torch.arange(k.size(2)-T, k.size(2), device=x.device)\n            pos_k=torch.arange(0, k.size(2), device=x.device)\n            q=self.rope(q, pos_q); k_exp=k.repeat_interleave(self.rep,1); k_exp=self.rope(k_exp, pos_k)\n        else:\n            k_exp=k.repeat_interleave(self.rep,1)\n        v_exp=v.repeat_interleave(self.rep,1)\n        Tc=k_exp.size(2)\n        if self.use_swa:\n            M = band_mask(Tc, self.swa_window, x.device, q.dtype)[:, -T:]\n        else:\n            i = torch.arange(Tc, device=x.device); j = torch.arange(Tc, device=x.device)\n            causal = (j[None,:] <= i[:,None]); causal = causal[:, -T:]\n            M = torch.zeros((T,Tc), device=x.device, dtype=q.dtype).masked_fill(~causal, float(\"-inf\"))\n        if global_mask is not None:\n            full = torch.zeros_like(M)\n            M = torch.where(global_mask[:, :, None], full[None,:,:], M[None,:,:]).squeeze(0)\n        q_=q.reshape(B*self.H,T,self.Dh); k_=k_exp.reshape(B*self.H,Tc,self.Dh); v_=v_exp.reshape(B*self.H,Tc,self.Dh)\n        M_=M.unsqueeze(0).expand(B*self.H, -1, -1)\n        out = F.scaled_dot_product_attention(q_, k_, v_, attn_mask=M_, is_causal=False,\n                                             dropout_p=float(self.drop.p) if self.training else 0.0)\n        out=out.view(B,self.H,T,self.Dh).transpose(1,2).reshape(B,T,self.H*self.Dh)\n        out=self.o(out)\n        new_cache=(k.detach(), v.detach())\n        return out, new_cache\n\ntry:\n    from mamba_ssm import Mamba as _Mamba\nexcept Exception:\n    _Mamba=None\nclass MambaLayer(nn.Module):\n    def __init__(self, d_model, d_state=16, d_conv=4, expand=2):\n        super().__init__(); self.enabled=(d_model>0); \n        self.inner = (_Mamba(d_model=d_model, d_state=d_state, d_conv=d_conv, expand=expand) if (_Mamba and self.enabled) else nn.Identity())\n    def forward(self, x): return self.inner(x)\n\nclass HymbaBlock(nn.Module):\n    def __init__(self, d_model, attn_dim, attn_heads, kv_heads, use_swa, swa_window, rope,\n                 mamba_dim, ffn_mult=4.0, dropout=0.0, fusion:str=\"mean\"):\n        super().__init__()\n        self.attn_dim=attn_dim; self.mamba_dim=mamba_dim; self.fusion=fusion\n        self.pre=RMSNorm(d_model)\n        self.to_a=nn.Linear(d_model, attn_dim, bias=False) if attn_dim>0 else None\n        self.to_m=nn.Linear(d_model, mamba_dim, bias=False) if mamba_dim>0 else None\n        self.attn=GQA(attn_dim, attn_heads, kv_heads, rope=rope, use_swa=use_swa, swa_window=swa_window, dropout=dropout) if attn_dim>0 else None\n        self.mamba=MambaLayer(mamba_dim) if mamba_dim>0 else None\n        if fusion==\"concat\":\n            self.mix = nn.Linear(attn_dim+mamba_dim, d_model, bias=False)\n            self.mix2= None\n        else:  # mean\n            self.mix = nn.Linear(attn_dim, d_model, bias=False) if attn_dim>0 else None\n            self.mix2= nn.Linear(mamba_dim, d_model, bias=False) if mamba_dim>0 else None\n        self.ffn=SwiGLU_FFN(d_model, mult=ffn_mult, dropout=dropout); self.drop=nn.Dropout(dropout)\n\n    def forward(self, x, kv_cache=None, global_mask=None):\n        h=self.pre(x); outs=[]; new_cache=None\n        if self.attn is not None:\n            a_in=self.to_a(h); a, new_cache = self.attn(a_in, kv_cache=kv_cache, global_mask=global_mask)\n            outs.append(a)\n        if self.mamba is not None:\n            m_in=self.to_m(h); m=self.mamba(m_in); outs.append(m)\n        if len(outs)==2:\n            if self.fusion==\"concat\":\n                y=self.mix(torch.cat(outs, -1))\n            else:\n                y = 0\n                if self.attn is not None: y = y + self.mix(outs[0])\n                if self.mamba is not None: y = y + self.mix2(outs[1])\n                y = y / ((1 if self.attn is not None else 0) + (1 if self.mamba is not None else 0))\n        else:\n            if self.fusion==\"concat\":\n                y=self.mix(outs[0])\n            else:\n                y = self.mix(outs[0]) if (self.attn is not None) else self.mix2(outs[0])\n        x = x + self.drop(y)\n        x = x + self.drop(self.ffn(self.pre(x)))\n        return x, new_cache\n\nclass Hymba(nn.Module):\n    def __init__(self, vocab_size:int, d_model:int, n_layers:int, n_heads:int, n_kv_heads:int,\n                 attn_dim:int, swa_layers:set, swa_window:int, mamba_dim:int, fusion:str=\"mean\",\n                 num_meta_tokens:int=0, kv_share:bool=True, max_position:int=65536, ffn_mult:float=4.0, dropout:float=0.0):\n        super().__init__()\n        self.vocab_size=vocab_size; self.d_model=d_model; self.n_layers=n_layers\n        self.num_meta_tokens=num_meta_tokens; self.kv_share=kv_share\n        self.tok=nn.Embedding(vocab_size, d_model)\n        self.rope=RotaryEmbedding(dim=(attn_dim//max(1,n_heads)), max_position=max_position) if attn_dim>0 else None\n        self.swa_layers=swa_layers\n        self.blocks=nn.ModuleList([\n            HymbaBlock(d_model, attn_dim, n_heads, n_kv_heads, (i in swa_layers), swa_window, self.rope, mamba_dim,\n                      ffn_mult=ffn_mult, dropout=dropout, fusion=fusion)\n            for i in range(n_layers)\n        ])\n        self.norm=RMSNorm(d_model); self.head=nn.Linear(d_model, vocab_size, bias=False)\n        self.meta = nn.Parameter(torch.randn(1, num_meta_tokens, d_model)*0.02) if num_meta_tokens>0 else None\n        # KV owner mapping for cross-layer sharing\n        self.owner=list(range(n_layers))\n        if kv_share:\n            for a in range(0,n_layers-1,2): self.owner[a+1]=a\n\n    def forward(self, idx, targets=None, kv_caches=None):\n        B,T=idx.shape; x=self.tok(idx); meta_add=0\n        if self.meta is not None and T>1:\n            x=torch.cat([self.meta.expand(B,-1,-1), x], 1); meta_add=self.num_meta_tokens\n        gmask=None\n        if meta_add>0:\n            gm=torch.zeros((B,x.size(1)), dtype=torch.bool, device=x.device); gm[:,:meta_add]=True; gmask=gm[:, -x.size(1):]\n        new=[None]*self.n_layers if kv_caches is not None else None\n        h=x\n        for li,blk in enumerate(self.blocks):\n            kv_in = kv_caches[self.owner[li]] if (kv_caches is not None) else None\n            h, kv_out = blk(h, kv_cache=kv_in, global_mask=gmask)\n            if new is not None and li==self.owner[li]: new[self.owner[li]]=kv_out\n        h=self.norm(h); logits=self.head(h)\n        loss=None\n        if targets is not None:\n            s=meta_add; lf=logits[:, s:s+targets.size(1), :]\n            loss = F.cross_entropy(lf.reshape(-1, lf.size(-1)), targets.reshape(-1))\n        return {\"logits\":logits,\"loss\":loss,\"kv_caches\":new}\n\n    def estimate_kv_cache_bytes(self, seq_len:int, attn_dim:int, n_kv_heads:int, n_heads:int, dtype=torch.float16):\n        if attn_dim<=0 or n_heads<=0: return 0\n        Dh = (attn_dim//n_heads)\n        KV = n_kv_heads if n_kv_heads>0 else n_heads\n        bytes_per = torch.finfo(dtype).bits//8\n        per_owner = 2 * KV * seq_len * Dh * bytes_per  # K & V\n        owners = len(set(self.owner))\n        return per_owner * owners\n\ndef dims_from_ratio(d_model:int, ratio_attn:float, ratio_mamba:float):\n    if ratio_attn==0 and ratio_mamba>0:\n        return 0, d_model\n    if ratio_mamba==0 and ratio_attn>0:\n        return d_model, 0\n    frac = ratio_attn / (ratio_attn + ratio_mamba)\n    attn_dim = int(round(d_model * frac))\n    attn_dim = max(0, min(d_model, attn_dim))\n    mamba_dim = d_model - attn_dim\n    return attn_dim, mamba_dim\n",
      "outputs": []
    },
    {
      "id": "04725c48",
      "cell_type": "markdown",
      "source": "\n## 4) ÌõàÎ†®/ÌèâÍ∞Ä Ïú†Ìã∏(AMP, Cosine+Warmup, ÌÜ†ÌÅ∞ Ï†ïÌôïÎèÑ, Throughput)\n",
      "metadata": {}
    },
    {
      "id": "f3a8ab37",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "from transformers import get_cosine_schedule_with_warmup\n\ndef build_model_variant(ratio_attn, ratio_mamba, n_heads, n_kv_heads, swa_mode:str, fusion:str,\n                        use_meta:bool, kv_share:bool):\n    attn_dim, mamba_dim = dims_from_ratio(cfg.d_model, ratio_attn, ratio_mamba)\n    # SWA layer set\n    if swa_mode==\"none\":\n        swa_layers=set()\n    elif swa_mode==\"all\":\n        swa_layers=set(range(cfg.n_layers))\n    elif swa_mode==\"default\":  # first/middle/last are global; others SWA\n        mid=cfg.n_layers//2\n        swa_layers=set([i for i in range(cfg.n_layers) if i not in (0,mid,cfg.n_layers-1)])\n    else:\n        swa_layers=set()\n    model = Hymba(\n        vocab_size=VOCAB, d_model=cfg.d_model, n_layers=cfg.n_layers, n_heads=n_heads, n_kv_heads=n_kv_heads,\n        attn_dim=attn_dim, swa_layers=swa_layers, swa_window=cfg.swa_window, mamba_dim=mamba_dim, fusion=fusion,\n        num_meta_tokens=(cfg.num_meta_tokens if use_meta else 0), kv_share=kv_share, max_position=cfg.max_position,\n        ffn_mult=cfg.ffn_mult if hasattr(cfg, \"ffn_mult\") else 4.0, dropout=0.0\n    ).to(device)\n    return model, attn_dim, mamba_dim\n\ndef tokens_accuracy(logits, targets):\n    pred = logits.argmax(-1)\n    correct = (pred == targets).float()\n    return correct.mean().item()\n\ndef train_one(model:nn.Module, train_dl, val_dl, attn_dim, n_kv_heads, n_heads):\n    opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay, betas=(0.9,0.95), eps=1e-8)\n    total_steps = cfg.max_steps\n    sch = get_cosine_schedule_with_warmup(opt, num_warmup_steps=cfg.warmup_steps, num_training_steps=total_steps)\n    scaler = torch.cuda.amp.GradScaler(enabled=cfg.amp)\n    step=0; t0=time.time(); tokens=0\n    for ep in range(cfg.epochs):\n        for xb,yb in train_dl:\n            xb, yb = xb.to(device), yb.to(device)\n            with torch.cuda.amp.autocast(enabled=cfg.amp):\n                out = model(xb, targets=yb); loss = out[\"loss\"]\n            scaler.scale(loss).backward()\n            if ((step+1)%cfg.grad_accum)==0:\n                scaler.unscale_(opt)\n                if cfg.grad_clip>0: nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n                scaler.step(opt); scaler.update(); opt.zero_grad(set_to_none=True); sch.step()\n            step+=1; tokens+=xb.numel()\n            if step>=total_steps: break\n        if step>=total_steps: break\n    elapsed = time.time()-t0\n    tps = tokens/max(1e-9, elapsed)\n\n    # Evaluate\n    model.eval(); nll=0.0; n_tok=0; accs=[]; \n    with torch.no_grad(), torch.cuda.amp.autocast(enabled=cfg.amp):\n        for xb,yb in val_dl:\n            xb, yb = xb.to(device), yb.to(device)\n            out = model(xb, targets=yb)\n            loss = out[\"loss\"]\n            n = xb.numel(); nll += loss.item()*n; n_tok += n\n            accs.append(tokens_accuracy(out[\"logits\"][:, :yb.size(1), :], yb))\n    ppl = math.exp(nll/max(1,n_tok)); acc = float(np.mean(accs))\n    cache_mb = model.estimate_kv_cache_bytes(cfg.seq_len, attn_dim, n_kv_heads, n_heads)/2**20\n    return {\"ppl\":ppl, \"acc\":acc, \"tps\":tps, \"cache_mb\":cache_mb, \"steps\":step}\n",
      "outputs": []
    },
    {
      "id": "781740c4",
      "cell_type": "markdown",
      "source": "\n## 5) Ïã§Ìóò Îß§Ìä∏Î¶≠Ïä§(1‚Üí13)\n- **Param Ratio (Attn:Mamba)**Îäî Ìëú ÏàòÏπòÎ•º Î∞òÏòÅ(Ïòà: `1:8.48` ‚Üí ÎπÑÏú®=1/8.48)\n- **6)**ÏùÄ **Attention-Only** (Mamba 0), **10)**ÏùÄ **6Ïùò GQA Î≤ÑÏ†Ñ**\n- **7,8**ÏùÄ SWA Î∞∞Ïπò(Ï†Ñ Î†àÏù¥Ïñ¥ / Ï≤´¬∑Ï§ë¬∑Îßà Ï†úÏô∏)\n- **9**Îäî 8)+**Cross-layer KV sharing**\n- **11**ÏùÄ 9)+**Fusion=Concat**\n- **12**Îäî 1)+**Meta Tokens**\n- **13**ÏùÄ 9)+**Meta Tokens**\n",
      "metadata": {}
    },
    {
      "id": "8282af02",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "exps = [\n    # (id, desc, (attn, mamba) ratio, n_heads, n_kv_heads, swa_mode, fusion, meta, kv_share)\n    (1, \"Mamba Heads Only\",               (0.0, 1.0), cfg.n_heads, cfg.n_kv_heads, \"none\",    \"mean\",   False, False),\n    (2, \"Mamba + 4 Attn Heads\",           (1.0, 8.48), cfg.n_heads, cfg.n_heads,   \"none\",    \"mean\",   False, False),  # no GQA\n    (3, \"Mamba + 8 Attn Heads\",           (1.0, 4.24), cfg.n_heads, cfg.n_heads,   \"none\",    \"mean\",   False, False),  # no GQA\n    (4, \"Mamba + 16 Attn Heads\",          (1.0, 2.12), cfg.n_heads, cfg.n_heads,   \"none\",    \"mean\",   False, False),  # no GQA\n    (5, \"4) + GQA\",                       (1.0, 3.64), cfg.n_heads, max(1,cfg.n_kv_heads//1), \"none\",    \"mean\",   False, False),\n    (6, \"Attn Heads Only (Llama)\",        (1.0, 0.0), cfg.n_heads, cfg.n_heads,    \"none\",    \"mean\",   False, False),  # attn-only\n    (7, \"5) + All SWA's\",                 (1.0, 3.64), cfg.n_heads, max(1,cfg.n_kv_heads//1), \"all\",     \"mean\",   False, False),\n    (8, \"5) + SWA's + Full Attn\",         (1.0, 3.64), cfg.n_heads, max(1,cfg.n_kv_heads//1), \"default\", \"mean\",   False, False),\n    (9, \"8) + Cross-layer KV sharing\",    (1.0, 5.23), cfg.n_heads, max(1,cfg.n_kv_heads//1), \"default\", \"mean\",   False, True),\n    (10,\"6) + Same KV compression\",       (1.0, 0.0), cfg.n_heads, max(1,cfg.n_kv_heads//1), \"none\",    \"mean\",   False, False),\n    (11,\"9) Replace Mean by Concat\",      (1.0, 5.23), cfg.n_heads, max(1,cfg.n_kv_heads//1), \"default\", \"concat\", False, True),\n    (12,\"1) + Meta Tokens\",               (0.0, 1.0), cfg.n_heads, cfg.n_kv_heads, \"none\",    \"mean\",   True,  False),\n    (13,\"9) + Meta Tokens\",               (1.0, 5.23), cfg.n_heads, max(1,cfg.n_kv_heads//1), \"default\", \"mean\",   True,  True),\n]\nresults = []\nfor (eid, desc, ratio, n_heads, n_kv_heads, swa_mode, fusion, meta, kv_share) in exps:\n    print(f\"=== Exp {eid}: {desc} ===\")\n    model, attn_dim, mamba_dim = build_model_variant(ratio[0], ratio[1], n_heads, n_kv_heads, swa_mode, fusion, meta, kv_share)\n    out = train_one(model, train_dl, val_dl, attn_dim, n_kv_heads, n_heads)\n    attn_share = attn_dim / max(1, (attn_dim+mamba_dim))\n    results.append({\n        \"ID\": eid,\n        \"Configuration\": desc,\n        \"ParamRatio(Attn:Mamba)\": f\"{ratio[0]}:{ratio[1]}\",\n        \"AttnShare(%)\": round(100*attn_share, 2),\n        \"PPL(‚Üì)\": round(out[\"ppl\"], 3),\n        \"Acc(‚Üë)\": round(out[\"acc\"], 4),\n        \"Throughput(tokens/s)‚Üë\": int(out[\"tps\"]),\n        \"Cache(MB)‚Üì\": round(out[\"cache_mb\"], 2),\n        \"Steps\": out[\"steps\"],\n        \"Fusion\": fusion,\n        \"SWA\": swa_mode,\n        \"Meta\": meta,\n        \"KVshare\": kv_share,\n    })\n\ndf = pd.DataFrame(results).sort_values(\"ID\").reset_index(drop=True)\ndf\n",
      "outputs": []
    },
    {
      "id": "3558e5df",
      "cell_type": "markdown",
      "source": "\n## 6) Í≤∞Í≥º ÏöîÏïΩ\n- Ìëú ÌòïÌÉúÎ°ú Ï†ïÎ¶¨ÌïòÍ≥†, Íµ¨ÏÑ± ÏöîÏÜåÎ≥Ñ Ï∞®Ïù¥Î•º Îπ†Î•¥Í≤å ÎπÑÍµêÌï©ÎãàÎã§.\n",
      "metadata": {}
    },
    {
      "id": "a6c7cf94",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "import pandas as pd\nfrom IPython.display import display\npd.set_option(\"display.max_colwidth\", 120)\ndisplay(df)\n",
      "outputs": []
    }
  ]
}