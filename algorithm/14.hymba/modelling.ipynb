{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd5739ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-03 02:00:26,820] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-03 02:00:28.259547: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-09-03 02:00:28.956201: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 디바이스: NVIDIA A100 80GB PCIe\n",
      "Mamba의 CUDA 커널을 사용하여 최적화된 성능으로 실행됩니다.\n",
      "\n",
      "--- Hymba Ablation Study 단계별 모델 구현 테스트 ---\n",
      "\n",
      "--- 단계: 1_Transformer_Baseline ---\n",
      "모델 파라미터 수: 20,187,392\n",
      "입력 shape: torch.Size([2, 512])\n",
      "출력 shape: torch.Size([2, 512, 32000])\n",
      "테스트 성공!\n",
      "\n",
      "--- 단계: 2_Plus_Meta_Tokens ---\n",
      "모델 파라미터 수: 20,188,416\n",
      "입력 shape: torch.Size([2, 512])\n",
      "출력 shape: torch.Size([2, 512, 32000])\n",
      "테스트 성공!\n",
      "\n",
      "--- 단계: 3_Plus_Shared_KV ---\n",
      "모델 파라미터 수: 20,188,416\n",
      "입력 shape: torch.Size([2, 512])\n",
      "출력 shape: torch.Size([2, 512, 32000])\n",
      "테스트 성공!\n",
      "\n",
      "--- 단계: 4_Plus_SWA ---\n",
      "모델 파라미터 수: 20,188,416\n",
      "입력 shape: torch.Size([2, 512])\n",
      "출력 shape: torch.Size([2, 512, 32000])\n",
      "테스트 성공!\n",
      "\n",
      "--- 단계: 5_Hymba_Final ---\n",
      "모델 파라미터 수: 22,463,744\n",
      "입력 shape: torch.Size([2, 512])\n",
      "출력 shape: torch.Size([2, 512, 32000])\n",
      "테스트 성공!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional, Tuple\n",
    "import time\n",
    "\n",
    "# --- 중요 ---\n",
    "# Hymba의 Mamba 구성 요소는 성능 최적화를 위해 CUDA 커널을 사용합니다.\n",
    "# 이 코드는 반드시 GPU가 활성화된 환경(예: Google Colab의 GPU 런타임)에서 실행해야 합니다.\n",
    "# !pip install mamba-ssm causal-conv1d einops\n",
    "from mamba_ssm import Mamba\n",
    "\n",
    "# --- 1단계: Llama 기반 Transformer 구성 요소 구현 ---\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, d_model: int, eps: float = 1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(d_model))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) * self.weight\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim: int, max_seq_len: int, base: int = 10000):\n",
    "        super().__init__()\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "        \n",
    "        t = torch.arange(max_seq_len, device=self.inv_freq.device, dtype=self.inv_freq.dtype)\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "        \n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :])\n",
    "        self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :])\n",
    "\n",
    "    def forward(self, x: torch.Tensor, seq_len: int):\n",
    "        return (\n",
    "            self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
    "            self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
    "        )\n",
    "\n",
    "def apply_rotary_emb(xq: torch.Tensor, xk: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    xq_out = (xq * cos) + (rotate_half(xq) * sin)\n",
    "    xk_out = (xk * cos) + (rotate_half(xk) * sin)\n",
    "    return xq_out, xk_out\n",
    "\n",
    "def rotate_half(x: torch.Tensor):\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, n_kv_heads: int, max_seq_len: int, window_size: int = -1):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.n_kv_heads = n_kv_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "        self.window_size = window_size\n",
    "\n",
    "        self.q_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.k_proj = nn.Linear(d_model, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(d_model, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        \n",
    "        self.rotary_emb = RotaryEmbedding(self.head_dim, max_seq_len)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, past_kv: Optional[Tuple[torch.Tensor, torch.Tensor]] = None, attn_mask: Optional[torch.Tensor] = None, use_cache: bool = False):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        q = self.q_proj(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_proj(x).view(batch_size, seq_len, self.n_kv_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(x).view(batch_size, seq_len, self.n_kv_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        cos, sin = self.rotary_emb(q, seq_len)\n",
    "        q, k = apply_rotary_emb(q, k, cos, sin)\n",
    "\n",
    "        if use_cache and past_kv is not None:\n",
    "            past_k, past_v = past_kv\n",
    "            k = torch.cat([past_k, k], dim=2)\n",
    "            v = torch.cat([past_v, v], dim=2)\n",
    "        \n",
    "        present_kv = (k, v) if use_cache else None\n",
    "        \n",
    "        if self.n_kv_heads != self.n_heads:\n",
    "            n_repeats = self.n_heads // self.n_kv_heads\n",
    "            k = k.repeat_interleave(n_repeats, dim=1)\n",
    "            v = v.repeat_interleave(n_repeats, dim=1)\n",
    "            \n",
    "        attn_output = F.scaled_dot_product_attention(q, k, v, attn_mask=attn_mask, is_causal=(attn_mask is None))\n",
    "        \n",
    "        output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n",
    "        return self.o_proj(output), present_kv\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model: int, hidden_dim: int):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(d_model, hidden_dim, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_dim, d_model, bias=False)\n",
    "        self.w3 = nn.Linear(d_model, hidden_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w2(F.silu(self.w1(x)) * self.w3(x))\n",
    "\n",
    "# --- 2단계: 최종 Hymba 레이어 및 모델 구현 ---\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, n_kv_heads: int, max_seq_len: int, ffn_hidden_dim: int, **kwargs):\n",
    "        super().__init__()\n",
    "        attn_args = {'d_model': d_model, 'n_heads': n_heads, 'n_kv_heads': n_kv_heads, 'max_seq_len': max_seq_len}\n",
    "        if 'window_size' in kwargs:\n",
    "            attn_args['window_size'] = kwargs['window_size']\n",
    "            \n",
    "        self.attention = GroupedQueryAttention(**attn_args)\n",
    "        self.feed_forward = FeedForward(d_model, ffn_hidden_dim)\n",
    "        self.attention_norm = RMSNorm(d_model)\n",
    "        self.ffn_norm = RMSNorm(d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, past_kv: Optional[Tuple[torch.Tensor, torch.Tensor]] = None, attn_mask: Optional[torch.Tensor] = None, use_cache: bool = False):\n",
    "        h, present_kv = self.attention(self.attention_norm(x), past_kv, attn_mask, use_cache=use_cache)\n",
    "        h = x + h\n",
    "        out = h + self.feed_forward(self.ffn_norm(h))\n",
    "        return out, present_kv\n",
    "\n",
    "class HymbaLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, mamba_params: dict, attn_params: dict, ffn_hidden_dim: int):\n",
    "        super().__init__()\n",
    "        self.norm = RMSNorm(d_model)\n",
    "        self.mamba_block = Mamba(d_model=d_model, **mamba_params)\n",
    "        self.attn_block = GroupedQueryAttention(**attn_params)\n",
    "        self.feed_forward = FeedForward(d_model, ffn_hidden_dim)\n",
    "        self.ffn_norm = RMSNorm(d_model)\n",
    "        self.gate = nn.Linear(d_model * 2, d_model, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, past_kv: Optional[Tuple[torch.Tensor, torch.Tensor]] = None, attn_mask: Optional[torch.Tensor] = None, use_cache: bool = False):\n",
    "        residual = x\n",
    "        x_norm = self.norm(x)\n",
    "        \n",
    "        mamba_out = self.mamba_block(x_norm)\n",
    "        attn_out, present_kv = self.attn_block(x_norm, past_kv, attn_mask, use_cache=use_cache)\n",
    "        \n",
    "        combined = torch.cat([mamba_out, attn_out], dim=-1)\n",
    "        gated_output = self.gate(combined)\n",
    "        \n",
    "        h = residual + gated_output\n",
    "        out = h + self.feed_forward(self.ffn_norm(h))\n",
    "        \n",
    "        return out, present_kv\n",
    "\n",
    "class HymbaModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.tok_embeddings = nn.Embedding(config['vocab_size'], config['d_model'])\n",
    "        \n",
    "        if config['use_meta_tokens']:\n",
    "            self.meta_tokens = nn.Parameter(torch.randn(1, config['n_meta_tokens'], config['d_model']))\n",
    "\n",
    "        layers = []\n",
    "        for _ in range(config['n_layers']):\n",
    "            attn_params = {\n",
    "                'd_model': config['d_model'],\n",
    "                'n_heads': config['n_heads'],\n",
    "                'n_kv_heads': config['n_kv_heads'],\n",
    "                'max_seq_len': config['max_seq_len'],\n",
    "                'window_size': config['window_size'] if config['use_swa'] else -1,\n",
    "            }\n",
    "            if config['use_ssm_head']:\n",
    "                layers.append(HymbaLayer(\n",
    "                    d_model=config['d_model'],\n",
    "                    mamba_params=config['mamba_params'],\n",
    "                    attn_params=attn_params,\n",
    "                    ffn_hidden_dim=config['ffn_hidden_dim']\n",
    "                ))\n",
    "            else:\n",
    "                layers.append(TransformerBlock(\n",
    "                    d_model=config['d_model'],\n",
    "                    n_heads=config['n_heads'],\n",
    "                    n_kv_heads=config['n_kv_heads'],\n",
    "                    max_seq_len=config['max_seq_len'],\n",
    "                    ffn_hidden_dim=config['ffn_hidden_dim'],\n",
    "                    window_size=attn_params['window_size']\n",
    "                ))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.norm = RMSNorm(config['d_model'])\n",
    "        self.output = nn.Linear(config['d_model'], config['vocab_size'], bias=False)\n",
    "\n",
    "    def _create_sliding_window_mask(self, seq_len: int, device: torch.device) -> Optional[torch.Tensor]:\n",
    "        if not self.config['use_swa'] or self.config['window_size'] <= 0:\n",
    "            return None\n",
    "        \n",
    "        mask = torch.full((1, 1, seq_len, seq_len), float(\"-inf\"), device=device)\n",
    "        mask = torch.triu(mask, diagonal=1)\n",
    "        \n",
    "        window_mask = torch.triu(torch.ones_like(mask), diagonal=self.config['window_size'] + 1)\n",
    "        mask = mask.masked_fill(window_mask.bool(), float('-inf'))\n",
    "        \n",
    "        return mask\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor, use_cache: bool = False):\n",
    "        batch_size, seq_len = tokens.shape\n",
    "        h = self.tok_embeddings(tokens)\n",
    "\n",
    "        if self.config['use_meta_tokens']:\n",
    "            meta_h = self.meta_tokens.expand(batch_size, -1, -1)\n",
    "            h = torch.cat([meta_h, h], dim=1)\n",
    "            seq_len += self.config['n_meta_tokens']\n",
    "\n",
    "        attn_mask = self._create_sliding_window_mask(seq_len, tokens.device)\n",
    "        \n",
    "        kv_cache = [None] * self.config['n_layers']\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            past_kv = kv_cache[i] if use_cache else None\n",
    "            \n",
    "            if use_cache and self.config['use_shared_kv_cache'] and i > 0:\n",
    "                past_kv = kv_cache[i-1]\n",
    "                \n",
    "            h, present_kv = layer(h, past_kv=past_kv, attn_mask=attn_mask, use_cache=use_cache)\n",
    "            \n",
    "            if use_cache:\n",
    "                kv_cache[i] = present_kv\n",
    "\n",
    "        h = self.norm(h)\n",
    "\n",
    "        if self.config['use_meta_tokens']:\n",
    "            h = h[:, self.config['n_meta_tokens']:, :]\n",
    "\n",
    "        return self.output(h)\n",
    "\n",
    "# --- 3단계: Ablation Study 재현을 위한 설정 ---\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # 1. 실행 환경 확인 및 디바이스 설정\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f\"사용 디바이스: {torch.cuda.get_device_name(0)}\")\n",
    "        print(\"Mamba의 CUDA 커널을 사용하여 최적화된 성능으로 실행됩니다.\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"사용 디바이스: CPU\")\n",
    "        print(\"!!! 경고: Mamba의 핵심 기능은 CUDA를 필요로 합니다. 이 코드는 CPU 환경에서 실행되지 않습니다. !!!\")\n",
    "        print(\"!!! Google Colab 사용 시, [런타임] > [런타임 유형 변경]에서 GPU를 선택해주세요. !!!\")\n",
    "        # CPU 환경에서는 Mamba가 실행되지 않으므로, 테스트를 중단합니다.\n",
    "        exit()\n",
    "\n",
    "    base_config = {\n",
    "        'vocab_size': 32000, 'd_model': 256, 'n_layers': 4,\n",
    "        'n_heads': 8, 'n_kv_heads': 2, 'max_seq_len': 1024,\n",
    "        'ffn_hidden_dim': 256 * 4, 'window_size': 256, 'n_meta_tokens': 4,\n",
    "        'mamba_params': {'d_state': 16, 'd_conv': 4, 'expand': 2},\n",
    "        'use_meta_tokens': False, 'use_shared_kv_cache': False,\n",
    "        'use_swa': False, 'use_ssm_head': False,\n",
    "    }\n",
    "\n",
    "    ablation_steps = {\n",
    "        \"1_Transformer_Baseline\": {},\n",
    "        \"2_Plus_Meta_Tokens\": {'use_meta_tokens': True},\n",
    "        \"3_Plus_Shared_KV\": {'use_meta_tokens': True, 'use_shared_kv_cache': True},\n",
    "        \"4_Plus_SWA\": {'use_meta_tokens': True, 'use_shared_kv_cache': True, 'use_swa': True},\n",
    "        \"5_Hymba_Final\": {'use_meta_tokens': True, 'use_shared_kv_cache': True, 'use_swa': True, 'use_ssm_head': True},\n",
    "    }\n",
    "\n",
    "    print(\"\\n--- Hymba Ablation Study 단계별 모델 구현 테스트 ---\")\n",
    "    for name, flags in ablation_steps.items():\n",
    "        print(f\"\\n--- 단계: {name} ---\")\n",
    "        config = base_config.copy()\n",
    "        config.update(flags)\n",
    "        \n",
    "        # 2. 모델과 데이터를 설정된 디바이스로 이동\n",
    "        model = HymbaModel(config).to(device)\n",
    "        \n",
    "        num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        print(f\"모델 파라미터 수: {num_params:,}\")\n",
    "\n",
    "        dummy_input = torch.randint(0, config['vocab_size'], (2, 512)).to(device)\n",
    "        \n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                output_logits = model(dummy_input, use_cache=False)\n",
    "                \n",
    "            expected_seq_len = 512\n",
    "            print(f\"입력 shape: {dummy_input.shape}\")\n",
    "            print(f\"출력 shape: {output_logits.shape}\")\n",
    "            assert output_logits.shape == (2, expected_seq_len, config['vocab_size'])\n",
    "            print(\"테스트 성공!\")\n",
    "        except RuntimeError as e:\n",
    "            print(f\"!!! 테스트 실패: {e} !!!\")\n",
    "            print(\"GPU 환경에서 실행 중인지 다시 확인해주세요.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b333343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 디바이스: NVIDIA A100 80GB PCIe\n",
      "\n",
      "--- 3단계: + Shared KV Cache 모델 성능 측정 ---\n",
      "\n",
      "--- KV Cache 크기 측정 ---\n",
      "입력 shape: (2, 512)\n",
      "실제 측정된 총 KV 캐시 크기: 5.04 MB\n",
      "이론적인 총 KV 캐시 크기: 1.01 MB (공유 O)\n",
      "\n",
      "--- 처리량(Throughput) 측정 ---\n",
      "워밍업 실행 (5회)...\n",
      "성능 측정 실행 (20회)...\n",
      "총 처리 시간: 0.070 초\n",
      "총 처리 토큰: 20,480 개\n",
      "처리량: 292,229.31 tokens/sec\n",
      "\n",
      "--- 지연 시간(Latency) 측정 ---\n",
      "평균 지연 시간: 4.13 ms\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional, Tuple\n",
    "import time\n",
    "\n",
    "# --- 중요 ---\n",
    "# Hymba의 Mamba 구성 요소는 성능 최적화를 위해 CUDA 커널을 사용합니다.\n",
    "# 이 코드는 반드시 GPU가 활성화된 환경(예: Google Colab의 GPU 런타임)에서 실행해야 합니다.\n",
    "# !pip install mamba-ssm causal-conv1d einops\n",
    "from mamba_ssm import Mamba\n",
    "\n",
    "# --- 1단계: Llama 기반 Transformer 구성 요소 구현 ---\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, d_model: int, eps: float = 1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(d_model))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) * self.weight\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim: int, max_seq_len: int, base: int = 10000):\n",
    "        super().__init__()\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "        \n",
    "        t = torch.arange(max_seq_len, device=self.inv_freq.device, dtype=self.inv_freq.dtype)\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "        \n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :])\n",
    "        self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :])\n",
    "\n",
    "    def forward(self, x: torch.Tensor, seq_len: int):\n",
    "        return (\n",
    "            self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
    "            self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
    "        )\n",
    "\n",
    "def apply_rotary_emb(xq: torch.Tensor, xk: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    xq_out = (xq * cos) + (rotate_half(xq) * sin)\n",
    "    xk_out = (xk * cos) + (rotate_half(xk) * sin)\n",
    "    return xq_out, xk_out\n",
    "\n",
    "def rotate_half(x: torch.Tensor):\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, n_kv_heads: int, max_seq_len: int, window_size: int = -1):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.n_kv_heads = n_kv_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "        self.window_size = window_size\n",
    "\n",
    "        self.q_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.k_proj = nn.Linear(d_model, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(d_model, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        \n",
    "        self.rotary_emb = RotaryEmbedding(self.head_dim, max_seq_len)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, past_kv: Optional[Tuple[torch.Tensor, torch.Tensor]] = None, attn_mask: Optional[torch.Tensor] = None, use_cache: bool = False):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        q = self.q_proj(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_proj(x).view(batch_size, seq_len, self.n_kv_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(x).view(batch_size, seq_len, self.n_kv_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        cos, sin = self.rotary_emb(q, seq_len)\n",
    "        q, k = apply_rotary_emb(q, k, cos, sin)\n",
    "\n",
    "        if use_cache and past_kv is not None:\n",
    "            past_k, past_v = past_kv\n",
    "            k = torch.cat([past_k, k], dim=2)\n",
    "            v = torch.cat([past_v, v], dim=2)\n",
    "        \n",
    "        present_kv = (k, v)\n",
    "        \n",
    "        if self.n_kv_heads != self.n_heads:\n",
    "            n_repeats = self.n_heads // self.n_kv_heads\n",
    "            k = k.repeat_interleave(n_repeats, dim=1)\n",
    "            v = v.repeat_interleave(n_repeats, dim=1)\n",
    "            \n",
    "        attn_output = F.scaled_dot_product_attention(q, k, v, attn_mask=attn_mask, is_causal=(attn_mask is None))\n",
    "        \n",
    "        output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n",
    "        return self.o_proj(output), present_kv\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model: int, hidden_dim: int):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(d_model, hidden_dim, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_dim, d_model, bias=False)\n",
    "        self.w3 = nn.Linear(d_model, hidden_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w2(F.silu(self.w1(x)) * self.w3(x))\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, n_kv_heads: int, max_seq_len: int, ffn_hidden_dim: int, **kwargs):\n",
    "        super().__init__()\n",
    "        attn_args = {'d_model': d_model, 'n_heads': n_heads, 'n_kv_heads': n_kv_heads, 'max_seq_len': max_seq_len}\n",
    "        if 'window_size' in kwargs:\n",
    "            attn_args['window_size'] = kwargs['window_size']\n",
    "            \n",
    "        self.attention = GroupedQueryAttention(**attn_args)\n",
    "        self.feed_forward = FeedForward(d_model, ffn_hidden_dim)\n",
    "        self.attention_norm = RMSNorm(d_model)\n",
    "        self.ffn_norm = RMSNorm(d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, past_kv: Optional[Tuple[torch.Tensor, torch.Tensor]] = None, attn_mask: Optional[torch.Tensor] = None, use_cache: bool = False):\n",
    "        h, present_kv = self.attention(self.attention_norm(x), past_kv, attn_mask, use_cache=use_cache)\n",
    "        h = x + h\n",
    "        out = h + self.feed_forward(self.ffn_norm(h))\n",
    "        return out, present_kv\n",
    "\n",
    "class HymbaLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, mamba_params: dict, attn_params: dict, ffn_hidden_dim: int):\n",
    "        super().__init__()\n",
    "        self.norm = RMSNorm(d_model)\n",
    "        self.mamba_block = Mamba(d_model=d_model, **mamba_params)\n",
    "        self.attn_block = GroupedQueryAttention(**attn_params)\n",
    "        self.feed_forward = FeedForward(d_model, ffn_hidden_dim)\n",
    "        self.ffn_norm = RMSNorm(d_model)\n",
    "        self.gate = nn.Linear(d_model * 2, d_model, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, past_kv: Optional[Tuple[torch.Tensor, torch.Tensor]] = None, attn_mask: Optional[torch.Tensor] = None, use_cache: bool = False):\n",
    "        residual = x\n",
    "        x_norm = self.norm(x)\n",
    "        \n",
    "        mamba_out = self.mamba_block(x_norm)\n",
    "        attn_out, present_kv = self.attn_block(x_norm, past_kv, attn_mask, use_cache=use_cache)\n",
    "        \n",
    "        combined = torch.cat([mamba_out, attn_out], dim=-1)\n",
    "        gated_output = self.gate(combined)\n",
    "        \n",
    "        h = residual + gated_output\n",
    "        out = h + self.feed_forward(self.ffn_norm(h))\n",
    "        \n",
    "        return out, present_kv\n",
    "\n",
    "class HymbaModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.tok_embeddings = nn.Embedding(config['vocab_size'], config['d_model'])\n",
    "        \n",
    "        if config['use_meta_tokens']:\n",
    "            self.meta_tokens = nn.Parameter(torch.randn(1, config['n_meta_tokens'], config['d_model']))\n",
    "\n",
    "        layers = []\n",
    "        for _ in range(config['n_layers']):\n",
    "            attn_params = {\n",
    "                'd_model': config['d_model'],\n",
    "                'n_heads': config['n_heads'],\n",
    "                'n_kv_heads': config['n_kv_heads'],\n",
    "                'max_seq_len': config['max_seq_len'],\n",
    "                'window_size': config['window_size'] if config['use_swa'] else -1,\n",
    "            }\n",
    "            if config['use_ssm_head']:\n",
    "                layers.append(HymbaLayer(\n",
    "                    d_model=config['d_model'],\n",
    "                    mamba_params=config['mamba_params'],\n",
    "                    attn_params=attn_params,\n",
    "                    ffn_hidden_dim=config['ffn_hidden_dim']\n",
    "                ))\n",
    "            else:\n",
    "                layers.append(TransformerBlock(\n",
    "                    d_model=config['d_model'],\n",
    "                    n_heads=config['n_heads'],\n",
    "                    n_kv_heads=config['n_kv_heads'],\n",
    "                    max_seq_len=config['max_seq_len'],\n",
    "                    ffn_hidden_dim=config['ffn_hidden_dim'],\n",
    "                    window_size=attn_params['window_size']\n",
    "                ))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.norm = RMSNorm(config['d_model'])\n",
    "        self.output = nn.Linear(config['d_model'], config['vocab_size'], bias=False)\n",
    "\n",
    "    def _create_sliding_window_mask(self, seq_len: int, device: torch.device) -> Optional[torch.Tensor]:\n",
    "        if not self.config['use_swa'] or self.config['window_size'] <= 0:\n",
    "            return None\n",
    "        \n",
    "        mask = torch.full((1, 1, seq_len, seq_len), float(\"-inf\"), device=device)\n",
    "        mask = torch.triu(mask, diagonal=1)\n",
    "        \n",
    "        window_mask = torch.triu(torch.ones_like(mask), diagonal=self.config['window_size'] + 1)\n",
    "        mask = mask.masked_fill(window_mask.bool(), float('-inf'))\n",
    "        \n",
    "        return mask\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor, use_cache: bool = False, return_kv_cache: bool = False):\n",
    "        \"\"\"\n",
    "        수정된 forward 메소드\n",
    "        - use_cache: KV 캐시를 사용할지(추론) 안 할지(학습) 결정\n",
    "        - return_kv_cache: 외부에서 캐시를 확인하기 위해 반환할지 결정\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = tokens.shape\n",
    "        h = self.tok_embeddings(tokens)\n",
    "\n",
    "        if self.config['use_meta_tokens']:\n",
    "            meta_h = self.meta_tokens.expand(batch_size, -1, -1)\n",
    "            h = torch.cat([meta_h, h], dim=1)\n",
    "            seq_len += self.config['n_meta_tokens']\n",
    "\n",
    "        attn_mask = self._create_sliding_window_mask(seq_len, tokens.device)\n",
    "        \n",
    "        kv_cache = [None] * self.config['n_layers']\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            past_kv = kv_cache[i] if use_cache else None\n",
    "            \n",
    "            # Shared KV Cache는 추론 시에만 의미가 있음\n",
    "            if use_cache and self.config['use_shared_kv_cache'] and i > 0:\n",
    "                past_kv = kv_cache[i-1]\n",
    "                \n",
    "            h, present_kv = layer(h, past_kv=past_kv, attn_mask=attn_mask, use_cache=use_cache)\n",
    "            \n",
    "            # use_cache가 True일 때만 캐시를 저장\n",
    "            if use_cache:\n",
    "                kv_cache[i] = present_kv\n",
    "\n",
    "        h = self.norm(h)\n",
    "\n",
    "        if self.config['use_meta_tokens']:\n",
    "            h = h[:, self.config['n_meta_tokens']:, :]\n",
    "\n",
    "        logits = self.output(h)\n",
    "\n",
    "        if return_kv_cache:\n",
    "            return logits, kv_cache\n",
    "        else:\n",
    "            return logits\n",
    "\n",
    "# --- 성능 측정 스크립트 ---\n",
    "if __name__ == '__main__':\n",
    "    # GPU 사용 가능 여부 확인\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"!!! 경고: 이 성능 측정 스크립트는 GPU 환경에서 실행해야 의미가 있습니다. !!!\")\n",
    "        print(\"!!! Google Colab 사용 시, [런타임] > [런타임 유형 변경]에서 GPU를 선택해주세요. !!!\")\n",
    "        exit()\n",
    "        \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"사용 디바이스: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "    # 기본 Llama 스타일 설정\n",
    "    base_config = {\n",
    "        'vocab_size': 32000, 'd_model': 256, 'n_layers': 4,\n",
    "        'n_heads': 8, 'n_kv_heads': 2, 'max_seq_len': 1024,\n",
    "        'ffn_hidden_dim': 256 * 4, 'window_size': 256, 'n_meta_tokens': 4,\n",
    "        'mamba_params': {'d_state': 16, 'd_conv': 4, 'expand': 2},\n",
    "        'use_meta_tokens': False, 'use_shared_kv_cache': False,\n",
    "        'use_swa': False, 'use_ssm_head': False,\n",
    "    }\n",
    "\n",
    "    # 3단계 설정: Transformer + Meta Tokens + Shared KV\n",
    "    config = base_config.copy()\n",
    "    config.update({\n",
    "        'use_meta_tokens': True,\n",
    "        'use_shared_kv_cache': True,\n",
    "    })\n",
    "\n",
    "    model = HymbaModel(config).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    batch_size = 2\n",
    "    seq_len = 512\n",
    "    dummy_input = torch.randint(0, config['vocab_size'], (batch_size, seq_len)).to(device)\n",
    "\n",
    "    print(\"\\n--- 3단계: + Shared KV Cache 모델 성능 측정 ---\")\n",
    "\n",
    "    # --- KV Cache 크기 측정 ---\n",
    "    print(\"\\n--- KV Cache 크기 측정 ---\")\n",
    "    with torch.no_grad():\n",
    "        # use_cache와 return_kv_cache를 True로 설정하여 캐시를 채우고 반환받음\n",
    "        _, kv_cache = model(dummy_input, use_cache=True, return_kv_cache=True)\n",
    "\n",
    "    unique_tensors = {}\n",
    "    for cache_tuple in kv_cache:\n",
    "        if cache_tuple is not None:\n",
    "            k, v = cache_tuple\n",
    "            if id(k) not in unique_tensors:\n",
    "                unique_tensors[id(k)] = k\n",
    "            if id(v) not in unique_tensors:\n",
    "                unique_tensors[id(v)] = v\n",
    "\n",
    "    total_cache_bytes = sum(t.numel() * t.element_size() for t in unique_tensors.values())\n",
    "    total_cache_mb = total_cache_bytes / (1024 * 1024)\n",
    "\n",
    "    # 이론적 계산 (비교용)\n",
    "    num_unique_layers = config['n_layers'] if not config['use_shared_kv_cache'] else (config['n_layers'] // 2 + config['n_layers'] % 2)\n",
    "    expected_bytes = num_unique_layers * batch_size * config['n_kv_heads'] * (seq_len + config['n_meta_tokens']) * (config['d_model'] // config['n_heads']) * 2 * 4\n",
    "    expected_mb = expected_bytes / (1024 * 1024)\n",
    "\n",
    "    print(f\"입력 shape: ({batch_size}, {seq_len})\")\n",
    "    print(f\"실제 측정된 총 KV 캐시 크기: {total_cache_mb:.2f} MB\")\n",
    "    print(f\"이론적인 총 KV 캐시 크기: {expected_mb:.2f} MB (공유 O)\")\n",
    "\n",
    "    # --- 처리량(Throughput) 측정 ---\n",
    "    print(\"\\n--- 처리량(Throughput) 측정 ---\")\n",
    "    warmup_iterations = 5\n",
    "    measurement_iterations = 20\n",
    "\n",
    "    print(f\"워밍업 실행 ({warmup_iterations}회)...\")\n",
    "    for _ in range(warmup_iterations):\n",
    "        # use_cache=True로 설정하여 추론과 유사한 상황(캐시 채우기)을 시뮬레이션\n",
    "        _ = model(dummy_input, use_cache=True)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    print(f\"성능 측정 실행 ({measurement_iterations}회)...\")\n",
    "    start_time = time.time()\n",
    "    for _ in range(measurement_iterations):\n",
    "        _ = model(dummy_input, use_cache=True)\n",
    "    torch.cuda.synchronize()\n",
    "    end_time = time.time()\n",
    "\n",
    "    total_time = end_time - start_time\n",
    "    total_tokens = batch_size * seq_len * measurement_iterations\n",
    "    tokens_per_second = total_tokens / total_time\n",
    "\n",
    "    print(f\"총 처리 시간: {total_time:.3f} 초\")\n",
    "    print(f\"총 처리 토큰: {total_tokens:,} 개\")\n",
    "    print(f\"처리량: {tokens_per_second:,.2f} tokens/sec\")\n",
    "\n",
    "    # --- 지연 시간(Latency) 측정 ---\n",
    "    print(\"\\n--- 지연 시간(Latency) 측정 ---\")\n",
    "    latency_iterations = 50\n",
    "    latencies = []\n",
    "    for _ in range(latency_iterations):\n",
    "        start = time.time()\n",
    "        _ = model(dummy_input, use_cache=True)\n",
    "        torch.cuda.synchronize()\n",
    "        end = time.time()\n",
    "        latencies.append(end - start)\n",
    "    avg_latency = sum(latencies) / latency_iterations\n",
    "    print(f\"평균 지연 시간: {avg_latency * 1000:.2f} ms\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
