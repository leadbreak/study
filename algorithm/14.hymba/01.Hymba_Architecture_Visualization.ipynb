{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hymba 아키텍처 시각화 및 검증\n",
    "\n",
    "## 목표\n",
    "1. 공식 Hymba-1.5B 구성 시각화\n",
    "2. Global/Local Attention 패턴 검증\n",
    "3. 어텐션 맵 시각화 (Global vs Local 분리)\n",
    "4. KV-Cache 공유 메커니즘 확인\n",
    "\n",
    "## 공식 Hymba-1.5B 구성\n",
    "- 총 32 레이어\n",
    "- **Global Attention**: 레이어 0, 15, 31 (첫/중간/마지막)\n",
    "- **Local Attention (SWA)**: 나머지 레이어 (1-14, 16-30)\n",
    "- **Sliding Window**: 1024 토큰\n",
    "- **Meta Tokens**: 128개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "디바이스: cuda\n",
      "결과 저장 폴더: ./results\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('./backbone')\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.patches import FancyBboxPatch, Rectangle\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from hymba import Hymba, HymbaConfig, ArchType, AttentionType\n",
    "\n",
    "# 결과 저장 폴더\n",
    "RESULTS_DIR = './results'\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# 시각화 설정\n",
    "plt.rcParams['figure.figsize'] = (16, 10)\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# 디바이스\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"디바이스: {device}\")\n",
    "print(f\"결과 저장 폴더: {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 공식 Hymba-1.5B 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "HymbaConfig.__init__() got an unexpected keyword argument 'attn_ratio'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Hymba-1.5B 스케일 모델 (테스트용으로 작게)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43mHymbaConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43md_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m768\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 공식 구성\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_kv_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43march_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mArchType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHYBRID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# 공식 Global/Local 패턴\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mglobal_attn_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m31\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 첫/중간/마지막\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mswa_window\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 공식 구현\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Meta Tokens\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_meta_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_meta_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 공식 구현\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# KV 공유\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_kv_sharing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m model \u001b[38;5;241m=\u001b[39m Hymba(config)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== Hymba-1.5B 스케일 모델 ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: HymbaConfig.__init__() got an unexpected keyword argument 'attn_ratio'"
     ]
    }
   ],
   "source": [
    "# Hymba-1.5B 스케일 모델 (테스트용으로 작게)\n",
    "config = HymbaConfig(\n",
    "    vocab_size=8000,\n",
    "    d_model=768,\n",
    "    n_layers=32,  # 공식 구성\n",
    "    n_heads=12,\n",
    "    n_kv_heads=4,\n",
    "    arch_type=ArchType.HYBRID,\n",
    "    attn_ratio=0.5,\n",
    "    \n",
    "    # 공식 Global/Local 패턴\n",
    "    global_attn_indices=[0, 15, 31],  # 첫/중간/마지막\n",
    "    swa_window=1024,  # 공식 구현\n",
    "    \n",
    "    # Meta Tokens\n",
    "    use_meta_tokens=True,\n",
    "    num_meta_tokens=128,  # 공식 구현\n",
    "    \n",
    "    # KV 공유\n",
    "    use_kv_sharing=True,\n",
    ")\n",
    "\n",
    "model = Hymba(config)\n",
    "\n",
    "print(\"=== Hymba-1.5B 스케일 모델 ===\")\n",
    "params = model.count_parameters()\n",
    "print(f\"파라미터: {params['total']:,}\")\n",
    "print(f\"모델 크기: {params['total'] * 4 / 1024**3:.2f} GB (FP32)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Attention 패턴 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어텐션 패턴 정보\n",
    "attn_info = model.get_attention_pattern_info()\n",
    "\n",
    "print(\"=== Attention 패턴 ===\")\n",
    "print(f\"전체 레이어: {attn_info['total_layers']}\")\n",
    "print(f\"\\nGlobal Attention 레이어: {attn_info['global_layers']}\")\n",
    "print(f\"개수: {attn_info['num_global']}\")\n",
    "print(f\"\\nLocal Attention 레이어: {attn_info['local_layers']}\")\n",
    "print(f\"개수: {attn_info['num_local']}\")\n",
    "\n",
    "# DataFrame으로 정리\n",
    "layer_data = []\n",
    "attn_types = config.get_attention_types()\n",
    "for i in range(config.n_layers):\n",
    "    layer_data.append({\n",
    "        \"Layer\": i,\n",
    "        \"Attention\": \"Global\" if attn_types[i] == AttentionType.GLOBAL else \"Local (SWA)\",\n",
    "        \"Window\": \"Full\" if attn_types[i] == AttentionType.GLOBAL else config.swa_window,\n",
    "        \"KV Owner\": model.owner[i],\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(layer_data)\n",
    "print(\"\\n=== 레이어별 구성 ===\")\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 레이어 구조 시각화\n",
    "def visualize_layer_structure(config, attn_info):\n",
    "    fig, ax = plt.subplots(figsize=(18, 12))\n",
    "    \n",
    "    n_layers = config.n_layers\n",
    "    attn_types = config.get_attention_types()\n",
    "    \n",
    "    # 색상\n",
    "    color_global = '#FFB6C1'  # 연한 빨강 (Global)\n",
    "    color_local = '#B6D7FF'   # 연한 파랑 (Local)\n",
    "    \n",
    "    # 레이어 박스 그리기\n",
    "    box_height = 0.8\n",
    "    for i in range(n_layers):\n",
    "        y = n_layers - i - 1\n",
    "        \n",
    "        is_global = attn_types[i] == AttentionType.GLOBAL\n",
    "        color = color_global if is_global else color_local\n",
    "        label = \"Global\" if is_global else \"Local (SWA)\"\n",
    "        \n",
    "        # 레이어 박스\n",
    "        rect = FancyBboxPatch(\n",
    "            (1, y), 12, box_height,\n",
    "            boxstyle=\"round,pad=0.05\",\n",
    "            edgecolor='black', facecolor=color, linewidth=2\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # 텍스트\n",
    "        ax.text(1.5, y + box_height/2, f\"Layer {i}\", \n",
    "                va='center', fontsize=10, fontweight='bold')\n",
    "        ax.text(5, y + box_height/2, label,\n",
    "                va='center', fontsize=9)\n",
    "        \n",
    "        # Window 크기\n",
    "        window_text = \"Full Sequence\" if is_global else f\"Window={config.swa_window}\"\n",
    "        ax.text(9, y + box_height/2, window_text,\n",
    "                va='center', fontsize=9, style='italic')\n",
    "        \n",
    "        # 특별 표시 (첫/중간/마지막)\n",
    "        if i == 0:\n",
    "            ax.text(13.5, y + box_height/2, \"← First\",\n",
    "                    va='center', fontsize=9, color='red', fontweight='bold')\n",
    "        elif i == n_layers // 2:\n",
    "            ax.text(13.5, y + box_height/2, \"← Middle\",\n",
    "                    va='center', fontsize=9, color='red', fontweight='bold')\n",
    "        elif i == n_layers - 1:\n",
    "            ax.text(13.5, y + box_height/2, \"← Last\",\n",
    "                    va='center', fontsize=9, color='red', fontweight='bold')\n",
    "    \n",
    "    ax.set_xlim(0, 16)\n",
    "    ax.set_ylim(-1, n_layers)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # 범례\n",
    "    global_patch = mpatches.Patch(color=color_global, label=f'Global Attention ({attn_info[\"num_global\"]} layers)')\n",
    "    local_patch = mpatches.Patch(color=color_local, label=f'Local Attention ({attn_info[\"num_local\"]} layers)')\n",
    "    ax.legend(handles=[global_patch, local_patch], loc='upper right', fontsize=11)\n",
    "    \n",
    "    plt.title(f'Hymba-1.5B Layer Configuration ({config.n_layers} layers)', \n",
    "              fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "fig = visualize_layer_structure(config, attn_info)\n",
    "plt.savefig(f'{RESULTS_DIR}/hymba_layer_structure.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"저장: {RESULTS_DIR}/hymba_layer_structure.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. KV-Cache 공유 패턴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kv_info = model.get_kv_sharing_info()\n",
    "\n",
    "print(\"=== KV-Cache 공유 정보 ===\")\n",
    "print(f\"전체 레이어: {kv_info['total_layers']}\")\n",
    "print(f\"독립 KV 캐시: {kv_info['independent_caches']}\")\n",
    "print(f\"메모리 절감: {kv_info['reduction']:.2f}x\")\n",
    "\n",
    "print(\"\\n=== KV 공유 그룹 ===\")\n",
    "for owner, layers in sorted(kv_info['groups'].items()):\n",
    "    if len(layers) > 1:\n",
    "        print(f\"Owner Layer {owner} → Shared by {layers}\")\n",
    "    else:\n",
    "        print(f\"Layer {owner}: 독립 (공유 안 함)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Attention Map 시각화\n",
    "\n",
    "### Global vs Local 어텐션 패턴 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 작은 테스트 입력\n",
    "batch_size = 1\n",
    "seq_len = 128\n",
    "test_input = torch.randint(0, config.vocab_size, (batch_size, seq_len)).to(device)\n",
    "\n",
    "# 어텐션 가중치 추출\n",
    "model = model.to(device).eval()\n",
    "with torch.no_grad():\n",
    "    output = model(test_input, return_attn=True)\n",
    "\n",
    "attn_weights = output['attn_weights']\n",
    "print(f\"추출된 어텐션 맵: {len(attn_weights)}개 레이어\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global vs Local 어텐션 맵 비교 (개선된 시각화)\n",
    "def visualize_attention_maps(attn_weights, config, num_meta=128):\n",
    "    \"\"\"\n",
    "    Global과 Local 어텐션 맵을 분리하여 시각화\n",
    "    \n",
    "    개선사항:\n",
    "    - Log scale 적용으로 작은 값도 잘 보이게\n",
    "    - 더 선명한 색상 맵 사용\n",
    "    - 메타 토큰 영역 명확히 표시\n",
    "    - 어텐션 값 범위 조정\n",
    "    \"\"\"\n",
    "    import matplotlib.colors as mcolors\n",
    "    \n",
    "    attn_types = config.get_attention_types()\n",
    "    \n",
    "    # Global/Local 레이어 분리\n",
    "    global_indices = [i for i, t in enumerate(attn_types) if t == AttentionType.GLOBAL]\n",
    "    local_indices = [i for i, t in enumerate(attn_types) if t == AttentionType.LOCAL][:3]\n",
    "    \n",
    "    n_global = len(global_indices)\n",
    "    n_local = len(local_indices)\n",
    "    n_total = n_global + n_local\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    plot_idx = 0\n",
    "    \n",
    "    # Global 레이어 플롯\n",
    "    for layer_idx in global_indices:\n",
    "        if layer_idx >= len(attn_weights) or attn_weights[layer_idx] is None:\n",
    "            continue\n",
    "            \n",
    "        attn = attn_weights[layer_idx][0, 0].cpu().numpy()  # [B, H, T, T] -> [T, T]\n",
    "        \n",
    "        ax = axes[plot_idx]\n",
    "        \n",
    "        # 어텐션 값 정규화 및 로그 스케일 적용\n",
    "        attn_vis = attn.copy()\n",
    "        attn_vis = np.clip(attn_vis, 1e-6, 1.0)  # 0 방지\n",
    "        attn_log = np.log10(attn_vis + 1e-8)\n",
    "        \n",
    "        # 더 선명한 색상맵 사용\n",
    "        im = ax.imshow(attn_log, cmap='magma', aspect='auto', \n",
    "                       vmin=-4, vmax=0)  # log scale: 10^-4 ~ 10^0\n",
    "        \n",
    "        ax.set_title(f'Layer {layer_idx}: Global Attention', fontweight='bold', \n",
    "                     fontsize=12, color='darkred')\n",
    "        ax.set_xlabel('Key Position', fontsize=10)\n",
    "        ax.set_ylabel('Query Position', fontsize=10)\n",
    "        \n",
    "        # 메타 토큰 경계 표시\n",
    "        if num_meta > 0:\n",
    "            ax.axvline(x=num_meta-0.5, color='lime', linestyle='-', linewidth=2, alpha=0.9)\n",
    "            ax.axhline(y=num_meta-0.5, color='lime', linestyle='-', linewidth=2, alpha=0.9)\n",
    "            ax.text(num_meta/2, -5, 'Meta', ha='center', va='bottom', \n",
    "                    fontsize=9, color='lime', fontweight='bold')\n",
    "            ax.text(-5, num_meta/2, 'Meta', ha='right', va='center', \n",
    "                    fontsize=9, color='lime', fontweight='bold', rotation=90)\n",
    "        \n",
    "        cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "        cbar.set_label('log₁₀(attention)', fontsize=9)\n",
    "        \n",
    "        plot_idx += 1\n",
    "    \n",
    "    # Local 레이어 플롯\n",
    "    for layer_idx in local_indices:\n",
    "        if layer_idx >= len(attn_weights) or attn_weights[layer_idx] is None:\n",
    "            continue\n",
    "            \n",
    "        attn = attn_weights[layer_idx][0, 0].cpu().numpy()\n",
    "        \n",
    "        ax = axes[plot_idx]\n",
    "        \n",
    "        # 어텐션 값 정규화 및 로그 스케일 적용\n",
    "        attn_vis = attn.copy()\n",
    "        attn_vis = np.clip(attn_vis, 1e-6, 1.0)\n",
    "        attn_log = np.log10(attn_vis + 1e-8)\n",
    "        \n",
    "        im = ax.imshow(attn_log, cmap='viridis', aspect='auto',\n",
    "                       vmin=-4, vmax=0)\n",
    "        \n",
    "        ax.set_title(f'Layer {layer_idx}: Local (SWA, w={config.swa_window})', \n",
    "                     fontweight='bold', fontsize=12, color='darkblue')\n",
    "        ax.set_xlabel('Key Position', fontsize=10)\n",
    "        ax.set_ylabel('Query Position', fontsize=10)\n",
    "        \n",
    "        # 메타 토큰 경계\n",
    "        if num_meta > 0:\n",
    "            ax.axvline(x=num_meta-0.5, color='yellow', linestyle='-', linewidth=2, alpha=0.9)\n",
    "            ax.axhline(y=num_meta-0.5, color='yellow', linestyle='-', linewidth=2, alpha=0.9)\n",
    "        \n",
    "        cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "        cbar.set_label('log₁₀(attention)', fontsize=9)\n",
    "        \n",
    "        plot_idx += 1\n",
    "    \n",
    "    # 사용하지 않는 서브플롯 제거\n",
    "    for i in range(plot_idx, len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "    \n",
    "    plt.suptitle('Hymba Attention Maps: Global vs Local\\n(Log Scale for Better Visibility)', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "fig = visualize_attention_maps(attn_weights, config, num_meta=config.num_meta_tokens)\n",
    "plt.savefig(f'{RESULTS_DIR}/attention_maps_global_vs_local.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"저장: {RESULTS_DIR}/attention_maps_global_vs_local.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sliding Window 패턴 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local 레이어의 Sliding Window 패턴 확인\n",
    "def analyze_sliding_window(attn_weights, config):\n",
    "    \"\"\"\n",
    "    Sliding Window가 올바르게 적용되었는지 확인\n",
    "    \"\"\"\n",
    "    attn_types = config.get_attention_types()\n",
    "    local_indices = [i for i, t in enumerate(attn_types) if t == AttentionType.LOCAL]\n",
    "    \n",
    "    print(\"=== Sliding Window 패턴 분석 ===\")\n",
    "    \n",
    "    for layer_idx in local_indices[:3]:  # 처음 3개만\n",
    "        if layer_idx >= len(attn_weights) or attn_weights[layer_idx] is None:\n",
    "            continue\n",
    "        \n",
    "        attn = attn_weights[layer_idx][0, 0].cpu().numpy()\n",
    "        T = attn.shape[0]\n",
    "        \n",
    "        # 각 쿼리 위치에서 non-zero attention의 범위 확인\n",
    "        print(f\"\\nLayer {layer_idx}:\")\n",
    "        \n",
    "        # 샘플 쿼리 위치 (메타 토큰 이후)\n",
    "        sample_queries = [config.num_meta_tokens + 10, \n",
    "                         config.num_meta_tokens + 50, \n",
    "                         T - 10]\n",
    "        \n",
    "        for q_pos in sample_queries:\n",
    "            if q_pos >= T:\n",
    "                continue\n",
    "            \n",
    "            # Non-zero attention 찾기\n",
    "            attn_row = attn[q_pos]\n",
    "            nonzero_pos = np.where(attn_row > 1e-6)[0]\n",
    "            \n",
    "            if len(nonzero_pos) > 0:\n",
    "                min_key = nonzero_pos[0]\n",
    "                max_key = nonzero_pos[-1]\n",
    "                window_size = max_key - min_key + 1\n",
    "                \n",
    "                print(f\"  Query {q_pos}: attends to keys [{min_key}, {max_key}] \"\n",
    "                      f\"(window_size={window_size})\")\n",
    "                \n",
    "                # 메타 토큰 영역 확인\n",
    "                meta_attention = attn_row[:config.num_meta_tokens].sum()\n",
    "                if meta_attention > 0:\n",
    "                    print(f\"    → Meta tokens attention: {meta_attention:.4f}\")\n",
    "\n",
    "analyze_sliding_window(attn_weights, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 메타 토큰 효과 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_meta_tokens(attn_weights, config):\n",
    "    \"\"\"\n",
    "    메타 토큰에 대한 어텐션 패턴 분석\n",
    "    \"\"\"\n",
    "    M = config.num_meta_tokens\n",
    "    \n",
    "    print(\"=== 메타 토큰 어텐션 분석 ===\")\n",
    "    print(f\"메타 토큰 수: {M}\\n\")\n",
    "    \n",
    "    attn_types = config.get_attention_types()\n",
    "    \n",
    "    for layer_idx in [0, 15, 31]:  # Global 레이어\n",
    "        if layer_idx >= len(attn_weights) or attn_weights[layer_idx] is None:\n",
    "            continue\n",
    "        \n",
    "        attn = attn_weights[layer_idx][0, 0].cpu().numpy()\n",
    "        T = attn.shape[0]\n",
    "        \n",
    "        # 일반 토큰(메타 이후)이 메타 토큰에 주는 어텐션\n",
    "        content_to_meta = attn[M:, :M].mean()\n",
    "        # 일반 토큰끼리의 어텐션\n",
    "        content_to_content = attn[M:, M:].mean()\n",
    "        # 메타 토큰끼리의 어텐션\n",
    "        meta_to_meta = attn[:M, :M].mean()\n",
    "        \n",
    "        layer_type = \"Global\" if attn_types[layer_idx] == AttentionType.GLOBAL else \"Local\"\n",
    "        print(f\"Layer {layer_idx} ({layer_type}):\")\n",
    "        print(f\"  Content → Meta: {content_to_meta:.6f}\")\n",
    "        print(f\"  Content → Content: {content_to_content:.6f}\")\n",
    "        print(f\"  Meta → Meta: {meta_to_meta:.6f}\")\n",
    "        print(f\"  비율 (Content→Meta / Content→Content): {content_to_meta / (content_to_content + 1e-9):.2f}x\\n\")\n",
    "\n",
    "analyze_meta_tokens(attn_weights, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 종합 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Hymba 아키텍처 검증 요약\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. 모델 구성:\")\n",
    "print(f\"   - 전체 레이어: {config.n_layers}\")\n",
    "print(f\"   - 파라미터: {params['total']:,}\")\n",
    "print(f\"   - 모델 크기: {params['total'] * 4 / 1024**3:.2f} GB (FP32)\")\n",
    "\n",
    "print(\"\\n2. Attention 패턴:\")\n",
    "print(f\"   - Global Attention: {attn_info['global_layers']} (총 {attn_info['num_global']}개)\")\n",
    "print(f\"   - Local Attention: 나머지 {attn_info['num_local']}개 레이어\")\n",
    "print(f\"   - Sliding Window: {config.swa_window} 토큰\")\n",
    "\n",
    "print(\"\\n3. 메타 토큰:\")\n",
    "print(f\"   - 개수: {config.num_meta_tokens}개\")\n",
    "print(f\"   - 역할: 전역 컨텍스트 저장, Attention Sink 방지\")\n",
    "\n",
    "print(\"\\n4. KV-Cache 공유:\")\n",
    "print(f\"   - 독립 캐시: {kv_info['independent_caches']}개\")\n",
    "print(f\"   - 메모리 절감: {kv_info['reduction']:.2f}x\")\n",
    "\n",
    "print(\"\\n5. 공식 논문 일치성:\")\n",
    "print(\"   ✅ Global Attention: 첫/중간/마지막 (0, 15, 31)\")\n",
    "print(\"   ✅ Local Attention: Sliding Window (1024)\")\n",
    "print(\"   ✅ Meta Tokens: 128개\")\n",
    "print(\"   ✅ KV-Cache 공유: Local 레이어끼리 페어링\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
