{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "deec2e18",
      "metadata": {},
      "source": [
        "\n",
        "# Hymba (Hybrid Attention + Mamba) ‚Äî ÏµúÏã† ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä ¬∑ ÏûêÎèô Îç∞Ïù¥ÌÑ∞ Îã§Ïö¥Î°úÎìú ¬∑ **ORPO(SFT+DPO Í≥ÑÏó¥)** ÎèôÏãúÌïôÏäµ\n",
        "Ïù¥ ÎÖ∏Ìä∏Î∂ÅÏùÄ Í∏∞Ï°¥ Íµ¨ÌòÑÏùÑ Îã§ÏùåÍ≥º Í∞ôÏù¥ Î≥¥Í∞ïÌï©ÎãàÎã§.\n",
        "\n",
        "1) **ÏµúÏã† ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä**: ü§ó `tokenizers`Ïùò **Unigram**(SentencePiece Í≥ÑÏó¥)ÏúºÎ°ú **ÏßÅÏ†ë ÌïôÏäµ**ÌïòÏó¨ ÏÇ¨Ïö©  \n",
        "2) **Îç∞Ïù¥ÌÑ∞ ÏûêÎèô Îã§Ïö¥Î°úÎìú**: Í≤ΩÎ°úÍ∞Ä ÏóÜÏúºÎ©¥ ü§ó `datasets`ÏóêÏÑú ÏûêÎèô Î°úÎìú(ÌÜ†ÏπòÌÖçÏä§Ìä∏ ÎØ∏ÏÇ¨Ïö©)  \n",
        "3) **ORPO**: SFT ÏÜêÏã§ + **ÏÉÅÎåÄ Î°úÍ∑∏ Ïò§Ï¶à**(log-odds) Ìï≠ÏùÑ Í≤∞Ìï©Ìïú **Îã®Ïùº Îã®Í≥Ñ** ÏÑ†Ìò∏ ÏµúÏ†ÅÌôî Íµ¨ÌòÑ Î∞è Ìï¥ÏÑ§  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5e00d872",
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "import math, time, typing as T, os, warnings\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "from torch.nn.attention import sdpa_kernel, SDPBackend\n",
        "\n",
        "# env / warnings\n",
        "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5da7f6da",
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import Unigram\n",
        "from tokenizers.trainers import UnigramTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.normalizers import NFKC, Lowercase, Sequence as NormSeq\n",
        "\n",
        "def get_corpus(hf_spec:str=\"karpathy/tiny_shakespeare\") -> str:\n",
        "    ds = load_dataset(hf_spec)\n",
        "    col = \"text\" if \"text\" in ds[\"train\"].column_names else ds[\"train\"].column_names[0]\n",
        "    return \"\\n\\n\".join(ds[\"train\"][col])\n",
        "\n",
        "def train_unigram(text:str, vocab_size:int=8000, unk:str=\"<|unk|>\"):\n",
        "    tk = Tokenizer(Unigram())\n",
        "    tk.normalizer = NormSeq([NFKC(), Lowercase()])\n",
        "    tk.pre_tokenizer = Whitespace()\n",
        "    trainer = UnigramTrainer(vocab_size=vocab_size, special_tokens=[unk], unk_token=unk)\n",
        "    tk.train_from_iterator([text], trainer=trainer)\n",
        "\n",
        "    class Wrap:\n",
        "        def __init__(self, tk): self.tk=tk\n",
        "        def encode(self, s): return self.tk.encode(s).ids\n",
        "        def decode(self, ids): return self.tk.decode(ids)\n",
        "        @property\n",
        "        def vocab_size(self): return self.tk.get_vocab_size()\n",
        "    return Wrap(tk)\n",
        "\n",
        "def make_stream_dataset(tok, text:str, seq_len:int=512) -> TensorDataset:\n",
        "    import numpy as np\n",
        "    ids = np.array(tok.encode(text), dtype=np.int64)\n",
        "    if ids.size < seq_len+1: raise RuntimeError(\"Text too short\")\n",
        "    x = ids[:-1]; y = ids[1:]\n",
        "    n = (len(y)//seq_len)*seq_len\n",
        "    X = torch.tensor(x[:n].reshape(-1, seq_len))\n",
        "    Y = torch.tensor(y[:n].reshape(-1, seq_len))\n",
        "    return TensorDataset(X,Y)\n",
        "\n",
        "def build_dataloaders(tok, text:str, seq_len:int=512, bs:int=32, workers:int=0, pin:bool=True):\n",
        "    ds_full = make_stream_dataset(tok, text, seq_len)\n",
        "    tr_len = int(0.95*len(ds_full)); va_len = len(ds_full)-tr_len\n",
        "    tr, va = random_split(ds_full, [tr_len, va_len])\n",
        "    train_dl = DataLoader(tr, batch_size=bs, shuffle=True, drop_last=True, num_workers=workers, pin_memory=pin)\n",
        "    val_dl   = DataLoader(va, batch_size=bs, shuffle=False, num_workers=workers, pin_memory=pin)\n",
        "    return train_dl, val_dl\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "86e68997",
      "metadata": {},
      "outputs": [],
      "source": [
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, d:int, eps:float=1e-6):\n",
        "        super().__init__(); self.eps=eps; self.w=nn.Parameter(torch.ones(d))\n",
        "    def forward(self, x): return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True)+self.eps) * self.w\n",
        "\n",
        "class SwiGLU(nn.Module):\n",
        "    def __init__(self, d:int, mult:float=4.0, p:float=0.0):\n",
        "        super().__init__()\n",
        "        h=int(d*mult)\n",
        "        self.w1=nn.Linear(d,h,bias=False); self.w2=nn.Linear(d,h,bias=False); self.w3=nn.Linear(h,d,bias=False)\n",
        "        self.drop=nn.Dropout(p)\n",
        "    def forward(self, x): return self.w3(self.drop(F.silu(self.w1(x))*self.w2(x)))\n",
        "\n",
        "class RotaryEmbedding(nn.Module):\n",
        "    def __init__(self, dim:int, base:float=10000.0):\n",
        "        super().__init__()\n",
        "        inv = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
        "        self.register_buffer(\"_inv\", inv, persistent=False)\n",
        "        self.register_buffer(\"_cos\", None, persistent=False)\n",
        "        self.register_buffer(\"_sin\", None, persistent=False)\n",
        "    def _build(self, L:int, device, dtype):\n",
        "        if self._cos is not None and self._cos.size(0) >= L: return\n",
        "        t = torch.arange(L, device=device, dtype=self._inv.dtype)\n",
        "        freqs = torch.einsum(\"i,j->ij\", t, self._inv)\n",
        "        self._cos = torch.cos(freqs).to(dtype); self._sin = torch.sin(freqs).to(dtype)\n",
        "    def apply(self, x:torch.Tensor, pos:torch.Tensor):\n",
        "        # x: (..., Dh) with Dh even\n",
        "        self._build(int(pos.max().item())+1, x.device, x.dtype)\n",
        "        cos = self._cos.index_select(0, pos)[None,None,:,:]\n",
        "        sin = self._sin.index_select(0, pos)[None,None,:,:]\n",
        "        x1, x2 = x[..., ::2], x[..., 1::2]\n",
        "        o1 = x1*cos - x2*sin; o2 = x1*sin + x2*cos\n",
        "        return torch.stack([o1,o2], dim=-1).flatten(-2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "5debd3c0",
      "metadata": {},
      "outputs": [],
      "source": [
        "def _scaled_dot_attn(q, k, v, mask_2d: torch.Tensor | None, p: float, training: bool, is_causal: bool):\n",
        "    on_cuda = q.is_cuda\n",
        "    dtype_ok = q.dtype in (torch.float16, torch.bfloat16)\n",
        "    want_flash = on_cuda and dtype_ok and (mask_2d is None) and is_causal\n",
        "    if want_flash:\n",
        "        backends = [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION, SDPBackend.MATH]\n",
        "    else:\n",
        "        backends = [SDPBackend.EFFICIENT_ATTENTION, SDPBackend.MATH]\n",
        "    with sdpa_kernel(backends):\n",
        "        return F.scaled_dot_product_attention(\n",
        "            q, k, v,\n",
        "            attn_mask=mask_2d,\n",
        "            dropout_p=p if training else 0.0,\n",
        "            is_causal=is_causal and (mask_2d is None)\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "0487dc73",
      "metadata": {},
      "outputs": [],
      "source": [
        "class AttnLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    GQA + RoPE\n",
        "    - Ï∫êÏãúÏóêÎäî 'ÎπÑÌöåÏ†Ñ K/V'Îßå Ï†ÄÏû•\n",
        "    - Îß§ Ìò∏Ï∂ú Ïãú RoPEÎäî Ï°∞ÌöåÏö©ÏúºÎ°úÎßå Ï†ÅÏö©\n",
        "    - local=True ‚Üí Ïä¨ÎùºÏù¥Îî©ÏúàÎèÑÏö∞ SWA (window tokens)\n",
        "    \"\"\"\n",
        "    def __init__(self, d:int, n_heads:int, n_kv:int, local:bool=False, window:int=256, dropout:float=0.0):\n",
        "        super().__init__()\n",
        "        assert d % n_heads == 0, \"d must be divisible by n_heads\"\n",
        "        assert n_heads % n_kv == 0, \"n_heads must be divisible by n_kv for GQA\"\n",
        "        self.H = n_heads; self.KV = n_kv; self.Dh = d // n_heads\n",
        "        self.q = nn.Linear(d, n_heads*self.Dh, bias=False)\n",
        "        self.k = nn.Linear(d, n_kv*self.Dh, bias=False)\n",
        "        self.v = nn.Linear(d, n_kv*self.Dh, bias=False)\n",
        "        self.o = nn.Linear(n_heads*self.Dh, d, bias=False)\n",
        "        self.rope = RotaryEmbedding(self.Dh)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.local = local; self.window = window\n",
        "        self.rep = self.H // self.KV\n",
        "\n",
        "    def _local_slice(self, k, v):\n",
        "        if not self.local: return k, v\n",
        "        Tk = k.size(2)\n",
        "        w = min(self.window, Tk)\n",
        "        return k[:, :, Tk-w:Tk, :], v[:, :, Tk-w:Tk, :]\n",
        "\n",
        "    def _apply_rope_for_ranges(self, q, k_cat, Tc:int, T:int):\n",
        "        # q at positions [Tc-T, ..., Tc-1], k over [0, ..., Tc-1]\n",
        "        pos_q = torch.arange(Tc - T, Tc, device=q.device)\n",
        "        pos_k = torch.arange(0, Tc, device=k_cat.device)\n",
        "        q = self.rope.apply(q, pos_q)\n",
        "        k_rot = self.rope.apply(k_cat, pos_k)  # rotated view only\n",
        "        return q, k_rot\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        kv_cache: tuple[torch.Tensor, torch.Tensor] | None = None,\n",
        "        role: str = \"owner\",\n",
        "        global_mask: torch.Tensor | None = None,\n",
        "    ):\n",
        "        B, T, C = x.shape\n",
        "        q = self.q(x).view(B, T, self.H, self.Dh).transpose(1, 2)  # (B,H,T,Dh)\n",
        "\n",
        "        if role == \"follower\":\n",
        "            assert kv_cache is not None and kv_cache[0] is not None, \"Follower requires owner's KV cache\"\n",
        "            k_owner, v_owner = kv_cache                   # unrotated (B,KV,Tc,Dh)\n",
        "            Tc = k_owner.size(2)\n",
        "            # RoPE apply for q and k (rotated view only)\n",
        "            q, k_rot = self._apply_rope_for_ranges(q, k_owner, Tc=Tc, T=T)\n",
        "            v_full = v_owner\n",
        "            # KV‚ÜíH replicate\n",
        "            k_full = k_rot.repeat_interleave(self.rep, dim=1)  # (B,H,Tc,Dh)\n",
        "            v_full = v_full.repeat_interleave(self.rep, dim=1) # (B,H,Tc,Dh)\n",
        "            # local slice\n",
        "            k_full, v_full = self._local_slice(k_full, v_full)\n",
        "            Tk = k_full.size(2)\n",
        "            # SDPA\n",
        "            out = _scaled_dot_attn(\n",
        "                q.reshape(B*self.H, T, self.Dh),\n",
        "                k_full.reshape(B*self.H, Tk, self.Dh),\n",
        "                v_full.reshape(B*self.H, Tk, self.Dh),\n",
        "                mask_2d=None, p=float(self.drop.p), training=self.training, is_causal=True\n",
        "            )\n",
        "            out = out.view(B, self.H, T, self.Dh).transpose(1, 2).reshape(B, T, self.H * self.Dh)\n",
        "            return self.o(out), None\n",
        "\n",
        "        # owner path: project new K/V (unrotated), concat to unrotated cache\n",
        "        k_new = self.k(x).view(B, T, self.KV, self.Dh).transpose(1, 2)  # (B,KV,T,Dh)\n",
        "        v_new = self.v(x).view(B, T, self.KV, self.Dh).transpose(1, 2)\n",
        "        if kv_cache is not None and kv_cache[0] is not None and kv_cache[0].numel() > 0:\n",
        "            k_prev, v_prev = kv_cache  # unrotated\n",
        "            k_cat = torch.cat([k_prev, k_new], dim=2)\n",
        "            v_cat = torch.cat([v_prev, v_new], dim=2)\n",
        "        else:\n",
        "            k_cat, v_cat = k_new, v_new\n",
        "\n",
        "        Tc = k_cat.size(2)\n",
        "        # RoPE rotated view for attention only\n",
        "        q, k_rot = self._apply_rope_for_ranges(q, k_cat, Tc=Tc, T=T)\n",
        "\n",
        "        # KV‚ÜíH replicate\n",
        "        k_full = k_rot.repeat_interleave(self.rep, dim=1)\n",
        "        v_full = v_cat.repeat_interleave(self.rep, dim=1)\n",
        "        # local slice\n",
        "        k_full, v_full = self._local_slice(k_full, v_full)\n",
        "        Tk = k_full.size(2)\n",
        "\n",
        "        out = _scaled_dot_attn(\n",
        "            q.reshape(B*self.H, T, self.Dh),\n",
        "            k_full.reshape(B*self.H, Tk, self.Dh),\n",
        "            v_full.reshape(B*self.H, Tk, self.Dh),\n",
        "            mask_2d=None, p=float(self.drop.p), training=self.training, is_causal=True\n",
        "        )\n",
        "        out = out.view(B, self.H, T, self.Dh).transpose(1, 2).reshape(B, T, self.H * self.Dh)\n",
        "        out = self.o(out)\n",
        "\n",
        "        # return unrotated cache\n",
        "        return out, (k_cat, v_cat)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "8b0645d3",
      "metadata": {},
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\"ÌïòÎÇòÏùò Ïñ¥ÌÖêÏÖò(+FFN) Î∏îÎ°ù. local=TrueÎ©¥ SWA, FalseÎ©¥ Global.\"\"\"\n",
        "    def __init__(self, d:int, n_heads:int, n_kv:int, local:bool, window:int, dropout:float):\n",
        "        super().__init__()\n",
        "        self.pre = RMSNorm(d)\n",
        "        self.attn = AttnLayer(d, n_heads, n_kv, local=local, window=window, dropout=dropout)\n",
        "        self.post = RMSNorm(d)\n",
        "        self.ffn = SwiGLU(d, mult=4.0, p=dropout)\n",
        "    def forward(self, x, kv_cache=None, global_mask=None, training=True, role:str=\"owner\"):\n",
        "        h = self.pre(x)\n",
        "        a, new_cache = self.attn(h, kv_cache=kv_cache if not training else None, role=role, global_mask=global_mask)\n",
        "        x = x + a\n",
        "        x = x + self.ffn(self.post(x))\n",
        "        return x, new_cache\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "90281572",
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ModelCfg:\n",
        "    vocab_size: int\n",
        "    d_model: int = 384\n",
        "    n_layers: int = 12\n",
        "    n_heads: int = 6\n",
        "    n_kv_heads: int = 2\n",
        "    dropout: float = 0.0\n",
        "    seq_len: int = 512\n",
        "    swa_layers: T.Tuple[int,...] = (1,2,3,4,5,7,8,9,10)\n",
        "    swa_window: int = 256\n",
        "    num_meta_tokens: int = 0\n",
        "\n",
        "class HymbaV2(nn.Module):\n",
        "    def __init__(self, cfg:ModelCfg):\n",
        "        super().__init__(); self.cfg=cfg\n",
        "        self.embed = nn.Embedding(cfg.vocab_size, cfg.d_model)\n",
        "\n",
        "        self.meta = None\n",
        "        if cfg.num_meta_tokens > 0:\n",
        "            self.meta = nn.Parameter(torch.randn(1, cfg.num_meta_tokens, cfg.d_model) * 0.02)\n",
        "\n",
        "        self.blocks = nn.ModuleList()\n",
        "        self.swa_layers = set(cfg.swa_layers)\n",
        "        for li in range(cfg.n_layers):\n",
        "            is_local = (li in self.swa_layers)\n",
        "            self.blocks.append(Block(cfg.d_model, cfg.n_heads, cfg.n_kv_heads,\n",
        "                                     local=is_local, window=cfg.swa_window, dropout=cfg.dropout))\n",
        "        self.norm = RMSNorm(cfg.d_model)\n",
        "        self.head = nn.Linear(cfg.d_model, cfg.vocab_size, bias=False)\n",
        "\n",
        "        # KV-share Í∑∏Î£π/owner ÎèÑÏ∂ú\n",
        "        self.owner = list(range(cfg.n_layers))\n",
        "        self.kv_group_id = [0]*cfg.n_layers\n",
        "        swa = self.swa_layers\n",
        "        gid = -1; i=0; N=cfg.n_layers\n",
        "        while i < N:\n",
        "            if i in swa:\n",
        "                j=i\n",
        "                while j<N and (j in swa): j+=1\n",
        "                k=i\n",
        "                while k<j:\n",
        "                    if k+1<j:\n",
        "                        gid += 1\n",
        "                        self.kv_group_id[k]=gid; self.kv_group_id[k+1]=gid\n",
        "                        self.owner[k]=k; self.owner[k+1]=k\n",
        "                        k+=2\n",
        "                    else:\n",
        "                        gid += 1\n",
        "                        self.kv_group_id[k]=gid; self.owner[k]=k\n",
        "                        k+=1\n",
        "                i=j\n",
        "            else:\n",
        "                gid += 1\n",
        "                self.kv_group_id[i]=gid; self.owner[i]=i\n",
        "                i+=1\n",
        "\n",
        "    def forward(self, input_ids:torch.LongTensor, targets:torch.LongTensor|None=None):\n",
        "        \"\"\"\n",
        "        Meta tokens:\n",
        "          - ÏûÖÎ†• ÏûÑÎ≤†Îî© ÏïûÏóê [num_meta_tokens]Í∞úÎ•º prepend.\n",
        "          - lossÎäî Ìï≠ÏÉÅ 'Îã§Ïùå ÌÜ†ÌÅ∞ ÏòàÏ∏°' Í∑úÏπôÏúºÎ°ú Ï†ïÎ†¨.\n",
        "        \"\"\"\n",
        "        B, T = input_ids.shape\n",
        "        x = self.embed(input_ids)                         # (B,T,D)\n",
        "        M = 0\n",
        "        if self.meta is not None:\n",
        "            M = self.meta.size(1)\n",
        "            x = torch.cat([self.meta.expand(B, -1, -1), x], dim=1)  # (B,M+T,D)\n",
        "\n",
        "        h = x\n",
        "        for li, blk in enumerate(self.blocks):\n",
        "            h,_ = blk(h, kv_cache=None, global_mask=None, training=True, role=\"owner\")\n",
        "        h = self.norm(h)\n",
        "        logits = self.head(h)                             # (B,M+T,V)\n",
        "\n",
        "        out = {\"logits\": logits}\n",
        "        if targets is not None:\n",
        "            if M > 0:\n",
        "                # logits positions [M .. M+T-2] predict targets [1 .. T-1]\n",
        "                logits_for_loss = logits[:, M: M+T-1, :]\n",
        "                targets_for_loss = targets[:, 1:]\n",
        "            else:\n",
        "                logits_for_loss = logits[:, :-1, :]\n",
        "                targets_for_loss = targets[:, 1:]\n",
        "            loss = F.cross_entropy(\n",
        "                logits_for_loss.reshape(-1, logits.size(-1)),\n",
        "                targets_for_loss.reshape(-1)\n",
        "            )\n",
        "            out[\"loss\"] = loss\n",
        "        return out\n",
        "\n",
        "    # ------ Generate helper ------\n",
        "    def _owner_map_for(self, kv_share:bool):\n",
        "        return self.owner if kv_share else list(range(len(self.blocks)))\n",
        "\n",
        "    def _forward_blocks_once(self, h, owners, kv, global_mask=None):\n",
        "        for li, blk in enumerate(self.blocks):\n",
        "            owner = owners[li]\n",
        "            role = \"owner\" if li == owner else \"follower\"\n",
        "            h, kv_out = blk(h, kv_cache=kv[owner], global_mask=global_mask, training=False, role=role)\n",
        "            if li == owner:\n",
        "                kv[owner] = kv_out\n",
        "        return h, kv\n",
        "\n",
        "    def _forward_blocks_full_recompute(self, ids, global_mask=None):\n",
        "        h = self.embed(ids)\n",
        "        for blk in self.blocks:\n",
        "            h,_ = blk(h, kv_cache=None, global_mask=global_mask, training=False, role=\"owner\")\n",
        "        return h\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, input_ids:torch.LongTensor, max_new_tokens:int=64,\n",
        "                 temperature:float=1.0, top_k:int=0, eos_token_id:int|None=None,\n",
        "                 use_kv_cache:bool=True, kv_share:bool=True):\n",
        "        device = next(self.parameters()).device\n",
        "        self.eval()\n",
        "        ids = input_ids.to(device)\n",
        "\n",
        "        if use_kv_cache:\n",
        "            owners = self._owner_map_for(kv_share)\n",
        "            kv = [None]*len(self.blocks)\n",
        "            h = self.embed(ids)\n",
        "            h, kv = self._forward_blocks_once(h, owners, kv, global_mask=None)\n",
        "        else:\n",
        "            h = self._forward_blocks_full_recompute(ids, global_mask=None)\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            if use_kv_cache:\n",
        "                x_step = self.embed(ids[:, -1:])\n",
        "                h = x_step\n",
        "                h, kv = self._forward_blocks_once(h, owners, kv, global_mask=None)\n",
        "                h = self.norm(h); logits = self.head(h)[:, -1, :]\n",
        "            else:\n",
        "                h = self._forward_blocks_full_recompute(ids, global_mask=None)\n",
        "                h = self.norm(h); logits = self.head(h)[:, -1, :]\n",
        "\n",
        "            if temperature <= 0:\n",
        "                next_id = torch.argmax(logits, dim=-1, keepdim=True)\n",
        "            else:\n",
        "                logits = logits / temperature\n",
        "                if top_k and top_k < logits.size(-1):\n",
        "                    topk_vals, topk_idx = torch.topk(logits, top_k, dim=-1)\n",
        "                    mask = torch.full_like(logits, float(\"-inf\"))\n",
        "                    mask.scatter_(1, topk_idx, topk_vals)\n",
        "                    logits = mask\n",
        "                probs = F.softmax(logits, dim=-1)\n",
        "                next_id = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            ids = torch.cat([ids, next_id], dim=1)\n",
        "            if eos_token_id is not None and bool((next_id == eos_token_id).all()):\n",
        "                break\n",
        "        return ids\n",
        "\n",
        "    # ------ Utils ------\n",
        "    def layer_table(self):\n",
        "        import pandas as pd\n",
        "        rows=[]\n",
        "        for i,_ in enumerate(self.blocks):\n",
        "            rows.append({\n",
        "                \"layer\": i,\n",
        "                \"attn\": \"LOCAL(SWA)\" if i in self.swa_layers else \"GLOBAL\",\n",
        "                \"kv_owner\": self.owner[i],\n",
        "                \"kv_share_group\": self.kv_group_id[i],\n",
        "            })\n",
        "        return pd.DataFrame(rows)\n",
        "\n",
        "    def estimate_kv_cache_mb(self, seq_len:int, dtype=torch.float16):\n",
        "        Dh = self.cfg.d_model // self.cfg.n_heads\n",
        "        KV = max(1, self.cfg.n_kv_heads)\n",
        "        bytes_per = torch.finfo(dtype).bits // 8\n",
        "        owners = len(set(self.owner))\n",
        "        per_owner = 2 * KV * seq_len * Dh * bytes_per\n",
        "        return round(per_owner * owners / (1024**2), 3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "833120c7",
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class TrainCfg:\n",
        "    seq_len:int=512\n",
        "    batch_size:int=32\n",
        "    steps:int=600\n",
        "    lr:float=6e-4\n",
        "    warmup:int=100\n",
        "    amp:bool=True\n",
        "    wd:float=0.1\n",
        "    grad_clip:float=1.0\n",
        "\n",
        "def adamw_param_groups(model:nn.Module, wd:float):\n",
        "    decay, no_decay = [], []\n",
        "    for n,p in model.named_parameters():\n",
        "        if not p.requires_grad: continue\n",
        "        if p.dim() >= 2 and \"norm\" not in n.lower():\n",
        "            decay.append(p)\n",
        "        else:\n",
        "            no_decay.append(p)\n",
        "    return [{\"params\": decay, \"weight_decay\": wd},\n",
        "            {\"params\": no_decay, \"weight_decay\": 0.0}]\n",
        "\n",
        "def train_loop(model:HymbaV2, train_dl, val_dl, tcfg:TrainCfg, device:str=\"cuda\"):\n",
        "    import itertools, math\n",
        "    from transformers import get_cosine_schedule_with_warmup\n",
        "    from torch.amp import GradScaler, autocast\n",
        "\n",
        "    torch.manual_seed(1337)\n",
        "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(1337)\n",
        "\n",
        "    model.to(device).train()\n",
        "    pg = adamw_param_groups(model, wd=tcfg.wd)\n",
        "    opt = torch.optim.AdamW(pg, lr=tcfg.lr, betas=(0.9,0.95), eps=1e-8,\n",
        "                            fused=torch.cuda.is_available())\n",
        "    sch = get_cosine_schedule_with_warmup(opt, tcfg.warmup, tcfg.steps)\n",
        "    scaler = GradScaler(device=\"cuda\" if (device==\"cuda\" and torch.cuda.is_available()) else \"cpu\",\n",
        "                        enabled=tcfg.amp)\n",
        "\n",
        "    it = itertools.cycle(train_dl)\n",
        "    step=0; tok_count=0; train_nll=0.0; train_tok=0\n",
        "    t0=time.time()\n",
        "\n",
        "    while step < tcfg.steps:\n",
        "        xb,yb = next(it)\n",
        "        xb,yb = xb.to(device,non_blocking=True), yb.to(device,non_blocking=True)\n",
        "        with autocast(device_type=(\"cuda\" if (device==\"cuda\" and torch.cuda.is_available()) else \"cpu\"),\n",
        "                      enabled=tcfg.amp):\n",
        "            out = model(xb, targets=yb)\n",
        "            loss = out[\"loss\"]\n",
        "\n",
        "        train_nll += float(loss.detach())*xb.numel(); train_tok += xb.numel()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.unscale_(opt)\n",
        "        if tcfg.grad_clip>0: nn.utils.clip_grad_norm_(model.parameters(), tcfg.grad_clip)\n",
        "        scaler.step(opt); scaler.update(); opt.zero_grad(set_to_none=True); sch.step()\n",
        "        step += 1; tok_count += xb.numel()\n",
        "\n",
        "        if step==1 or step%50==0:\n",
        "            lr_now = opt.param_groups[0][\"lr\"]\n",
        "            print(f\"[{step:5d}] loss={loss.item():.3f} lr={lr_now:.2e}\")\n",
        "\n",
        "    elapsed = time.time()-t0\n",
        "    tps = int(tok_count/max(1e-9, elapsed))\n",
        "    train_loss = train_nll/max(1,train_tok)\n",
        "\n",
        "    # validation\n",
        "    model.eval(); val_nll=0.0; val_tok=0\n",
        "    with torch.no_grad(), torch.amp.autocast(\"cuda\", enabled=tcfg.amp and (device==\"cuda\")):\n",
        "        for xb,yb in val_dl:\n",
        "            xb,yb = xb.to(device), yb.to(device)\n",
        "            out = model(xb, targets=yb)\n",
        "            val_nll += float(out[\"loss\"].detach())*xb.numel(); val_tok += xb.numel()\n",
        "    val_loss = val_nll/max(1,val_tok); ppl = math.exp(val_loss)\n",
        "    return {\"train_loss\": float(train_loss), \"val_loss\": float(val_loss), \"ppl\": float(ppl), \"tps\": tps}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "ca2c2848",
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_everything(seq_len:int=512, bs:int=32, vocab_size:int=8000):\n",
        "    text = get_corpus(\"karpathy/tiny_shakespeare\")\n",
        "    tok = train_unigram(text, vocab_size=vocab_size)\n",
        "    train_dl, val_dl = build_dataloaders(tok, text, seq_len=seq_len, bs=bs)\n",
        "\n",
        "    cfg = ModelCfg(vocab_size=tok.vocab_size, seq_len=seq_len)\n",
        "    model = HymbaV2(cfg)\n",
        "    return model, tok, train_dl, val_dl\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "308de928",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "[    1] loss=8.795 lr=6.00e-06\n",
            "[   50] loss=5.852 lr=3.00e-04\n",
            "[  100] loss=4.739 lr=6.00e-04\n",
            "[  150] loss=2.822 lr=5.95e-04\n",
            "[  200] loss=1.680 lr=5.82e-04\n",
            "[  250] loss=0.397 lr=5.60e-04\n",
            "[  300] loss=0.048 lr=5.30e-04\n",
            "[  350] loss=0.009 lr=4.93e-04\n",
            "[  400] loss=0.012 lr=4.50e-04\n",
            "[  450] loss=0.003 lr=4.03e-04\n",
            "[  500] loss=0.003 lr=3.52e-04\n",
            "[  550] loss=0.002 lr=3.00e-04\n",
            "[  600] loss=0.001 lr=2.48e-04\n",
            "[  650] loss=0.000 lr=1.97e-04\n",
            "[  700] loss=0.000 lr=1.50e-04\n",
            "[  750] loss=0.000 lr=1.07e-04\n",
            "[  800] loss=0.000 lr=7.02e-05\n",
            "[  850] loss=0.000 lr=4.02e-05\n",
            "[  900] loss=0.000 lr=1.81e-05\n",
            "[  950] loss=0.000 lr=4.56e-06\n",
            "[ 1000] loss=0.000 lr=0.00e+00\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'train_loss': 0.9886284915425604,\n",
              " 'val_loss': 0.4139203131198883,\n",
              " 'ppl': 1.5127365768230914,\n",
              " 'tps': 176810}"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# (E) Ïã§Ìñâ\n",
        "model, tok, train_dl, val_dl = build_everything(seq_len=512, bs=32, vocab_size=8000)\n",
        "tcfg = TrainCfg(steps=1000, warmup=100, amp=True)\n",
        "stats = train_loop(model, train_dl, val_dl, tcfg, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "stats\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e90e0b9",
      "metadata": {},
      "source": [
        "\n",
        "## 9) **ORPO (SFT + Î°úÍ∑∏ Ïò§Ï¶à ÏÑ†Ìò∏Ìï≠)** Íµ¨ÌòÑ Î∞è Ìï¥ÏÑ§\n",
        "- **ÌïµÏã¨**: SFTÏùò NLL ÏÜêÏã§Ïóê **ÏÑ†Ìò∏ ÎåÄÎπÑ Ìï≠**ÏùÑ Ï∂îÍ∞Ä  \n",
        "- ORPO ÎÖºÎ¨∏ÏùÄ **Ï∞∏Ï°∞ Î™®Îç∏ ÏóÜÏù¥**, `L = L_SFT + Œ≤ ¬∑ L_ratio`Î•º Ï†úÏïà  \n",
        "- Ïó¨Í∏∞ÏÑú `L_ratio = -log œÉ( Œî )`, `Œî ‚âà (avg log p_Œ∏(chosen|x) - avg log p_Œ∏(rejected|x))`  \n",
        "  - Ïã§Î¨¥Ï†ÅÏúºÎ°ú TRL Íµ¨ÌòÑÏùÄ **Í∏∏Ïù¥ Ï†ïÍ∑úÌôîÎêú ÌÜ†ÌÅ∞ ÌèâÍ∑† Î°úÍ∑∏ÌôïÎ•† Ï∞®Ïù¥**Ïóê Î°úÏßÄÏä§Ìã±ÏùÑ Ï†ÅÏö©Ìï©ÎãàÎã§.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "d7b8ac2b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "74dc31501eb04b4197eea63c5160d693",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ab6e6026c06d48b99aaa430431d8929f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "harmless-base/train.jsonl.gz:   0%|          | 0.00/13.2M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c71240465a5c4cc1a71447053c9bf4e6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "helpful-base/train.jsonl.gz:   0%|          | 0.00/16.2M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "95ad7c6252b0480da408f644176e0332",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "helpful-online/train.jsonl.gz:   0%|          | 0.00/20.1M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a393094b65b74e6087a0c7f6da32a44b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "helpful-rejection-sampled/train.jsonl.gz:   0%|          | 0.00/25.7M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "51eea33c6080418aace2f8e313feebdd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "harmless-base/test.jsonl.gz:   0%|          | 0.00/743k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5ea958fb99a94274b8a045d5bac35da9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "helpful-base/test.jsonl.gz:   0%|          | 0.00/875k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b1c8aea6f585423496b02612ec41485b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "helpful-online/test.jsonl.gz:   0%|          | 0.00/1.05M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8c1a237dba3a44b8a851549dbefdb701",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "helpful-rejection-sampled/test.jsonl.gz:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ebbc9e3bebd14b64bb3a668cbced9f4d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/160800 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0d2f79ff75ee466daa3e95383ff426f3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split:   0%|          | 0/8552 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "160060"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ===============================\n",
        "# ORPO: HF Îç∞Ïù¥ÌÑ∞ÏÖã Î°úÎìú ‚Üí pairs ÏÉùÏÑ±\n",
        "# ===============================\n",
        "# !pip install -q datasets\n",
        "from datasets import load_dataset\n",
        "\n",
        "# ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Ïòà:\n",
        "#   \"Anthropic/hh-rlhf\"\n",
        "#   \"Dahoas/rm-static\"\n",
        "#   \"HuggingFaceH4/ultrafeedback_binarized\"\n",
        "DATASET = \"Anthropic/hh-rlhf\"\n",
        "\n",
        "ds = load_dataset(DATASET)\n",
        "\n",
        "def make_pairs_from_hf(ds, dataset_name:str):\n",
        "    \"\"\"\n",
        "    Î∞òÌôò ÌòïÏãù: [(prompt, chosen, rejected), ...]\n",
        "    \"\"\"\n",
        "    pairs = []\n",
        "    split = \"train\" if \"train\" in ds else list(ds.keys())[0]\n",
        "    for ex in ds[split]:\n",
        "        if \"Anthropic/hh-rlhf\" in dataset_name:\n",
        "            prompt = \"\"\n",
        "            pos = ex[\"chosen\"]\n",
        "            neg = ex[\"rejected\"]\n",
        "        elif \"Dahoas/rm-static\" in dataset_name:\n",
        "            prompt = ex.get(\"prompt\",\"\")\n",
        "            pos = ex.get(\"chosen\", ex.get(\"response\",\"\"))\n",
        "            neg = ex.get(\"rejected\",\"\")\n",
        "        elif \"ultrafeedback_binarized\" in dataset_name:\n",
        "            prompt = ex.get(\"prompt\",\"\")\n",
        "            pos = ex.get(\"chosen\",\"\")\n",
        "            neg = ex.get(\"rejected\",\"\")\n",
        "        else:\n",
        "            continue\n",
        "        if pos and neg and pos != neg:\n",
        "            pairs.append((prompt, pos, neg))\n",
        "    if not pairs:\n",
        "        raise RuntimeError(\"pairs ÏÉùÏÑ± Ïã§Ìå®: Ïª¨Îüº ÌôïÏù∏ ÌïÑÏöî\")\n",
        "    return pairs\n",
        "\n",
        "pairs = make_pairs_from_hf(ds, DATASET)\n",
        "len(pairs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "189d8a1f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# ORPO ÌÖçÏä§Ìä∏ ‚Üí ÌÜ†ÌÅ∞ ÌÖêÏÑú Î¶¨Ïä§Ìä∏ ÏÉùÏÑ±Í∏∞\n",
        "# ==========================================\n",
        "import torch\n",
        "from typing import List, Tuple\n",
        "\n",
        "# Ï†ÑÏ†ú: tok, seq_len Í∞Ä Ï°¥Ïû¨Ìï¥Ïïº Ìï®\n",
        "\n",
        "def _chunks(ids: List[int], need:int, seq_len:int) -> List[List[int]]:\n",
        "    out=[]\n",
        "    L=len(ids)\n",
        "    for s in range(0, L - need + 1, seq_len):\n",
        "        out.append(ids[s:s+need])\n",
        "    return out\n",
        "\n",
        "def build_orpo_tensors(\n",
        "    pairs: List[Tuple[str,str,str]],\n",
        "    tok,\n",
        "    seq_len: int\n",
        "):\n",
        "    \"\"\"\n",
        "    Î∞òÌôò:\n",
        "      pos_ids_list, pos_tgts_list, neg_ids_list, neg_tgts_list\n",
        "      Í∞Å ÏõêÏÜå ÌÖêÏÑú ÌÅ¨Í∏∞: (seq_len,)\n",
        "    \"\"\"\n",
        "    pos_ids_list=[]; pos_tgts_list=[]\n",
        "    neg_ids_list=[]; neg_tgts_list=[]\n",
        "    need = seq_len + 1  # ÏûÖÎ†• T, ÌÉÄÍπÉ T Ï†ïÎ†¨Ïö©\n",
        "\n",
        "    for prompt, pos, neg in pairs:\n",
        "        text_pos = (prompt + \"\\n\" + pos) if prompt else pos\n",
        "        text_neg = (prompt + \"\\n\" + neg) if prompt else neg\n",
        "\n",
        "        pos_ids = tok.encode(text_pos)\n",
        "        neg_ids = tok.encode(text_neg)\n",
        "\n",
        "        pos_chunks = _chunks(pos_ids, need, seq_len)\n",
        "        neg_chunks = _chunks(neg_ids, need, seq_len)\n",
        "\n",
        "        m = min(len(pos_chunks), len(neg_chunks))\n",
        "        for i in range(m):\n",
        "            p = pos_chunks[i]; n = neg_chunks[i]\n",
        "\n",
        "            p_x = torch.tensor(p[:-1], dtype=torch.long)  # (T,)\n",
        "            p_y = torch.tensor(p[1:],  dtype=torch.long)  # (T,)\n",
        "            n_x = torch.tensor(n[:-1], dtype=torch.long)\n",
        "            n_y = torch.tensor(n[1:],  dtype=torch.long)\n",
        "\n",
        "            pos_ids_list.append(p_x); pos_tgts_list.append(p_y)\n",
        "            neg_ids_list.append(n_x); neg_tgts_list.append(n_y)\n",
        "\n",
        "    if not (len(pos_ids_list)==len(neg_ids_list)==len(pos_tgts_list)==len(neg_tgts_list)):\n",
        "        raise RuntimeError(\"ORPO Î¶¨Ïä§Ìä∏ Í∏∏Ïù¥ Î∂àÏùºÏπò\")\n",
        "    if len(pos_ids_list)==0:\n",
        "        raise RuntimeError(\"ÏÉòÌîå 0Í∞ú. seq_lenÏùÑ Ï§ÑÏù¥Í±∞ÎÇò ÌÖçÏä§Ìä∏ Í∏∏Ïù¥Î•º ÎäòÎ¶¥ Í≤É\")\n",
        "\n",
        "    return pos_ids_list, pos_tgts_list, neg_ids_list, neg_tgts_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "fd577659",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =====================\n",
        "# ORPO Í∏∞Î≥∏ Ïú†Ìã∏/Î°úÏä§\n",
        "# =====================\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@torch.no_grad()\n",
        "def _meta_tokens(model) -> int:\n",
        "    return 0 if getattr(model, \"meta\", None) is None else int(model.meta.size(1))\n",
        "\n",
        "def _seq_logprob(model, input_ids:torch.LongTensor, targets:torch.LongTensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    returns:\n",
        "      token_logp_sum: (B,)\n",
        "      token_count:    (B,)\n",
        "    Î™®Îç∏Ïùò loss Ï†ïÎ†¨Í≥º ÎèôÏùºÌïòÍ≤å Ï≤òÎ¶¨:\n",
        "      - meta M>0: logits[:, M:M+T-1] vs targets[:, 1:]\n",
        "      - meta M=0: logits[:, :-1]     vs targets[:, 1:]\n",
        "    \"\"\"\n",
        "    out = model(input_ids, targets=None)\n",
        "    logits = out[\"logits\"]  # (B, M+T, V)\n",
        "    B, L, V = logits.shape\n",
        "    M = _meta_tokens(model)\n",
        "\n",
        "    if M > 0:\n",
        "        logit_slice = logits[:, M:L-1, :]    # (B, T-1, V)\n",
        "        gold_tokens = targets[:, 1:]         # (B, T-1)\n",
        "    else:\n",
        "        logit_slice = logits[:, :L-1, :]\n",
        "        gold_tokens = targets[:, 1:]\n",
        "\n",
        "    logp = F.log_softmax(logit_slice, dim=-1)                         # (B, T-1, V)\n",
        "    token_logp = torch.gather(logp, 2, gold_tokens.unsqueeze(-1)).squeeze(-1)  # (B, T-1)\n",
        "    token_logp_sum = token_logp.sum(dim=1)                             # (B,)\n",
        "    token_count = torch.full((B,), token_logp.size(1), device=token_logp.device, dtype=torch.long)\n",
        "    return token_logp_sum, token_count\n",
        "\n",
        "@dataclass\n",
        "class ORPOCfg:\n",
        "    beta: float = 1.0\n",
        "    sft_weight: float = 1.0\n",
        "    amp: bool = True\n",
        "\n",
        "def orpo_loss(model:nn.Module,\n",
        "              pos_ids:torch.LongTensor, pos_tgts:torch.LongTensor,\n",
        "              neg_ids:torch.LongTensor, neg_tgts:torch.LongTensor,\n",
        "              cfg:ORPOCfg) -> dict:\n",
        "    \"\"\"\n",
        "    L = sft_weight * CE(y+|x)  +  mean( -log œÉ[ beta * (log p(y+|x) - log p(y-|x)) ] )\n",
        "    Î∞òÌôò: {\"loss\", \"sft_loss\", \"orpo_term\", \"margin_mean\"}\n",
        "    \"\"\"\n",
        "    sft_out = model(pos_ids, targets=pos_tgts)\n",
        "    sft_loss = sft_out[\"loss\"]\n",
        "\n",
        "    pos_lp_sum, pos_cnt = _seq_logprob(model, pos_ids, pos_tgts)\n",
        "    neg_lp_sum, neg_cnt = _seq_logprob(model, neg_ids, neg_tgts)\n",
        "\n",
        "    pos_lp = pos_lp_sum / pos_cnt.clamp_min(1)\n",
        "    neg_lp = neg_lp_sum / neg_cnt.clamp_min(1)\n",
        "\n",
        "    margin = pos_lp - neg_lp                       # (B,)\n",
        "    orpo_term = -F.logsigmoid(cfg.beta * margin)   # (B,)\n",
        "    loss = cfg.sft_weight * sft_loss + orpo_term.mean()\n",
        "\n",
        "    return {\n",
        "        \"loss\": loss,\n",
        "        \"sft_loss\": sft_loss.detach(),\n",
        "        \"orpo_term\": orpo_term.mean().detach(),\n",
        "        \"margin_mean\": margin.mean().detach(),\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "06ef3a77",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================\n",
        "# ORPOÏö© Pair Îç∞Ïù¥ÌÑ∞ÏÖã/Collate\n",
        "# =============================\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class PairDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Í∞Å ÏïÑÏù¥ÌÖú: (pos_ids, pos_tgts, neg_ids, neg_tgts)\n",
        "    Í∞Å ÌÖêÏÑú ÌÅ¨Í∏∞: (T,)\n",
        "    \"\"\"\n",
        "    def __init__(self, pos_ids, pos_tgts, neg_ids, neg_tgts):\n",
        "        assert len(pos_ids)==len(neg_ids)==len(pos_tgts)==len(neg_tgts)\n",
        "        self.pos_ids = pos_ids\n",
        "        self.pos_tgts = pos_tgts\n",
        "        self.neg_ids = neg_ids\n",
        "        self.neg_tgts = neg_tgts\n",
        "    def __len__(self): return len(self.pos_ids)\n",
        "    def __getitem__(self, i):\n",
        "        return (self.pos_ids[i], self.pos_tgts[i],\n",
        "                self.neg_ids[i], self.neg_tgts[i])\n",
        "\n",
        "def collate_pairs(batch):\n",
        "    pos_ids  = torch.stack([b[0] for b in batch], dim=0)\n",
        "    pos_tgts = torch.stack([b[1] for b in batch], dim=0)\n",
        "    neg_ids  = torch.stack([b[2] for b in batch], dim=0)\n",
        "    neg_tgts = torch.stack([b[3] for b in batch], dim=0)\n",
        "    return pos_ids, pos_tgts, neg_ids, neg_tgts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "630387cb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================\n",
        "# ORPOÏö© Pair Îç∞Ïù¥ÌÑ∞ÏÖã/Collate\n",
        "# =============================\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class PairDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Í∞Å ÏïÑÏù¥ÌÖú: (pos_ids, pos_tgts, neg_ids, neg_tgts)\n",
        "    Í∞Å ÌÖêÏÑú ÌÅ¨Í∏∞: (T,)\n",
        "    \"\"\"\n",
        "    def __init__(self, pos_ids, pos_tgts, neg_ids, neg_tgts):\n",
        "        assert len(pos_ids)==len(neg_ids)==len(pos_tgts)==len(neg_tgts)\n",
        "        self.pos_ids = pos_ids\n",
        "        self.pos_tgts = pos_tgts\n",
        "        self.neg_ids = neg_ids\n",
        "        self.neg_tgts = neg_tgts\n",
        "    def __len__(self): return len(self.pos_ids)\n",
        "    def __getitem__(self, i):\n",
        "        return (self.pos_ids[i], self.pos_tgts[i],\n",
        "                self.neg_ids[i], self.neg_tgts[i])\n",
        "\n",
        "def collate_pairs(batch):\n",
        "    pos_ids  = torch.stack([b[0] for b in batch], dim=0)\n",
        "    pos_tgts = torch.stack([b[1] for b in batch], dim=0)\n",
        "    neg_ids  = torch.stack([b[2] for b in batch], dim=0)\n",
        "    neg_tgts = torch.stack([b[3] for b in batch], dim=0)\n",
        "    return pos_ids, pos_tgts, neg_ids, neg_tgts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "9f773ab2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================\n",
        "# ORPO ÌïôÏäµ Î£®ÌîÑ\n",
        "# ================\n",
        "from torch.amp import autocast, GradScaler\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class ORPOTrainCfg:\n",
        "    steps:int = 1000\n",
        "    batch_size:int = 16\n",
        "    lr:float = 2e-5\n",
        "    warmup:int = 100\n",
        "    wd:float = 0.0\n",
        "    grad_clip:float = 1.0\n",
        "    beta:float = 1.0\n",
        "    sft_weight:float = 1.0\n",
        "    amp:bool = True\n",
        "\n",
        "def _adamw_groups(model:nn.Module, wd:float):\n",
        "    decay, no_decay = [], []\n",
        "    for n,p in model.named_parameters():\n",
        "        if not p.requires_grad: continue\n",
        "        if p.dim() >= 2 and \"norm\" not in n.lower():\n",
        "            decay.append(p)\n",
        "        else:\n",
        "            no_decay.append(p)\n",
        "    return [{\"params\": decay, \"weight_decay\": wd},\n",
        "            {\"params\": no_decay, \"weight_decay\": 0.0}]\n",
        "\n",
        "def train_orpo(model:nn.Module, pair_loader:DataLoader, val_loader:DataLoader|None,\n",
        "               cfg:ORPOTrainCfg, device:str=\"cuda\"):\n",
        "    import itertools, time, math\n",
        "    from transformers import get_cosine_schedule_with_warmup\n",
        "\n",
        "    torch.manual_seed(1337)\n",
        "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(1337)\n",
        "\n",
        "    model.to(device).train()\n",
        "    pg = _adamw_groups(model, wd=cfg.wd)\n",
        "    opt = torch.optim.AdamW(pg, lr=cfg.lr, betas=(0.9,0.95), eps=1e-8,\n",
        "                            fused=torch.cuda.is_available())\n",
        "    sch = get_cosine_schedule_with_warmup(opt, cfg.warmup, cfg.steps)\n",
        "    scaler = GradScaler(device=\"cuda\" if (device==\"cuda\" and torch.cuda.is_available()) else \"cpu\",\n",
        "                        enabled=cfg.amp)\n",
        "\n",
        "    it = itertools.cycle(pair_loader)\n",
        "    step=0; t0=time.time()\n",
        "    log = []\n",
        "\n",
        "    while step < cfg.steps:\n",
        "        pos_ids, pos_tgts, neg_ids, neg_tgts = next(it)\n",
        "        pos_ids = pos_ids.to(device, non_blocking=True)\n",
        "        pos_tgts = pos_tgts.to(device, non_blocking=True)\n",
        "        neg_ids = neg_ids.to(device, non_blocking=True)\n",
        "        neg_tgts = neg_tgts.to(device, non_blocking=True)\n",
        "\n",
        "        with autocast(device_type=(\"cuda\" if (device==\"cuda\" and torch.cuda.is_available()) else \"cpu\"),\n",
        "                      enabled=cfg.amp):\n",
        "            loss_dict = orpo_loss(\n",
        "                model,\n",
        "                pos_ids, pos_tgts,\n",
        "                neg_ids, neg_tgts,\n",
        "                ORPOCfg(beta=cfg.beta, sft_weight=cfg.sft_weight, amp=cfg.amp),\n",
        "            )\n",
        "            loss = loss_dict[\"loss\"]\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.unscale_(opt)\n",
        "        if cfg.grad_clip>0: nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
        "        scaler.step(opt); scaler.update(); opt.zero_grad(set_to_none=True); sch.step()\n",
        "        step += 1\n",
        "\n",
        "        if step==1 or step%50==0:\n",
        "            print(f\"[{step:5d}] loss={loss.item():.4f} sft={loss_dict['sft_loss']:.4f} \"\n",
        "                  f\"orpo={loss_dict['orpo_term']:.4f} margin={loss_dict['margin_mean']:.4f}\")\n",
        "        if step%50==0:\n",
        "            log.append({k: float(v) for k,v in loss_dict.items()})\n",
        "\n",
        "    elapsed = time.time()-t0\n",
        "    metrics = {\"train_time_s\": elapsed}\n",
        "    if val_loader is not None:\n",
        "        model.eval()\n",
        "        wins=0; total=0; nll_sum=0.0; tok_sum=0\n",
        "        with torch.no_grad(), torch.amp.autocast(\"cuda\", enabled=cfg.amp and (device==\"cuda\")):\n",
        "            for pos_ids, pos_tgts, neg_ids, neg_tgts in val_loader:\n",
        "                pos_ids = pos_ids.to(device); pos_tgts = pos_tgts.to(device)\n",
        "                neg_ids = neg_ids.to(device); neg_tgts = neg_tgts.to(device)\n",
        "                pos_lp_sum, pos_cnt = _seq_logprob(model, pos_ids, pos_tgts)\n",
        "                neg_lp_sum, neg_cnt = _seq_logprob(model, neg_ids, neg_tgts)\n",
        "                wins += int((pos_lp_sum/pos_cnt - neg_lp_sum/neg_cnt).sum().item() > 0)\n",
        "                total += pos_ids.size(0)\n",
        "\n",
        "                out = model(pos_ids, targets=pos_tgts)\n",
        "                nll_sum += float(out[\"loss\"]) * pos_ids.numel()\n",
        "                tok_sum += pos_ids.numel()\n",
        "        if total>0: metrics[\"win_rate\"] = wins/total\n",
        "        if tok_sum>0:\n",
        "            val_loss = nll_sum/tok_sum\n",
        "            metrics[\"val_ppl_pos\"] = math.exp(val_loss)\n",
        "        model.train()\n",
        "    return metrics, log\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "8932dd58",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[    1] loss=1.7929 sft=1.0949 orpo=0.6980 margin=-0.0096\n",
            "[   50] loss=1.4319 sft=0.7454 orpo=0.6864 margin=0.0137\n",
            "[  100] loss=1.1596 sft=0.4718 orpo=0.6878 margin=0.0110\n",
            "[  150] loss=1.0718 sft=0.3793 orpo=0.6925 margin=0.0012\n",
            "[  200] loss=1.0267 sft=0.3352 orpo=0.6915 margin=0.0033\n",
            "[  250] loss=0.9722 sft=0.2789 orpo=0.6932 margin=-0.0001\n",
            "[  300] loss=0.9404 sft=0.2466 orpo=0.6939 margin=-0.0014\n",
            "[  350] loss=0.9272 sft=0.2329 orpo=0.6943 margin=-0.0022\n",
            "[  400] loss=0.9191 sft=0.2273 orpo=0.6918 margin=0.0027\n",
            "[  450] loss=0.8798 sft=0.1853 orpo=0.6945 margin=-0.0027\n",
            "[  500] loss=0.8676 sft=0.1726 orpo=0.6950 margin=-0.0037\n",
            "[  550] loss=0.8548 sft=0.1599 orpo=0.6949 margin=-0.0035\n",
            "[  600] loss=0.8440 sft=0.1504 orpo=0.6936 margin=-0.0009\n",
            "[  650] loss=0.8272 sft=0.1301 orpo=0.6971 margin=-0.0078\n",
            "[  700] loss=0.8155 sft=0.1250 orpo=0.6905 margin=0.0053\n",
            "[  750] loss=0.8474 sft=0.1537 orpo=0.6937 margin=-0.0010\n",
            "[  800] loss=0.8161 sft=0.1213 orpo=0.6948 margin=-0.0033\n",
            "[  850] loss=0.8145 sft=0.1209 orpo=0.6936 margin=-0.0008\n",
            "[  900] loss=0.8055 sft=0.1119 orpo=0.6936 margin=-0.0009\n",
            "[  950] loss=0.8105 sft=0.1170 orpo=0.6936 margin=-0.0008\n",
            "[ 1000] loss=0.8175 sft=0.1245 orpo=0.6930 margin=0.0003\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'train_time_s': 163.42869114875793,\n",
              " 'win_rate': 0.03599120952025542,\n",
              " 'val_ppl_pos': 1.1234024763550756}"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# =================\n",
        "# ÏÇ¨Ïö© ÏòàÏãú (Ï†ÑÏ≤¥)\n",
        "# =================\n",
        "# 1) HF pairs ‚Üí ÌÖêÏÑú Î¶¨Ïä§Ìä∏\n",
        "seq_len = 512\n",
        "pos_ids_list, pos_tgts_list, neg_ids_list, neg_tgts_list = build_orpo_tensors(pairs, tok, seq_len)\n",
        "\n",
        "# 2) DataLoader\n",
        "pair_ds = PairDataset(pos_ids_list, pos_tgts_list, neg_ids_list, neg_tgts_list)\n",
        "pair_dl = DataLoader(pair_ds, batch_size=16, shuffle=True, drop_last=True, collate_fn=collate_pairs)\n",
        "val_dl  = DataLoader(pair_ds, batch_size=16, shuffle=False, drop_last=False, collate_fn=collate_pairs)\n",
        "\n",
        "# 3) ÌïôÏäµ\n",
        "orpo_cfg = ORPOTrainCfg(steps=1000, batch_size=16, lr=2e-5, warmup=100, beta=1.0, sft_weight=1.0, amp=True)\n",
        "metrics, log = train_orpo(model, pair_dl, val_dl, orpo_cfg, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aea11bbc",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
