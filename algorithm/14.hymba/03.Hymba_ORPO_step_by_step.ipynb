{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "deec2e18",
      "metadata": {},
      "source": [
        "\n",
        "# Hymba (Hybrid Attention + Mamba) — 최신 토크나이저 · 자동 데이터 다운로드 · **ORPO(SFT+DPO 계열)** 동시학습\n",
        "이 노트북은 기존 구현을 다음과 같이 보강합니다.\n",
        "\n",
        "1) **최신 토크나이저**: 🤗 `tokenizers`의 **Unigram**(SentencePiece 계열)으로 **직접 학습**하여 사용  \n",
        "2) **데이터 자동 다운로드**: 경로가 없으면 🤗 `datasets`에서 자동 로드(토치텍스트 미사용)  \n",
        "3) **ORPO**: SFT 손실 + **상대 로그 오즈**(log-odds) 항을 결합한 **단일 단계** 선호 최적화 구현 및 해설  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c61796c3",
      "metadata": {},
      "source": [
        "\n",
        "## 0) 의존성 설치 (파이썬 코드로 실행)\n",
        "- `tokenizers`의 Unigram 모델, `datasets`, `transformers`, `mamba-ssm`(가능 시) 등 설치\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b672696",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys, subprocess, pkg_resources\n",
        "\n",
        "def pip_install(pkgs):\n",
        "    for p in pkgs:\n",
        "        try:\n",
        "            pkg_resources.get_distribution(p.split('==')[0].split('>=')[0])\n",
        "        except Exception:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", p])\n",
        "\n",
        "pip_install([\n",
        "    \"torch>=2.2.0\",\n",
        "    \"datasets>=2.18.0\",\n",
        "    \"transformers>=4.41.0\",\n",
        "    \"tokenizers>=0.15.2\",\n",
        "    \"accelerate>=0.32.0\",\n",
        "    \"mamba-ssm>=2.2.2\",\n",
        "    \"matplotlib>=3.8.0\"\n",
        "])\n",
        "\n",
        "import os, math, time, dataclasses, typing as T\n",
        "import numpy as np\n",
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
        "from datasets import load_dataset, Dataset as HFDataset\n",
        "from transformers import get_cosine_schedule_with_warmup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"device:\", device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "680604e2",
      "metadata": {},
      "source": [
        "\n",
        "## 1) 설정\n",
        "- 토크나이저는 **unigram**으로 기본 설정  \n",
        "- 데이터셋 지정이 **로컬 경로**면 해당 파일/폴더에서 읽고, 없으면 **HuggingFace에서 자동 다운로드**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c6cbf4f",
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclasses.dataclass\n",
        "class CFG:\n",
        "    # 데이터\n",
        "    sft_dataset: str = \"karpathy/tiny_shakespeare\"  # 로컬 경로 or HF hub 이름\n",
        "    pref_dataset: str = \"trl-lib/hh-rlhf-helpful-base\"  # ORPO용 (prompt, chosen, rejected)\n",
        "    seq_len: int = 512\n",
        "    batch_size: int = 16\n",
        "    epochs: int = 1\n",
        "    max_steps: int = 150\n",
        "    lr: float = 3e-4\n",
        "    weight_decay: float = 0.1\n",
        "    warmup_steps: int = 40\n",
        "    grad_accum: int = 1\n",
        "    grad_clip: float = 1.0\n",
        "    amp: bool = True\n",
        "\n",
        "    # 토크나이저\n",
        "    tokenizer_type: str = \"unigram\"   # [\"unigram\", \"gpt2_fallback\"]\n",
        "    vocab_size: int = 32000\n",
        "    bos_token: str = \"<|bos|>\"\n",
        "    eos_token: str = \"<|eos|>\"\n",
        "    pad_token: str = \"<|pad|>\"\n",
        "\n",
        "    # 모델\n",
        "    d_model: int = 512\n",
        "    n_layers: int = 12\n",
        "    n_heads: int = 8\n",
        "    n_kv_heads: int = 4\n",
        "    attn_ratio: float = 0.5\n",
        "    swa_window: int = 256\n",
        "    swa_layers: T.Optional[T.List[int]] = None  # None이면 (첫/가운데/마지막 제외 전체 SWA)\n",
        "    num_meta_tokens: int = 4\n",
        "    kv_share: bool = True\n",
        "    ffn_mult: float = 4.0\n",
        "    dropout: float = 0.0\n",
        "    max_position: int = 65536\n",
        "    return_attn: bool = False\n",
        "\n",
        "cfg = CFG()\n",
        "cfg\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66a1b5ba",
      "metadata": {},
      "source": [
        "\n",
        "## 2) 데이터 준비 — 로컬이 없으면 HF에서 자동 로드\n",
        "- SFT: 순수 텍스트(예: Tiny Shakespeare) → 토큰 청크\n",
        "- ORPO: **선호 데이터**(prompt, chosen, rejected) → 토큰 쌍\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf53b378",
      "metadata": {},
      "outputs": [],
      "source": [
        "def read_local_text(path:str)->str:\n",
        "    if not os.path.exists(path):\n",
        "        return None\n",
        "    if os.path.isdir(path):\n",
        "        texts=[]\n",
        "        for root,_,files in os.walk(path):\n",
        "            for f in files:\n",
        "                if f.endswith(\".txt\"):\n",
        "                    with open(os.path.join(root,f),\"r\",encoding=\"utf-8\",errors=\"ignore\") as fh:\n",
        "                        texts.append(fh.read())\n",
        "        return \"\\n\\n\".join(texts) if texts else None\n",
        "    else:\n",
        "        with open(path,\"r\",encoding=\"utf-8\",errors=\"ignore\") as fh:\n",
        "            return fh.read()\n",
        "\n",
        "def get_sft_corpus(spec:str)->str:\n",
        "    # 로컬 경로 텍스트 우선\n",
        "    text = read_local_text(spec)\n",
        "    if text is not None:\n",
        "        return text\n",
        "    # 없으면 HF datasets에서 다운로드\n",
        "    ds = load_dataset(spec)\n",
        "    # text/ content/ document 등 가능성 처리\n",
        "    keys = [\"text\",\"content\",\"document\",\"raw\",\"data\"]\n",
        "    for k in keys:\n",
        "        if k in ds[\"train\"].column_names:\n",
        "            return \"\\n\\n\".join(ds[\"train\"][k])\n",
        "    # 마지막 수단: train split 통째로 문자열화\n",
        "    return \"\\n\\n\".join(map(str, ds[\"train\"][:1000]))\n",
        "\n",
        "def load_pref_dataset(name:str, split:str=\"train\"):\n",
        "    # HH-RLHF helpful-base 또는 UltraFeedback binarized 등 (prompt, chosen, rejected) 지원\n",
        "    ds = load_dataset(name, split=split)\n",
        "    cols = ds.column_names\n",
        "    def pick(x, keys):\n",
        "        for k in keys:\n",
        "            if k in x and x[k] is not None:\n",
        "                return x[k]\n",
        "        return \"\"\n",
        "    out = {\"prompt\":[],\"chosen\":[],\"rejected\":[]}\n",
        "    for ex in ds:\n",
        "        # 대화형/문자열 형식 모두 처리\n",
        "        prompt = pick(ex, [\"prompt\",\"question\",\"input\",\"instruction\"])\n",
        "        chosen = pick(ex, [\"chosen\",\"chosen_response\",\"chosen_text\"])\n",
        "        rejected= pick(ex, [\"rejected\",\"rejected_response\",\"rejected_text\"])\n",
        "        # 대화형 형식인 경우(list of dict: role/content)\n",
        "        if isinstance(prompt, list):\n",
        "            prompt = \"\\n\".join([f\"{m.get('role','user').capitalize()}: {m.get('content','')}\".strip() for m in prompt])\n",
        "        if isinstance(chosen, list):\n",
        "            chosen = \"\\n\".join([m.get(\"content\",\"\") for m in chosen])\n",
        "        if isinstance(rejected, list):\n",
        "            rejected = \"\\n\".join([m.get(\"content\",\"\") for m in rejected])\n",
        "        if prompt and chosen and rejected:\n",
        "            out[\"prompt\"].append(prompt)\n",
        "            out[\"chosen\"].append(chosen)\n",
        "            out[\"rejected\"].append(rejected)\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31e79bb3",
      "metadata": {},
      "source": [
        "\n",
        "## 3) 최신 토크나이저 — **Unigram(SentencePiece 계열)** 직접 학습\n",
        "- 데이터에서 직접 학습 → 도메인 적합 & 최신 서브워드 품질  \n",
        "- 특수 토큰 **BOS/EOS/PAD** 포함, 후처리 템플릿으로 자동 부착\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe8a2e34",
      "metadata": {},
      "outputs": [],
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import Unigram\n",
        "from tokenizers.trainers import UnigramTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.normalizers import NFKC, Lowercase, Sequence as NormSeq\n",
        "from tokenizers.processors import TemplateProcessing\n",
        "\n",
        "def train_unigram_tokenizer(corpus_text:str, vocab_size:int, bos:str, eos:str, pad:str):\n",
        "    # 간단한 iterator로 메모리 사용 절감\n",
        "    def batch_iter(text, bs=1000000):\n",
        "        for i in range(0, len(text), bs):\n",
        "            yield text[i:i+bs]\n",
        "    tok = Tokenizer(Unigram())\n",
        "    tok.normalizer = NormSeq([NFKC(), Lowercase()])\n",
        "    tok.pre_tokenizer = Whitespace()\n",
        "    trainer = UnigramTrainer(vocab_size=vocab_size, special_tokens=[pad,bos,eos])\n",
        "    tok.train_from_iterator(batch_iter(corpus_text), trainer=trainer)\n",
        "    # 특수 토큰 ID\n",
        "    pad_id = tok.token_to_id(pad)\n",
        "    bos_id = tok.token_to_id(bos)\n",
        "    eos_id = tok.token_to_id(eos)\n",
        "    tok.post_processor = TemplateProcessing(\n",
        "        single=f\"{bos} $A {eos}\",\n",
        "        pair=f\"{bos} $A {eos} $B:1 {eos}:1\",\n",
        "        special_tokens=[(bos, bos_id), (eos, eos_id)]\n",
        "    )\n",
        "    class ModernTok:\n",
        "        def __init__(self, tk, bos_id, eos_id, pad_id):\n",
        "            self.tk=tk; self.bos_token_id=bos_id; self.eos_token_id=eos_id; self.pad_token_id=pad_id\n",
        "        def encode(self, s): return self.tk.encode(s).ids\n",
        "        def decode(self, ids): return self.tk.decode(ids)\n",
        "        @property\n",
        "        def vocab_size(self): return self.tk.get_vocab_size()\n",
        "    return ModernTok(tok, bos_id, eos_id, pad_id)\n",
        "\n",
        "# 코퍼스 수집 → 토크나이저 학습\n",
        "corpus = get_sft_corpus(cfg.sft_dataset)\n",
        "modern_tok = train_unigram_tokenizer(corpus, cfg.vocab_size, cfg.bos_token, cfg.eos_token, cfg.pad_token)\n",
        "VOCAB = modern_tok.vocab_size; EOS_ID = modern_tok.eos_token_id\n",
        "print(\"vocab:\", VOCAB, \"eos:\", EOS_ID)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96ae8238",
      "metadata": {},
      "source": [
        "\n",
        "## 4) SFT 데이터 → (X, Y) 청크\n",
        "- 입력 `X`는 토큰 시퀀스, 타깃 `Y`는 **다음 토큰**(shifted)  \n",
        "- `seq_len` 단위로 자르고 마지막 토큰은 EOS로 채움\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f49b2bc",
      "metadata": {},
      "outputs": [],
      "source": [
        "def chunk_sft_tokens(text:str, seq_len:int, tok=modern_tok, eos_id=EOS_ID):\n",
        "    ids = modern_tok.encode(text)\n",
        "    n = (len(ids)//seq_len)*seq_len\n",
        "    ids = ids[:n]\n",
        "    arr = np.array(ids, dtype=np.int64).reshape(-1, seq_len)\n",
        "    y = np.copy(arr); y[:, :-1] = arr[:, 1:]; y[:, -1] = eos_id\n",
        "    X = torch.tensor(arr); Y = torch.tensor(y)\n",
        "    return TensorDataset(X, Y)\n",
        "\n",
        "sft_ds = chunk_sft_tokens(corpus, cfg.seq_len, modern_tok, EOS_ID)\n",
        "sft_dl = DataLoader(sft_ds, batch_size=cfg.batch_size, shuffle=True, drop_last=True)\n",
        "len(sft_ds), cfg.seq_len\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "066c9399",
      "metadata": {},
      "source": [
        "\n",
        "## 5) 모델: Hymba (하이브리드 블록: Global/SWA 어텐션 + Mamba) + 메타 토큰 + 인접 KV 공유\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44523090",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 핵심 모듈 (RoPE, RMSNorm, FFN, GQA(SWA/Global), Mamba, Block, Model) ---\n",
        "class RotaryEmbedding(nn.Module):\n",
        "    def __init__(self, dim, max_position=131072, base=10000.0):\n",
        "        super().__init__()\n",
        "        inv = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
        "        t = torch.arange(max_position, dtype=torch.float32)\n",
        "        freqs = torch.einsum(\"i,j->ij\", t, inv)\n",
        "        self.register_buffer(\"cos_cached\", torch.cos(freqs), persistent=False)\n",
        "        self.register_buffer(\"sin_cached\", torch.sin(freqs), persistent=False)\n",
        "    def forward(self, x, positions):\n",
        "        cos = self.cos_cached[positions][:, None, None, :]\n",
        "        sin = self.sin_cached[positions][:, None, None, :]\n",
        "        x1 = x[..., ::2]; x2 = x[..., 1::2]\n",
        "        return torch.stack([x1*cos - x2*sin, x1*sin + x2*cos], dim=-1).flatten(-2)\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, d, eps=1e-6): super().__init__(); self.eps=eps; self.w=nn.Parameter(torch.ones(d))\n",
        "    def forward(self, x): return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) * self.w\n",
        "\n",
        "class SwiGLU_FFN(nn.Module):\n",
        "    def __init__(self, d, mult=4.0, dropout=0.0):\n",
        "        super().__init__()\n",
        "        h=int(d*mult); self.w1=nn.Linear(d,h, bias=False); self.w2=nn.Linear(d,h, bias=False)\n",
        "        self.w3=nn.Linear(h,d, bias=False); self.drop=nn.Dropout(dropout)\n",
        "    def forward(self, x): return self.w3(self.drop(F.silu(self.w1(x)) * self.w2(x)))\n",
        "\n",
        "def band_mask(T, w, device, dtype):\n",
        "    i = torch.arange(T, device=device); j = torch.arange(T, device=device)\n",
        "    m = (j[None,:] <= i[:,None]) & (j[None,:] >= (i[:,None]-w+1))\n",
        "    M = torch.zeros((T,T), device=device, dtype=dtype)\n",
        "    return M.masked_fill(~m, float(\"-inf\"))\n",
        "\n",
        "class GQA(nn.Module):\n",
        "    def __init__(self, d_model, H, KV, rope=None, use_swa=False, swa_window=4096, dropout=0.0, ret_attn=False):\n",
        "        super().__init__(); assert d_model%H==0\n",
        "        self.H=H; self.KV=KV; self.rep=H//KV; self.Dh=d_model//H\n",
        "        self.q=nn.Linear(d_model, H*self.Dh, bias=False)\n",
        "        self.k=nn.Linear(d_model, KV*self.Dh, bias=False)\n",
        "        self.v=nn.Linear(d_model, KV*self.Dh, bias=False)\n",
        "        self.o=nn.Linear(H*self.Dh, d_model, bias=False)\n",
        "        self.drop=nn.Dropout(dropout); self.rope=rope; self.use_swa=use_swa; self.swa_window=swa_window; self.ret=ret_attn\n",
        "    def forward(self, x, kv_cache=None, global_mask=None, need_weights=False):\n",
        "        B,T,C=x.shape\n",
        "        q=self.q(x).view(B,T,self.H,self.Dh).transpose(1,2)\n",
        "        k=self.k(x).view(B,T,self.KV,self.Dh).transpose(1,2)\n",
        "        v=self.v(x).view(B,T,self.KV,self.Dh).transpose(1,2)\n",
        "        if kv_cache is not None and kv_cache[0] is not None and kv_cache[0].numel()>0:\n",
        "            pk,pv=kv_cache; k=torch.cat([pk,k],2); v=torch.cat([pv,v],2)\n",
        "        if self.rope is not None:\n",
        "            pos_q=torch.arange(k.size(2)-T, k.size(2), device=x.device)\n",
        "            pos_k=torch.arange(0, k.size(2), device=x.device)\n",
        "            q=self.rope(q, pos_q); k_exp=k.repeat_interleave(self.rep,1); k_exp=self.rope(k_exp, pos_k)\n",
        "        else:\n",
        "            k_exp=k.repeat_interleave(self.rep,1)\n",
        "        v_exp=v.repeat_interleave(self.rep,1)\n",
        "        Tc=k_exp.size(2)\n",
        "        if self.use_swa:\n",
        "            M = band_mask(Tc, self.swa_window, x.device, q.dtype)[:, -T:]\n",
        "        else:\n",
        "            i = torch.arange(Tc, device=x.device); j = torch.arange(Tc, device=x.device)\n",
        "            causal = (j[None,:] <= i[:,None]); causal = causal[:, -T:]\n",
        "            M = torch.zeros((T,Tc), device=x.device, dtype=q.dtype).masked_fill(~causal, float(\"-inf\"))\n",
        "        if global_mask is not None:\n",
        "            full = torch.zeros_like(M)\n",
        "            M = torch.where(global_mask[:, :, None], full[None,:,:], M[None,:,:]).squeeze(0)\n",
        "        q_=q.reshape(B*self.H,T,self.Dh); k_=k_exp.reshape(B*self.H,Tc,self.Dh); v_=v_exp.reshape(B*self.H,Tc,self.Dh)\n",
        "        M_=M.unsqueeze(0).expand(B*self.H, -1, -1)\n",
        "        out = F.scaled_dot_product_attention(q_, k_, v_, attn_mask=M_, is_causal=False,\n",
        "                                             dropout_p=float(self.drop.p) if self.training else 0.0)\n",
        "        attn=None\n",
        "        out=out.view(B,self.H,T,self.Dh).transpose(1,2).reshape(B,T,self.H*self.Dh)\n",
        "        out=self.o(out); new_cache=(k.detach(), v.detach())\n",
        "        return out, new_cache, attn\n",
        "\n",
        "\n",
        "\n",
        "from mamba_ssm import Mamba as _Mamba\n",
        "\n",
        "class MambaLayer(nn.Module):\n",
        "    def __init__(self, d_model, d_state=16, d_conv=4, expand=2):\n",
        "        super().__init__(); self.inner=_Mamba(d_model=d_model, d_state=d_state, d_conv=d_conv, expand=expand) if _Mamba else nn.Identity()\n",
        "    def forward(self, x): return self.inner(x)\n",
        "\n",
        "class HymbaBlock(nn.Module):\n",
        "    def __init__(self, d_model, H, KV, attn_dim, use_swa, swa_window, rope, mamba_dim, ffn_mult=4.0, dropout=0.0, ret_attn=False):\n",
        "        super().__init__()\n",
        "        self.pre=RMSNorm(d_model); self.to_a=nn.Linear(d_model, attn_dim, bias=False) if attn_dim>0 else None\n",
        "        self.to_m=nn.Linear(d_model, mamba_dim, bias=False) if mamba_dim>0 else None\n",
        "        self.attn=GQA(attn_dim, H, KV, rope, use_swa, swa_window, dropout, ret_attn) if attn_dim>0 else None\n",
        "        self.mamba=MambaLayer(mamba_dim) if mamba_dim>0 else None\n",
        "        self.from_paths=nn.Linear(attn_dim+mamba_dim, d_model, bias=False)\n",
        "        self.ffn=SwiGLU_FFN(d_model, mult=ffn_mult, dropout=dropout); self.drop=nn.Dropout(dropout)\n",
        "    def forward(self, x, kv_cache=None, global_mask=None, need_attn=False):\n",
        "        h=self.pre(x); outs=[]; new_cache=None\n",
        "        if self.attn is not None:\n",
        "            a, new_cache, _ = self.attn(self.to_a(h), kv_cache=kv_cache, global_mask=global_mask, need_weights=need_attn); outs.append(a)\n",
        "        if self.mamba is not None:\n",
        "            m=self.mamba(self.to_m(h)); outs.append(m)\n",
        "        y=self.from_paths(torch.cat(outs, -1) if len(outs)>1 else outs[0])\n",
        "        x=x+self.drop(y); x=x+self.drop(self.ffn(self.pre(x)))\n",
        "        return x, new_cache, None\n",
        "\n",
        "class HymbaForCausalLM(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=512, n_layers=12, n_heads=8, n_kv_heads=4, attn_ratio=0.5,\n",
        "                 swa_layers=None, swa_window=256, num_meta_tokens=4, kv_share=True,\n",
        "                 max_position=65536, ffn_mult=4.0, dropout=0.0, return_attn=False):\n",
        "        super().__init__()\n",
        "        self.vocab_size=vocab_size; self.d_model=d_model; self.n_layers=n_layers\n",
        "        self.num_meta_tokens=num_meta_tokens; self.kv_share=kv_share; self.ret=return_attn\n",
        "        self.tok=nn.Embedding(vocab_size, d_model)\n",
        "        attn_dim=int(d_model*attn_ratio); mamba_dim=d_model-attn_dim\n",
        "        self.rope=RotaryEmbedding(dim=(attn_dim//n_heads), max_position=max_position)\n",
        "        if swa_layers is None:\n",
        "            mid=n_layers//2; swa_layers=[i for i in range(n_layers) if i not in (0,mid,n_layers-1)]\n",
        "        self.swa=set(swa_layers)\n",
        "        self.layers=nn.ModuleList([\n",
        "            HymbaBlock(d_model, n_heads, n_kv_heads, attn_dim, (i in self.swa), swa_window, self.rope, mamba_dim, ffn_mult, dropout, return_attn)\n",
        "            for i in range(n_layers)\n",
        "        ])\n",
        "        self.norm=RMSNorm(d_model); self.head=nn.Linear(d_model, vocab_size, bias=False)\n",
        "        self.meta = nn.Parameter(torch.randn(1, num_meta_tokens, d_model)*0.02) if num_meta_tokens>0 else None\n",
        "        # 인접 레이어 캐시 공유: (0→1), (2→3), ...\n",
        "        self.owner=list(range(n_layers))\n",
        "        for a in range(0,n_layers-1,2): self.owner[a+1]=a\n",
        "\n",
        "    def forward(self, idx, targets=None, kv_caches=None, return_attn=False):\n",
        "        B,T=idx.shape; x=self.tok(idx); meta_add=0\n",
        "        if self.meta is not None and T>1:\n",
        "            x=torch.cat([self.meta.expand(B,-1,-1), x], 1); meta_add=self.num_meta_tokens\n",
        "        gmask=None\n",
        "        if meta_add>0:\n",
        "            gm=torch.zeros((B,x.size(1)), dtype=torch.bool, device=x.device); gm[:,:meta_add]=True; gmask=gm[:, -x.size(1):]\n",
        "        new=[None]*self.n_layers if kv_caches is not None else None\n",
        "        h=x\n",
        "        for li,L in enumerate(self.layers):\n",
        "            kv = kv_caches[self.owner[li]] if kv_caches is not None else None\n",
        "            h, kv_out, _ = L(h, kv_cache=kv, global_mask=gmask, need_attn=(self.ret or return_attn))\n",
        "            if new is not None and li==self.owner[li]: new[self.owner[li]]=kv_out\n",
        "        h=self.norm(h); logits=self.head(h); loss=None\n",
        "        if targets is not None:\n",
        "            s=meta_add; lf=logits[:, s:s+targets.size(1), :]\n",
        "            loss=F.cross_entropy(lf.reshape(-1, lf.size(-1)), targets.reshape(-1))\n",
        "        return {\"logits\":logits,\"loss\":loss,\"kv_caches\":new}\n",
        "\n",
        "    def estimate_kv_cache_bytes(self, seq_len:int, dtype=torch.float16):\n",
        "        blk=self.layers[0]; Dh=blk.attn.Dh if blk.attn else 0; KV=blk.attn.KV if blk.attn else 0\n",
        "        b= torch.finfo(dtype).bits//8; per = 2*KV*seq_len*Dh*b; owners=len(set(self.owner));\n",
        "        return per*owners\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd25e197",
      "metadata": {},
      "source": [
        "\n",
        "## 6) 학습 유틸\n",
        "- **Cosine 스케줄러 + Warmup**, **AMP**, **Gradient Clipping**  \n",
        "- 간단 **Perplexity** 평가\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0537d517",
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_model(cfg:CFG, *, swa_layers=None, use_meta=True, kv_share=True):\n",
        "    return HymbaForCausalLM(\n",
        "        vocab_size=VOCAB, d_model=cfg.d_model, n_layers=cfg.n_layers, n_heads=cfg.n_heads,\n",
        "        n_kv_heads=cfg.n_kv_heads, attn_ratio=cfg.attn_ratio, swa_layers=swa_layers,\n",
        "        swa_window=cfg.swa_window, num_meta_tokens=(cfg.num_meta_tokens if use_meta else 0),\n",
        "        kv_share=kv_share, max_position=cfg.max_position, ffn_mult=cfg.ffn_mult,\n",
        "        dropout=cfg.dropout, return_attn=cfg.return_attn\n",
        "    ).to(device)\n",
        "\n",
        "def evaluate_ppl(model:nn.Module, data_loader:DataLoader, max_batches:int=20):\n",
        "    model.eval(); nll=0.0; n_tok=0\n",
        "    with torch.no_grad(), torch.cuda.amp.autocast(enabled=cfg.amp):\n",
        "        for i,(xb,yb) in enumerate(data_loader):\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            out = model(xb, targets=yb)\n",
        "            loss = out[\"loss\"]\n",
        "            n = xb.numel()\n",
        "            nll += loss.item()*n; n_tok += n\n",
        "            if i+1>=max_batches: break\n",
        "    ppl = math.exp(nll/max(1,n_tok))\n",
        "    model.train()\n",
        "    return ppl\n",
        "\n",
        "def train_sft(model:nn.Module, data_loader:DataLoader, cfg:CFG):\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay, betas=(0.9,0.95), eps=1e-8)\n",
        "    total_steps = cfg.max_steps\n",
        "    sch = get_cosine_schedule_with_warmup(opt, num_warmup_steps=cfg.warmup_steps, num_training_steps=total_steps)\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=cfg.amp)\n",
        "    step=0; t0=time.time(); tokens=0\n",
        "    for epoch in range(cfg.epochs):\n",
        "        for xb,yb in data_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            with torch.cuda.amp.autocast(enabled=cfg.amp):\n",
        "                out = model(xb, targets=yb); loss = out[\"loss\"]\n",
        "            scaler.scale(loss).backward()\n",
        "            if ((step+1)%cfg.grad_accum)==0:\n",
        "                scaler.unscale_(opt)\n",
        "                if cfg.grad_clip>0: nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
        "                scaler.step(opt); scaler.update(); opt.zero_grad(set_to_none=True); sch.step()\n",
        "            step+=1; tokens+=xb.numel()\n",
        "            if step%20==0:\n",
        "                tps = tokens/max(1e-9, time.time()-t0)\n",
        "                print(f\"[SFT] step {step}/{total_steps} loss {loss.item():.3f}  ~{tps:.1f} tok/s\")\n",
        "            if step>=total_steps: break\n",
        "        if step>=total_steps: break\n",
        "    return {\"steps\":step, \"tokens\":tokens, \"time_sec\":time.time()-t0}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5b2d20f",
      "metadata": {},
      "source": [
        "\n",
        "## 7) SFT 베이스라인 (Global-only, 메타 off, KV 공유 off)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d51adb9",
      "metadata": {},
      "outputs": [],
      "source": [
        "swa_layers_none = []\n",
        "baseline = build_model(cfg, swa_layers=swa_layers_none, use_meta=False, kv_share=False)\n",
        "baseline_summary = train_sft(baseline, sft_dl, cfg)\n",
        "baseline_ppl = evaluate_ppl(baseline, sft_dl, max_batches=20)\n",
        "print(\"Baseline PPL:\", baseline_ppl); baseline_summary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f6f501b",
      "metadata": {},
      "source": [
        "\n",
        "## 8) Hybrid: Global+SWA(+메타 on), 인접 **KV 공유**\n",
        "- 첫/가운데/마지막은 Global, 그 외는 SWA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0828b9e",
      "metadata": {},
      "outputs": [],
      "source": [
        "mid = cfg.n_layers//2\n",
        "default_swa = [i for i in range(cfg.n_layers) if i not in (0, mid, cfg.n_layers-1)]\n",
        "hybrid_meta = build_model(cfg, swa_layers=default_swa, use_meta=True, kv_share=True)\n",
        "hybrid_meta_sum = train_sft(hybrid_meta, sft_dl, cfg)\n",
        "hybrid_meta_ppl = evaluate_ppl(hybrid_meta, sft_dl)\n",
        "print(\"KV bytes estimate (no-share vs share) with seq_len):\")\n",
        "tmp_noshare = build_model(cfg, swa_layers=default_swa, use_meta=True, kv_share=False)\n",
        "bytes_noshare = tmp_noshare.estimate_kv_cache_bytes(cfg.seq_len)\n",
        "bytes_share   = hybrid_meta.estimate_kv_cache_bytes(cfg.seq_len)\n",
        "print(\"no-share:\", round(bytes_noshare/2**20,2), \"MB  | share:\", round(bytes_share/2**20,2), \"MB\")\n",
        "hybrid_meta_ppl, hybrid_meta_sum\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e90e0b9",
      "metadata": {},
      "source": [
        "\n",
        "## 9) **ORPO (SFT + 로그 오즈 선호항)** 구현 및 해설\n",
        "- **핵심**: SFT의 NLL 손실에 **선호 대비 항**을 추가  \n",
        "- ORPO 논문은 **참조 모델 없이**, `L = L_SFT + β · L_ratio`를 제안  \n",
        "- 여기서 `L_ratio = -log σ( Δ )`, `Δ ≈ (avg log p_θ(chosen|x) - avg log p_θ(rejected|x))`  \n",
        "  - 실무적으로 TRL 구현은 **길이 정규화된 토큰 평균 로그확률 차이**에 로지스틱을 적용합니다.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04ea183a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def mean_logprob_for_response(model, prompt_ids, resp_ids):\n",
        "    # prompt + response를 이어서 forward → response 구간의 평균 log P 토큰 계산\n",
        "    B = prompt_ids.size(0)\n",
        "    x = torch.cat([prompt_ids, resp_ids], dim=1)\n",
        "    with torch.no_grad():\n",
        "        out = model(x.to(device))\n",
        "        logits = out[\"logits\"][:, :-1, :]  # predict next\n",
        "    target = x[:, 1:].to(device)\n",
        "    # response token 위치 마스크\n",
        "    resp_mask = torch.zeros_like(target, dtype=torch.bool)\n",
        "    resp_mask[:, prompt_ids.size(1)-1 : prompt_ids.size(1)-1 + resp_ids.size(1)] = True\n",
        "    logp = F.log_softmax(logits, dim=-1).gather(-1, target.unsqueeze(-1)).squeeze(-1)\n",
        "    resp_logp = logp.masked_select(resp_mask.to(device)).view(B, -1)\n",
        "    return resp_logp.mean(dim=1)  # (B,)\n",
        "\n",
        "def encode_prompt_and_resp(batch_prompts, batch_resps, max_len=256):\n",
        "    # 간단한 포맷: \"User: ...\\nAssistant: ...\"\n",
        "    prompts = [f\"User: {p}\\nAssistant:\" for p in batch_prompts]\n",
        "    inputs  = [modern_tok.encode(t) for t in prompts]\n",
        "    resps   = [modern_tok.encode(r) for r in batch_resps]\n",
        "    # 길이 제한 및 텐서화(+패딩)\n",
        "    def pad_to(arrs, L):\n",
        "        t = torch.full((len(arrs), L), modern_tok.pad_token_id, dtype=torch.long)\n",
        "        for i,a in enumerate(arrs): t[i, :min(L,len(a))]=torch.tensor(a[:L])\n",
        "        return t\n",
        "    P = pad_to(inputs, max_len)\n",
        "    R = pad_to(resps, max_len//2)\n",
        "    return P, R\n",
        "\n",
        "class PrefDataset(Dataset):\n",
        "    def __init__(self, pref_dict, max_prompt_len=256):\n",
        "        self.d=pref_dict; self.max_prompt_len=max_prompt_len\n",
        "    def __len__(self): return len(self.d[\"prompt\"])\n",
        "    def __getitem__(self, i):\n",
        "        return self.d[\"prompt\"][i], self.d[\"chosen\"][i], self.d[\"rejected\"][i]\n",
        "\n",
        "def collate_pref(batch, max_prompt_len=256):\n",
        "    prompts, chosens, rejecteds = zip(*batch)\n",
        "    P, C = encode_prompt_and_resp(prompts, chosens, max_len=max_prompt_len)\n",
        "    _, R = encode_prompt_and_resp(prompts, rejecteds, max_len=max_prompt_len)\n",
        "    return P, C, R\n",
        "\n",
        "pref_raw = load_pref_dataset(cfg.pref_dataset, split=\"train[:2000]\")\n",
        "pref_dl = DataLoader(PrefDataset(pref_raw), batch_size=cfg.batch_size, shuffle=True, drop_last=True, collate_fn=collate_pref)\n",
        "len(pref_raw[\"prompt\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34aa6b78",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_orpo(model:nn.Module, pref_loader:DataLoader, cfg:CFG, beta:float=0.1, lam_or:float=0.5):\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay, betas=(0.9,0.95), eps=1e-8)\n",
        "    sch = get_cosine_schedule_with_warmup(opt, num_warmup_steps=cfg.warmup_steps, num_training_steps=cfg.max_steps)\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=cfg.amp)\n",
        "    step=0; t0=time.time()\n",
        "    for ep in range(cfg.epochs):\n",
        "        for P,C,R in pref_loader:\n",
        "            P, C, R = P.to(device), C.to(device), R.to(device)\n",
        "            with torch.cuda.amp.autocast(enabled=cfg.amp):\n",
        "                # 1) SFT - chosen에 대한 NLL (prompt+chosen 합쳐서 target=shifted)\n",
        "                x = torch.cat([P, C], dim=1)\n",
        "                targets = x[:, 1:].contiguous()\n",
        "                out = model(x[:, :-1], targets=None)  # logits for next\n",
        "                logp = F.log_softmax(out[\"logits\"], dim=-1)\n",
        "                tgt_logp = logp.gather(-1, targets.unsqueeze(-1)).squeeze(-1)\n",
        "                # chosen 부분만 평균 NLL\n",
        "                mask = torch.zeros_like(targets, dtype=torch.bool); mp = P.size(1)-1; mc = C.size(1)\n",
        "                mask[:, mp:mp+mc] = True\n",
        "                sft_loss = -(tgt_logp.masked_select(mask.to(device))).mean()\n",
        "\n",
        "                # 2) ORPO ratio: chosen vs rejected 평균 로그확률 차이를 로지스틱 분류\n",
        "                logp_ch = mean_logprob_for_response(model, P, C)  # (B,)\n",
        "                logp_rj = mean_logprob_for_response(model, P, R)  # (B,)\n",
        "                # 안정화: 극단값 clamp (bf16에서 NaN 방지)\n",
        "                diff = torch.clamp(logp_ch - logp_rj, -50.0, 50.0)\n",
        "                ratio_loss = -F.logsigmoid(beta * diff).mean()\n",
        "\n",
        "                loss = sft_loss + lam_or * ratio_loss\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            if ((step+1)%cfg.grad_accum)==0:\n",
        "                scaler.unscale_(opt)\n",
        "                if cfg.grad_clip>0: nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
        "                scaler.step(opt); scaler.update(); opt.zero_grad(set_to_none=True); sch.step()\n",
        "            step+=1\n",
        "            if step%20==0:\n",
        "                print(f\"[ORPO] step {step}/{cfg.max_steps}  L_sft {sft_loss.item():.3f}  L_or {ratio_loss.item():.3f}  -> L {loss.item():.3f}\")\n",
        "            if step>=cfg.max_steps: break\n",
        "        if step>=cfg.max_steps: break\n",
        "    return {\"steps\":step, \"time_sec\":time.time()-t0}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "380f4c6c",
      "metadata": {},
      "source": [
        "\n",
        "### ORPO 실행 (하이브리드 모델 기반)\n",
        "- 베이스로 **Global+SWA(+메타 on, KV 공유 on)** 모델을 사용\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fc681da",
      "metadata": {},
      "outputs": [],
      "source": [
        "orpo_model = build_model(cfg, swa_layers=default_swa, use_meta=True, kv_share=True)\n",
        "orpo_summary = train_orpo(orpo_model, pref_dl, cfg, beta=0.1, lam_or=0.5)\n",
        "# SFT perplexity로 대략 확인(선호 데이터는 ppl 지표가 직접적이지 않음)\n",
        "orpo_ppl = evaluate_ppl(orpo_model, sft_dl, max_batches=20)\n",
        "orpo_ppl, orpo_summary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f65fc09b",
      "metadata": {},
      "source": [
        "\n",
        "## 10) 추가 Ablation\n",
        "- **메타 토큰 off**  \n",
        "- **KV 공유 off** (메모리/속도 비교)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "574e6188",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 메타 토큰 OFF\n",
        "hybrid_no_meta = build_model(cfg, swa_layers=default_swa, use_meta=False, kv_share=True)\n",
        "hnm_sum = train_sft(hybrid_no_meta, sft_dl, cfg)\n",
        "hnm_ppl = evaluate_ppl(hybrid_no_meta, sft_dl)\n",
        "print(\"Hybrid(meta OFF) PPL:\", hnm_ppl); hnm_sum\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c794e03",
      "metadata": {},
      "outputs": [],
      "source": [
        "# KV 공유 OFF\n",
        "hybrid_noshare = build_model(cfg, swa_layers=default_swa, use_meta=True, kv_share=False)\n",
        "hns_sum = train_sft(hybrid_noshare, sft_dl, cfg)\n",
        "hns_ppl = evaluate_ppl(hybrid_noshare, sft_dl)\n",
        "b_share = hybrid_meta.estimate_kv_cache_bytes(cfg.seq_len)\n",
        "b_noshare = hybrid_noshare.estimate_kv_cache_bytes(cfg.seq_len)\n",
        "print(\"KV bytes  share:\", round(b_share/2**20,2),\"MB  |  no-share:\", round(b_noshare/2**20,2),\"MB\")\n",
        "hns_ppl, hns_sum\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
