{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87de1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "import torch, time, psutil, os\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7dd3ac",
   "metadata": {},
   "source": [
    "### KV Cache Sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74546344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19c66acfdbcd49acbec7973ec10cad21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=24, description='max_layers', max=48, min=4, step=2), IntSlider(value=10…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def kv_cache_tradeoff(max_layers=24, seq_len=1024, hidden_dim=1024):\n",
    "    layers = np.arange(2, max_layers+1, 2)\n",
    "    \n",
    "    # 단순화된 메모리 모델 (단위: MB)\n",
    "    token_size = hidden_dim * 2  # Key + Value\n",
    "    bytes_per_token = token_size * 4  # float32\n",
    "    base_mem = seq_len * bytes_per_token / (1024**2)\n",
    "    \n",
    "    mem_no_share = layers * base_mem\n",
    "    mem_pair_share = (layers/2) * base_mem\n",
    "    \n",
    "    # Latency 모델 (임의 스케일링: ms 단위)\n",
    "    base_latency = 0.05  # 1 cache access = 0.05ms 가정\n",
    "    latency_no_share = layers * base_latency\n",
    "    latency_pair_share = (layers/2) * base_latency\n",
    "    \n",
    "    # Plot\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=layers, y=mem_no_share, mode=\"lines+markers\",\n",
    "        name=\"Memory No Sharing\", line=dict(color=\"red\", width=3)\n",
    "    ))\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=layers, y=mem_pair_share, mode=\"lines+markers\",\n",
    "        name=\"Memory Pair Sharing\", line=dict(color=\"green\", width=3, dash=\"dot\")\n",
    "    ))\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=layers, y=latency_no_share*100, mode=\"lines+markers\",\n",
    "        name=\"Latency No Sharing (scaled)\", line=dict(color=\"orange\", width=3)\n",
    "    ))\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=layers, y=latency_pair_share*100, mode=\"lines+markers\",\n",
    "        name=\"Latency Pair Sharing (scaled)\", line=dict(color=\"blue\", width=3, dash=\"dot\")\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"KV Cache Trade-off: Memory vs Latency\",\n",
    "        xaxis_title=\"Number of Layers\",\n",
    "        yaxis_title=\"Relative Units (Memory MB / Latency*100)\",\n",
    "        template=\"plotly_white\"\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "widgets.interact(\n",
    "    kv_cache_tradeoff,\n",
    "    max_layers=(4, 48, 2),\n",
    "    seq_len=(512, 8192, 512),\n",
    "    hidden_dim=(256, 2048, 256)\n",
    ");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4142e773",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KVCache:\n",
    "    \"\"\"KV 캐시 관리: 2개 레이어가 하나의 캐시를 공유\"\"\"\n",
    "    def __init__(self, num_layers, share_pairs=True):\n",
    "        self.share_pairs = share_pairs\n",
    "        self.caches = {}\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def get_key(self, layer_idx):\n",
    "        if not self.share_pairs:\n",
    "            return f\"layer_{layer_idx}\"\n",
    "        # 두 레이어를 묶어 같은 캐시 키 반환\n",
    "        pair_idx = layer_idx // 2\n",
    "        return f\"pair_{pair_idx}\"\n",
    "\n",
    "    def get(self, layer_idx):\n",
    "        return self.caches.get(self.get_key(layer_idx))\n",
    "\n",
    "    def set(self, layer_idx, kv):\n",
    "        self.caches[self.get_key(layer_idx)] = kv\n",
    "\n",
    "class KVLayer(nn.Module):\n",
    "    def __init__(self, d_model, layer_idx, kv_cache: KVCache):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "        self.layer_idx = layer_idx\n",
    "        self.kv_cache = kv_cache\n",
    "\n",
    "    def forward(self, x):\n",
    "        cached = self.kv_cache.get(self.layer_idx)\n",
    "        if cached is not None:\n",
    "            return cached\n",
    "        out = self.fc(x)\n",
    "        self.kv_cache.set(self.layer_idx, out)\n",
    "        return out\n",
    "\n",
    "# 사용 예시\n",
    "d_model = 64\n",
    "layers = 6\n",
    "kv_cache = KVCache(num_layers=layers, share_pairs=True)\n",
    "x = torch.randn(1, 10, d_model)\n",
    "\n",
    "model = nn.ModuleList([KVLayer(d_model, i, kv_cache) for i in range(layers)])\n",
    "for layer in model:\n",
    "    out = layer(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d3e878",
   "metadata": {},
   "source": [
    "### SWA(Sliding Window Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d33eba2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9d199e8548b44a285e663cee2768262",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=16, description='seq_len', max=32, min=8), IntSlider(value=4, descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def swa_attention_map(seq_len=16, window_size=4, highlight_row=8):\n",
    "    attn = np.zeros((seq_len, seq_len))\n",
    "    for i in range(seq_len):\n",
    "        for j in range(seq_len):\n",
    "            if j <= i and j >= i - window_size + 1:\n",
    "                attn[i,j] = 1\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=attn,\n",
    "        x=[f\"K{j}\" for j in range(seq_len)],\n",
    "        y=[f\"Q{i}\" for i in range(seq_len)],\n",
    "        colorscale=[[0,\"#f3f4f6\"],[1,\"#22c55e\"]],\n",
    "        showscale=False,\n",
    "        hovertemplate=\"Query=%{y}, Key=%{x}, Active=%{z}<extra></extra>\"\n",
    "    ))\n",
    "    # 강조 row에 라인\n",
    "    fig.add_shape(type=\"rect\", x0=-0.5, y0=highlight_row-0.5, x1=seq_len-0.5, y1=highlight_row+0.5,\n",
    "                  line=dict(color=\"blue\", width=2))\n",
    "    fig.update_layout(\n",
    "        title=f\"SWA Attention Map (Window={window_size}, Highlight Q{highlight_row})\",\n",
    "        xaxis=dict(side=\"top\")\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "widgets.interact(swa_attention_map, seq_len=(8,32,1), window_size=(1,16,1), highlight_row=(1,32,1));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c64849d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full mask: tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "Window mask (w=3): tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [-inf, 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [-inf, -inf, 0., 0., 0., 0., -inf, -inf],\n",
      "        [-inf, -inf, -inf, 0., 0., 0., 0., -inf],\n",
      "        [-inf, -inf, -inf, -inf, 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# PyTorch 코드로 mask 비교\n",
    "def build_mask(n,w,dev):\n",
    "    m=torch.full((n,n),float(\"-inf\"),device=dev)\n",
    "    for i in range(n):\n",
    "        m[i,max(0,i-w):i+1]=0\n",
    "    return m\n",
    "\n",
    "print(\"Full mask:\", build_mask(8,8,\"cpu\"))\n",
    "print(\"Window mask (w=3):\", build_mask(8,3,\"cpu\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725e791e",
   "metadata": {},
   "source": [
    "### Meta Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b7b8ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a8d5cd3fc244aa6bb565206c0666831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=12, description='num_tokens', max=16, min=4), Dropdown(description='use_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def meta_attention_map(num_tokens=12, use_meta=True):\n",
    "    size = num_tokens + (1 if use_meta else 0)\n",
    "    attn = np.zeros((size, size))\n",
    "    for i in range(size):\n",
    "        for j in range(size):\n",
    "            if j <= i:  # causal\n",
    "                if use_meta and j == 0:\n",
    "                    attn[i,j] = 0.9\n",
    "                elif abs(i-j) <= 1:\n",
    "                    attn[i,j] = 0.6\n",
    "                else:\n",
    "                    attn[i,j] = 0.2\n",
    "    \n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=attn,\n",
    "        x=[f\"K{j}\" if not(use_meta and j==0) else \"META\" for j in range(size)],\n",
    "        y=[f\"Q{i}\" if not(use_meta and i==0) else \"META\" for i in range(size)],\n",
    "        colorscale=\"Blues\",\n",
    "        hoverongaps=False,\n",
    "        colorbar=dict(title=\"Attention\")\n",
    "    ))\n",
    "    fig.update_layout(\n",
    "        title=\"Meta Token Heatmap (허브 역할)\" if use_meta else \"Standard Attention Heatmap\",\n",
    "        xaxis=dict(side=\"top\")\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "widgets.interact(meta_attention_map, num_tokens=(4,16,1), use_meta=[False, True]);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "199e2afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without Meta weights shape: torch.Size([1, 2, 16, 16])\n",
      "With Meta weights shape: torch.Size([1, 2, 17, 17])\n"
     ]
    }
   ],
   "source": [
    "# PyTorch 코드 예시 (Meta Token 적용/비적용)\n",
    "class MetaBlock(nn.Module):\n",
    "    def __init__(self,d,heads=2,use_meta=True):\n",
    "        super().__init__(); self.attn=nn.MultiheadAttention(d,heads,batch_first=True); self.use_meta=use_meta\n",
    "    def forward(self,x):\n",
    "        if self.use_meta:\n",
    "            meta=torch.zeros(x.size(0),1,x.size(-1))\n",
    "            x=torch.cat([meta,x],dim=1)\n",
    "        out,w=self.attn(x,x,x,need_weights=True,average_attn_weights=False)\n",
    "        return out,w\n",
    "\n",
    "x=torch.randn(1,16,8)\n",
    "for use_meta in [False,True]:\n",
    "    blk=MetaBlock(8,use_meta=use_meta)\n",
    "    _,w=blk(x)\n",
    "    print(\"With Meta\" if use_meta else \"Without Meta\", \"weights shape:\",w.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
