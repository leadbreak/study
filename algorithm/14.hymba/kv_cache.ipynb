{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b58c8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- KV ìºì‹œ ë™ì‘ ê³¼ì • ì‹œë®¬ë ˆì´ì…˜ ---\n",
      "\n",
      "[1. Prefill ë‹¨ê³„]\n",
      "ì…ë ¥ í”„ë¡¬í”„íŠ¸ shape: torch.Size([1, 10])\n",
      "ìƒì„±ëœ KV ìºì‹œ shape: K=torch.Size([1, 4, 10, 16]), V=torch.Size([1, 4, 10, 16])\n",
      "-> í”„ë¡¬í”„íŠ¸ 10ê°œ í† í°ì— ëŒ€í•œ K, Vê°€ ìºì‹œì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "[2. Decoding ë‹¨ê³„ (1-step)]\n",
      "ì…ë ¥ í† í° shape: torch.Size([1, 1])\n",
      "ì „ë‹¬ë˜ëŠ” ê³¼ê±° KV ìºì‹œ shape: K=torch.Size([1, 4, 10, 16]), V=torch.Size([1, 4, 10, 16])\n",
      "ì—…ë°ì´íŠ¸ëœ KV ìºì‹œ shape: K=torch.Size([1, 4, 11, 16]), V=torch.Size([1, 4, 11, 16])\n",
      "-> ìƒˆë¡œìš´ í† í° 1ê°œì— ëŒ€í•œ K, Vê°€ ê¸°ì¡´ ìºì‹œì— ì¶”ê°€ë˜ì–´ ê¸¸ì´ê°€ 11ì´ ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "[2. Decoding ë‹¨ê³„ (2-step)]\n",
      "ì…ë ¥ í† í° shape: torch.Size([1, 1])\n",
      "ì „ë‹¬ë˜ëŠ” ê³¼ê±° KV ìºì‹œ shape: K=torch.Size([1, 4, 11, 16]), V=torch.Size([1, 4, 11, 16])\n",
      "ì—…ë°ì´íŠ¸ëœ KV ìºì‹œ shape: K=torch.Size([1, 4, 12, 16]), V=torch.Size([1, 4, 12, 16])\n",
      "-> ë‹¤ì‹œ ìƒˆë¡œìš´ í† í° 1ê°œì— ëŒ€í•œ K, Vê°€ ì¶”ê°€ë˜ì–´ ê¸¸ì´ê°€ 12ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "# KV ìºì‹œì˜ ë™ì‘ì„ ë³´ì—¬ì£¼ê¸° ìœ„í•œ ìµœì†Œí•œì˜ ì–´í…ì…˜ ëª¨ë“ˆ\n",
    "class SimpleAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "        \n",
    "        self.qkv_proj = nn.Linear(d_model, d_model * 3)\n",
    "        self.o_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, past_kv: Optional[Tuple[torch.Tensor, torch.Tensor]] = None):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "\n",
    "        # 1. Q, K, V ê³„ì‚°\n",
    "        q, k, v = self.qkv_proj(x).chunk(3, dim=-1)\n",
    "        \n",
    "        # Multi-head í˜•íƒœë¡œ ë³€í™˜\n",
    "        q = q.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # 2. KV ìºì‹œ ì²˜ë¦¬\n",
    "        if past_kv is not None:\n",
    "            # Decoding ë‹¨ê³„: ê³¼ê±°ì˜ K, Vë¥¼ ê°€ì ¸ì™€ í˜„ì¬ K, Vì™€ ì—°ê²°\n",
    "            past_k, past_v = past_kv\n",
    "            k = torch.cat([past_k, k], dim=2)\n",
    "            v = torch.cat([past_v, v], dim=2)\n",
    "        \n",
    "        # í˜„ì¬ ìŠ¤í…ê¹Œì§€ì˜ K, Vë¥¼ ë‹¤ìŒ ìŠ¤í…ìœ¼ë¡œ ì „ë‹¬í•˜ê¸° ìœ„í•´ ì €ì¥\n",
    "        present_kv = (k, v)\n",
    "\n",
    "        # 3. ì–´í…ì…˜ ê³„ì‚°\n",
    "        attn_output = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "        \n",
    "        output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        \n",
    "        return self.o_proj(output), present_kv\n",
    "\n",
    "# ê°„ë‹¨í•œ ëª¨ë¸\n",
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(100, d_model) # vocab_size=100\n",
    "        self.attention = SimpleAttention(d_model, n_heads)\n",
    "        self.lm_head = nn.Linear(d_model, 100) # vocab_size=100\n",
    "        \n",
    "    def forward(self, tokens, past_kv=None):\n",
    "        x = self.embedding(tokens)\n",
    "        x, present_kv = self.attention(x, past_kv)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits, present_kv\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    d_model = 64\n",
    "    n_heads = 4\n",
    "    batch_size = 1\n",
    "    \n",
    "    model = SimpleTransformer(d_model, n_heads)\n",
    "    model.eval()\n",
    "\n",
    "    print(\"--- KV ìºì‹œ ë™ì‘ ê³¼ì • ì‹œë®¬ë ˆì´ì…˜ ---\")\n",
    "\n",
    "    # --- 1ë‹¨ê³„: Prefill (í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬) ---\n",
    "    print(\"\\n[1. Prefill ë‹¨ê³„]\")\n",
    "    prompt_tokens = torch.randint(0, 100, (batch_size, 10)) # (B, 10) í¬ê¸°ì˜ í”„ë¡¬í”„íŠ¸\n",
    "    print(f\"ì…ë ¥ í”„ë¡¬í”„íŠ¸ shape: {prompt_tokens.shape}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits, kv_cache = model(prompt_tokens)\n",
    "    \n",
    "    # kv_cache[0]ì€ Key, kv_cache[1]ì€ Value\n",
    "    print(f\"ìƒì„±ëœ KV ìºì‹œ shape: K={kv_cache[0].shape}, V={kv_cache[1].shape}\")\n",
    "    print(\"-> í”„ë¡¬í”„íŠ¸ 10ê°œ í† í°ì— ëŒ€í•œ K, Vê°€ ìºì‹œì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    # --- 2ë‹¨ê³„: Decoding (ë‹¤ìŒ í† í° ìƒì„±) ---\n",
    "    print(\"\\n[2. Decoding ë‹¨ê³„ (1-step)]\")\n",
    "    # ì´ì œë¶€í„°ëŠ” ë‹¨ í•˜ë‚˜ì˜ í† í°ë§Œ ì…ë ¥ìœ¼ë¡œ ë„£ìŠµë‹ˆë‹¤.\n",
    "    next_token = torch.randint(0, 100, (batch_size, 1))\n",
    "    print(f\"ì…ë ¥ í† í° shape: {next_token.shape}\")\n",
    "    print(f\"ì „ë‹¬ë˜ëŠ” ê³¼ê±° KV ìºì‹œ shape: K={kv_cache[0].shape}, V={kv_cache[1].shape}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # ì´ì „ ìŠ¤í…ì˜ ìºì‹œ(kv_cache)ë¥¼ past_kvë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.\n",
    "        new_logits, new_kv_cache = model(next_token, past_kv=kv_cache)\n",
    "\n",
    "    print(f\"ì—…ë°ì´íŠ¸ëœ KV ìºì‹œ shape: K={new_kv_cache[0].shape}, V={new_kv_cache[1].shape}\")\n",
    "    print(\"-> ìƒˆë¡œìš´ í† í° 1ê°œì— ëŒ€í•œ K, Vê°€ ê¸°ì¡´ ìºì‹œì— ì¶”ê°€ë˜ì–´ ê¸¸ì´ê°€ 11ì´ ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    print(\"\\n[2. Decoding ë‹¨ê³„ (2-step)]\")\n",
    "    next_token_2 = torch.randint(0, 100, (batch_size, 1))\n",
    "    print(f\"ì…ë ¥ í† í° shape: {next_token_2.shape}\")\n",
    "    print(f\"ì „ë‹¬ë˜ëŠ” ê³¼ê±° KV ìºì‹œ shape: K={new_kv_cache[0].shape}, V={new_kv_cache[1].shape}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        _, final_kv_cache = model(next_token_2, past_kv=new_kv_cache)\n",
    "\n",
    "    print(f\"ì—…ë°ì´íŠ¸ëœ KV ìºì‹œ shape: K={final_kv_cache[0].shape}, V={final_kv_cache[1].shape}\")\n",
    "    print(\"-> ë‹¤ì‹œ ìƒˆë¡œìš´ í† í° 1ê°œì— ëŒ€í•œ K, Vê°€ ì¶”ê°€ë˜ì–´ ê¸¸ì´ê°€ 12ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13b234a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì‚¬ìš© ë””ë°”ì´ìŠ¤: NVIDIA A100 80GB PCIe\n",
      "\n",
      "--- KV ìºì‹œ ìœ ë¬´ì— ë”°ë¥¸ ì¶”ë¡  ì†ë„ ë¹„êµ ---\n",
      "í”„ë¡¬í”„íŠ¸ ê¸¸ì´: 128, ìƒì„±í•  í† í° ìˆ˜: 128\n",
      "\n",
      "âœ… KV ìºì‹œ ì‚¬ìš© ì‹œ: 0.0315 ì´ˆ\n",
      "âŒ KV ìºì‹œ ë¯¸ì‚¬ìš© ì‹œ: 0.0997 ì´ˆ\n",
      "\n",
      "ğŸš€ ì„±ëŠ¥ í–¥ìƒ: KV ìºì‹œ ì‚¬ìš© ì‹œ ì•½ 3.17ë°° ë” ë¹ ë¦…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional, Tuple\n",
    "import time\n",
    "\n",
    "# GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸ ë° ë””ë°”ì´ìŠ¤ ì„¤ì •\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"!!! ê²½ê³ : ì´ ì„±ëŠ¥ ì‹œì—°ì€ GPU í™˜ê²½ì—ì„œ ì‹¤í–‰í•´ì•¼ ìœ ì˜ë¯¸í•œ ì°¨ì´ë¥¼ ë³´ì…ë‹ˆë‹¤. !!!\")\n",
    "\n",
    "# ì´ì „ ì˜ˆì œì™€ ë™ì¼í•œ ê°„ë‹¨í•œ ì–´í…ì…˜ ë° ëª¨ë¸ ì •ì˜\n",
    "class SimpleAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "        self.qkv_proj = nn.Linear(d_model, d_model * 3)\n",
    "        self.o_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, past_kv: Optional[Tuple[torch.Tensor, torch.Tensor]] = None):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        q, k, v = self.qkv_proj(x).chunk(3, dim=-1)\n",
    "        q = q.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        if past_kv is not None:\n",
    "            past_k, past_v = past_kv\n",
    "            k = torch.cat([past_k, k], dim=2)\n",
    "            v = torch.cat([past_v, v], dim=2)\n",
    "        \n",
    "        present_kv = (k, v)\n",
    "        attn_output = F.scaled_dot_product_attention(q, k, v, is_causal=(past_kv is None))\n",
    "        output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        return self.o_proj(output), present_kv\n",
    "\n",
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.attention = SimpleAttention(d_model, n_heads)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, tokens, past_kv=None):\n",
    "        x = self.embedding(tokens)\n",
    "        x, present_kv = self.attention(x, past_kv)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits, present_kv\n",
    "\n",
    "# --- ìƒì„± í•¨ìˆ˜ ì •ì˜ ---\n",
    "\n",
    "def generate_with_cache(model, prompt, n_tokens_to_gen):\n",
    "    \"\"\"KV ìºì‹œë¥¼ ì‚¬ìš©í•˜ì—¬ í† í°ì„ ìƒì„±í•˜ëŠ” íš¨ìœ¨ì ì¸ ë°©ë²•\"\"\"\n",
    "    model.eval()\n",
    "    kv_cache = None\n",
    "    generated_tokens = prompt\n",
    "    \n",
    "    # 1. Prefill\n",
    "    with torch.no_grad():\n",
    "        _, kv_cache = model(prompt, past_kv=None)\n",
    "    \n",
    "    # 2. Decoding\n",
    "    next_token = torch.argmax(model(prompt, past_kv=None)[0][:, -1, :], dim=-1, keepdim=True)\n",
    "    \n",
    "    for _ in range(n_tokens_to_gen - 1):\n",
    "        with torch.no_grad():\n",
    "            logits, kv_cache = model(next_token, past_kv=kv_cache)\n",
    "            next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "            generated_tokens = torch.cat([generated_tokens, next_token], dim=1)\n",
    "            \n",
    "    return generated_tokens\n",
    "\n",
    "def generate_without_cache(model, prompt, n_tokens_to_gen):\n",
    "    \"\"\"KV ìºì‹œë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ë¹„íš¨ìœ¨ì ì¸ ë°©ë²•\"\"\"\n",
    "    model.eval()\n",
    "    generated_tokens = prompt\n",
    "    \n",
    "    for _ in range(n_tokens_to_gen):\n",
    "        with torch.no_grad():\n",
    "            # ë§¤ë²ˆ ì „ì²´ ì‹œí€€ìŠ¤ë¥¼ ë‹¤ì‹œ ì…ë ¥\n",
    "            logits, _ = model(generated_tokens, past_kv=None)\n",
    "            next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "            generated_tokens = torch.cat([generated_tokens, next_token], dim=1)\n",
    "            \n",
    "    return generated_tokens\n",
    "\n",
    "# --- ì„±ëŠ¥ ì¸¡ì • ---\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # ëª¨ë¸ ë° ì„¤ì •\n",
    "    vocab_size = 50257\n",
    "    d_model = 512\n",
    "    n_heads = 8\n",
    "    \n",
    "    model = SimpleTransformer(vocab_size, d_model, n_heads).to(device)\n",
    "\n",
    "    prompt_len = 128\n",
    "    n_gen = 128\n",
    "    prompt = torch.randint(0, vocab_size, (1, prompt_len)).to(device)\n",
    "\n",
    "    print(\"\\n--- KV ìºì‹œ ìœ ë¬´ì— ë”°ë¥¸ ì¶”ë¡  ì†ë„ ë¹„êµ ---\")\n",
    "    print(f\"í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {prompt_len}, ìƒì„±í•  í† í° ìˆ˜: {n_gen}\\n\")\n",
    "\n",
    "    # ì›Œë°ì—…\n",
    "    for _ in range(3):\n",
    "        generate_with_cache(model, prompt, 10)\n",
    "        generate_without_cache(model, prompt, 10)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    # KV ìºì‹œ ì‚¬ìš© ì„±ëŠ¥ ì¸¡ì •\n",
    "    t0 = time.time()\n",
    "    generate_with_cache(model, prompt, n_gen)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "    time_with_cache = t1 - t0\n",
    "    print(f\"âœ… KV ìºì‹œ ì‚¬ìš© ì‹œ: {time_with_cache:.4f} ì´ˆ\")\n",
    "\n",
    "    # KV ìºì‹œ ë¯¸ì‚¬ìš© ì„±ëŠ¥ ì¸¡ì •\n",
    "    t0 = time.time()\n",
    "    generate_without_cache(model, prompt, n_gen)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "    time_without_cache = t1 - t0\n",
    "    print(f\"âŒ KV ìºì‹œ ë¯¸ì‚¬ìš© ì‹œ: {time_without_cache:.4f} ì´ˆ\")\n",
    "\n",
    "    # ê²°ê³¼ ë¹„êµ\n",
    "    speedup = time_without_cache / time_with_cache\n",
    "    print(f\"\\nğŸš€ ì„±ëŠ¥ í–¥ìƒ: KV ìºì‹œ ì‚¬ìš© ì‹œ ì•½ {speedup:.2f}ë°° ë” ë¹ ë¦…ë‹ˆë‹¤.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
