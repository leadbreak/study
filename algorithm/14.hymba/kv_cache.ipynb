{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b58c8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- KV 캐시 동작 과정 시뮬레이션 ---\n",
      "\n",
      "[1. Prefill 단계]\n",
      "입력 프롬프트 shape: torch.Size([1, 10])\n",
      "생성된 KV 캐시 shape: K=torch.Size([1, 4, 10, 16]), V=torch.Size([1, 4, 10, 16])\n",
      "-> 프롬프트 10개 토큰에 대한 K, V가 캐시에 저장되었습니다.\n",
      "\n",
      "[2. Decoding 단계 (1-step)]\n",
      "입력 토큰 shape: torch.Size([1, 1])\n",
      "전달되는 과거 KV 캐시 shape: K=torch.Size([1, 4, 10, 16]), V=torch.Size([1, 4, 10, 16])\n",
      "업데이트된 KV 캐시 shape: K=torch.Size([1, 4, 11, 16]), V=torch.Size([1, 4, 11, 16])\n",
      "-> 새로운 토큰 1개에 대한 K, V가 기존 캐시에 추가되어 길이가 11이 되었습니다.\n",
      "\n",
      "[2. Decoding 단계 (2-step)]\n",
      "입력 토큰 shape: torch.Size([1, 1])\n",
      "전달되는 과거 KV 캐시 shape: K=torch.Size([1, 4, 11, 16]), V=torch.Size([1, 4, 11, 16])\n",
      "업데이트된 KV 캐시 shape: K=torch.Size([1, 4, 12, 16]), V=torch.Size([1, 4, 12, 16])\n",
      "-> 다시 새로운 토큰 1개에 대한 K, V가 추가되어 길이가 12가 되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "# KV 캐시의 동작을 보여주기 위한 최소한의 어텐션 모듈\n",
    "class SimpleAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "        \n",
    "        self.qkv_proj = nn.Linear(d_model, d_model * 3)\n",
    "        self.o_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, past_kv: Optional[Tuple[torch.Tensor, torch.Tensor]] = None):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "\n",
    "        # 1. Q, K, V 계산\n",
    "        q, k, v = self.qkv_proj(x).chunk(3, dim=-1)\n",
    "        \n",
    "        # Multi-head 형태로 변환\n",
    "        q = q.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # 2. KV 캐시 처리\n",
    "        if past_kv is not None:\n",
    "            # Decoding 단계: 과거의 K, V를 가져와 현재 K, V와 연결\n",
    "            past_k, past_v = past_kv\n",
    "            k = torch.cat([past_k, k], dim=2)\n",
    "            v = torch.cat([past_v, v], dim=2)\n",
    "        \n",
    "        # 현재 스텝까지의 K, V를 다음 스텝으로 전달하기 위해 저장\n",
    "        present_kv = (k, v)\n",
    "\n",
    "        # 3. 어텐션 계산\n",
    "        attn_output = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "        \n",
    "        output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        \n",
    "        return self.o_proj(output), present_kv\n",
    "\n",
    "# 간단한 모델\n",
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(100, d_model) # vocab_size=100\n",
    "        self.attention = SimpleAttention(d_model, n_heads)\n",
    "        self.lm_head = nn.Linear(d_model, 100) # vocab_size=100\n",
    "        \n",
    "    def forward(self, tokens, past_kv=None):\n",
    "        x = self.embedding(tokens)\n",
    "        x, present_kv = self.attention(x, past_kv)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits, present_kv\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    d_model = 64\n",
    "    n_heads = 4\n",
    "    batch_size = 1\n",
    "    \n",
    "    model = SimpleTransformer(d_model, n_heads)\n",
    "    model.eval()\n",
    "\n",
    "    print(\"--- KV 캐시 동작 과정 시뮬레이션 ---\")\n",
    "\n",
    "    # --- 1단계: Prefill (프롬프트 처리) ---\n",
    "    print(\"\\n[1. Prefill 단계]\")\n",
    "    prompt_tokens = torch.randint(0, 100, (batch_size, 10)) # (B, 10) 크기의 프롬프트\n",
    "    print(f\"입력 프롬프트 shape: {prompt_tokens.shape}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits, kv_cache = model(prompt_tokens)\n",
    "    \n",
    "    # kv_cache[0]은 Key, kv_cache[1]은 Value\n",
    "    print(f\"생성된 KV 캐시 shape: K={kv_cache[0].shape}, V={kv_cache[1].shape}\")\n",
    "    print(\"-> 프롬프트 10개 토큰에 대한 K, V가 캐시에 저장되었습니다.\")\n",
    "\n",
    "    # --- 2단계: Decoding (다음 토큰 생성) ---\n",
    "    print(\"\\n[2. Decoding 단계 (1-step)]\")\n",
    "    # 이제부터는 단 하나의 토큰만 입력으로 넣습니다.\n",
    "    next_token = torch.randint(0, 100, (batch_size, 1))\n",
    "    print(f\"입력 토큰 shape: {next_token.shape}\")\n",
    "    print(f\"전달되는 과거 KV 캐시 shape: K={kv_cache[0].shape}, V={kv_cache[1].shape}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # 이전 스텝의 캐시(kv_cache)를 past_kv로 전달합니다.\n",
    "        new_logits, new_kv_cache = model(next_token, past_kv=kv_cache)\n",
    "\n",
    "    print(f\"업데이트된 KV 캐시 shape: K={new_kv_cache[0].shape}, V={new_kv_cache[1].shape}\")\n",
    "    print(\"-> 새로운 토큰 1개에 대한 K, V가 기존 캐시에 추가되어 길이가 11이 되었습니다.\")\n",
    "\n",
    "    print(\"\\n[2. Decoding 단계 (2-step)]\")\n",
    "    next_token_2 = torch.randint(0, 100, (batch_size, 1))\n",
    "    print(f\"입력 토큰 shape: {next_token_2.shape}\")\n",
    "    print(f\"전달되는 과거 KV 캐시 shape: K={new_kv_cache[0].shape}, V={new_kv_cache[1].shape}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        _, final_kv_cache = model(next_token_2, past_kv=new_kv_cache)\n",
    "\n",
    "    print(f\"업데이트된 KV 캐시 shape: K={final_kv_cache[0].shape}, V={final_kv_cache[1].shape}\")\n",
    "    print(\"-> 다시 새로운 토큰 1개에 대한 K, V가 추가되어 길이가 12가 되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13b234a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 디바이스: NVIDIA A100 80GB PCIe\n",
      "\n",
      "--- KV 캐시 유무에 따른 추론 속도 비교 ---\n",
      "프롬프트 길이: 128, 생성할 토큰 수: 128\n",
      "\n",
      "✅ KV 캐시 사용 시: 0.0315 초\n",
      "❌ KV 캐시 미사용 시: 0.0997 초\n",
      "\n",
      "🚀 성능 향상: KV 캐시 사용 시 약 3.17배 더 빠릅니다.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional, Tuple\n",
    "import time\n",
    "\n",
    "# GPU 사용 가능 여부 확인 및 디바이스 설정\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"사용 디바이스: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"!!! 경고: 이 성능 시연은 GPU 환경에서 실행해야 유의미한 차이를 보입니다. !!!\")\n",
    "\n",
    "# 이전 예제와 동일한 간단한 어텐션 및 모델 정의\n",
    "class SimpleAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "        self.qkv_proj = nn.Linear(d_model, d_model * 3)\n",
    "        self.o_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, past_kv: Optional[Tuple[torch.Tensor, torch.Tensor]] = None):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        q, k, v = self.qkv_proj(x).chunk(3, dim=-1)\n",
    "        q = q.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        if past_kv is not None:\n",
    "            past_k, past_v = past_kv\n",
    "            k = torch.cat([past_k, k], dim=2)\n",
    "            v = torch.cat([past_v, v], dim=2)\n",
    "        \n",
    "        present_kv = (k, v)\n",
    "        attn_output = F.scaled_dot_product_attention(q, k, v, is_causal=(past_kv is None))\n",
    "        output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        return self.o_proj(output), present_kv\n",
    "\n",
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.attention = SimpleAttention(d_model, n_heads)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, tokens, past_kv=None):\n",
    "        x = self.embedding(tokens)\n",
    "        x, present_kv = self.attention(x, past_kv)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits, present_kv\n",
    "\n",
    "# --- 생성 함수 정의 ---\n",
    "\n",
    "def generate_with_cache(model, prompt, n_tokens_to_gen):\n",
    "    \"\"\"KV 캐시를 사용하여 토큰을 생성하는 효율적인 방법\"\"\"\n",
    "    model.eval()\n",
    "    kv_cache = None\n",
    "    generated_tokens = prompt\n",
    "    \n",
    "    # 1. Prefill\n",
    "    with torch.no_grad():\n",
    "        _, kv_cache = model(prompt, past_kv=None)\n",
    "    \n",
    "    # 2. Decoding\n",
    "    next_token = torch.argmax(model(prompt, past_kv=None)[0][:, -1, :], dim=-1, keepdim=True)\n",
    "    \n",
    "    for _ in range(n_tokens_to_gen - 1):\n",
    "        with torch.no_grad():\n",
    "            logits, kv_cache = model(next_token, past_kv=kv_cache)\n",
    "            next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "            generated_tokens = torch.cat([generated_tokens, next_token], dim=1)\n",
    "            \n",
    "    return generated_tokens\n",
    "\n",
    "def generate_without_cache(model, prompt, n_tokens_to_gen):\n",
    "    \"\"\"KV 캐시를 사용하지 않는 비효율적인 방법\"\"\"\n",
    "    model.eval()\n",
    "    generated_tokens = prompt\n",
    "    \n",
    "    for _ in range(n_tokens_to_gen):\n",
    "        with torch.no_grad():\n",
    "            # 매번 전체 시퀀스를 다시 입력\n",
    "            logits, _ = model(generated_tokens, past_kv=None)\n",
    "            next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "            generated_tokens = torch.cat([generated_tokens, next_token], dim=1)\n",
    "            \n",
    "    return generated_tokens\n",
    "\n",
    "# --- 성능 측정 ---\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 모델 및 설정\n",
    "    vocab_size = 50257\n",
    "    d_model = 512\n",
    "    n_heads = 8\n",
    "    \n",
    "    model = SimpleTransformer(vocab_size, d_model, n_heads).to(device)\n",
    "\n",
    "    prompt_len = 128\n",
    "    n_gen = 128\n",
    "    prompt = torch.randint(0, vocab_size, (1, prompt_len)).to(device)\n",
    "\n",
    "    print(\"\\n--- KV 캐시 유무에 따른 추론 속도 비교 ---\")\n",
    "    print(f\"프롬프트 길이: {prompt_len}, 생성할 토큰 수: {n_gen}\\n\")\n",
    "\n",
    "    # 워밍업\n",
    "    for _ in range(3):\n",
    "        generate_with_cache(model, prompt, 10)\n",
    "        generate_without_cache(model, prompt, 10)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    # KV 캐시 사용 성능 측정\n",
    "    t0 = time.time()\n",
    "    generate_with_cache(model, prompt, n_gen)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "    time_with_cache = t1 - t0\n",
    "    print(f\"✅ KV 캐시 사용 시: {time_with_cache:.4f} 초\")\n",
    "\n",
    "    # KV 캐시 미사용 성능 측정\n",
    "    t0 = time.time()\n",
    "    generate_without_cache(model, prompt, n_gen)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "    time_without_cache = t1 - t0\n",
    "    print(f\"❌ KV 캐시 미사용 시: {time_without_cache:.4f} 초\")\n",
    "\n",
    "    # 결과 비교\n",
    "    speedup = time_without_cache / time_with_cache\n",
    "    print(f\"\\n🚀 성능 향상: KV 캐시 사용 시 약 {speedup:.2f}배 더 빠릅니다.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
