{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7d607a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class HybridHead(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, ssm_module, gate_init=(1.0, 1.0)):\n",
    "        \"\"\"\n",
    "        ssm_module: 모듈 인스턴스 (예: Mamba 등)\n",
    "        gate_init: (alpha_init, beta_init)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(d_model, num_heads, batch_first=True)\n",
    "        self.ssm = ssm_module  # 예: Mamba module\n",
    "        self.gate_attn = nn.Parameter(torch.tensor(gate_init[0]))\n",
    "        self.gate_ssm = nn.Parameter(torch.tensor(gate_init[1]))\n",
    "        # 또는 채널별 스케일 벡터를 쓸 수도 있음 (논문에서는 채널 스케일링 언급됨)\n",
    "        # self.scale_attn = nn.Parameter(torch.ones(d_model))\n",
    "        # self.scale_ssm = nn.Parameter(torch.ones(d_model))\n",
    "\n",
    "    def forward(self, x, attn_mask=None, **ssm_kwargs):\n",
    "        \"\"\"\n",
    "        x: (batch, seq_len, d_model)\n",
    "        attn_mask: attention 마스크 (예: sliding window)\n",
    "        ssm_kwargs: SSM 모듈이 필요로 하는 추가 인자\n",
    "        \"\"\"\n",
    "        # 1) Attention branch\n",
    "        attn_out, _ = self.attn(x, x, x, attn_mask=attn_mask)\n",
    "        # 2) SSM branch\n",
    "        ssm_out = self.ssm(x, **ssm_kwargs)\n",
    "        # 3) 스케일 / 게이트 조정 (채널 단위 또는 스칼라)\n",
    "        out = self.gate_attn * attn_out + self.gate_ssm * ssm_out\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "186e4010",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KVCachePool:\n",
    "    def __init__(self, num_layers, share_pairs: bool = True):\n",
    "        self.share_pairs = share_pairs\n",
    "        self.caches = {}\n",
    "\n",
    "    def _key(self, layer_idx: int):\n",
    "        if not self.share_pairs:\n",
    "            return layer_idx\n",
    "        return layer_idx // 2  # 예: 레이어 0 & 1 공유, 2 & 3 공유, etc.\n",
    "\n",
    "    def get(self, layer_idx: int):\n",
    "        return self.caches.get(self._key(layer_idx), None)\n",
    "\n",
    "    def set(self, layer_idx: int, kv):\n",
    "        self.caches[self._key(layer_idx)] = kv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ef168b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sliding_mask(seq_len: int, window_size: int, device):\n",
    "    mask = torch.full((seq_len, seq_len), float(\"-inf\"), device=device)\n",
    "    for i in range(seq_len):\n",
    "        start = max(0, i - window_size + 1)\n",
    "        mask[i, start : i+1] = 0\n",
    "    return mask\n",
    "\n",
    "# 사용할지/말지 옵션\n",
    "class AttentionWithOption(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, use_swa=False, window_size=16):\n",
    "        super().__init__()\n",
    "        self.use_swa = use_swa\n",
    "        self.window_size = window_size\n",
    "        self.attn = nn.MultiheadAttention(d_model, num_heads, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mask = None\n",
    "        if self.use_swa:\n",
    "            mask = build_sliding_mask(x.size(1), self.window_size, x.device)\n",
    "        out, weights = self.attn(x, x, x, attn_mask=mask, need_weights=True)\n",
    "        return out, weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "282ab10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaTokenPrepend(nn.Module):\n",
    "    def __init__(self, num_meta: int, d_model: int, use_meta: bool = True):\n",
    "        super().__init__()\n",
    "        self.use_meta = use_meta\n",
    "        if use_meta:\n",
    "            self.meta = nn.Parameter(torch.randn(1, num_meta, d_model))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.use_meta:\n",
    "            return x\n",
    "        B = x.size(0)\n",
    "        meta = self.meta.expand(B, -1, -1)\n",
    "        x2 = torch.cat([meta, x], dim=1)\n",
    "        return x2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce119304",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HymbaBlock(nn.Module):\n",
    "    def __init__(self, layer_idx, d_model, num_heads, ssm_module,\n",
    "                 kv_cache_pool: KVCachePool,\n",
    "                 use_kv_share=False,\n",
    "                 use_swa=False, window_size=16,\n",
    "                 use_meta=False, num_meta=1):\n",
    "        super().__init__()\n",
    "        self.layer_idx = layer_idx\n",
    "        self.kv_cache_pool = kv_cache_pool\n",
    "        self.use_kv_share = use_kv_share\n",
    "\n",
    "        self.meta_block = MetaTokenPrepend(num_meta, d_model, use_meta)\n",
    "        self.attn_opt = AttentionWithOption(d_model, num_heads, use_swa, window_size)\n",
    "        self.hybrid = HybridHead(d_model, num_heads, ssm_module)  # 병합된 head\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, 4*d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4*d_model, d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 입력 + meta token\n",
    "        x = self.meta_block(x)\n",
    "\n",
    "        # attention part: optional sliding mask\n",
    "        attn_out, _ = self.attn_opt(x)\n",
    "\n",
    "        # hybrid head (attention + SSM)\n",
    "        # 여기선 하이브리드가 내부에서 attn 적용하므로, 단순히 전달\n",
    "        out = self.hybrid(x)\n",
    "\n",
    "        # 캐시 저장 / 재사용 (필요시)\n",
    "        if self.use_kv_share and self.kv_cache_pool is not None:\n",
    "            cached = self.kv_cache_pool.get(self.layer_idx)\n",
    "            if cached is not None:\n",
    "                out = cached\n",
    "            else:\n",
    "                self.kv_cache_pool.set(self.layer_idx, out)\n",
    "\n",
    "        # residual + norm + FF\n",
    "        x2 = self.norm(x + out)\n",
    "        y = x2 + self.ff(x2)\n",
    "\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "824cd43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HymbaModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers,\n",
    "                 ssm_constructor, use_kv_share=True,\n",
    "                 swa_ratio=0.9, window_size=16, num_meta=1):\n",
    "        \"\"\"\n",
    "        swa_ratio: 몇 %의 레이어에 SWA를 적용할지 (예: 0.9 → 90%)\n",
    "        ssm_constructor: 예: lambda: Mamba(...)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.kv_cache_pool = KVCachePool(num_layers, share_pairs=use_kv_share)\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            use_swa = (i / num_layers) >= (1 - swa_ratio)  # 예: 하위 일부 레이어만 global\n",
    "            self.layers.append(\n",
    "                HymbaBlock(layer_idx=i,\n",
    "                           d_model=d_model,\n",
    "                           num_heads=num_heads,\n",
    "                           ssm_module=ssm_constructor(),\n",
    "                           kv_cache_pool=self.kv_cache_pool,\n",
    "                           use_kv_share=use_kv_share,\n",
    "                           use_swa=use_swa,\n",
    "                           window_size=window_size,\n",
    "                           use_meta=(i == 0),\n",
    "                           num_meta=num_meta\n",
    "                )\n",
    "            )\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        x = self.embed(input_ids)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.norm(x)\n",
    "        logits = self.head(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16af3fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/root/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torch-2.5.1%2Bcu121-cp310-cp310-linux_x86_64.whl (780.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m780.4/780.4 MB\u001b[0m \u001b[31m108.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.20.1%2Bcu121-cp310-cp310-linux_x86_64.whl (7.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m122.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.5.1%2Bcu121-cp310-cp310-linux_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m129.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torchtext\n",
      "  Downloading https://download.pytorch.org/whl/torchtext-0.17.0%2Bcpu-cp310-cp310-linux_x86_64.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m137.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.0)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
      "Collecting nvidia-nccl-cu12==2.21.5\n",
      "  Downloading https://download.pytorch.org/whl/nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m109.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m116.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting triton==3.1.0\n",
      "  Downloading https://download.pytorch.org/whl/triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m116.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
      "Collecting sympy==1.13.1\n",
      "  Downloading https://download.pytorch.org/whl/sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m121.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.8.93)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.5.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.32.3)\n",
      "Collecting torchtext\n",
      "  Downloading https://download.pytorch.org/whl/torchtext-0.16.2%2Bcpu-cp310-cp310-linux_x86_64.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m133.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading https://download.pytorch.org/whl/torchtext-0.16.1%2Bcpu-cp310-cp310-linux_x86_64.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m110.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading https://download.pytorch.org/whl/torchtext-0.16.0%2Bcpu-cp310-cp310-linux_x86_64.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m130.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading https://download.pytorch.org/whl/torchtext-0.15.2%2Bcpu-cp310-cp310-linux_x86_64.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m124.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading https://download.pytorch.org/whl/torchtext-0.15.1%2Bcpu-cp310-cp310-linux_x86_64.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m113.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torchdata==0.6.0\n",
      "  Downloading https://download.pytorch.org/whl/torchdata-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m120.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchtext\n",
      "  Downloading https://download.pytorch.org/whl/torchtext-0.15.0%2Bcpu-cp310-cp310-linux_x86_64.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m112.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading https://download.pytorch.org/whl/torchtext-0.14.1-cp310-cp310-linux_x86_64.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m129.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading https://download.pytorch.org/whl/torchtext-0.14.0-cp310-cp310-linux_x86_64.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m114.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading https://download.pytorch.org/whl/torchtext-0.13.1-cp310-cp310-linux_x86_64.whl (1.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m124.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading https://download.pytorch.org/whl/torchtext-0.13.0-cp310-cp310-linux_x86_64.whl (1.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m125.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading https://download.pytorch.org/whl/torchtext-0.12.0-cp310-cp310-linux_x86_64.whl (10.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m118.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading https://download.pytorch.org/whl/torchtext-0.6.0-py3-none-any.whl (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.4/64.4 KB\u001b[0m \u001b[31m313.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.16.0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from torchtext) (0.2.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2024.6.2)\n",
      "Installing collected packages: triton, sympy, nvidia-nccl-cu12, nvidia-cudnn-cu12, torch, torchvision, torchtext, torchaudio\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.1.0\n",
      "    Uninstalling triton-2.1.0:\n",
      "      Successfully uninstalled triton-2.1.0\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.14.0\n",
      "    Uninstalling sympy-1.14.0:\n",
      "      Successfully uninstalled sympy-1.14.0\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.20.5\n",
      "    Uninstalling nvidia-nccl-cu12-2.20.5:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.20.5\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 8.9.2.26\n",
      "    Uninstalling nvidia-cudnn-cu12-8.9.2.26:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-8.9.2.26\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gfpgan 1.3.8 requires tb-nightly, which is not installed.\n",
      "basicsr 1.4.2 requires tb-nightly, which is not installed.\n",
      "torchdata 0.7.0 requires torch==2.1.0, but you have torch 2.5.1+cu121 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nvidia-cudnn-cu12-9.1.0.70 nvidia-nccl-cu12-2.21.5 sympy-1.13.1 torch-2.5.1+cu121 torchaudio-2.5.1+cu121 torchtext-0.6.0 torchvision-0.20.1+cu121 triton-3.1.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio torchtext --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf2ee53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/root/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting mamba-ssm\n",
      "  Downloading mamba_ssm-2.2.5.tar.gz (113 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.8/113.8 KB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l/"
     ]
    }
   ],
   "source": [
    "!pip install mamba-ssm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d14987c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "/usr/local/lib/python3.10/dist-packages/selective_scan_cuda.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN3c104cuda9SetDeviceEab",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmamba_ssm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Mamba\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/mamba_ssm/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.2.5\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmamba_ssm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mselective_scan_interface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m selective_scan_fn, mamba_inner_fn\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmamba_ssm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmamba_simple\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Mamba\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmamba_ssm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmamba2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Mamba2\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/mamba_ssm/ops/selective_scan_interface.py:20\u001b[0m\n\u001b[1;32m     16\u001b[0m     causal_conv1d_update_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmamba_ssm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtriton\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayer_norm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _layer_norm_fwd\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mselective_scan_cuda\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSelectiveScanFn\u001b[39;00m(torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mFunction):\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(ctx, u, delta, A, B, C, D\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, z\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, delta_bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, delta_softplus\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     27\u001b[0m                 return_last_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[0;31mImportError\u001b[0m: /usr/local/lib/python3.10/dist-packages/selective_scan_cuda.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN3c104cuda9SetDeviceEab"
     ]
    }
   ],
   "source": [
    "from mamba_ssm import Mamba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be1054a9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "/usr/local/lib/python3.10/dist-packages/selective_scan_cuda.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN3c1021throwNullDataPtrErrorEv",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmamba_ssm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Mamba\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Device 설정\u001b[39;00m\n\u001b[1;32m      6\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/mamba_ssm/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.2.5\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmamba_ssm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mselective_scan_interface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m selective_scan_fn, mamba_inner_fn\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmamba_ssm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmamba_simple\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Mamba\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmamba_ssm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmamba2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Mamba2\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/mamba_ssm/ops/selective_scan_interface.py:20\u001b[0m\n\u001b[1;32m     16\u001b[0m     causal_conv1d_update_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmamba_ssm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtriton\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayer_norm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _layer_norm_fwd\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mselective_scan_cuda\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSelectiveScanFn\u001b[39;00m(torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mFunction):\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(ctx, u, delta, A, B, C, D\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, z\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, delta_bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, delta_softplus\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     27\u001b[0m                 return_last_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[0;31mImportError\u001b[0m: /usr/local/lib/python3.10/dist-packages/selective_scan_cuda.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN3c1021throwNullDataPtrErrorEv"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from mamba_ssm import Mamba\n",
    "\n",
    "# Device 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 예제 HymbaModel 생성\n",
    "vocab_size = 10000\n",
    "d_model = 128\n",
    "num_heads = 4\n",
    "num_layers = 6\n",
    "window_size = 32\n",
    "num_meta = 2\n",
    "\n",
    "def make_ssm():\n",
    "    return Mamba(d_model, d_state=16, d_conv=4, expand=2).to(device)\n",
    "\n",
    "model = HymbaModel(vocab_size, d_model, num_heads, num_layers,\n",
    "                   ssm_constructor=make_ssm,\n",
    "                   use_kv_share=True,\n",
    "                   swa_ratio=0.8, window_size=window_size,\n",
    "                   num_meta=num_meta).to(device)\n",
    "\n",
    "# 입력을 device로 이동\n",
    "x = torch.randint(0, vocab_size, (2, 20), device=device)\n",
    "\n",
    "# Forward pass\n",
    "logits = model(x)\n",
    "print(\"logits shape:\", logits.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d93480c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# WikiText-2 불러오기\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "train_text = \" \".join(dataset[\"train\"][\"text\"])\n",
    "valid_text = \" \".join(dataset[\"validation\"][\"text\"])\n",
    "test_text  = \" \".join(dataset[\"test\"][\"text\"])\n",
    "\n",
    "print(\"Train sample:\", train_text[:200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64326c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import torch\n",
    "\n",
    "def build_vocab(texts, min_freq=2):\n",
    "    counter = Counter()\n",
    "    for t in texts.split():\n",
    "        counter[t] += 1\n",
    "    vocab = {w: i for i, (w, c) in enumerate(counter.items()) if c >= min_freq}\n",
    "    vocab[\"<unk>\"] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "vocab = build_vocab(train_text)\n",
    "\n",
    "def encode(text):\n",
    "    return torch.tensor([vocab.get(w, vocab[\"<unk>\"]) for w in text.split()], dtype=torch.long)\n",
    "\n",
    "train_ids = encode(train_text)\n",
    "val_ids   = encode(valid_text)\n",
    "test_ids  = encode(test_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22c8ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 64\n",
    "batch_size = 32\n",
    "\n",
    "def batchify(data, bsz):\n",
    "    nbatch = data.size(0) // bsz\n",
    "    data = data[:nbatch*bsz]\n",
    "    return data.view(bsz, -1).t().contiguous()\n",
    "\n",
    "train_data = batchify(train_ids, batch_size).to(\"cuda\")\n",
    "val_data   = batchify(val_ids, batch_size).to(\"cuda\")\n",
    "\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(bptt, source.size(0)-1-i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    return data, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b2ea89",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model, num_heads, num_layers = 128, 4, 6\n",
    "\n",
    "model = HymbaModel(len(vocab), d_model, num_heads, num_layers,\n",
    "                   swa_layers_ratio=0.8,   # 하위 80% 레이어는 SWA\n",
    "                   use_kv_share=True,      # KV Cache Sharing\n",
    "                   num_meta=2).to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332d727a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4)\n",
    "scaler = GradScaler()\n",
    "\n",
    "def evaluate(data_source):\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0)-1, bptt):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            with autocast(device_type=\"cuda\"):\n",
    "                output = model(data)\n",
    "                loss = criterion(output.view(-1, len(vocab)), targets)\n",
    "            total_loss += loss.item() * len(data)\n",
    "    return total_loss / (len(data_source)-1)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(1):\n",
    "    for i in range(0, train_data.size(0)-1, bptt):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        optimizer.zero_grad()\n",
    "        with autocast(device_type=\"cuda\"):\n",
    "            output = model(data)\n",
    "            loss = criterion(output.view(-1, len(vocab)), targets)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        if i % (bptt*100) == 0:\n",
    "            print(f\"step {i}, loss {loss.item():.4f}\")\n",
    "\n",
    "val_loss = evaluate(val_data)\n",
    "print(\"Validation Loss:\", val_loss, \" | PPL:\", torch.exp(torch.tensor(val_loss)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e3faac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, start_text=\"the\", max_len=30):\n",
    "    model.eval()\n",
    "    tokens = torch.tensor([[vocab.get(w, vocab[\"<unk>\"]) for w in start_text.split()]], device=\"cuda\")\n",
    "    for _ in range(max_len):\n",
    "        with torch.no_grad(), autocast(device_type=\"cuda\"):\n",
    "            logits = model(tokens)\n",
    "            next_token = torch.argmax(logits[:, -1], dim=-1).unsqueeze(0)\n",
    "            tokens = torch.cat([tokens, next_token], dim=1)\n",
    "    inv_vocab = {i: w for w, i in vocab.items()}\n",
    "    return \" \".join(inv_vocab[t.item()] for t in tokens[0])\n",
    "\n",
    "print(\"Sample generation:\", generate(model, \"the king\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
