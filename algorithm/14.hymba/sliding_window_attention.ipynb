{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92955e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 디바이스: cuda\n",
      "\n",
      "--- 마스크 생성 시작 (seq_len=16, window_size=4) ---\n",
      "\n",
      "[1단계] 인과성(Causal) 마스크 (미래 차단):\n",
      " tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "\n",
      "[2단계] 슬라이딩 윈도우 전용 마스크 (먼 과거 차단):\n",
      " tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "\n",
      "[3단계] 최종 결합된 SWA 마스크:\n",
      " tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "---------------------------------------------\n",
      "\n",
      "--- 실제 어텐션 계산 예시 ---\n",
      "입력 Q, K, V Shape: torch.Size([1, 2, 16, 4])\n",
      "SWA 적용 후 출력 Shape: torch.Size([1, 2, 16, 4])\n",
      "계산이 성공적으로 완료되었습니다.\n",
      "\n",
      "--- 연산량 비교 ---\n",
      "Full Attention 상대적 연산량: 256\n",
      "SWA 상대적 연산량: 64\n",
      "효율성 향상: 4.00 배\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def create_sliding_window_causal_mask(seq_len: int, window_size: int, device: torch.device = None):\n",
    "    \"\"\"\n",
    "    언어 모델을 위한 인과적 슬라이딩 윈도우 어텐션 마스크를 생성하고,\n",
    "    그 과정을 단계별로 출력하여 이해를 돕습니다.\n",
    "    \n",
    "    Args:\n",
    "        seq_len (int): 전체 시퀀스의 길이입니다.\n",
    "        window_size (int): 어텐션을 허용할 창의 크기입니다.\n",
    "        device (torch.device, optional): 텐서를 생성할 디바이스입니다.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: 어텐션 계산에 사용될 최종 불리언(boolean) 마스크입니다. \n",
    "    \"\"\"\n",
    "    print(f\"\\n--- 마스크 생성 시작 (seq_len={seq_len}, window_size={window_size}) ---\")\n",
    "    \n",
    "    # 1. 인과성 마스크 (Causal Mask) 생성\n",
    "    #    미래 토큰을 보지 못하도록 대각선을 포함한 하삼각 행렬을 만듭니다.\n",
    "    causal_mask = torch.tril(torch.ones((seq_len, seq_len), device=device, dtype=torch.bool), diagonal=0)\n",
    "    print(\"\\n[1단계] 인과성(Causal) 마스크 (미래 차단):\\n\", causal_mask.int())\n",
    "\n",
    "    # 2. 슬라이딩 윈도우 전용 마스크 생성\n",
    "    #    각 토큰이 자신으로부터 window_size 만큼 떨어진 과거까지만 보도록 제한하는 '밴드' 형태의 마스크를 만듭니다.\n",
    "    #    (오류 수정: bool 텐서의 뺄셈(-)을 논리적 XOR(^) 연산으로 변경)\n",
    "    sliding_window_only_mask = torch.tril(torch.ones_like(causal_mask), diagonal=0) ^ torch.tril(torch.ones_like(causal_mask), diagonal=-window_size)\n",
    "    print(\"\\n[2단계] 슬라이딩 윈도우 전용 마스크 (먼 과거 차단):\\n\", sliding_window_only_mask.int())\n",
    "    \n",
    "    # 3. 최종 마스크 결합\n",
    "    #    두 조건을 모두 만족하는 최종 마스크를 생성합니다.\n",
    "    final_mask = causal_mask & sliding_window_only_mask\n",
    "    print(\"\\n[3단계] 최종 결합된 SWA 마스크:\\n\", final_mask.int())\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    return final_mask\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # --- 설정 ---\n",
    "    seq_len = 16\n",
    "    window_size = 4\n",
    "    d_model = 8\n",
    "    n_heads = 2\n",
    "    batch_size = 1\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"사용 디바이스: {device}\")\n",
    "\n",
    "    # --- 마스크 생성 ---\n",
    "    attention_mask = create_sliding_window_causal_mask(seq_len, window_size, device)\n",
    "    \n",
    "    # --- 실제 어텐션 계산에 적용 ---\n",
    "    print(\"\\n--- 실제 어텐션 계산 예시 ---\")\n",
    "    \n",
    "    q = torch.randn(batch_size, n_heads, seq_len, d_model // n_heads, device=device)\n",
    "    k = torch.randn(batch_size, n_heads, seq_len, d_model // n_heads, device=device)\n",
    "    v = torch.randn(batch_size, n_heads, seq_len, d_model // n_heads, device=device)\n",
    "    \n",
    "    print(f\"입력 Q, K, V Shape: {q.shape}\")\n",
    "    \n",
    "    # Full Attention (비교용)\n",
    "    # 연산량은 seq_len^2에 비례합니다.\n",
    "    output_full = F.scaled_dot_product_attention(q, k, v, attn_mask=torch.tril(torch.ones(seq_len, seq_len, device=device, dtype=torch.bool)))\n",
    "    \n",
    "    # SWA\n",
    "    # 연산량은 seq_len * window_size에 비례합니다.\n",
    "    output_swa = F.scaled_dot_product_attention(q, k, v, attn_mask=attention_mask)\n",
    "    \n",
    "    print(f\"SWA 적용 후 출력 Shape: {output_swa.shape}\")\n",
    "    print(\"계산이 성공적으로 완료되었습니다.\")\n",
    "    \n",
    "    # 연산량 비교\n",
    "    print(\"\\n--- 연산량 비교 ---\")\n",
    "    full_attn_ops = seq_len * seq_len\n",
    "    swa_ops = seq_len * window_size\n",
    "    print(f\"Full Attention 상대적 연산량: {full_attn_ops}\")\n",
    "    print(f\"SWA 상대적 연산량: {swa_ops}\")\n",
    "    print(f\"효율성 향상: {full_attn_ops / swa_ops:.2f} 배\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
