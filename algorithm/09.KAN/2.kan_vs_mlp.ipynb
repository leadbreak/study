{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 3D 시각화를 위한 라이브러리\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# 모델 요약을 위한 라이브러리\n",
    "from torchsummary import summary\n",
    "\n",
    "# 데이터셋과 데이터로더 생성\n",
    "from torch.utils.data import DataLoader, TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 데이터 생성\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# 입력 데이터 생성\n",
    "x1 = np.linspace(-2 * np.pi, 2 * np.pi, 200)\n",
    "x2 = np.linspace(-2 * np.pi, 2 * np.pi, 200)\n",
    "X1, X2 = np.meshgrid(x1, x2)\n",
    "\n",
    "# 입력 데이터를 벡터 형태로 변환\n",
    "X_input = np.stack([X1.flatten(), X2.flatten()], axis=1)\n",
    "\n",
    "# 실제 함수 값 계산\n",
    "def f_true(x):\n",
    "    return np.sin(x[:, 0] + x[:, 1])\n",
    "\n",
    "Y_true = f_true(X_input)\n",
    "\n",
    "# 데이터를 PyTorch 텐서로 변환\n",
    "X_input_tensor = torch.from_numpy(X_input).float()\n",
    "Y_true_tensor = torch.from_numpy(Y_true).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "dataset = TensorDataset(X_input_tensor, Y_true_tensor)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kolmogorov-Arnold Network 정의\n",
    "class KAN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size):\n",
    "        super(KAN, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        # 각 입력 차원에 대한 일변량 함수 φ_i\n",
    "        self.phi_layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(1, hidden_size),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(hidden_size, hidden_size),\n",
    "                nn.Tanh()\n",
    "            ) for _ in range(input_dim)\n",
    "        ])\n",
    "        # 최종 일변량 함수 ψ\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, input_dim]\n",
    "        outputs = []\n",
    "        for i in range(self.input_dim):\n",
    "            xi = x[:, i].unsqueeze(1)  # [batch_size, 1]\n",
    "            phi_i = self.phi_layers[i](xi)  # [batch_size, hidden_size]\n",
    "            outputs.append(phi_i)\n",
    "        # 출력 합산\n",
    "        s = sum(outputs)  # [batch_size, hidden_size]\n",
    "        # 최종 함수 적용\n",
    "        out = self.psi(s)  # [batch_size, 1]\n",
    "        return out.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP 모델 정의\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saver 클래스 정의\n",
    "class Saver:\n",
    "    def __init__(self):\n",
    "        self.outputs = []\n",
    "\n",
    "    def save(self, epoch, Y_pred):\n",
    "        self.outputs.append((epoch, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 초기화\n",
    "input_dim = 2\n",
    "hidden_size = 10\n",
    "\n",
    "kan_model = KAN(input_dim=input_dim, hidden_size=hidden_size)\n",
    "mlp_model = MLP(input_dim=input_dim, hidden_size=hidden_size*20) # mlp model의 파라미터가 100배 이상 많도록\n",
    "\n",
    "# 모델 요약 출력\n",
    "print(\"KAN 모델 요약:\")\n",
    "summary(kan_model, input_size=(input_dim,), device='cpu')\n",
    "\n",
    "print(\"\\nMLP 모델 요약:\")\n",
    "summary(mlp_model, input_size=(input_dim,), device='cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실 함수와 옵티마이저 정의\n",
    "criterion = nn.MSELoss()\n",
    "kan_optimizer = optim.Adam(kan_model.parameters(), lr=0.001)\n",
    "mlp_optimizer = optim.Adam(mlp_model.parameters(), lr=0.001)\n",
    "\n",
    "# 학습 루프\n",
    "epochs = 100\n",
    "\n",
    "# 에포크별 손실 저장용 리스트\n",
    "kan_losses = []\n",
    "mlp_losses = []\n",
    "\n",
    "# 시각화를 위한 예측값 저장용 Saver 객체 생성\n",
    "kan_saver = Saver()\n",
    "mlp_saver = Saver()\n",
    "\n",
    "# 시각화할 에포크 지정 (5개의 에포크로 줄임)\n",
    "visualization_epochs = [19, 39, 59, 79, 99]  # 총 5개의 에포크\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    kan_model.train()\n",
    "    mlp_model.train()\n",
    "    \n",
    "    kan_epoch_loss = 0\n",
    "    mlp_epoch_loss = 0\n",
    "    \n",
    "    for X_batch, Y_batch in loader:\n",
    "        # 그래디언트 초기화\n",
    "        kan_optimizer.zero_grad()\n",
    "        mlp_optimizer.zero_grad()\n",
    "        \n",
    "        # KAN 모델 학습\n",
    "        kan_outputs = kan_model(X_batch)\n",
    "        kan_loss = criterion(kan_outputs, Y_batch)\n",
    "        kan_loss.backward()\n",
    "        kan_optimizer.step()\n",
    "        \n",
    "        kan_epoch_loss += kan_loss.item() * X_batch.size(0)\n",
    "        \n",
    "        # MLP 모델 학습\n",
    "        mlp_outputs = mlp_model(X_batch)\n",
    "        mlp_loss = criterion(mlp_outputs, Y_batch)\n",
    "        mlp_loss.backward()\n",
    "        mlp_optimizer.step()\n",
    "        \n",
    "        mlp_epoch_loss += mlp_loss.item() * X_batch.size(0)\n",
    "    \n",
    "    # 에포크별 평균 손실 계산\n",
    "    kan_epoch_loss /= len(dataset)\n",
    "    mlp_epoch_loss /= len(dataset)\n",
    "    \n",
    "    # 손실 저장\n",
    "    kan_losses.append(kan_epoch_loss)\n",
    "    mlp_losses.append(mlp_epoch_loss)\n",
    "    \n",
    "    # 손실 출력\n",
    "    print(f'Epoch {epoch+1}/{epochs}, KAN Loss: {kan_epoch_loss:.4f}, MLP Loss: {mlp_epoch_loss:.4f}')\n",
    "    \n",
    "    # 특정 에포크마다 예측값 저장\n",
    "    if epoch in visualization_epochs:\n",
    "        # 모델 평가 모드로 전환\n",
    "        kan_model.eval()\n",
    "        mlp_model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            kan_pred = kan_model(X_input_tensor)\n",
    "            mlp_pred = mlp_model(X_input_tensor)\n",
    "        \n",
    "        # 예측 값 저장\n",
    "        kan_saver.save(epoch, kan_pred.numpy())\n",
    "        mlp_saver.save(epoch, mlp_pred.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제 함수 값\n",
    "Z_true = Y_true.reshape(X1.shape)\n",
    "\n",
    "# 손실 곡선 시각화\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(range(1, epochs+1), kan_losses, label='KAN Loss')\n",
    "plt.plot(range(1, epochs+1), mlp_losses, label='MLP Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 에포크별 결과 시각화 함수 정의\n",
    "def plot_approximations(X1, X2, Y_true, kan_saver, mlp_saver):\n",
    "    num_cols = 5  # 한 행에 보여줄 그래프 수\n",
    "    num_rows = 3  # 실제 함수 + KAN + MLP\n",
    "\n",
    "    fig = plt.figure(figsize=(4*num_cols, 4*num_rows))\n",
    "\n",
    "    # 실제 함수 시각화 (첫 번째 행)\n",
    "    for i in range(num_cols):\n",
    "        ax = fig.add_subplot(num_rows, num_cols, i+1, projection='3d')\n",
    "        if i == 0:\n",
    "            ax.plot_surface(X1, X2, Y_true.reshape(X1.shape), cmap='viridis')\n",
    "            ax.set_title('Actual Function')\n",
    "            ax.set_xlabel('x1')\n",
    "            ax.set_ylabel('x2')\n",
    "            ax.set_zlabel('f(x1, x2)')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            ax.set_zticks([])\n",
    "        else:\n",
    "            # 빈 플롯 생성\n",
    "            ax.axis('off')\n",
    "\n",
    "    # KAN 에포크별 근사 결과 시각화 (두 번째 행)\n",
    "    for i, (epoch, Y_pred) in enumerate(kan_saver.outputs):\n",
    "        ax = fig.add_subplot(num_rows, num_cols, num_cols + i+1, projection='3d')\n",
    "        ax.plot_surface(X1, X2, Y_pred.reshape(X1.shape), cmap='viridis')\n",
    "        ax.set_title(f'KAN Epoch {epoch+1}')\n",
    "        ax.set_xlabel('x1')\n",
    "        ax.set_ylabel('x2')\n",
    "        ax.set_zlabel('Approximation')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_zticks([])\n",
    "\n",
    "    # MLP 에포크별 근사 결과 시각화 (세 번째 행)\n",
    "    for i, (epoch, Y_pred) in enumerate(mlp_saver.outputs):\n",
    "        ax = fig.add_subplot(num_rows, num_cols, 2*num_cols + i+1, projection='3d')\n",
    "        ax.plot_surface(X1, X2, Y_pred.reshape(X1.shape), cmap='viridis')\n",
    "        ax.set_title(f'MLP Epoch {epoch+1}')\n",
    "        ax.set_xlabel('x1')\n",
    "        ax.set_ylabel('x2')\n",
    "        ax.set_zlabel('Approximation')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_zticks([])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 전체 결과 시각화\n",
    "plot_approximations(X1, X2, Y_true, kan_saver, mlp_saver)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KAN 모델 요약:\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                   [-1, 64]             128\n",
      "              SiLU-2                   [-1, 64]               0\n",
      "            Linear-3                   [-1, 64]           4,160\n",
      "              SiLU-4                   [-1, 64]               0\n",
      "            Linear-5                   [-1, 64]             128\n",
      "              SiLU-6                   [-1, 64]               0\n",
      "            Linear-7                   [-1, 64]           4,160\n",
      "              SiLU-8                   [-1, 64]               0\n",
      "            Linear-9                   [-1, 64]           4,160\n",
      "             SiLU-10                   [-1, 64]               0\n",
      "           Linear-11                    [-1, 1]              65\n",
      "================================================================\n",
      "Total params: 12,801\n",
      "Trainable params: 12,801\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.05\n",
      "Estimated Total Size (MB): 0.05\n",
      "----------------------------------------------------------------\n",
      "\n",
      "MLP 모델 요약:\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 640]           1,920\n",
      "              SiLU-2                  [-1, 640]               0\n",
      "            Linear-3                  [-1, 640]         410,240\n",
      "              SiLU-4                  [-1, 640]               0\n",
      "            Linear-5                  [-1, 640]         410,240\n",
      "              SiLU-6                  [-1, 640]               0\n",
      "            Linear-7                    [-1, 1]             641\n",
      "================================================================\n",
      "Total params: 823,041\n",
      "Trainable params: 823,041\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.03\n",
      "Params size (MB): 3.14\n",
      "Estimated Total Size (MB): 3.17\n",
      "----------------------------------------------------------------\n",
      "Epoch 1/100, KAN Loss: 0.4987, MLP Loss: 0.5106\n",
      "Epoch 2/100, KAN Loss: 0.4779, MLP Loss: 0.1509\n",
      "Epoch 3/100, KAN Loss: 0.3789, MLP Loss: 0.0430\n",
      "Epoch 4/100, KAN Loss: 0.2077, MLP Loss: 0.0182\n",
      "Epoch 5/100, KAN Loss: 0.0758, MLP Loss: 0.0138\n",
      "Epoch 6/100, KAN Loss: 0.0301, MLP Loss: 0.0078\n",
      "Epoch 7/100, KAN Loss: 0.0205, MLP Loss: 0.0054\n",
      "Epoch 8/100, KAN Loss: 0.0159, MLP Loss: 0.0068\n",
      "Epoch 9/100, KAN Loss: 0.0120, MLP Loss: 0.0037\n",
      "Epoch 10/100, KAN Loss: 0.0093, MLP Loss: 0.0053\n",
      "Epoch 11/100, KAN Loss: 0.0073, MLP Loss: 0.0083\n",
      "Epoch 12/100, KAN Loss: 0.0059, MLP Loss: 0.0025\n",
      "Epoch 13/100, KAN Loss: 0.0049, MLP Loss: 0.0018\n",
      "Epoch 14/100, KAN Loss: 0.0039, MLP Loss: 0.0014\n",
      "Epoch 15/100, KAN Loss: 0.0034, MLP Loss: 0.0012\n",
      "Epoch 16/100, KAN Loss: 0.0029, MLP Loss: 0.0047\n",
      "Epoch 17/100, KAN Loss: 0.0026, MLP Loss: 0.0017\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 3D 시각화를 위한 라이브러리\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# 모델 요약을 위한 라이브러리\n",
    "from torchsummary import summary\n",
    "\n",
    "# 데이터 생성\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# 입력 데이터 생성\n",
    "x1 = np.linspace(-2 * np.pi, 2 * np.pi, 200)\n",
    "x2 = np.linspace(-2 * np.pi, 2 * np.pi, 200)\n",
    "X1, X2 = np.meshgrid(x1, x2)\n",
    "\n",
    "# 입력 데이터를 벡터 형태로 변환\n",
    "X_input = np.stack([X1.flatten(), X2.flatten()], axis=1)\n",
    "\n",
    "# 실제 함수 값 계산\n",
    "def f_true(x):\n",
    "    return np.sin(x[:, 0] + x[:, 1])\n",
    "\n",
    "Y_true = f_true(X_input)\n",
    "\n",
    "# 데이터를 PyTorch 텐서로 변환\n",
    "X_input_tensor = torch.from_numpy(X_input).float()\n",
    "Y_true_tensor = torch.from_numpy(Y_true).float()\n",
    "\n",
    "# 데이터셋과 데이터로더 생성\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "batch_size = 1000\n",
    "dataset = TensorDataset(X_input_tensor, Y_true_tensor)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# KAN 모델 정의\n",
    "class KAN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size):\n",
    "        super(KAN, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # 각 입력 차원에 대한 일변량 함수 φ_i\n",
    "        self.phi_layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(1, hidden_size),\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(hidden_size, hidden_size),\n",
    "                nn.SiLU()\n",
    "            ) for _ in range(input_dim)\n",
    "        ])\n",
    "\n",
    "        # 최종 일변량 함수 ψ\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, input_dim]\n",
    "        outputs = []\n",
    "        for i in range(self.input_dim):\n",
    "            xi = x[:, i].unsqueeze(1)  # [batch_size, 1]\n",
    "            phi_i = self.phi_layers[i](xi)  # [batch_size, hidden_size]\n",
    "            outputs.append(phi_i)\n",
    "        # 출력 합산\n",
    "        s = sum(outputs)  # [batch_size, hidden_size]\n",
    "        # 최종 함수 적용\n",
    "        out = self.psi(s)  # [batch_size, 1]\n",
    "        return out.squeeze()\n",
    "\n",
    "# MLP 모델 정의 (KAN과 구조를 맞춤)\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size, n_hidden_layers=3):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(input_dim, hidden_size))\n",
    "        layers.append(nn.SiLU())\n",
    "        for _ in range(n_hidden_layers - 1):\n",
    "            layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            layers.append(nn.SiLU())\n",
    "        layers.append(nn.Linear(hidden_size, 1))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze()\n",
    "\n",
    "# Saver 클래스 정의\n",
    "class Saver:\n",
    "    def __init__(self):\n",
    "        self.outputs = []\n",
    "\n",
    "    def save(self, epoch, Y_pred):\n",
    "        self.outputs.append((epoch, Y_pred))\n",
    "\n",
    "# 모델 초기화\n",
    "input_dim = 2\n",
    "hidden_size = 64  # 히든 사이즈를 늘려서 복잡한 함수 근사에 적합하게 조정\n",
    "n_hidden_layers = 3  # 히든 레이어 수를 늘림\n",
    "\n",
    "kan_model = KAN(input_dim=input_dim, hidden_size=hidden_size)\n",
    "mlp_model = MLP(input_dim=input_dim, hidden_size=hidden_size*10, n_hidden_layers=n_hidden_layers)\n",
    "\n",
    "# 모델 요약 출력\n",
    "print(\"KAN 모델 요약:\")\n",
    "summary(kan_model, input_size=(input_dim,), device='cpu')\n",
    "\n",
    "print(\"\\nMLP 모델 요약:\")\n",
    "summary(mlp_model, input_size=(input_dim,), device='cpu')\n",
    "\n",
    "# 손실 함수와 옵티마이저 정의\n",
    "criterion = nn.MSELoss()\n",
    "kan_optimizer = optim.Adam(kan_model.parameters(), lr=0.001)\n",
    "mlp_optimizer = optim.Adam(mlp_model.parameters(), lr=0.001)\n",
    "\n",
    "# 학습 루프\n",
    "epochs = 100\n",
    "\n",
    "# 에포크별 손실 저장용 리스트\n",
    "kan_losses = []\n",
    "mlp_losses = []\n",
    "\n",
    "# 시각화를 위한 예측값 저장용 Saver 객체 생성\n",
    "kan_saver = Saver()\n",
    "mlp_saver = Saver()\n",
    "\n",
    "# 시각화할 에포크 지정 (5개의 에포크로 줄임)\n",
    "visualization_epochs = [19, 39, 59, 79, 99]  # 총 5개의 에포크\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    kan_model.train()\n",
    "    mlp_model.train()\n",
    "    \n",
    "    kan_epoch_loss = 0\n",
    "    mlp_epoch_loss = 0\n",
    "    \n",
    "    for X_batch, Y_batch in loader:\n",
    "        # 그래디언트 초기화\n",
    "        kan_optimizer.zero_grad()\n",
    "        mlp_optimizer.zero_grad()\n",
    "        \n",
    "        # KAN 모델 학습\n",
    "        kan_outputs = kan_model(X_batch)\n",
    "        kan_loss = criterion(kan_outputs, Y_batch)\n",
    "        kan_loss.backward()\n",
    "        kan_optimizer.step()\n",
    "        \n",
    "        kan_epoch_loss += kan_loss.item() * X_batch.size(0)\n",
    "        \n",
    "        # MLP 모델 학습\n",
    "        mlp_outputs = mlp_model(X_batch)\n",
    "        mlp_loss = criterion(mlp_outputs, Y_batch)\n",
    "        mlp_loss.backward()\n",
    "        mlp_optimizer.step()\n",
    "        \n",
    "        mlp_epoch_loss += mlp_loss.item() * X_batch.size(0)\n",
    "    \n",
    "    # 에포크별 평균 손실 계산\n",
    "    kan_epoch_loss /= len(dataset)\n",
    "    mlp_epoch_loss /= len(dataset)\n",
    "    \n",
    "    # 손실 저장\n",
    "    kan_losses.append(kan_epoch_loss)\n",
    "    mlp_losses.append(mlp_epoch_loss)\n",
    "    \n",
    "    # 손실 출력\n",
    "    print(f'Epoch {epoch+1}/{epochs}, KAN Loss: {kan_epoch_loss:.4f}, MLP Loss: {mlp_epoch_loss:.4f}')\n",
    "    \n",
    "    # 특정 에포크마다 예측값 저장\n",
    "    if epoch in visualization_epochs:\n",
    "        # 모델 평가 모드로 전환\n",
    "        kan_model.eval()\n",
    "        mlp_model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            kan_pred = kan_model(X_input_tensor)\n",
    "            mlp_pred = mlp_model(X_input_tensor)\n",
    "        \n",
    "        # 예측 값 저장\n",
    "        kan_saver.save(epoch, kan_pred.numpy())\n",
    "        mlp_saver.save(epoch, mlp_pred.numpy())\n",
    "\n",
    "# 실제 함수 값\n",
    "Z_true = Y_true.reshape(X1.shape)\n",
    "\n",
    "# 손실 곡선 시각화\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(range(1, epochs+1), kan_losses, label='KAN Loss')\n",
    "plt.plot(range(1, epochs+1), mlp_losses, label='MLP Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 에포크별 결과 시각화 함수 정의\n",
    "def plot_approximations(X1, X2, Y_true, kan_saver, mlp_saver):\n",
    "    num_cols = 5  # 한 행에 보여줄 그래프 수\n",
    "    num_rows = 3  # 실제 함수 + KAN + MLP\n",
    "\n",
    "    fig = plt.figure(figsize=(4*num_cols, 4*num_rows))\n",
    "\n",
    "    # 실제 함수 시각화 (첫 번째 행)\n",
    "    for i in range(num_cols):\n",
    "        ax = fig.add_subplot(num_rows, num_cols, i+1, projection='3d')\n",
    "        if i == 0:\n",
    "            ax.plot_surface(X1, X2, Y_true.reshape(X1.shape), cmap='viridis')\n",
    "            ax.set_title('Actual Function')\n",
    "            ax.set_xlabel('x1')\n",
    "            ax.set_ylabel('x2')\n",
    "            ax.set_zlabel('f(x1, x2)')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            ax.set_zticks([])\n",
    "        else:\n",
    "            # 빈 플롯 생성\n",
    "            ax.axis('off')\n",
    "\n",
    "    # KAN 에포크별 근사 결과 시각화 (두 번째 행)\n",
    "    for i, (epoch, Y_pred) in enumerate(kan_saver.outputs):\n",
    "        ax = fig.add_subplot(num_rows, num_cols, num_cols + i+1, projection='3d')\n",
    "        ax.plot_surface(X1, X2, Y_pred.reshape(X1.shape), cmap='viridis')\n",
    "        ax.set_title(f'KAN Epoch {epoch+1}')\n",
    "        ax.set_xlabel('x1')\n",
    "        ax.set_ylabel('x2')\n",
    "        ax.set_zlabel('Approximation')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_zticks([])\n",
    "\n",
    "    # MLP 에포크별 근사 결과 시각화 (세 번째 행)\n",
    "    for i, (epoch, Y_pred) in enumerate(mlp_saver.outputs):\n",
    "        ax = fig.add_subplot(num_rows, num_cols, 2*num_cols + i+1, projection='3d')\n",
    "        ax.plot_surface(X1, X2, Y_pred.reshape(X1.shape), cmap='viridis')\n",
    "        ax.set_title(f'MLP Epoch {epoch+1}')\n",
    "        ax.set_xlabel('x1')\n",
    "        ax.set_ylabel('x2')\n",
    "        ax.set_zlabel('Approximation')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_zticks([])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 전체 결과 시각화\n",
    "plot_approximations(X1, X2, Y_true, kan_saver, mlp_saver)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "KAN 모델 요약:\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         KANLinear-1                [-1, 2, 16]               0\n",
      "              SiLU-2                [-1, 2, 16]               0\n",
      "            Linear-3                [-1, 2, 16]             272\n",
      "              SiLU-4                [-1, 2, 16]               0\n",
      "         KANLinear-5                [-1, 2, 16]               0\n",
      "              SiLU-6                [-1, 2, 16]               0\n",
      "            Linear-7                [-1, 2, 16]             272\n",
      "              SiLU-8                [-1, 2, 16]               0\n",
      "            Linear-9                [-1, 2, 16]             272\n",
      "             SiLU-10                [-1, 2, 16]               0\n",
      "           Linear-11                 [-1, 2, 1]              17\n",
      "================================================================\n",
      "Total params: 833\n",
      "Trainable params: 833\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.01\n",
      "----------------------------------------------------------------\n",
      "\n",
      "MLP 모델 요약:\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 160]             480\n",
      "              SiLU-2                  [-1, 160]               0\n",
      "            Linear-3                  [-1, 160]          25,760\n",
      "              SiLU-4                  [-1, 160]               0\n",
      "            Linear-5                  [-1, 160]          25,760\n",
      "              SiLU-6                  [-1, 160]               0\n",
      "            Linear-7                  [-1, 160]          25,760\n",
      "              SiLU-8                  [-1, 160]               0\n",
      "            Linear-9                    [-1, 1]             161\n",
      "================================================================\n",
      "Total params: 77,921\n",
      "Trainable params: 77,921\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.30\n",
      "Estimated Total Size (MB): 0.31\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/sam_env2/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([4096])) that is different to the input size (torch.Size([4096, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/root/miniconda3/envs/sam_env2/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([3136])) that is different to the input size (torch.Size([3136, 3136])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, KAN Loss: 0.6809, MLP Loss: 0.4992\n",
      "Epoch 2/100, KAN Loss: 0.6080, MLP Loss: 0.4920\n",
      "Epoch 3/100, KAN Loss: 0.6134, MLP Loss: 0.4739\n",
      "Epoch 4/100, KAN Loss: 0.6143, MLP Loss: 0.4150\n",
      "Epoch 5/100, KAN Loss: 0.5603, MLP Loss: 0.3337\n",
      "Epoch 6/100, KAN Loss: 0.5520, MLP Loss: 0.2470\n",
      "Epoch 7/100, KAN Loss: 0.5293, MLP Loss: 0.1660\n",
      "Epoch 8/100, KAN Loss: 0.5939, MLP Loss: 0.1036\n",
      "Epoch 9/100, KAN Loss: 0.5221, MLP Loss: 0.0716\n",
      "Epoch 10/100, KAN Loss: 0.5159, MLP Loss: 0.0558\n",
      "Epoch 11/100, KAN Loss: 0.5282, MLP Loss: 0.0450\n",
      "Epoch 12/100, KAN Loss: 0.5110, MLP Loss: 0.0363\n",
      "Epoch 13/100, KAN Loss: 0.5164, MLP Loss: 0.0299\n",
      "Epoch 14/100, KAN Loss: 0.5163, MLP Loss: 0.0243\n",
      "Epoch 15/100, KAN Loss: 0.5188, MLP Loss: 0.0206\n",
      "Epoch 16/100, KAN Loss: 0.5112, MLP Loss: 0.0180\n",
      "Epoch 17/100, KAN Loss: 0.5218, MLP Loss: 0.0141\n",
      "Epoch 18/100, KAN Loss: 0.5073, MLP Loss: 0.0121\n",
      "Epoch 19/100, KAN Loss: 0.5082, MLP Loss: 0.0107\n",
      "Epoch 20/100, KAN Loss: 0.5121, MLP Loss: 0.0101\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 5.96 GiB. GPU 0 has a total capacity of 79.15 GiB of which 1019.62 MiB is free. Process 31775 has 78.15 GiB memory in use. Of the allocated memory 77.57 GiB is allocated by PyTorch, and 77.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 275\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;66;03m# AMP를 사용하여 예측\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 275\u001b[0m     kan_pred \u001b[38;5;241m=\u001b[39m \u001b[43mkan_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_input_tensor_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m     mlp_pred \u001b[38;5;241m=\u001b[39m mlp_model(X_input_tensor_device)\n\u001b[1;32m    278\u001b[0m \u001b[38;5;66;03m# 예측 값 저장 (CPU로 이동)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sam_env2/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sam_env2/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 139\u001b[0m, in \u001b[0;36mKAN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_dim):\n\u001b[1;32m    138\u001b[0m     xi \u001b[38;5;241m=\u001b[39m x[:, i]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [batch_size, 1]\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m     phi_i \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mphi_layers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxi\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch_size, hidden_size]\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(phi_i)\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# 출력 합산\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sam_env2/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sam_env2/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sam_env2/lib/python3.10/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/sam_env2/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sam_env2/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 70\u001b[0m, in \u001b[0;36mKANLinear.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# x: [batch_size, 1]\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     xi \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [batch_size]\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     basis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbspline_basis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mknots\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdegree\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch_size, n_basis]\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     out \u001b[38;5;241m=\u001b[39m basis \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights  \u001b[38;5;66;03m# [batch_size, out_features]\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "Cell \u001b[0;32mIn[1], line 100\u001b[0m, in \u001b[0;36mKANLinear.bspline_basis\u001b[0;34m(self, x, knots, degree)\u001b[0m\n\u001b[1;32m     98\u001b[0m     term1 \u001b[38;5;241m=\u001b[39m ((x \u001b[38;5;241m-\u001b[39m knots[i]) \u001b[38;5;241m/\u001b[39m denom1) \u001b[38;5;241m*\u001b[39m basis[:, \u001b[38;5;241m0\u001b[39m, i]\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m denom2 \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 100\u001b[0m     term2 \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mknots\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdenom2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbasis\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    102\u001b[0m     term2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 5.96 GiB. GPU 0 has a total capacity of 79.15 GiB of which 1019.62 MiB is free. Process 31775 has 78.15 GiB memory in use. Of the allocated memory 77.57 GiB is allocated by PyTorch, and 77.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 3D 시각화를 위한 라이브러리\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# 모델 요약을 위한 라이브러리\n",
    "from torchsummary import summary\n",
    "\n",
    "# CUDA 사용 여부 확인 및 장치 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# 데이터 생성\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# 입력 데이터 생성\n",
    "x1 = np.linspace(-2 * np.pi, 2 * np.pi, 200)\n",
    "x2 = np.linspace(-2 * np.pi, 2 * np.pi, 200)\n",
    "X1, X2 = np.meshgrid(x1, x2)\n",
    "\n",
    "# 입력 데이터를 벡터 형태로 변환\n",
    "X_input = np.stack([X1.flatten(), X2.flatten()], axis=1)\n",
    "\n",
    "# 실제 함수 값 계산\n",
    "def f_true(x):\n",
    "    return np.sin(x[:, 0] + x[:, 1])\n",
    "\n",
    "Y_true = f_true(X_input)\n",
    "\n",
    "# 데이터를 PyTorch 텐서로 변환하고 장치로 이동\n",
    "X_input_tensor = torch.from_numpy(X_input).float().to(device)\n",
    "Y_true_tensor = torch.from_numpy(Y_true).float().to(device)\n",
    "\n",
    "# 데이터셋과 데이터로더 생성\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "batch_size = 4096  # 배치 크기를 늘려서 GPU 활용을 극대화\n",
    "dataset = TensorDataset(X_input_tensor, Y_true_tensor)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# KANLinear layer with spline basis\n",
    "class KANLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, grid_size=10, degree=3):\n",
    "        super(KANLinear, self).__init__()\n",
    "        self.in_features = in_features  # Should be 1 for φ_i functions\n",
    "        self.out_features = out_features\n",
    "        self.degree = degree\n",
    "        self.grid_size = grid_size\n",
    "\n",
    "        # Knot vector (uniformly spaced)\n",
    "        self.num_knots = grid_size + 2 * degree  # Corrected number of knots\n",
    "        knots = np.linspace(-2 * np.pi, 2 * np.pi, self.num_knots)\n",
    "        self.register_buffer('knots', torch.from_numpy(knots).float())\n",
    "\n",
    "        # Number of basis functions\n",
    "        self.n_basis = self.num_knots - degree - 1  # Corrected number of basis functions\n",
    "\n",
    "        # Coefficients for each basis function\n",
    "        # Since in_features is 1, we can simplify weights to a single nn.Parameter\n",
    "        self.weights = nn.Parameter(torch.randn(self.n_basis, out_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, 1]\n",
    "        xi = x.squeeze(1)  # [batch_size]\n",
    "        basis = self.bspline_basis(xi, self.knots, self.degree)  # [batch_size, n_basis]\n",
    "        out = basis @ self.weights  # [batch_size, out_features]\n",
    "        return out\n",
    "\n",
    "    def bspline_basis(self, x, knots, degree):\n",
    "        # x: [batch_size]\n",
    "        # knots: [num_knots]\n",
    "        x = x.unsqueeze(1)  # [batch_size, 1]\n",
    "\n",
    "        # Number of basis functions\n",
    "        n_basis = self.n_basis\n",
    "\n",
    "        # Initialize degree 0 basis functions\n",
    "        basis = []\n",
    "        for i in range(n_basis + degree):\n",
    "            cond = ((x >= knots[i]) & (x < knots[i + 1])).float()\n",
    "            basis.append(cond)\n",
    "        basis = torch.stack(basis, dim=2)  # [batch_size, 1, n_basis + degree]\n",
    "\n",
    "        # Recursive computation of basis functions\n",
    "        for k in range(1, degree + 1):\n",
    "            new_basis = []\n",
    "            for i in range(n_basis + degree - k):\n",
    "                denom1 = knots[i + k] - knots[i]\n",
    "                denom2 = knots[i + k + 1] - knots[i + 1]\n",
    "\n",
    "                term1 = 0\n",
    "                if denom1 != 0:\n",
    "                    term1 = ((x - knots[i]) / denom1) * basis[:, 0, i]\n",
    "                if denom2 != 0:\n",
    "                    term2 = ((knots[i + k + 1] - x) / denom2) * basis[:, 0, i + 1]\n",
    "                else:\n",
    "                    term2 = 0\n",
    "\n",
    "                new_basis.append(term1 + term2)\n",
    "            basis = torch.stack(new_basis, dim=2)  # [batch_size, 1, n_basis + degree - k]\n",
    "\n",
    "        return basis.squeeze(1)  # [batch_size, n_basis]\n",
    "\n",
    "# KAN model definition\n",
    "class KAN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size, grid_size=10, degree=3):\n",
    "        super(KAN, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # 각 입력 차원에 대한 일변량 함수 φ_i\n",
    "        self.phi_layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                KANLinear(1, hidden_size, grid_size, degree),\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(hidden_size, hidden_size),\n",
    "                nn.SiLU()\n",
    "            ) for _ in range(input_dim)\n",
    "        ])\n",
    "\n",
    "        # 최종 함수 ψ\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )  \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, input_dim]\n",
    "        outputs = []\n",
    "        for i in range(self.input_dim):\n",
    "            xi = x[:, i].unsqueeze(1)  # [batch_size, 1]\n",
    "            phi_i = self.phi_layers[i](xi)  # [batch_size, hidden_size]\n",
    "            outputs.append(phi_i)\n",
    "        # 출력 합산\n",
    "        s = sum(outputs)  # [batch_size, hidden_size]\n",
    "        # 최종 함수 적용\n",
    "        out = self.psi(s)  # [batch_size, 1]\n",
    "        return out.squeeze()\n",
    "\n",
    "# MLP 모델 정의 (KAN과 구조를 맞춤)\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size, n_hidden_layers=3):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(input_dim, hidden_size))\n",
    "        layers.append(nn.SiLU())\n",
    "        for _ in range(n_hidden_layers - 1):\n",
    "            layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            layers.append(nn.SiLU())\n",
    "        layers.append(nn.Linear(hidden_size, 1))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze()\n",
    "\n",
    "# Saver 클래스 정의\n",
    "class Saver:\n",
    "    def __init__(self):\n",
    "        self.outputs = []\n",
    "\n",
    "    def save(self, epoch, Y_pred):\n",
    "        self.outputs.append((epoch, Y_pred))\n",
    "\n",
    "# 모델 초기화 및 장치로 이동\n",
    "input_dim = 2\n",
    "hidden_size = 16  # 히든 사이즈를 늘려서 복잡한 함수 근사에 적합하게 조정\n",
    "n_hidden_layers = 4  # 히든 레이어 수를 늘림\n",
    "grid_size = 10\n",
    "degree = 3\n",
    "\n",
    "kan_model = KAN(input_dim=input_dim, hidden_size=hidden_size, grid_size=grid_size, degree=degree).to(device)\n",
    "mlp_model = MLP(input_dim=input_dim, hidden_size=hidden_size * 10, n_hidden_layers=n_hidden_layers).to(device)\n",
    "\n",
    "# 모델 요약 출력\n",
    "print(\"KAN 모델 요약:\")\n",
    "summary(kan_model, input_size=(input_dim,), device=str(device))\n",
    "\n",
    "print(\"\\nMLP 모델 요약:\")\n",
    "summary(mlp_model, input_size=(input_dim,), device=str(device))\n",
    "\n",
    "# 손실 함수와 옵티마이저 정의\n",
    "criterion = nn.MSELoss()\n",
    "kan_optimizer = optim.Adam(kan_model.parameters(), lr=0.001)\n",
    "mlp_optimizer = optim.Adam(mlp_model.parameters(), lr=0.001)\n",
    "\n",
    "# AMP를 위한 GradScaler 초기화\n",
    "kan_scaler = torch.amp.GradScaler('cuda')\n",
    "mlp_scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "# 학습 루프\n",
    "epochs = 100\n",
    "\n",
    "# 에포크별 손실 저장용 리스트\n",
    "kan_losses = []\n",
    "mlp_losses = []\n",
    "\n",
    "# 시각화를 위한 예측값 저장용 Saver 객체 생성\n",
    "kan_saver = Saver()\n",
    "mlp_saver = Saver()\n",
    "\n",
    "# 시각화할 에포크 지정 (5개의 에포크로 줄임)\n",
    "visualization_epochs = [19, 39, 59, 79, 99]  # 총 5개의 에포크\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    kan_model.train()\n",
    "    mlp_model.train()\n",
    "    \n",
    "    kan_epoch_loss = 0\n",
    "    mlp_epoch_loss = 0\n",
    "    \n",
    "    for X_batch, Y_batch in loader:\n",
    "        # 데이터를 장치로 이동\n",
    "        X_batch = X_batch.to(device)\n",
    "        Y_batch = Y_batch.to(device)\n",
    "        \n",
    "        # 그래디언트 초기화\n",
    "        kan_optimizer.zero_grad()\n",
    "        mlp_optimizer.zero_grad()\n",
    "        \n",
    "        # AMP를 사용하여 순전파 및 역전파\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            # KAN 모델 학습\n",
    "            kan_outputs = kan_model(X_batch)\n",
    "            kan_loss = criterion(kan_outputs, Y_batch)\n",
    "        \n",
    "        # 손실 스케일링 및 역전파\n",
    "        kan_scaler.scale(kan_loss).backward()\n",
    "        kan_scaler.step(kan_optimizer)\n",
    "        kan_scaler.update()\n",
    "        \n",
    "        kan_epoch_loss += kan_loss.item() * X_batch.size(0)\n",
    "        \n",
    "        with torch.amp.autocast('cuda'):\n",
    "            # MLP 모델 학습\n",
    "            mlp_outputs = mlp_model(X_batch)\n",
    "            mlp_loss = criterion(mlp_outputs, Y_batch)\n",
    "        \n",
    "        # 손실 스케일링 및 역전파\n",
    "        mlp_scaler.scale(mlp_loss).backward()\n",
    "        mlp_scaler.step(mlp_optimizer)\n",
    "        mlp_scaler.update()\n",
    "        \n",
    "        mlp_epoch_loss += mlp_loss.item() * X_batch.size(0)\n",
    "    \n",
    "    # 에포크별 평균 손실 계산\n",
    "    kan_epoch_loss /= len(dataset)\n",
    "    mlp_epoch_loss /= len(dataset)\n",
    "    \n",
    "    # 손실 저장\n",
    "    kan_losses.append(kan_epoch_loss)\n",
    "    mlp_losses.append(mlp_epoch_loss)\n",
    "    \n",
    "    # 손실 출력\n",
    "    print(f'Epoch {epoch+1}/{epochs}, KAN Loss: {kan_epoch_loss:.4f}, MLP Loss: {mlp_epoch_loss:.4f}')\n",
    "    \n",
    "    # 특정 에포크마다 예측값 저장\n",
    "    if epoch in visualization_epochs:\n",
    "        # 모델 평가 모드로 전환\n",
    "        kan_model.eval()\n",
    "        mlp_model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # 전체 데이터셋을 장치로 이동\n",
    "            X_input_tensor_device = X_input_tensor.to(device)\n",
    "            \n",
    "            # AMP를 사용하여 예측\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                kan_pred = kan_model(X_input_tensor_device)\n",
    "                mlp_pred = mlp_model(X_input_tensor_device)\n",
    "            \n",
    "            # 예측 값 저장 (CPU로 이동)\n",
    "            kan_saver.save(epoch, kan_pred.cpu().numpy())\n",
    "            mlp_saver.save(epoch, mlp_pred.cpu().numpy())\n",
    "\n",
    "# 실제 함수 값 (CPU로 이동)\n",
    "Z_true = Y_true.reshape(X1.shape)\n",
    "\n",
    "# 손실 곡선 시각화\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(range(1, epochs+1), kan_losses, label='KAN Loss')\n",
    "plt.plot(range(1, epochs+1), mlp_losses, label='MLP Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 에포크별 결과 시각화 함수 정의\n",
    "def plot_approximations(X1, X2, Y_true, kan_saver, mlp_saver):\n",
    "    num_cols = 5  # 한 행에 보여줄 그래프 수\n",
    "    num_rows = 3  # 실제 함수 + KAN + MLP\n",
    "\n",
    "    fig = plt.figure(figsize=(4*num_cols, 4*num_rows))\n",
    "\n",
    "    # 실제 함수 시각화 (첫 번째 행)\n",
    "    for i in range(num_cols):\n",
    "        ax = fig.add_subplot(num_rows, num_cols, i+1, projection='3d')\n",
    "        if i == 0:\n",
    "            ax.plot_surface(X1, X2, Y_true.reshape(X1.shape), cmap='viridis')\n",
    "            ax.set_title('Actual Function')\n",
    "            ax.set_xlabel('x1')\n",
    "            ax.set_ylabel('x2')\n",
    "            ax.set_zlabel('f(x1, x2)')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            ax.set_zticks([])\n",
    "        else:\n",
    "            # 빈 플롯 생성\n",
    "            ax.axis('off')\n",
    "\n",
    "    # KAN 에포크별 근사 결과 시각화 (두 번째 행)\n",
    "    for i, (epoch, Y_pred) in enumerate(kan_saver.outputs):\n",
    "        ax = fig.add_subplot(num_rows, num_cols, num_cols + i+1, projection='3d')\n",
    "        ax.plot_surface(X1, X2, Y_pred.reshape(X1.shape), cmap='viridis')\n",
    "        ax.set_title(f'KAN Epoch {epoch+1}')\n",
    "        ax.set_xlabel('x1')\n",
    "        ax.set_ylabel('x2')\n",
    "        ax.set_zlabel('Approximation')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_zticks([])\n",
    "\n",
    "    # MLP 에포크별 근사 결과 시각화 (세 번째 행)\n",
    "    for i, (epoch, Y_pred) in enumerate(mlp_saver.outputs):\n",
    "        ax = fig.add_subplot(num_rows, num_cols, 2*num_cols + i+1, projection='3d')\n",
    "        ax.plot_surface(X1, X2, Y_pred.reshape(X1.shape), cmap='viridis')\n",
    "        ax.set_title(f'MLP Epoch {epoch+1}')\n",
    "        ax.set_xlabel('x1')\n",
    "        ax.set_ylabel('x2')\n",
    "        ax.set_zlabel('Approximation')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_zticks([])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 전체 결과 시각화\n",
    "plot_approximations(X1, X2, Z_true, kan_saver, mlp_saver)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam_env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
