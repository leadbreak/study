{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. B-Spline 기저 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def B_batch(x, grid, k=0, device='cpu'):\n",
    "    '''\n",
    "    주어진 점 x에서 B-스플라인 기저 함수를 평가합니다.\n",
    "\n",
    "    Args:\n",
    "    -----\n",
    "        x : torch.Tensor\n",
    "            입력 텐서로, 크기는 (batch_size, in_dim).\n",
    "        grid : torch.Tensor\n",
    "            노드 벡터로, 크기는 (in_dim, num_knots).\n",
    "        k : int\n",
    "            B-스플라인의 차수.\n",
    "        device : str\n",
    "            연산을 수행할 디바이스.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "        value : torch.Tensor\n",
    "            평가된 B-스플라인 기저 함수로, 크기는 (batch_size, in_dim, num_basis).\n",
    "    '''\n",
    "    x = x.unsqueeze(dim=2)\n",
    "    grid = grid.unsqueeze(dim=0)\n",
    "\n",
    "    if k == 0:\n",
    "        value = ((x >= grid[:, :, :-1]) & (x < grid[:, :, 1:])).float()\n",
    "    else:\n",
    "        B_km1 = B_batch(x[:, :, 0], grid[0], k=k - 1).to(device)\n",
    "        denom1 = grid[:, :, k:-1] - grid[:, :, :-(k + 1)]\n",
    "        denom2 = grid[:, :, k + 1:] - grid[:, :, 1:(-k)]\n",
    "        denom1[denom1 == 0] = 1e-8\n",
    "        denom2[denom2 == 0] = 1e-8\n",
    "        term1 = ((x - grid[:, :, :-(k + 1)]) / denom1) * B_km1[:, :, :-1]\n",
    "        term2 = ((grid[:, :, k + 1:] - x) / denom2) * B_km1[:, :, 1:]\n",
    "        value = term1 + term2\n",
    "\n",
    "    value = torch.nan_to_num(value)\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B-스플라인 기저 함수 값:\n",
      "tensor([[[0.0000, 0.0000]],\n",
      "\n",
      "        [[0.5000, 0.0000]],\n",
      "\n",
      "        [[0.5000, 0.5000]],\n",
      "\n",
      "        [[0.0000, 0.5000]],\n",
      "\n",
      "        [[0.0000, 0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "# 입력 데이터\n",
    "x = torch.linspace(-1, 1, steps=5).unsqueeze(1)  # (batch_size, in_dim)\n",
    "\n",
    "# 노드 벡터 생성\n",
    "grid = torch.linspace(-1, 1, steps=5).unsqueeze(0)  # (in_dim, num_knots)\n",
    "\n",
    "# 차수 설정\n",
    "k = 2\n",
    "\n",
    "# 함수 호출\n",
    "B_values = B_batch(x, grid, k=k)\n",
    "\n",
    "print(\"B-스플라인 기저 함수 값:\")\n",
    "print(B_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 계수와 곡선 간의 변환 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coef2curve(x_eval, grid, coef, k, device='cpu'):\n",
    "    '''\n",
    "    주어진 계수를 사용하여 x_eval에서 B-스플라인 곡선을 평가합니다.\n",
    "\n",
    "    Args:\n",
    "    -----\n",
    "        x_eval : torch.Tensor\n",
    "            크기 (batch_size, in_dim)의 입력 텐서.\n",
    "        grid : torch.Tensor\n",
    "            크기 (in_dim, num_knots)의 노드 벡터.\n",
    "        coef : torch.Tensor\n",
    "            크기 (in_dim, out_dim, num_basis)의 B-스플라인 계수.\n",
    "        k : int\n",
    "            B-스플라인의 차수.\n",
    "        device : str\n",
    "            연산을 수행할 디바이스.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "        y_eval : torch.Tensor\n",
    "            크기 (batch_size, in_dim, out_dim)의 평가된 스플라인 곡선.\n",
    "    '''\n",
    "    b_splines = B_batch(x_eval, grid, k=k, device=device)\n",
    "    y_eval = torch.einsum('ijk,jlk->ijl', b_splines, coef.to(device))\n",
    "    return y_eval\n",
    "\n",
    "def curve2coef(x_eval, y_eval, grid, k, lamb=1e-8):\n",
    "    '''\n",
    "    주어진 곡선 평가값으로부터 B-스플라인 계수를 계산합니다.\n",
    "\n",
    "    Args:\n",
    "    -----\n",
    "        x_eval : torch.Tensor\n",
    "            크기 (batch_size, in_dim)의 입력 텐서.\n",
    "        y_eval : torch.Tensor\n",
    "            크기 (batch_size, in_dim, out_dim)의 평가된 곡선 값.\n",
    "        grid : torch.Tensor\n",
    "            크기 (in_dim, num_knots)의 노드 벡터.\n",
    "        k : int\n",
    "            B-스플라인의 차수.\n",
    "        lamb : float\n",
    "            정규화 파라미터.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "        coef : torch.Tensor\n",
    "            크기 (in_dim, out_dim, num_basis)의 계산된 B-스플라인 계수.\n",
    "    '''\n",
    "    batch = x_eval.shape[0]\n",
    "    in_dim = x_eval.shape[1]\n",
    "    out_dim = y_eval.shape[2]\n",
    "    n_coef = grid.shape[1] - k - 1\n",
    "\n",
    "    mat = B_batch(x_eval, grid, k).permute(1, 0, 2)\n",
    "    mat = mat.unsqueeze(1).expand(-1, out_dim, -1, -1)\n",
    "    y_eval = y_eval.permute(1, 2, 0).unsqueeze(3)\n",
    "\n",
    "    XtX = torch.einsum('ijmn,ijnp->ijmp', mat.transpose(2, 3), mat)\n",
    "    Xty = torch.einsum('ijmn,ijnp->ijmp', mat.transpose(2, 3), y_eval)\n",
    "    n1, n2, n = XtX.shape[0], XtX.shape[1], XtX.shape[2]\n",
    "    identity = torch.eye(n, n, device=mat.device).unsqueeze(0).unsqueeze(0).expand(n1, n2, -1, -1)\n",
    "    A = XtX + lamb * identity\n",
    "    B = Xty\n",
    "    coef = torch.linalg.solve(A, B).squeeze(-1)\n",
    "    return coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원래 곡선 값:\n",
      "tensor([-0.8415, -0.7017, -0.5274, -0.3272, -0.1109,  0.1109,  0.3272,  0.5274,\n",
      "         0.7017,  0.8415])\n",
      "복원된 곡선 값:\n",
      "tensor([0.0000e+00, 2.4021e-10, 1.9217e-09, 6.0803e-09, 1.0224e-08, 1.0224e-08,\n",
      "        6.0803e-09, 1.9217e-09, 2.4021e-10, 0.0000e+00])\n"
     ]
    }
   ],
   "source": [
    "# 계수와 곡선 간의 변환 확인\n",
    "\n",
    "# 입력 데이터 생성\n",
    "x_eval = torch.linspace(-1, 1, steps=10).unsqueeze(1)  # (batch_size, in_dim)\n",
    "y_true = torch.sin(x_eval)  # 실제 곡선 값\n",
    "\n",
    "# 노드 벡터 생성\n",
    "grid = torch.linspace(-1, 1, steps=5).unsqueeze(0)  # (in_dim, num_knots)\n",
    "k = 3  # 차수\n",
    "\n",
    "# 곡선으로부터 계수 계산\n",
    "coef = curve2coef(x_eval, y_true.unsqueeze(2), grid, k)\n",
    "\n",
    "# 계수를 사용하여 곡선 복원\n",
    "y_pred = coef2curve(x_eval, grid, coef, k)\n",
    "\n",
    "print(\"원래 곡선 값:\")\n",
    "print(y_true.squeeze())\n",
    "print(\"복원된 곡선 값:\")\n",
    "print(y_pred.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 그리드 확장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_grid(grid, k_extend=0):\n",
    "    '''\n",
    "    스플라인의 경계 조건을 처리하기 위해 그리드를 확장합니다.\n",
    "\n",
    "    Args:\n",
    "    -----\n",
    "        grid : torch.Tensor\n",
    "            크기 (in_dim, num_knots)의 원래 노드 벡터.\n",
    "        k_extend : int\n",
    "            각 경계에서 확장할 노드의 수.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "        extended_grid : torch.Tensor\n",
    "            확장된 노드 벡터.\n",
    "    '''\n",
    "    h = (grid[:, -1:] - grid[:, :1]) / (grid.shape[1] - 1)\n",
    "    for _ in range(k_extend):\n",
    "        grid = torch.cat([grid[:, :1] - h, grid, grid[:, -1:] + h], dim=1)\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원래 그리드:\n",
      "tensor([[0.0000, 0.2500, 0.5000, 0.7500, 1.0000]])\n",
      "확장된 그리드:\n",
      "tensor([[-0.5000, -0.2500,  0.0000,  0.2500,  0.5000,  0.7500,  1.0000,  1.2500,\n",
      "          1.5000]])\n"
     ]
    }
   ],
   "source": [
    "# 그리드 확장 테스트\n",
    "\n",
    "# 원래 그리드 생성\n",
    "grid = torch.linspace(0, 1, steps=5).unsqueeze(0)  # (in_dim, num_knots)\n",
    "\n",
    "# 그리드 확장\n",
    "k_extend = 2  # 확장할 노드 수\n",
    "extended_grid = extend_grid(grid, k_extend=k_extend)\n",
    "\n",
    "print(\"원래 그리드:\")\n",
    "print(grid)\n",
    "print(\"확장된 그리드:\")\n",
    "print(extended_grid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. KAN Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class KANLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    KANLayer 클래스\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "        in_dim : int\n",
    "            입력 차원.\n",
    "        out_dim : int\n",
    "            출력 차원.\n",
    "        num : int\n",
    "            그리드 구간 수.\n",
    "        k : int\n",
    "            B-스플라인의 차수.\n",
    "        noise_scale : float\n",
    "            초기화 시 노이즈의 스케일.\n",
    "        coef : torch.Tensor\n",
    "            B-스플라인 기저 함수의 계수.\n",
    "        scale_base_mu : float\n",
    "            기본 함수 스케일링 계수의 평균.\n",
    "        scale_base_sigma : float\n",
    "            기본 함수 스케일링 계수의 표준편차.\n",
    "        scale_sp : float\n",
    "            스플라인 함수의 스케일링 계수.\n",
    "        base_fun : callable\n",
    "            기본 함수 b(x).\n",
    "        mask : torch.Tensor\n",
    "            활성/비활성 활성화를 나타내는 마스크.\n",
    "        grid_eps : float\n",
    "            그리드 적응을 제어하는 파라미터.\n",
    "        device : str\n",
    "            연산을 수행할 디바이스.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim=3, out_dim=2, num=5, k=3, noise_scale=0.1,\n",
    "                 scale_base_mu=0.0, scale_base_sigma=1.0, scale_sp=1.0,\n",
    "                 base_fun=torch.nn.SiLU(), grid_eps=0.02, grid_range=[-1, 1],\n",
    "                 sp_trainable=True, sb_trainable=True, device='cpu', sparse_init=False):\n",
    "        super(KANLayer, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.num = num\n",
    "        self.k = k\n",
    "        self.grid_eps = grid_eps\n",
    "        self.device = device\n",
    "\n",
    "        # 그리드 초기화\n",
    "        grid = torch.linspace(grid_range[0], grid_range[1], steps=num + 1).unsqueeze(0).repeat(in_dim, 1)\n",
    "        grid = extend_grid(grid, k_extend=k)  # (in_dim, num + 1 + 2k)\n",
    "        self.grid = nn.Parameter(grid, requires_grad=False)\n",
    "\n",
    "        # 계수 초기화\n",
    "        num_knots = num + 1 + 2 * k  # 그리드 점의 총 수\n",
    "        num_basis = num + k  # B-스플라인 기저 함수의 수\n",
    "\n",
    "        # x_eval의 차원: (num + 1, in_dim)\n",
    "        x_eval = self.grid[:, k:-k].t()  # 내부 그리드 점들\n",
    "\n",
    "        # noises의 차원을 x_eval과 맞춤: (num + 1, in_dim, out_dim)\n",
    "        noises = (torch.rand(num + 1, in_dim, out_dim) - 0.5) * noise_scale / num\n",
    "\n",
    "        # coef 계산\n",
    "        self.coef = nn.Parameter(curve2coef(x_eval, noises, self.grid, k), requires_grad=True)\n",
    "\n",
    "        # 나머지 초기화 코드는 그대로 유지\n",
    "        self.scale_base = nn.Parameter(\n",
    "            scale_base_mu / np.sqrt(in_dim) +\n",
    "            scale_base_sigma * (torch.rand(in_dim, out_dim) * 2 - 1) / np.sqrt(in_dim),\n",
    "            requires_grad=sb_trainable\n",
    "        )\n",
    "        self.scale_sp = nn.Parameter(torch.ones(in_dim, out_dim) * scale_sp, requires_grad=sp_trainable)\n",
    "\n",
    "        self.base_fun = base_fun\n",
    "\n",
    "        if sparse_init:\n",
    "            self.mask = nn.Parameter(torch.randint(0, 2, (in_dim, out_dim)).float(), requires_grad=False)\n",
    "        else:\n",
    "            self.mask = nn.Parameter(torch.ones(in_dim, out_dim), requires_grad=False)\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        preacts = x.unsqueeze(1).expand(batch_size, self.out_dim, self.in_dim)\n",
    "        base = self.base_fun(x)\n",
    "        y_spline = coef2curve(x, self.grid, self.coef, self.k, device=self.device)\n",
    "        postspline = y_spline.permute(0, 2, 1)\n",
    "        y = self.scale_base.unsqueeze(0) * base.unsqueeze(2) + self.scale_sp.unsqueeze(0) * y_spline\n",
    "        y = self.mask.unsqueeze(0) * y\n",
    "        postacts = y.permute(0, 2, 1)\n",
    "        y = y.sum(dim=1)\n",
    "        return y, preacts, postacts, postspline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "출력 y:\n",
      "tensor([[-0.0908,  0.1939],\n",
      "        [ 0.6888, -0.4251],\n",
      "        [ 0.9379, -0.7289],\n",
      "        [ 0.1921, -0.0386],\n",
      "        [ 0.1951,  0.0141],\n",
      "        [ 0.5350, -0.3471],\n",
      "        [-0.0432, -0.2205],\n",
      "        [ 0.5574, -0.2622],\n",
      "        [ 0.3228, -0.2377],\n",
      "        [-0.1825, -0.0668]], grad_fn=<SumBackward1>)\n",
      "활성화 함수 전의 값 preacts:\n",
      "tensor([[[-1.8268, -0.4919, -0.5655],\n",
      "         [-1.8268, -0.4919, -0.5655]],\n",
      "\n",
      "        [[-0.8122, -0.0561,  2.9039],\n",
      "         [-0.8122, -0.0561,  2.9039]],\n",
      "\n",
      "        [[ 2.1257, -0.1937,  1.4044],\n",
      "         [ 2.1257, -0.1937,  1.4044]],\n",
      "\n",
      "        [[-0.0954, -0.4374,  0.8750],\n",
      "         [-0.0954, -0.4374,  0.8750]],\n",
      "\n",
      "        [[ 0.5969, -1.4982, -0.0124],\n",
      "         [ 0.5969, -1.4982, -0.0124]],\n",
      "\n",
      "        [[ 0.4736, -0.1001,  1.8327],\n",
      "         [ 0.4736, -0.1001,  1.8327]],\n",
      "\n",
      "        [[-1.0392,  0.6603,  0.8547],\n",
      "         [-1.0392,  0.6603,  0.8547]],\n",
      "\n",
      "        [[ 0.2772, -0.5318,  1.8793],\n",
      "         [ 0.2772, -0.5318,  1.8793]],\n",
      "\n",
      "        [[ 1.4182, -0.2921, -1.3811],\n",
      "         [ 1.4182, -0.2921, -1.3811]],\n",
      "\n",
      "        [[-2.0757,  0.5231, -0.0996],\n",
      "         [-2.0757,  0.5231, -0.0996]]])\n",
      "활성화 함수 후의 값 postacts:\n",
      "tensor([[[-0.0812,  0.0541, -0.0638],\n",
      "         [ 0.0737,  0.0725,  0.0478]],\n",
      "\n",
      "        [[-0.0747,  0.0033,  0.7603],\n",
      "         [ 0.0699,  0.0130, -0.5080]],\n",
      "\n",
      "        [[ 0.6116,  0.0169,  0.3094],\n",
      "         [-0.5570,  0.0357, -0.2076]],\n",
      "\n",
      "        [[-0.0231,  0.0476,  0.1677],\n",
      "         [ 0.0078,  0.0665, -0.1129]],\n",
      "\n",
      "        [[ 0.1216,  0.0768, -0.0033],\n",
      "         [-0.1056,  0.1219, -0.0022]],\n",
      "\n",
      "        [[ 0.0912,  0.0074,  0.4364],\n",
      "         [-0.0768,  0.0208, -0.2911]],\n",
      "\n",
      "        [[-0.0848, -0.1220,  0.1637],\n",
      "         [ 0.0768, -0.1887, -0.1087]],\n",
      "\n",
      "        [[ 0.0485,  0.0586,  0.4503],\n",
      "         [-0.0383,  0.0767, -0.3005]],\n",
      "\n",
      "        [[ 0.3698,  0.0287, -0.0757],\n",
      "         [-0.3385,  0.0493,  0.0516]],\n",
      "\n",
      "        [[-0.0745, -0.0938, -0.0142],\n",
      "         [ 0.0679, -0.1418,  0.0072]]], grad_fn=<PermuteBackward0>)\n",
      "스플라인 함수 출력 postspline:\n",
      "tensor([[[ 4.0004e-04,  4.9675e-03, -7.1822e-03],\n",
      "         [-6.1732e-04, -6.4205e-03,  9.9799e-03]],\n",
      "\n",
      "        [[ 5.7013e-03, -3.9244e-03,  0.0000e+00],\n",
      "         [-3.3720e-03,  1.4528e-03,  0.0000e+00]],\n",
      "\n",
      "        [[ 8.1487e-07, -6.1475e-03, -2.0134e-03],\n",
      "         [-4.8214e-06, -1.2786e-03,  4.6075e-04]],\n",
      "\n",
      "        [[-8.4833e-03,  2.3436e-03, -2.8876e-03],\n",
      "         [-5.5548e-03, -5.9939e-03,  1.0841e-03]],\n",
      "\n",
      "        [[-2.3856e-03,  4.6888e-03, -1.5544e-03],\n",
      "         [ 7.2768e-03,  6.2451e-03, -3.3352e-03]],\n",
      "\n",
      "        [[-2.8230e-03, -5.1024e-03,  1.0032e-04],\n",
      "         [ 8.8407e-03,  6.6629e-04,  4.1554e-04]],\n",
      "\n",
      "        [[ 2.6452e-03, -7.3232e-03, -1.9182e-03],\n",
      "         [-2.8049e-03, -4.7010e-03,  1.9591e-03]],\n",
      "\n",
      "        [[-2.2727e-03,  6.7017e-03,  6.6811e-05],\n",
      "         [ 7.9108e-03, -6.4715e-03,  2.7675e-04]],\n",
      "\n",
      "        [[ 2.0967e-03, -4.1647e-03,  8.8887e-04],\n",
      "         [-3.6114e-03, -3.4741e-03,  3.7774e-04]],\n",
      "\n",
      "        [[ 1.4789e-05, -7.2700e-03, -1.1579e-03],\n",
      "         [-2.2821e-05, -3.0192e-03, -1.5397e-03]]], grad_fn=<PermuteBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# KANLayer 사용 예시\n",
    "\n",
    "# 입력 데이터 생성\n",
    "x = torch.randn(10, 3)  # (batch_size, in_dim)\n",
    "\n",
    "# KANLayer 초기화\n",
    "kan_layer = KANLayer(in_dim=3, out_dim=2, num=5, k=3)\n",
    "\n",
    "# 순전파 실행\n",
    "y, preacts, postacts, postspline = kan_layer(x)\n",
    "\n",
    "print(\"출력 y:\")\n",
    "print(y)\n",
    "print(\"활성화 함수 전의 값 preacts:\")\n",
    "print(preacts)\n",
    "print(\"활성화 함수 후의 값 postacts:\")\n",
    "print(postacts)\n",
    "print(\"스플라인 함수 출력 postspline:\")\n",
    "print(postspline)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Symbolic KAN Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import sympy as sp\n",
    "\n",
    "# 기호적 함수 라이브러리\n",
    "SYMBOLIC_LIB = {\n",
    "    'sin': (torch.sin, sp.sin, 'sin'),\n",
    "    'cos': (torch.cos, sp.cos, 'cos'),\n",
    "    'exp': (torch.exp, sp.exp, 'exp'),\n",
    "    'tanh': (torch.tanh, sp.tanh, 'tanh')\n",
    "}\n",
    "\n",
    "class Symbolic_KANLayer(nn.Module):\n",
    "    def __init__(self, in_dim=3, out_dim=2, device='cpu'):\n",
    "        super(Symbolic_KANLayer, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.device = device\n",
    "\n",
    "        # 마스크 초기화: 모든 연결을 비활성화\n",
    "        self.mask = nn.Parameter(torch.zeros(out_dim, in_dim, device=device), requires_grad=False)\n",
    "        \n",
    "        # 기호적 함수 리스트 초기화\n",
    "        self.funs = [[None for _ in range(in_dim)] for _ in range(out_dim)]\n",
    "        self.funs_name = [['0' for _ in range(in_dim)] for _ in range(out_dim)]\n",
    "        self.affine = nn.Parameter(torch.ones(out_dim, in_dim, 4, device=device), requires_grad=True)  # [a, b, c, d]\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x, singularity_avoiding=False, y_th=10.):\n",
    "        batch_size = x.shape[0]\n",
    "        postacts = []\n",
    "\n",
    "        for j in range(self.out_dim):\n",
    "            neuron_output = torch.zeros(batch_size, device=self.device)\n",
    "            for i in range(self.in_dim):\n",
    "                if self.mask[j, i] > 0 and self.funs[j][i] is not None:\n",
    "                    a, b, c, d = self.affine[j, i]\n",
    "                    transformed_x = a * x[:, i] + b\n",
    "                    func_output = self.funs[j][i](transformed_x)\n",
    "                    if singularity_avoiding and 'tan' in self.funs_name[j][i]:\n",
    "                        # 예시로 tan 함수에 대한 특이점 회피 처리\n",
    "                        func_output = func_output / (1 + (func_output / y_th).abs())\n",
    "                    neuron_output += c * func_output + d\n",
    "            postacts.append(neuron_output.unsqueeze(1))\n",
    "\n",
    "        postacts = torch.cat(postacts, dim=1)  # (batch_size, out_dim)\n",
    "        y = postacts\n",
    "        return y, postacts\n",
    "\n",
    "    def fix_symbolic(self, j, i, fun_name, a_init=None, b_init=None, c_init=None, d_init=None, verbose=True):\n",
    "        '''\n",
    "        특정 출력 뉴런 j와 입력 뉴런 i에 기호적 함수 할당 및 아핀 파라미터 초기화\n",
    "\n",
    "        Args:\n",
    "        -----\n",
    "            j : int\n",
    "                출력 뉴런 인덱스.\n",
    "            i : int\n",
    "                입력 뉴런 인덱스.\n",
    "            fun_name : str\n",
    "                할당할 기호적 함수 이름 (SYMBOLIC_LIB에 정의된 것).\n",
    "            a_init, b_init, c_init, d_init : float or None\n",
    "                아핀 변환 파라미터 초기값. None이면 기본값 사용.\n",
    "            verbose : bool\n",
    "                함수 할당 시 출력 여부.\n",
    "        '''\n",
    "        if fun_name in SYMBOLIC_LIB:\n",
    "            fun, fun_sympy, fun_str = SYMBOLIC_LIB[fun_name]\n",
    "            self.funs[j][i] = fun\n",
    "            self.funs_name[j][i] = fun_str\n",
    "\n",
    "            # 아핀 파라미터 초기화\n",
    "            with torch.no_grad():\n",
    "                self.affine[j, i, 0] = a_init if a_init is not None else 1.0\n",
    "                self.affine[j, i, 1] = b_init if b_init is not None else 0.0\n",
    "                self.affine[j, i, 2] = c_init if c_init is not None else 1.0\n",
    "                self.affine[j, i, 3] = d_init if d_init is not None else 0.0\n",
    "\n",
    "            # 마스크 활성화\n",
    "            with torch.no_grad():\n",
    "                self.mask[j, i] = 1.0\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Assigned function '{fun_name}' to Layer, Neuron {j}, Input {i}\")\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(f\"Symbolic function '{fun_name}' is not defined in SYMBOLIC_LIB.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "출력 y:\n",
      "tensor([[4.6168, 0.0000],\n",
      "        [1.5340, 0.0000],\n",
      "        [2.2689, 0.0000],\n",
      "        [4.5682, 0.0000],\n",
      "        [2.2214, 0.0000]])\n",
      "활성화 함수 후의 값 postacts:\n",
      "tensor([[[ 0.7525,  0.7819,  3.0824],\n",
      "         [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-0.9460,  1.0000,  1.4800],\n",
      "         [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-0.1148,  0.2095,  2.1742],\n",
      "         [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.7138,  0.6708,  3.1836],\n",
      "         [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-0.2315,  0.9824,  1.4705],\n",
      "         [ 0.0000,  0.0000,  0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "# Symbolic_KANLayer 사용 예제\n",
    "import torch\n",
    "\n",
    "# Symbolic_KANLayer 클래스 정의 (위에서 제공한 코드 사용)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Symbolic_KANLayer 초기화\n",
    "    symbolic_layer = Symbolic_KANLayer(in_dim=3, out_dim=2, device='cpu')\n",
    "\n",
    "    # 기호적 함수 할당 예제\n",
    "    # 레이어 0, 뉴런 0: sin 함수\n",
    "    symbolic_layer.fix_symbolic(j=0, i=0, fun_name='sin')\n",
    "    # 레이어 0, 뉴런 1: cos 함수\n",
    "    symbolic_layer.fix_symbolic(j=0, i=1, fun_name='cos')\n",
    "    # 레이어 0, 뉴런 2: exp 함수\n",
    "    symbolic_layer.fix_symbolic(j=0, i=2, fun_name='exp')\n",
    "\n",
    "    # 입력 데이터 생성\n",
    "    x = torch.randn(5, 3)  # (batch_size, in_dim)\n",
    "\n",
    "    # 순전파 실행\n",
    "    y, postacts = symbolic_layer(x)\n",
    "\n",
    "    print(\"출력 y:\")\n",
    "    print(y)\n",
    "    print(\"활성화 함수 후의 값 postacts:\")\n",
    "    print(postacts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. KAN Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class KAN(nn.Module):\n",
    "    def __init__(self, width, grid=5, k=3, base_fun='silu',\n",
    "                 symbolic_enabled=True, affine_trainable=True,  # affine_trainable=True로 변경\n",
    "                 device='cpu'):\n",
    "        super(KAN, self).__init__()\n",
    "        self.device = device\n",
    "        self.depth = len(width) - 1\n",
    "        self.width = width\n",
    "        self.symbolic_enabled = symbolic_enabled\n",
    "\n",
    "        self.act_fun = nn.ModuleList()\n",
    "        self.symbolic_fun = nn.ModuleList()\n",
    "        for l in range(self.depth):\n",
    "            kan_layer = KANLayer(\n",
    "                in_dim=width[l],\n",
    "                out_dim=width[l + 1],\n",
    "                num=grid,\n",
    "                k=k,\n",
    "                base_fun=torch.nn.SiLU(),\n",
    "                device=device\n",
    "            )\n",
    "            self.act_fun.append(kan_layer)\n",
    "\n",
    "            symbolic_layer = Symbolic_KANLayer(\n",
    "                in_dim=width[l],\n",
    "                out_dim=width[l + 1],\n",
    "                device=device\n",
    "            )\n",
    "            self.symbolic_fun.append(symbolic_layer)\n",
    "\n",
    "        self.node_bias = nn.ParameterList()\n",
    "        self.node_scale = nn.ParameterList()\n",
    "        self.subnode_bias = nn.ParameterList()\n",
    "        self.subnode_scale = nn.ParameterList()\n",
    "        for l in range(self.depth):\n",
    "            self.node_bias.append(nn.Parameter(torch.zeros(width[l + 1], device=device), requires_grad=affine_trainable))\n",
    "            self.node_scale.append(nn.Parameter(torch.ones(width[l + 1], device=device), requires_grad=affine_trainable))\n",
    "            self.subnode_bias.append(nn.Parameter(torch.zeros(width[l + 1], device=device), requires_grad=affine_trainable))\n",
    "            self.subnode_scale.append(nn.Parameter(torch.ones(width[l + 1], device=device), requires_grad=affine_trainable))\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for l in range(self.depth):\n",
    "            x_numerical, preacts, postacts_numerical, postspline = self.act_fun[l](x)\n",
    "            if self.symbolic_enabled:\n",
    "                x_symbolic, postacts_symbolic = self.symbolic_fun[l](x)\n",
    "            else:\n",
    "                x_symbolic = torch.zeros_like(x_numerical)\n",
    "\n",
    "            x = x_numerical + x_symbolic\n",
    "\n",
    "            x = self.subnode_scale[l] * x + self.subnode_bias[l]\n",
    "            x = self.node_scale[l] * x + self.node_bias[l]\n",
    "        return x\n",
    "\n",
    "    def symbolic_formula(self):\n",
    "        formulas = []\n",
    "        variables = [f\"x{i}\" for i in range(self.width[0])]\n",
    "        for l in range(self.depth):\n",
    "            layer_formulas = []\n",
    "            for j in range(self.width[l + 1]):\n",
    "                formula = \"\"\n",
    "                for i in range(self.width[l]):\n",
    "                    if self.symbolic_fun[l].mask.data[j, i] > 0 and self.symbolic_fun[l].funs[j][i] is not None:\n",
    "                        a, b, c, d = self.symbolic_fun[l].affine.data[j, i]\n",
    "                        func_name = self.symbolic_fun[l].funs_name[j][i]\n",
    "                        var = variables[i]\n",
    "                        term = f\"{c.item():.3f} * {func_name}({a.item():.3f} * {var} + {b.item():.3f}) + \"\n",
    "                        formula += term\n",
    "                formula = formula.rstrip(\" + \")\n",
    "                if formula == \"\":\n",
    "                    formula = \"0\"\n",
    "                layer_formulas.append(formula)\n",
    "            variables = [f\"h{l+1}_{j}\" for j in range(self.width[l + 1])]\n",
    "            formulas.append(layer_formulas)\n",
    "        return formulas\n",
    "\n",
    "    def fit(self, dataset, optimizer='Adam', steps=100, lr=0.001, lamb=0.0):\n",
    "        '''\n",
    "        KAN 모델을 학습합니다.\n",
    "\n",
    "        Args:\n",
    "        -----\n",
    "            dataset : dict\n",
    "                'train_input' 및 'train_label'을 포함하는 딕셔너리.\n",
    "            optimizer : str\n",
    "                사용할 옵티마이저 ('Adam' 또는 'LBFGS').\n",
    "            steps : int\n",
    "                학습 단계 수.\n",
    "            lr : float\n",
    "                학습률.\n",
    "            lamb : float\n",
    "                정규화 파라미터.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "            None\n",
    "        '''\n",
    "        loss_fn = nn.MSELoss()\n",
    "\n",
    "        if optimizer == 'Adam':\n",
    "            optim = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        elif optimizer == 'LBFGS':\n",
    "            optim = torch.optim.LBFGS(self.parameters(), lr=lr)\n",
    "        else:\n",
    "            raise ValueError(\"Optimizer not recognized.\")\n",
    "\n",
    "        x_train = dataset['train_input']\n",
    "        y_train = dataset['train_label']\n",
    "\n",
    "        for step in range(steps):\n",
    "            def closure():\n",
    "                optim.zero_grad()\n",
    "                output = self.forward(x_train)\n",
    "                loss = loss_fn(output, y_train)\n",
    "                # 필요 시 정규화 추가\n",
    "                loss.backward()\n",
    "                return loss\n",
    "\n",
    "            if optimizer == 'LBFGS':\n",
    "                optim.step(closure)\n",
    "            else:\n",
    "                loss = closure()\n",
    "                optim.step()\n",
    "\n",
    "            if step % 10 == 0:\n",
    "                print(f\"Step {step}: Loss = {loss.item()}\")\n",
    "\n",
    "    def prune(self, threshold=0.01):\n",
    "        '''\n",
    "        모델의 복잡도를 줄이기 위해 작은 계수를 가지는 연결을 제거합니다.\n",
    "\n",
    "        Args:\n",
    "        -----\n",
    "            threshold : float\n",
    "                가지치기 임계값. 이 값 이하의 절대값을 가진 계수는 제거됩니다.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "            None\n",
    "        '''\n",
    "        for l in range(self.depth):\n",
    "            # 수치적 층에서 작은 계수를 가지는 연결 제거\n",
    "            small_sp = self.act_fun[l].scale_sp.abs() < threshold\n",
    "            self.act_fun[l].mask.data[small_sp] = 0.0\n",
    "\n",
    "            # 기호적 층에서 작은 계수를 가지는 연결 제거 (c 파라미터 기준)\n",
    "            small_c = self.symbolic_fun[l].affine[:, :, 2].abs() < threshold\n",
    "            self.symbolic_fun[l].mask.data[small_c] = 0.0\n",
    "\n",
    "    def assign_symbolic_functions(self, assignments):\n",
    "        '''\n",
    "        여러 뉴런에 기호적 함수를 일괄 할당합니다.\n",
    "\n",
    "        Args:\n",
    "        -----\n",
    "            assignments : list of tuples\n",
    "                각 튜플은 (layer_index, output_neuron, input_neuron, function_name)\n",
    "        '''\n",
    "        for assignment in assignments:\n",
    "            l, j, i, func_name = assignment\n",
    "            if l < self.depth:\n",
    "                self.symbolic_fun[l].fix_symbolic(j, i, func_name)\n",
    "            else:\n",
    "                print(f\"Layer index {l} is out of range.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KAN 모델 출력:\n",
      "tensor([[-0.0638,  0.0495],\n",
      "        [-0.0439, -0.0179],\n",
      "        [ 0.2892,  0.1470],\n",
      "        [-0.0409, -0.0491],\n",
      "        [ 0.0861, -0.0115],\n",
      "        [ 0.1815, -0.0007],\n",
      "        [ 0.2272,  0.1203],\n",
      "        [-0.0292,  0.0007],\n",
      "        [ 0.0957,  0.0361],\n",
      "        [-0.0505, -0.0630]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# KAN 모델 사용 예시\n",
    "\n",
    "# 네트워크 아키텍처 정의\n",
    "width = [3, 5, 2]  # 입력 차원 3, 은닉층 5개 뉴런, 출력 차원 2\n",
    "\n",
    "# KAN 모델 초기화\n",
    "model = KAN(width=width, grid=5, k=3, device='cpu')\n",
    "\n",
    "# 입력 데이터 생성\n",
    "x = torch.randn(10, 3)\n",
    "\n",
    "# 순전파 실행\n",
    "output = model(x)\n",
    "\n",
    "print(\"KAN 모델 출력:\")\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Loss = 0.6423020362854004\n",
      "Step 10: Loss = 0.1253228336572647\n",
      "Step 20: Loss = 0.08409864455461502\n",
      "Step 30: Loss = 0.014542357996106148\n",
      "Step 40: Loss = 0.010408625937998295\n",
      "Step 50: Loss = 0.0030659902840852737\n",
      "Step 60: Loss = 0.0021975263953208923\n",
      "Step 70: Loss = 0.0007675166707485914\n",
      "Step 80: Loss = 0.00045903402497060597\n",
      "Step 90: Loss = 0.0002122166333720088\n",
      "Test Loss: 6.386656605172902e-05\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습 예제\n",
    "import torch\n",
    "\n",
    "# 네트워크 아키텍처 정의\n",
    "width = [2, 5, 1]\n",
    "\n",
    "# KAN 모델 초기화\n",
    "model = KAN(width=width, grid=5, k=3, device='cpu')\n",
    "\n",
    "# 데이터셋 생성\n",
    "x_train = torch.rand(100, 2)\n",
    "y_train = torch.sin(x_train[:, 0]) + x_train[:, 1] ** 2\n",
    "y_train = y_train.unsqueeze(1)\n",
    "\n",
    "dataset = {\n",
    "    'train_input': x_train,\n",
    "    'train_label': y_train\n",
    "}\n",
    "\n",
    "# 모델 학습\n",
    "model.fit(dataset, optimizer='Adam', steps=100, lr=0.01)\n",
    "\n",
    "# 테스트 데이터 생성\n",
    "x_test = torch.rand(20, 2)\n",
    "y_test = torch.sin(x_test[:, 0]) + x_test[:, 1] ** 2\n",
    "y_test = y_test.unsqueeze(1)\n",
    "\n",
    "# 예측 및 손실 계산\n",
    "y_pred = model(x_test)\n",
    "test_loss = nn.MSELoss()(y_pred, y_test)\n",
    "print(f\"Test Loss: {test_loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
