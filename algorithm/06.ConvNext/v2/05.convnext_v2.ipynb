{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 00 - Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "from timm.data import Mixup\n",
    "from timm.utils import ModelEmaV3\n",
    "from timm.loss import LabelSmoothingCrossEntropy, SoftTargetCrossEntropy\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "from convnext_v1 import load_convNext_v1\n",
    "from convnext_v2 import load_convNext\n",
    "import math\n",
    "import warnings\n",
    "from torch.optim.lr_scheduler import _LRScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineWarmupScheduler(_LRScheduler):\n",
    "    def __init__(self, optimizer, num_warmup_steps, num_training_steps, num_cycles=0.5, min_lr=1e-6, last_epoch=-1, verbose=False):\n",
    "        self.num_warmup_steps = num_warmup_steps\n",
    "        self.num_training_steps = num_training_steps\n",
    "        self.num_cycles = num_cycles\n",
    "        self.min_lr = min_lr\n",
    "        self.base_lrs = [group['lr'] for group in optimizer.param_groups]\n",
    "        super().__init__(optimizer, last_epoch, verbose)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if not self._get_lr_called_within_step:\n",
    "            warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
    "                          \"please use `get_last_lr()`.\", UserWarning)\n",
    "        \n",
    "        lrs = []\n",
    "        for base_lr in self.base_lrs:\n",
    "            if self.last_epoch < self.num_warmup_steps:\n",
    "                # Linear warmup\n",
    "                lr = (base_lr - self.min_lr) * self.last_epoch / max(1, self.num_warmup_steps) + self.min_lr\n",
    "            else:\n",
    "                # Cosine annealing\n",
    "                progress = (self.last_epoch - self.num_warmup_steps) / max(1, self.num_training_steps - self.num_warmup_steps)\n",
    "                lr = self.min_lr + (base_lr - self.min_lr) * 0.5 * (1 + math.cos(math.pi * self.num_cycles * 2.0 * progress))\n",
    "            lrs.append(lr)\n",
    "        return lrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 01 - Test ConvNeXt V1(sup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_v1 = load_convNext_v1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "Total Parameters: 27,897,028\n",
      "Trainable Parameters: 27,897,028\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 총 파라미터 수 계산\n",
    "total_params = sum(p.numel() for p in model_v1.parameters())\n",
    "\n",
    "# 학습 가능한 파라미터 수 계산\n",
    "trainable_params = sum(p.numel() for p in model_v1.parameters() if p.requires_grad)\n",
    "\n",
    "print('='*80)\n",
    "print(f\"\\nTotal Parameters: {total_params:,}\")\n",
    "print(f\"Trainable Parameters: {trainable_params:,}\\n\")\n",
    "print('='*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using EMA with decay = 0.9998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "# Transforms 정의하기\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.6,1), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    transforms.RandomErasing(p=1., scale=(0.02, 0.33)),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "data_dir = '../../data/sports'\n",
    "batch_size = 512\n",
    "\n",
    "train_path = data_dir+'/train'\n",
    "valid_path = data_dir+'/valid'\n",
    "test_path = data_dir+'/test'\n",
    "\n",
    "# dataset load\n",
    "train_data = ImageFolder(train_path, transform=train_transform)\n",
    "valid_data = ImageFolder(valid_path, transform=test_transform)\n",
    "test_data = ImageFolder(test_path, transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "device = 'cuda:3'\n",
    "max_norm = 3.0 \n",
    "\n",
    "model_v1.to(device)\n",
    "\n",
    "model_ema = None\n",
    "ema_active = True\n",
    "if ema_active:\n",
    "    ema_decay = 0.9998\n",
    "    model_ema = ModelEmaV3(\n",
    "        model_v1,\n",
    "        decay=ema_decay,\n",
    "    )\n",
    "    print(f\"Using EMA with decay = {ema_decay}\")\n",
    "\n",
    "model_path = ''\n",
    "\n",
    "mixup = True\n",
    "if mixup :\n",
    "    mixup_fn = Mixup(mixup_alpha=.8, \n",
    "                    cutmix_alpha=1., \n",
    "                    prob=1., \n",
    "                    switch_prob=0.5, \n",
    "                    mode='batch',\n",
    "                    label_smoothing=.1,\n",
    "                    num_classes=100)\n",
    "    \n",
    "    criterion = SoftTargetCrossEntropy()\n",
    "else :\n",
    "    criterion = LabelSmoothingCrossEntropy(.1)\n",
    "    \n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "optimizer = optim.AdamW(model_v1.parameters(), lr=4e-3, weight_decay=0.05)\n",
    "warmup_steps = int(len(train_loader)*(epochs)*0.1)\n",
    "train_steps = len(train_loader)*(epochs)\n",
    "scheduler = CosineWarmupScheduler(optimizer, \n",
    "                                num_warmup_steps=warmup_steps, \n",
    "                                num_training_steps=train_steps,\n",
    "                                num_cycles=0.5,\n",
    "                                min_lr=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 27/27 [00:56<00:00,  2.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.4774, Val_Loss: 4.0328, Total Mean Loss: 4.2551, LR: 0.00040008999999999997, Duration: 57.10 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 27/27 [00:55<00:00,  2.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.3636, Val_Loss: 3.9938, Total Mean Loss: 4.1787, LR: 0.0008000799999999999, Duration: 56.24 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 27/27 [00:55<00:00,  2.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.3047, Val_Loss: 3.7656, Total Mean Loss: 4.0352, LR: 0.0012000700000000001, Duration: 56.54 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 27/27 [00:55<00:00,  2.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.2676, Val_Loss: 3.7741, Total Mean Loss: 4.0209, LR: 0.00160006, Duration: 56.30 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.1701, Val_Loss: 3.5921, Total Mean Loss: 3.8811, LR: 0.0020000499999999997, Duration: 56.13 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.1979, Val_Loss: 3.5395, Total Mean Loss: 3.8687, LR: 0.00240004, Duration: 56.20 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.1892, Val_Loss: 3.5515, Total Mean Loss: 3.8703, LR: 0.00280003, Duration: 56.03 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.0846, Val_Loss: 3.3567, Total Mean Loss: 3.7207, LR: 0.0032000199999999996, Duration: 56.08 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 27/27 [00:55<00:00,  2.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.0832, Val_Loss: 3.2658, Total Mean Loss: 3.6745, LR: 0.00360001, Duration: 56.34 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.9733, Val_Loss: 3.2181, Total Mean Loss: 3.5957, LR: 0.004, Duration: 56.18 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.9585, Val_Loss: 3.1206, Total Mean Loss: 3.5395, LR: 0.003998781684496841, Duration: 56.13 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.9332, Val_Loss: 2.9625, Total Mean Loss: 3.4478, LR: 0.003995128222317136, Duration: 56.06 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 27/27 [00:54<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.9000, Val_Loss: 2.9625, Total Mean Loss: 3.4313, LR: 0.003989044064641779, Duration: 55.92 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.8427, Val_Loss: 2.8141, Total Mean Loss: 3.3284, LR: 0.0039805366240797035, Duration: 56.11 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 27/27 [00:54<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.7609, Val_Loss: 2.6553, Total Mean Loss: 3.2081, LR: 0.003969616265636766, Duration: 55.97 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 27/27 [00:54<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.8194, Val_Loss: 2.6457, Total Mean Loss: 3.2325, LR: 0.003956296294087574, Duration: 55.85 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 27/27 [00:54<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.6426, Val_Loss: 2.5182, Total Mean Loss: 3.0804, LR: 0.003940592937765679, Duration: 55.96 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.6091, Val_Loss: 2.3043, Total Mean Loss: 2.9567, LR: 0.003922525328791841, Duration: 56.04 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 27/27 [00:54<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.7503, Val_Loss: 2.3364, Total Mean Loss: 3.0433, LR: 0.0039021154797644923, Duration: 55.73 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 27/27 [00:54<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.5655, Val_Loss: 2.2997, Total Mean Loss: 2.9326, LR: 0.0038793882569407774, Duration: 55.96 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 27/27 [00:54<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.5359, Val_Loss: 2.1852, Total Mean Loss: 2.8605, LR: 0.0038543713499408464, Duration: 55.90 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.5495, Val_Loss: 2.2021, Total Mean Loss: 2.8758, LR: 0.003827095238012319, Duration: 56.02 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 27/27 [00:54<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.4314, Val_Loss: 1.8701, Total Mean Loss: 2.6508, LR: 0.003797593152896019, Duration: 55.81 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 27/27 [00:54<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.5395, Val_Loss: 2.0334, Total Mean Loss: 2.7864, LR: 0.0037659010383382105, Duration: 55.96 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 27/27 [00:54<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.3701, Val_Loss: 1.9876, Total Mean Loss: 2.6788, LR: 0.003732057506298688, Duration: 55.82 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 27/27 [00:54<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.3378, Val_Loss: 1.7445, Total Mean Loss: 2.5411, LR: 0.0036961037899080436, Duration: 55.90 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 27/27 [00:54<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.2249, Val_Loss: 1.6983, Total Mean Loss: 2.4616, LR: 0.0036580836932314552, Duration: 55.84 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 27/27 [00:55<00:00,  2.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.3712, Val_Loss: 1.7954, Total Mean Loss: 2.5833, LR: 0.003618043537900176, Duration: 56.32 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 27/27 [00:54<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.2627, Val_Loss: 1.6444, Total Mean Loss: 2.4536, LR: 0.003576032106675763, Duration: 55.88 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 27/27 [00:54<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.2453, Val_Loss: 1.6847, Total Mean Loss: 2.4650, LR: 0.0035321005840157995, Duration: 55.90 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|██████████| 27/27 [00:54<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.0739, Val_Loss: 1.4796, Total Mean Loss: 2.2767, LR: 0.0034863024937135142, Duration: 55.85 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.2991, Val_Loss: 1.4953, Total Mean Loss: 2.3972, LR: 0.003438693633687285, Duration: 56.03 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|██████████| 27/27 [00:54<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.0333, Val_Loss: 1.4960, Total Mean Loss: 2.2647, LR: 0.0033893320079994714, Duration: 55.97 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.1110, Val_Loss: 1.4444, Total Mean Loss: 2.2777, LR: 0.003338277756187398, Duration: 56.15 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.1195, Val_Loss: 1.5030, Total Mean Loss: 2.3113, LR: 0.003285593079992594, Duration: 56.06 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.1100, Val_Loss: 1.3832, Total Mean Loss: 2.2466, LR: 0.00323134216757755, Duration: 56.12 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.0795, Val_Loss: 1.4472, Total Mean Loss: 2.2634, LR: 0.0031755911153223313, Duration: 56.11 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.9890, Val_Loss: 1.2568, Total Mean Loss: 2.1229, LR: 0.0031184078472963196, Duration: 56.06 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|██████████| 27/27 [00:54<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.1263, Val_Loss: 1.3073, Total Mean Loss: 2.2168, LR: 0.003059862032503198, Duration: 55.97 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.0529, Val_Loss: 1.2554, Total Mean Loss: 2.1542, LR: 0.0030000249999999995, Duration: 56.05 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.7628, Val_Loss: 1.1780, Total Mean Loss: 1.9704, LR: 0.002938969651993642, Duration: 56.07 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42: 100%|██████████| 27/27 [00:54<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.8932, Val_Loss: 1.1685, Total Mean Loss: 2.0308, LR: 0.002876770375020815, Duration: 55.86 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.8992, Val_Loss: 1.2290, Total Mean Loss: 2.0641, LR: 0.0028135029493194467, Duration: 56.12 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44: 100%|██████████| 27/27 [00:55<00:00,  2.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.9412, Val_Loss: 1.2404, Total Mean Loss: 2.0908, LR: 0.0027492444565021534, Duration: 56.27 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45: 100%|██████████| 27/27 [00:54<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.9296, Val_Loss: 1.1521, Total Mean Loss: 2.0409, LR: 0.0026840731856441714, Duration: 55.90 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6970, Val_Loss: 1.0895, Total Mean Loss: 1.8932, LR: 0.0026180685379001757, Duration: 56.01 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47: 100%|██████████| 27/27 [00:54<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5881, Val_Loss: 0.9863, Total Mean Loss: 1.7872, LR: 0.002551310929766207, Duration: 55.95 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48: 100%|██████████| 27/27 [00:54<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.9589, Val_Loss: 1.0798, Total Mean Loss: 2.0193, LR: 0.002483881695104555, Duration: 55.89 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 27/27 [00:54<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.7937, Val_Loss: 1.0734, Total Mean Loss: 1.9336, LR: 0.0024158629860509774, Duration: 55.96 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.8662, Val_Loss: 1.0298, Total Mean Loss: 1.9480, LR: 0.0023473376729249776, Duration: 56.12 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51: 100%|██████████| 27/27 [00:54<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6298, Val_Loss: 1.0176, Total Mean Loss: 1.8237, LR: 0.0022783892432650826, Duration: 55.98 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2934, Val_Loss: 0.9682, Total Mean Loss: 1.6308, LR: 0.0022091017001121434, Duration: 56.07 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53: 100%|██████████| 27/27 [00:54<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.7997, Val_Loss: 1.0152, Total Mean Loss: 1.9075, LR: 0.002139559459664563, Duration: 55.89 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5031, Val_Loss: 0.9415, Total Mean Loss: 1.7223, LR: 0.0020698472484301667, Duration: 56.06 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55: 100%|██████████| 27/27 [00:54<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3984, Val_Loss: 0.9141, Total Mean Loss: 1.6562, LR: 0.0020000499999999997, Duration: 55.90 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56: 100%|██████████| 27/27 [00:54<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5740, Val_Loss: 0.8883, Total Mean Loss: 1.7312, LR: 0.0019302527515698336, Duration: 55.99 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57: 100%|██████████| 27/27 [00:54<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6690, Val_Loss: 0.9551, Total Mean Loss: 1.8120, LR: 0.0018605405403354365, Duration: 55.98 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6134, Val_Loss: 0.9461, Total Mean Loss: 1.7797, LR: 0.0017909982998878568, Duration: 56.05 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5852, Val_Loss: 0.9271, Total Mean Loss: 1.7561, LR: 0.0017217107567349176, Duration: 56.02 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60: 100%|██████████| 27/27 [00:55<00:00,  2.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5558, Val_Loss: 0.8173, Total Mean Loss: 1.6865, LR: 0.0016527623270750228, Duration: 56.21 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61: 100%|██████████| 27/27 [00:54<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5551, Val_Loss: 0.8222, Total Mean Loss: 1.6887, LR: 0.0015842370139490226, Duration: 55.97 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62: 100%|██████████| 27/27 [00:54<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5135, Val_Loss: 0.8065, Total Mean Loss: 1.6600, LR: 0.0015162183048954448, Duration: 55.95 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4218, Val_Loss: 0.8387, Total Mean Loss: 1.6302, LR: 0.0014487890702337925, Duration: 56.10 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3691, Val_Loss: 0.8183, Total Mean Loss: 1.5937, LR: 0.001382031462099824, Duration: 56.03 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4415, Val_Loss: 0.8186, Total Mean Loss: 1.6301, LR: 0.001316026814355829, Duration: 56.01 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4425, Val_Loss: 0.8346, Total Mean Loss: 1.6386, LR: 0.0012508555434978467, Duration: 56.06 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3497, Val_Loss: 0.7826, Total Mean Loss: 1.5661, LR: 0.0011865970506805537, Duration: 56.03 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68: 100%|██████████| 27/27 [00:54<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3265, Val_Loss: 0.7799, Total Mean Loss: 1.5532, LR: 0.0011233296249791845, Duration: 55.93 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5047, Val_Loss: 0.8032, Total Mean Loss: 1.6540, LR: 0.0010611303480063583, Duration: 56.02 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70: 100%|██████████| 27/27 [00:55<00:00,  2.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0608, Val_Loss: 0.7569, Total Mean Loss: 1.4088, LR: 0.0010000750000000004, Duration: 56.27 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2896, Val_Loss: 0.7690, Total Mean Loss: 1.5293, LR: 0.000940237967496802, Duration: 56.04 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72: 100%|██████████| 27/27 [00:54<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3211, Val_Loss: 0.7482, Total Mean Loss: 1.5347, LR: 0.0008816921527036801, Duration: 55.96 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 73: 100%|██████████| 27/27 [00:54<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3094, Val_Loss: 0.7497, Total Mean Loss: 1.5295, LR: 0.0008245088846776685, Duration: 55.97 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 74: 100%|██████████| 27/27 [00:54<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3828, Val_Loss: 0.7911, Total Mean Loss: 1.5869, LR: 0.0007687578324224496, Duration: 55.95 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3892, Val_Loss: 0.7802, Total Mean Loss: 1.5847, LR: 0.0007145069200074055, Duration: 56.00 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 76: 100%|██████████| 27/27 [00:55<00:00,  2.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3461, Val_Loss: 0.7231, Total Mean Loss: 1.5346, LR: 0.000661822243812602, Duration: 56.24 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 77: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1545, Val_Loss: 0.7566, Total Mean Loss: 1.4556, LR: 0.0006107679920005282, Duration: 56.03 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 78: 100%|██████████| 27/27 [00:55<00:00,  2.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2703, Val_Loss: 0.7705, Total Mean Loss: 1.5204, LR: 0.0005614063663127149, Duration: 56.34 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 79: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3561, Val_Loss: 0.7499, Total Mean Loss: 1.5530, LR: 0.000513797506286485, Duration: 56.12 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 80: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2656, Val_Loss: 0.7683, Total Mean Loss: 1.5169, LR: 0.00046799941598420013, Duration: 56.14 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 81: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2729, Val_Loss: 0.7432, Total Mean Loss: 1.5081, LR: 0.0004240678933242365, Duration: 56.04 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 82: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2517, Val_Loss: 0.7783, Total Mean Loss: 1.5150, LR: 0.00038205646209982404, Duration: 56.18 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 83: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2930, Val_Loss: 0.7482, Total Mean Loss: 1.5206, LR: 0.0003420163067685445, Duration: 56.11 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 84: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1917, Val_Loss: 0.7575, Total Mean Loss: 1.4746, LR: 0.0003039962100919559, Duration: 56.06 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 85: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2999, Val_Loss: 0.7637, Total Mean Loss: 1.5318, LR: 0.0002680424937013118, Duration: 56.07 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 86: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1889, Val_Loss: 0.7282, Total Mean Loss: 1.4586, LR: 0.00023419896166178896, Duration: 56.07 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 87: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1473, Val_Loss: 0.7745, Total Mean Loss: 1.4609, LR: 0.0002025068471039813, Duration: 56.04 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 88: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1531, Val_Loss: 0.8005, Total Mean Loss: 1.4768, LR: 0.00017300476198768016, Duration: 56.04 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 89: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2986, Val_Loss: 0.7652, Total Mean Loss: 1.5319, LR: 0.00014572865005915372, Duration: 56.19 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 90: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3740, Val_Loss: 0.7677, Total Mean Loss: 1.5708, LR: 0.00012071174305922266, Duration: 56.13 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 91: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2788, Val_Loss: 0.7759, Total Mean Loss: 1.5273, LR: 9.79845202355077e-05, Duration: 56.12 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 92: 100%|██████████| 27/27 [00:55<00:00,  2.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1289, Val_Loss: 0.7813, Total Mean Loss: 1.4551, LR: 7.757467120815912e-05, Duration: 56.22 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 93: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0349, Val_Loss: 0.7657, Total Mean Loss: 1.4003, LR: 5.950706223432085e-05, Duration: 56.15 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 94: 100%|██████████| 27/27 [00:54<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0171, Val_Loss: 0.7645, Total Mean Loss: 1.3908, LR: 4.3803705912425316e-05, Duration: 55.99 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 95: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0323, Val_Loss: 0.7693, Total Mean Loss: 1.4008, LR: 3.0483734363234566e-05, Duration: 56.10 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 96: 100%|██████████| 27/27 [00:54<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2488, Val_Loss: 0.7582, Total Mean Loss: 1.5035, LR: 1.9563375920296352e-05, Duration: 55.99 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 97: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.8898, Val_Loss: 0.7615, Total Mean Loss: 1.3256, LR: 1.1055935358221834e-05, Duration: 56.10 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 98: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1196, Val_Loss: 0.7607, Total Mean Loss: 1.4401, LR: 4.971777682864596e-06, Duration: 56.02 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0866, Val_Loss: 0.7598, Total Mean Loss: 1.4232, LR: 1.3183155031594304e-06, Duration: 56.12 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 100: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1089, Val_Loss: 0.7597, Total Mean Loss: 1.4343, LR: 1e-07, Duration: 56.03 sec\n",
      "\n",
      "[100 epoch result]\n",
      "       Metric     Value\n",
      "0   Accuracy  0.876000\n",
      "1  Precision  0.896143\n",
      "2     Recall  0.876000\n",
      "3   F1 Score  0.872440\n"
     ]
    }
   ],
   "source": [
    "training_time = 0\n",
    "losses = []\n",
    "val_losses = []\n",
    "lrs = []\n",
    "best_loss = float('inf')\n",
    "\n",
    "# GradScaler 초기화\n",
    "scaler = GradScaler()\n",
    "\n",
    "for i in range(epochs // 100):\n",
    "    for epoch in range(100):\n",
    "        model_v1.train()\n",
    "        start_time = time.time()\n",
    "        running_loss = 0.0\n",
    "        pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch + 1 + i*100}\")\n",
    "        \n",
    "        for _, data in pbar:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            inputs, labels = mixup_fn(inputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # AutoCast 적용\n",
    "            with autocast():\n",
    "                outputs = model_v1(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "            # 스케일링된 그라디언트 계산\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # 그라디언트 클리핑 전에 스케일링 제거\n",
    "            scaler.unscale_(optimizer)\n",
    "            clip_grad_norm_(model_v1.parameters(), max_norm=max_norm)\n",
    "\n",
    "            # 옵티마이저 스텝 및 스케일러 업데이트\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            # EMA 모델 업데이트\n",
    "            if model_ema is not None:\n",
    "                model_ema.update(model_v1)\n",
    "                \n",
    "            scheduler.step()\n",
    "                \n",
    "            lr = optimizer.param_groups[0][\"lr\"]\n",
    "            lrs.append(lr)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        losses.append(epoch_loss)        \n",
    "\n",
    "        model_v1.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data in valid_loader:\n",
    "                inputs, labels = data[0].to(device), data[1].to(device)\n",
    "                outputs = model_v1(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "        val_loss /= len(valid_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # 모델 저장\n",
    "        total_loss = val_loss + epoch_loss\n",
    "        if total_loss < best_loss:\n",
    "            best_loss = total_loss\n",
    "            model_save = False\n",
    "            if model_save:\n",
    "                torch.save(model_v1.state_dict(), model_path)\n",
    "\n",
    "        epoch_duration = time.time() - start_time\n",
    "        training_time += epoch_duration\n",
    "        \n",
    "        text = f'\\tLoss: {epoch_loss:.4f}, Val_Loss: {val_loss:.4f}, Total Mean Loss: {total_loss/2:.4f}, LR: {lr}, Duration: {epoch_duration:.2f} sec'\n",
    "        \n",
    "        if model_save:\n",
    "            text += f' - model saved!'\n",
    "            model_save = False\n",
    "\n",
    "        print(text)\n",
    "\n",
    "    # 예측 수행 및 레이블 저장\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model_v1(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # 혼동 행렬 생성\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    # 예측과 실제 레이블\n",
    "    y_true = all_labels  # 실제 레이블\n",
    "    y_pred = all_preds  # 모델에 의해 예측된 레이블\n",
    "\n",
    "    # 전체 데이터셋에 대한 정확도\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # 평균 정밀도, 리콜, F1-Score ('weighted')\n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "\n",
    "    # 판다스 데이터프레임으로 결과 정리\n",
    "    performance_metrics = pd.DataFrame({\n",
    "        'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "        'Value': [accuracy, precision, recall, f1_score]\n",
    "    })\n",
    "\n",
    "    # 데이터프레임 출력\n",
    "    print(f\"\\n[{i*100+100} epoch result]\\n\", performance_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_v1.cpu()\n",
    "del model_v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 02 - Test ConvNeXt V2(sup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "Total Parameters: 27,943,396\n",
      "Trainable Parameters: 27,943,396\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "model_v2 = load_convNext()\n",
    "\n",
    "# 총 파라미터 수 계산\n",
    "total_params = sum(p.numel() for p in model_v2.parameters())\n",
    "\n",
    "# 학습 가능한 파라미터 수 계산\n",
    "trainable_params = sum(p.numel() for p in model_v2.parameters() if p.requires_grad)\n",
    "\n",
    "print('='*80)\n",
    "print(f\"\\nTotal Parameters: {total_params:,}\")\n",
    "print(f\"Trainable Parameters: {trainable_params:,}\\n\")\n",
    "print('='*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using EMA with decay = 0.9998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "model_v2.to(device)\n",
    "\n",
    "model_ema = None\n",
    "ema_active = True\n",
    "if ema_active:\n",
    "    ema_decay = 0.9998\n",
    "    model_ema = ModelEmaV3(\n",
    "        model_v2,\n",
    "        decay=ema_decay,\n",
    "    )\n",
    "    print(f\"Using EMA with decay = {ema_decay}\")\n",
    "\n",
    "model_path = ''\n",
    "\n",
    "mixup = True\n",
    "if mixup :\n",
    "    mixup_fn = Mixup(mixup_alpha=.8, \n",
    "                    cutmix_alpha=1., \n",
    "                    prob=1., \n",
    "                    switch_prob=0.5, \n",
    "                    mode='batch',\n",
    "                    label_smoothing=.1,\n",
    "                    num_classes=100)\n",
    "    \n",
    "    criterion = SoftTargetCrossEntropy()\n",
    "else :\n",
    "    criterion = LabelSmoothingCrossEntropy(.1)\n",
    "    \n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "optimizer = optim.AdamW(model_v2.parameters(), lr=4e-3, weight_decay=0.05)\n",
    "warmup_steps = int(len(train_loader)*(epochs)*0.1)\n",
    "train_steps = len(train_loader)*(epochs)\n",
    "scheduler = CosineWarmupScheduler(optimizer, \n",
    "                                num_warmup_steps=warmup_steps, \n",
    "                                num_training_steps=train_steps,\n",
    "                                num_cycles=0.5,\n",
    "                                min_lr=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.4953, Val_Loss: 3.9957, Total Mean Loss: 4.2455, LR: 0.00040008999999999997, Duration: 67.57 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.3383, Val_Loss: 3.8066, Total Mean Loss: 4.0724, LR: 0.0008000799999999999, Duration: 67.57 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 27/27 [01:06<00:00,  2.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.2863, Val_Loss: 3.7261, Total Mean Loss: 4.0062, LR: 0.0012000700000000001, Duration: 67.70 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 27/27 [01:06<00:00,  2.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.2008, Val_Loss: 3.6685, Total Mean Loss: 3.9346, LR: 0.00160006, Duration: 67.98 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 27/27 [01:06<00:00,  2.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.2399, Val_Loss: 3.5638, Total Mean Loss: 3.9018, LR: 0.0020000499999999997, Duration: 67.65 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 27/27 [01:06<00:00,  2.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.1107, Val_Loss: 3.3985, Total Mean Loss: 3.7546, LR: 0.00240004, Duration: 67.89 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 27/27 [01:06<00:00,  2.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.0405, Val_Loss: 3.2016, Total Mean Loss: 3.6211, LR: 0.00280003, Duration: 67.75 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 27/27 [01:06<00:00,  2.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.0470, Val_Loss: 3.0241, Total Mean Loss: 3.5355, LR: 0.0032000199999999996, Duration: 67.74 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 27/27 [01:07<00:00,  2.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.0337, Val_Loss: 3.0783, Total Mean Loss: 3.5560, LR: 0.00360001, Duration: 68.12 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 27/27 [01:06<00:00,  2.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.9008, Val_Loss: 2.8049, Total Mean Loss: 3.3528, LR: 0.004, Duration: 67.85 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 27/27 [01:06<00:00,  2.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.7319, Val_Loss: 2.6172, Total Mean Loss: 3.1745, LR: 0.003998781684496841, Duration: 67.93 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.7359, Val_Loss: 2.5972, Total Mean Loss: 3.1665, LR: 0.003995128222317136, Duration: 67.60 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.7365, Val_Loss: 2.3938, Total Mean Loss: 3.0652, LR: 0.003989044064641779, Duration: 67.60 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.5635, Val_Loss: 2.3816, Total Mean Loss: 2.9726, LR: 0.0039805366240797035, Duration: 67.60 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.7393, Val_Loss: 2.2984, Total Mean Loss: 3.0189, LR: 0.003969616265636766, Duration: 67.59 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.5639, Val_Loss: 2.1167, Total Mean Loss: 2.8403, LR: 0.003956296294087574, Duration: 67.48 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.6771, Val_Loss: 2.0762, Total Mean Loss: 2.8767, LR: 0.003940592937765679, Duration: 67.58 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.4355, Val_Loss: 2.0092, Total Mean Loss: 2.7224, LR: 0.003922525328791841, Duration: 67.61 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.4700, Val_Loss: 2.1298, Total Mean Loss: 2.7999, LR: 0.0039021154797644923, Duration: 67.51 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.5844, Val_Loss: 2.1278, Total Mean Loss: 2.8561, LR: 0.0038793882569407774, Duration: 67.55 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.4391, Val_Loss: 1.9167, Total Mean Loss: 2.6779, LR: 0.0038543713499408464, Duration: 67.53 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.3674, Val_Loss: 1.7117, Total Mean Loss: 2.5396, LR: 0.003827095238012319, Duration: 67.48 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.1984, Val_Loss: 1.6721, Total Mean Loss: 2.4352, LR: 0.003797593152896019, Duration: 67.56 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.3351, Val_Loss: 1.7236, Total Mean Loss: 2.5294, LR: 0.0037659010383382105, Duration: 67.45 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 27/27 [01:06<00:00,  2.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.2980, Val_Loss: 1.5397, Total Mean Loss: 2.4188, LR: 0.003732057506298688, Duration: 67.72 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 27/27 [01:06<00:00,  2.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.2775, Val_Loss: 1.6634, Total Mean Loss: 2.4704, LR: 0.0036961037899080436, Duration: 67.67 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.1404, Val_Loss: 1.6914, Total Mean Loss: 2.4159, LR: 0.0036580836932314552, Duration: 67.52 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.2671, Val_Loss: 1.5354, Total Mean Loss: 2.4012, LR: 0.003618043537900176, Duration: 67.54 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.9860, Val_Loss: 1.4390, Total Mean Loss: 2.2125, LR: 0.003576032106675763, Duration: 67.41 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.1470, Val_Loss: 1.3110, Total Mean Loss: 2.2290, LR: 0.0035321005840157995, Duration: 67.41 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.8700, Val_Loss: 1.2069, Total Mean Loss: 2.0385, LR: 0.0034863024937135142, Duration: 67.52 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.0219, Val_Loss: 1.3896, Total Mean Loss: 2.2057, LR: 0.003438693633687285, Duration: 67.56 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.0175, Val_Loss: 1.3374, Total Mean Loss: 2.1774, LR: 0.0033893320079994714, Duration: 67.50 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.8511, Val_Loss: 1.1852, Total Mean Loss: 2.0182, LR: 0.003338277756187398, Duration: 67.57 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.0255, Val_Loss: 1.2675, Total Mean Loss: 2.1465, LR: 0.003285593079992594, Duration: 67.41 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.9926, Val_Loss: 1.1701, Total Mean Loss: 2.0814, LR: 0.00323134216757755, Duration: 67.49 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.8710, Val_Loss: 1.1276, Total Mean Loss: 1.9993, LR: 0.0031755911153223313, Duration: 67.41 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.8884, Val_Loss: 1.0406, Total Mean Loss: 1.9645, LR: 0.0031184078472963196, Duration: 67.40 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|██████████| 27/27 [01:06<00:00,  2.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6145, Val_Loss: 1.1127, Total Mean Loss: 1.8636, LR: 0.003059862032503198, Duration: 67.70 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.9169, Val_Loss: 1.0917, Total Mean Loss: 2.0043, LR: 0.0030000249999999995, Duration: 67.52 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.9894, Val_Loss: 1.0460, Total Mean Loss: 2.0177, LR: 0.002938969651993642, Duration: 67.55 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.7877, Val_Loss: 1.0938, Total Mean Loss: 1.9408, LR: 0.002876770375020815, Duration: 67.37 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5092, Val_Loss: 0.9956, Total Mean Loss: 1.7524, LR: 0.0028135029493194467, Duration: 67.48 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6443, Val_Loss: 1.0065, Total Mean Loss: 1.8254, LR: 0.0027492444565021534, Duration: 67.49 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45: 100%|██████████| 27/27 [01:06<00:00,  2.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4927, Val_Loss: 0.9553, Total Mean Loss: 1.7240, LR: 0.0026840731856441714, Duration: 67.69 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46: 100%|██████████| 27/27 [01:07<00:00,  2.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5427, Val_Loss: 0.9239, Total Mean Loss: 1.7333, LR: 0.0026180685379001757, Duration: 68.13 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47: 100%|██████████| 27/27 [01:06<00:00,  2.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6331, Val_Loss: 0.9464, Total Mean Loss: 1.7898, LR: 0.002551310929766207, Duration: 67.36 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6876, Val_Loss: 0.8948, Total Mean Loss: 1.7912, LR: 0.002483881695104555, Duration: 67.42 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6114, Val_Loss: 0.9237, Total Mean Loss: 1.7676, LR: 0.0024158629860509774, Duration: 67.55 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50: 100%|██████████| 27/27 [01:06<00:00,  2.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6508, Val_Loss: 0.9648, Total Mean Loss: 1.8078, LR: 0.0023473376729249776, Duration: 67.33 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5695, Val_Loss: 0.9561, Total Mean Loss: 1.7628, LR: 0.0022783892432650826, Duration: 67.48 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4391, Val_Loss: 0.8177, Total Mean Loss: 1.6284, LR: 0.0022091017001121434, Duration: 67.44 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4974, Val_Loss: 0.8427, Total Mean Loss: 1.6701, LR: 0.002139559459664563, Duration: 67.52 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4575, Val_Loss: 0.7762, Total Mean Loss: 1.6168, LR: 0.0020698472484301667, Duration: 67.46 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5227, Val_Loss: 0.8147, Total Mean Loss: 1.6687, LR: 0.0020000499999999997, Duration: 67.56 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56: 100%|██████████| 27/27 [01:06<00:00,  2.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4267, Val_Loss: 0.8300, Total Mean Loss: 1.6284, LR: 0.0019302527515698336, Duration: 67.35 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4398, Val_Loss: 0.7255, Total Mean Loss: 1.5827, LR: 0.0018605405403354365, Duration: 67.56 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2488, Val_Loss: 0.8539, Total Mean Loss: 1.5514, LR: 0.0017909982998878568, Duration: 67.61 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1985, Val_Loss: 0.7956, Total Mean Loss: 1.4970, LR: 0.0017217107567349176, Duration: 67.54 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3759, Val_Loss: 0.8497, Total Mean Loss: 1.6128, LR: 0.0016527623270750228, Duration: 67.58 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61: 100%|██████████| 27/27 [01:06<00:00,  2.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2043, Val_Loss: 0.8578, Total Mean Loss: 1.5311, LR: 0.0015842370139490226, Duration: 67.73 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3755, Val_Loss: 0.8482, Total Mean Loss: 1.6119, LR: 0.0015162183048954448, Duration: 67.44 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1355, Val_Loss: 0.8108, Total Mean Loss: 1.4731, LR: 0.0014487890702337925, Duration: 67.54 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1350, Val_Loss: 0.7789, Total Mean Loss: 1.4570, LR: 0.001382031462099824, Duration: 67.42 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1932, Val_Loss: 0.8184, Total Mean Loss: 1.5058, LR: 0.001316026814355829, Duration: 67.60 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1584, Val_Loss: 0.7591, Total Mean Loss: 1.4588, LR: 0.0012508555434978467, Duration: 67.54 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67: 100%|██████████| 27/27 [01:06<00:00,  2.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2821, Val_Loss: 0.8223, Total Mean Loss: 1.5522, LR: 0.0011865970506805537, Duration: 67.65 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2484, Val_Loss: 0.8220, Total Mean Loss: 1.5352, LR: 0.0011233296249791845, Duration: 67.50 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1439, Val_Loss: 0.7533, Total Mean Loss: 1.4486, LR: 0.0010611303480063583, Duration: 67.41 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2082, Val_Loss: 0.7790, Total Mean Loss: 1.4936, LR: 0.0010000750000000004, Duration: 67.47 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2692, Val_Loss: 0.7999, Total Mean Loss: 1.5346, LR: 0.000940237967496802, Duration: 67.54 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0755, Val_Loss: 0.7652, Total Mean Loss: 1.4203, LR: 0.0008816921527036801, Duration: 67.51 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 73: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1475, Val_Loss: 0.7764, Total Mean Loss: 1.4620, LR: 0.0008245088846776685, Duration: 67.45 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 74: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1947, Val_Loss: 0.7709, Total Mean Loss: 1.4828, LR: 0.0007687578324224496, Duration: 67.51 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1150, Val_Loss: 0.7763, Total Mean Loss: 1.4456, LR: 0.0007145069200074055, Duration: 67.46 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 76: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2568, Val_Loss: 0.7770, Total Mean Loss: 1.5169, LR: 0.000661822243812602, Duration: 67.61 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 77: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0956, Val_Loss: 0.8004, Total Mean Loss: 1.4480, LR: 0.0006107679920005282, Duration: 67.49 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 78: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2704, Val_Loss: 0.8184, Total Mean Loss: 1.5444, LR: 0.0005614063663127149, Duration: 67.54 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 79: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1208, Val_Loss: 0.7864, Total Mean Loss: 1.4536, LR: 0.000513797506286485, Duration: 67.39 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 80: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2827, Val_Loss: 0.7837, Total Mean Loss: 1.5332, LR: 0.00046799941598420013, Duration: 67.39 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 81: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.9240, Val_Loss: 0.7633, Total Mean Loss: 1.3437, LR: 0.0004240678933242365, Duration: 67.58 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 82: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1949, Val_Loss: 0.7407, Total Mean Loss: 1.4678, LR: 0.00038205646209982404, Duration: 67.43 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 83: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.9439, Val_Loss: 0.7818, Total Mean Loss: 1.3629, LR: 0.0003420163067685445, Duration: 67.42 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 84: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0535, Val_Loss: 0.7407, Total Mean Loss: 1.3971, LR: 0.0003039962100919559, Duration: 67.52 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 85: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0102, Val_Loss: 0.7809, Total Mean Loss: 1.3955, LR: 0.0002680424937013118, Duration: 67.37 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 86: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3759, Val_Loss: 0.7563, Total Mean Loss: 1.5661, LR: 0.00023419896166178896, Duration: 67.55 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 87: 100%|██████████| 27/27 [01:06<00:00,  2.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2072, Val_Loss: 0.7671, Total Mean Loss: 1.4871, LR: 0.0002025068471039813, Duration: 67.28 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 88: 100%|██████████| 27/27 [01:06<00:00,  2.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0811, Val_Loss: 0.8004, Total Mean Loss: 1.4408, LR: 0.00017300476198768016, Duration: 67.27 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 89: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0743, Val_Loss: 0.7554, Total Mean Loss: 1.4149, LR: 0.00014572865005915372, Duration: 67.54 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 90: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0216, Val_Loss: 0.7178, Total Mean Loss: 1.3697, LR: 0.00012071174305922266, Duration: 67.37 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 91: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.9036, Val_Loss: 0.7793, Total Mean Loss: 1.3415, LR: 9.79845202355077e-05, Duration: 67.45 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 92: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1194, Val_Loss: 0.7585, Total Mean Loss: 1.4390, LR: 7.757467120815912e-05, Duration: 67.51 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 93: 100%|██████████| 27/27 [01:06<00:00,  2.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1681, Val_Loss: 0.7490, Total Mean Loss: 1.4586, LR: 5.950706223432085e-05, Duration: 67.35 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 94: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0356, Val_Loss: 0.7410, Total Mean Loss: 1.3883, LR: 4.3803705912425316e-05, Duration: 67.52 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 95: 100%|██████████| 27/27 [01:06<00:00,  2.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.9751, Val_Loss: 0.7690, Total Mean Loss: 1.3720, LR: 3.0483734363234566e-05, Duration: 67.76 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 96: 100%|██████████| 27/27 [01:06<00:00,  2.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0435, Val_Loss: 0.7558, Total Mean Loss: 1.3996, LR: 1.9563375920296352e-05, Duration: 67.69 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 97: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0720, Val_Loss: 0.7562, Total Mean Loss: 1.4141, LR: 1.1055935358221834e-05, Duration: 67.62 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 98: 100%|██████████| 27/27 [01:06<00:00,  2.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1632, Val_Loss: 0.7543, Total Mean Loss: 1.4588, LR: 4.971777682864596e-06, Duration: 67.64 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1685, Val_Loss: 0.7549, Total Mean Loss: 1.4617, LR: 1.3183155031594304e-06, Duration: 67.57 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 100: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0983, Val_Loss: 0.7549, Total Mean Loss: 1.4266, LR: 1e-07, Duration: 67.38 sec\n",
      "\n",
      "[100 epoch result]\n",
      "       Metric     Value\n",
      "0   Accuracy  0.894000\n",
      "1  Precision  0.912238\n",
      "2     Recall  0.894000\n",
      "3   F1 Score  0.891306\n"
     ]
    }
   ],
   "source": [
    "training_time = 0\n",
    "losses = []\n",
    "val_losses = []\n",
    "lrs = []\n",
    "best_loss = float('inf')\n",
    "\n",
    "# GradScaler 초기화\n",
    "scaler = GradScaler()\n",
    "\n",
    "for i in range(epochs // 100):\n",
    "    for epoch in range(100):\n",
    "        model_v2.train()\n",
    "        start_time = time.time()\n",
    "        running_loss = 0.0\n",
    "        pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch + 1 + i*100}\")\n",
    "        \n",
    "        for _, data in pbar:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            inputs, labels = mixup_fn(inputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # AutoCast 적용\n",
    "            with autocast():\n",
    "                outputs = model_v2(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "            # 스케일링된 그라디언트 계산\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # 그라디언트 클리핑 전에 스케일링 제거\n",
    "            scaler.unscale_(optimizer)\n",
    "            clip_grad_norm_(model_v2.parameters(), max_norm=max_norm)\n",
    "\n",
    "            # 옵티마이저 스텝 및 스케일러 업데이트\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            # EMA 모델 업데이트\n",
    "            if model_ema is not None:\n",
    "                model_ema.update(model_v2)\n",
    "                \n",
    "            scheduler.step()\n",
    "                \n",
    "            lr = optimizer.param_groups[0][\"lr\"]\n",
    "            lrs.append(lr)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        losses.append(epoch_loss)        \n",
    "\n",
    "        model_v2.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data in valid_loader:\n",
    "                inputs, labels = data[0].to(device), data[1].to(device)\n",
    "                outputs = model_v2(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "        val_loss /= len(valid_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # 모델 저장\n",
    "        total_loss = val_loss + epoch_loss\n",
    "        if total_loss < best_loss:\n",
    "            best_loss = total_loss\n",
    "            model_save = False\n",
    "            if model_save:\n",
    "                torch.save(model_v2.state_dict(), model_path)\n",
    "\n",
    "        epoch_duration = time.time() - start_time\n",
    "        training_time += epoch_duration\n",
    "        \n",
    "        text = f'\\tLoss: {epoch_loss:.4f}, Val_Loss: {val_loss:.4f}, Total Mean Loss: {total_loss/2:.4f}, LR: {lr}, Duration: {epoch_duration:.2f} sec'\n",
    "        \n",
    "        if model_save:\n",
    "            text += f' - model saved!'\n",
    "            model_save = False\n",
    "\n",
    "        print(text)\n",
    "\n",
    "    # 예측 수행 및 레이블 저장\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model_v2(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # 혼동 행렬 생성\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    # 예측과 실제 레이블\n",
    "    y_true = all_labels  # 실제 레이블\n",
    "    y_pred = all_preds  # 모델에 의해 예측된 레이블\n",
    "\n",
    "    # 전체 데이터셋에 대한 정확도\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # 평균 정밀도, 리콜, F1-Score ('weighted')\n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "\n",
    "    # 판다스 데이터프레임으로 결과 정리\n",
    "    performance_metrics = pd.DataFrame({\n",
    "        'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "        'Value': [accuracy, precision, recall, f1_score]\n",
    "    })\n",
    "\n",
    "    # 데이터프레임 출력\n",
    "    print(f\"\\n[{i*100+100} epoch result]\\n\", performance_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 03 - Test ConvNeXt V2 with FCMAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 00.MASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "원본 데이터\n",
      "tensor([[[[ 1.,  2.,  3.,  4.],\n",
      "          [ 5.,  6.,  7.,  8.],\n",
      "          [ 9., 10., 11., 12.],\n",
      "          [13., 14., 15., 16.]]]])\n",
      "\n",
      "마스크\n",
      "tensor([[[[0, 0, 1, 1],\n",
      "          [0, 0, 1, 1],\n",
      "          [0, 0, 1, 1],\n",
      "          [0, 0, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 0, 0]]]])\n",
      "\n",
      "마스크 된 데이터\n",
      "tensor([[[[ 1.,  2.,  0.,  0.],\n",
      "          [ 5.,  6.,  0.,  0.],\n",
      "          [ 9., 10.,  0.,  0.],\n",
      "          [13., 14.,  0.,  0.]]],\n",
      "\n",
      "\n",
      "        [[[ 0.,  0.,  3.,  4.],\n",
      "          [ 0.,  0.,  7.,  8.],\n",
      "          [ 0.,  0., 11., 12.],\n",
      "          [ 0.,  0., 15., 16.]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define the upsample_mask and a simple example\n",
    "def upsample_mask(mask, scale):\n",
    "    assert len(mask.shape) == 2\n",
    "    p = int(mask.shape[1] ** .5)\n",
    "    return mask.reshape(-1, p, p).repeat_interleave(scale, axis=1).repeat_interleave(scale, axis=2)\n",
    "\n",
    "# 예시 마스크와 데이터\n",
    "mask = torch.tensor([[0, 1, 0, 1],\n",
    "                     [1, 0, 1, 0]])\n",
    "x = torch.arange(1., 17.).reshape(1, 1, 4, 4)  # 4x4 데이터, 채널과 배치 차원 포함\n",
    "\n",
    "# 마스크 업샘플링\n",
    "upsampled_mask = upsample_mask(mask, 2)  # 스케일을 2로 설정\n",
    "upsampled_mask = upsampled_mask.unsqueeze(1)  # 채널 차원 추가, 올바른 변수 이름 사용\n",
    "\n",
    "# 데이터에 마스크 적용\n",
    "x_orig = x.clone()  # 원본 데이터 복사\n",
    "x_masked = x * (1. - upsampled_mask.type_as(x))  # 마스크 적용, 타입 일치 시키기\n",
    "\n",
    "print(f\"\\n원본 데이터\\n{x_orig}\")\n",
    "print(f\"\\n마스크\\n{upsampled_mask}\")\n",
    "print(f\"\\n마스크 된 데이터\\n{x_masked}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01.TRAIN FCMAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/MinkowskiEngine-0.5.4-py3.10-linux-x86_64.egg/MinkowskiEngine/__init__.py:36: UserWarning: The environment variable `OMP_NUM_THREADS` not set. MinkowskiEngine will automatically set `OMP_NUM_THREADS=16`. If you want to set `OMP_NUM_THREADS` manually, please export it on the command line before running a python script. e.g. `export OMP_NUM_THREADS=12; python your_program.py`. It is recommended to set it below 24.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from model.fcmae import convnextv2_fcmae_tiny\n",
    "\n",
    "model = convnextv2_fcmae_tiny()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "# Transforms 정의하기\n",
    "train_transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224, scale=(0.2, 1.0), interpolation=3),  # 3 is bicubic\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "data_dir = '../../data/sports/'\n",
    "batch_size = 800\n",
    "train_path = data_dir\n",
    "\n",
    "# dataset load\n",
    "train_data = ImageFolder(train_path, transform=train_transform)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "len(train_loader)\n",
    "device = 'cuda:0'\n",
    "model.to(device)\n",
    "\n",
    "model_path = '../../model/convnext/fcmae.pt'\n",
    "\n",
    "epochs = 500\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1.5e-4, weight_decay=0.05, betas=(0.9, 0.95))\n",
    "\n",
    "warmup_steps = int(len(train_loader)*(epochs)*0.1)\n",
    "train_steps = len(train_loader)*(epochs)\n",
    "scheduler = CosineWarmupScheduler(optimizer, \n",
    "                                num_warmup_steps=50, \n",
    "                                num_training_steps=500,\n",
    "                                num_cycles=0.5,\n",
    "                                min_lr=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_time = 0\n",
    "losses = []\n",
    "val_losses = []\n",
    "lrs = []\n",
    "best_loss = float('inf')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch + 1}\")\n",
    "    \n",
    "    for _, data in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        samples= data[0].to(device)\n",
    "        loss, _, _ = model(samples, mask_ratio=0.6)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        lrs.append(lr)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)        \n",
    "    \n",
    "    # 모델 저장 로직 조정\n",
    "    if epoch_loss < best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        if epoch > (epochs // 2) :\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            model_saved_text = ' - model saved!'\n",
    "        else :\n",
    "            model_saved_text = ' - model save pass'\n",
    "    else:\n",
    "        model_saved_text = ''\n",
    "    \n",
    "    epoch_duration = time.time() - start_time\n",
    "    training_time += epoch_duration\n",
    "    \n",
    "    text = f'\\tLoss: {epoch_loss:,.4f}, LR: {lr}, Duration: {epoch_duration:.2f} sec{model_saved_text}'\n",
    "    print(text)\n",
    "\n",
    "    # 에폭마다 스케줄러 업데이트\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02.SUP after FCMAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model & pretrained Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.convnextv2 import load_convNext\n",
    "\n",
    "model = load_convNext(droppath=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['downsample_layers.0.stem_conv.weight', 'downsample_layers.0.stem_conv.bias', 'downsample_layers.0.stem_ln.weight', 'downsample_layers.0.stem_ln.bias', 'downsample_layers.1.ds_ln.weight', 'downsample_layers.1.ds_ln.bias', 'downsample_layers.1.ds_conv.weight', 'downsample_layers.1.ds_conv.bias', 'downsample_layers.2.ds_ln.weight', 'downsample_layers.2.ds_ln.bias', 'downsample_layers.2.ds_conv.weight', 'downsample_layers.2.ds_conv.bias', 'downsample_layers.3.ds_ln.weight', 'downsample_layers.3.ds_ln.bias', 'downsample_layers.3.ds_conv.weight', 'downsample_layers.3.ds_conv.bias', 'stages.0.0.dwconv.weight', 'stages.0.0.dwconv.bias', 'stages.0.0.layernorm.weight', 'stages.0.0.layernorm.bias', 'stages.0.0.pwconv1.weight', 'stages.0.0.pwconv1.bias', 'stages.0.0.grn.gamma', 'stages.0.0.grn.beta', 'stages.0.0.pwconv2.weight', 'stages.0.0.pwconv2.bias', 'stages.0.1.dwconv.weight', 'stages.0.1.dwconv.bias', 'stages.0.1.layernorm.weight', 'stages.0.1.layernorm.bias', 'stages.0.1.pwconv1.weight', 'stages.0.1.pwconv1.bias', 'stages.0.1.grn.gamma', 'stages.0.1.grn.beta', 'stages.0.1.pwconv2.weight', 'stages.0.1.pwconv2.bias', 'stages.0.2.dwconv.weight', 'stages.0.2.dwconv.bias', 'stages.0.2.layernorm.weight', 'stages.0.2.layernorm.bias', 'stages.0.2.pwconv1.weight', 'stages.0.2.pwconv1.bias', 'stages.0.2.grn.gamma', 'stages.0.2.grn.beta', 'stages.0.2.pwconv2.weight', 'stages.0.2.pwconv2.bias', 'stages.1.0.dwconv.weight', 'stages.1.0.dwconv.bias', 'stages.1.0.layernorm.weight', 'stages.1.0.layernorm.bias', 'stages.1.0.pwconv1.weight', 'stages.1.0.pwconv1.bias', 'stages.1.0.grn.gamma', 'stages.1.0.grn.beta', 'stages.1.0.pwconv2.weight', 'stages.1.0.pwconv2.bias', 'stages.1.1.dwconv.weight', 'stages.1.1.dwconv.bias', 'stages.1.1.layernorm.weight', 'stages.1.1.layernorm.bias', 'stages.1.1.pwconv1.weight', 'stages.1.1.pwconv1.bias', 'stages.1.1.grn.gamma', 'stages.1.1.grn.beta', 'stages.1.1.pwconv2.weight', 'stages.1.1.pwconv2.bias', 'stages.1.2.dwconv.weight', 'stages.1.2.dwconv.bias', 'stages.1.2.layernorm.weight', 'stages.1.2.layernorm.bias', 'stages.1.2.pwconv1.weight', 'stages.1.2.pwconv1.bias', 'stages.1.2.grn.gamma', 'stages.1.2.grn.beta', 'stages.1.2.pwconv2.weight', 'stages.1.2.pwconv2.bias', 'stages.2.0.dwconv.weight', 'stages.2.0.dwconv.bias', 'stages.2.0.layernorm.weight', 'stages.2.0.layernorm.bias', 'stages.2.0.pwconv1.weight', 'stages.2.0.pwconv1.bias', 'stages.2.0.grn.gamma', 'stages.2.0.grn.beta', 'stages.2.0.pwconv2.weight', 'stages.2.0.pwconv2.bias', 'stages.2.1.dwconv.weight', 'stages.2.1.dwconv.bias', 'stages.2.1.layernorm.weight', 'stages.2.1.layernorm.bias', 'stages.2.1.pwconv1.weight', 'stages.2.1.pwconv1.bias', 'stages.2.1.grn.gamma', 'stages.2.1.grn.beta', 'stages.2.1.pwconv2.weight', 'stages.2.1.pwconv2.bias', 'stages.2.2.dwconv.weight', 'stages.2.2.dwconv.bias', 'stages.2.2.layernorm.weight', 'stages.2.2.layernorm.bias', 'stages.2.2.pwconv1.weight', 'stages.2.2.pwconv1.bias', 'stages.2.2.grn.gamma', 'stages.2.2.grn.beta', 'stages.2.2.pwconv2.weight', 'stages.2.2.pwconv2.bias', 'stages.2.3.dwconv.weight', 'stages.2.3.dwconv.bias', 'stages.2.3.layernorm.weight', 'stages.2.3.layernorm.bias', 'stages.2.3.pwconv1.weight', 'stages.2.3.pwconv1.bias', 'stages.2.3.grn.gamma', 'stages.2.3.grn.beta', 'stages.2.3.pwconv2.weight', 'stages.2.3.pwconv2.bias', 'stages.2.4.dwconv.weight', 'stages.2.4.dwconv.bias', 'stages.2.4.layernorm.weight', 'stages.2.4.layernorm.bias', 'stages.2.4.pwconv1.weight', 'stages.2.4.pwconv1.bias', 'stages.2.4.grn.gamma', 'stages.2.4.grn.beta', 'stages.2.4.pwconv2.weight', 'stages.2.4.pwconv2.bias', 'stages.2.5.dwconv.weight', 'stages.2.5.dwconv.bias', 'stages.2.5.layernorm.weight', 'stages.2.5.layernorm.bias', 'stages.2.5.pwconv1.weight', 'stages.2.5.pwconv1.bias', 'stages.2.5.grn.gamma', 'stages.2.5.grn.beta', 'stages.2.5.pwconv2.weight', 'stages.2.5.pwconv2.bias', 'stages.2.6.dwconv.weight', 'stages.2.6.dwconv.bias', 'stages.2.6.layernorm.weight', 'stages.2.6.layernorm.bias', 'stages.2.6.pwconv1.weight', 'stages.2.6.pwconv1.bias', 'stages.2.6.grn.gamma', 'stages.2.6.grn.beta', 'stages.2.6.pwconv2.weight', 'stages.2.6.pwconv2.bias', 'stages.2.7.dwconv.weight', 'stages.2.7.dwconv.bias', 'stages.2.7.layernorm.weight', 'stages.2.7.layernorm.bias', 'stages.2.7.pwconv1.weight', 'stages.2.7.pwconv1.bias', 'stages.2.7.grn.gamma', 'stages.2.7.grn.beta', 'stages.2.7.pwconv2.weight', 'stages.2.7.pwconv2.bias', 'stages.2.8.dwconv.weight', 'stages.2.8.dwconv.bias', 'stages.2.8.layernorm.weight', 'stages.2.8.layernorm.bias', 'stages.2.8.pwconv1.weight', 'stages.2.8.pwconv1.bias', 'stages.2.8.grn.gamma', 'stages.2.8.grn.beta', 'stages.2.8.pwconv2.weight', 'stages.2.8.pwconv2.bias', 'stages.3.0.dwconv.weight', 'stages.3.0.dwconv.bias', 'stages.3.0.layernorm.weight', 'stages.3.0.layernorm.bias', 'stages.3.0.pwconv1.weight', 'stages.3.0.pwconv1.bias', 'stages.3.0.grn.gamma', 'stages.3.0.grn.beta', 'stages.3.0.pwconv2.weight', 'stages.3.0.pwconv2.bias', 'stages.3.1.dwconv.weight', 'stages.3.1.dwconv.bias', 'stages.3.1.layernorm.weight', 'stages.3.1.layernorm.bias', 'stages.3.1.pwconv1.weight', 'stages.3.1.pwconv1.bias', 'stages.3.1.grn.gamma', 'stages.3.1.grn.beta', 'stages.3.1.pwconv2.weight', 'stages.3.1.pwconv2.bias', 'stages.3.2.dwconv.weight', 'stages.3.2.dwconv.bias', 'stages.3.2.layernorm.weight', 'stages.3.2.layernorm.bias', 'stages.3.2.pwconv1.weight', 'stages.3.2.pwconv1.bias', 'stages.3.2.grn.gamma', 'stages.3.2.grn.beta', 'stages.3.2.pwconv2.weight', 'stages.3.2.pwconv2.bias', 'layernorm.weight', 'layernorm.bias', 'fc.weight', 'fc.bias'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0048,  0.1373, -0.1178, -0.0148],\n",
       "         [-0.1109,  0.0877, -0.1310,  0.1206],\n",
       "         [ 0.0211,  0.0512,  0.0611, -0.1399],\n",
       "         [-0.0541, -0.0021, -0.0820,  0.1090]],\n",
       "\n",
       "        [[-0.1274,  0.1100, -0.0220,  0.0409],\n",
       "         [-0.0799,  0.0074, -0.1124, -0.1037],\n",
       "         [-0.0679, -0.0906, -0.0561, -0.1157],\n",
       "         [-0.0516, -0.1274, -0.0867,  0.0821]],\n",
       "\n",
       "        [[-0.0379,  0.0448, -0.0285,  0.0171],\n",
       "         [-0.0741, -0.1290,  0.1428, -0.1275],\n",
       "         [-0.0575,  0.0482,  0.0613, -0.0229],\n",
       "         [ 0.1366,  0.0564,  0.0791,  0.0003]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['downsample_layers.0.stem_conv.weight'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1364, -0.0338,  0.0651,  0.0243,  0.0405,  0.1147,  0.1086],\n",
       "         [-0.0112,  0.0057, -0.1273,  0.0319,  0.0956,  0.0145,  0.0055],\n",
       "         [-0.0675, -0.0048,  0.1312, -0.0257,  0.1073, -0.0845,  0.0248],\n",
       "         [ 0.1088,  0.0761, -0.0161, -0.0377,  0.0640, -0.0932,  0.0649],\n",
       "         [ 0.0766,  0.0990,  0.0484, -0.0273, -0.0088,  0.0343,  0.0113],\n",
       "         [-0.0036, -0.0316, -0.0293, -0.0090,  0.0823, -0.1076,  0.0858],\n",
       "         [ 0.1420,  0.1262, -0.1276, -0.0563,  0.0826, -0.0250,  0.0812]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['stages.0.0.dwconv.weight'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remap Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def remap_checkpoint_keys(ckpt):\n",
    "    new_ckpt = OrderedDict()\n",
    "    for k, v in ckpt.items():\n",
    "        if k.startswith('encoder'):\n",
    "            k = '.'.join(k.split('.')[1:]) # remove encoder in the name\n",
    "        if k.endswith('kernel'):\n",
    "            k = '.'.join(k.split('.')[:-1]) # remove kernel in the name\n",
    "            new_k = k + '.weight'\n",
    "            if len(v.shape) == 3: # resahpe standard convolution\n",
    "                kv, in_dim, out_dim = v.shape\n",
    "                ks = int(math.sqrt(kv))\n",
    "                new_ckpt[new_k] = v.permute(2, 1, 0).\\\n",
    "                    reshape(out_dim, in_dim, ks, ks).transpose(3, 2)\n",
    "            elif len(v.shape) == 2: # reshape depthwise convolution\n",
    "                kv, dim = v.shape\n",
    "                ks = int(math.sqrt(kv))\n",
    "                new_ckpt[new_k] = v.permute(1, 0).\\\n",
    "                    reshape(dim, 1, ks, ks).transpose(3, 2)\n",
    "            continue\n",
    "        elif 'ln' in k or 'linear' in k:\n",
    "            k = k.split('.')\n",
    "            k.pop(-2) # remove ln and linear in the name\n",
    "            new_k = '.'.join(k)\n",
    "        else:\n",
    "            new_k = k\n",
    "        new_ckpt[new_k] = v\n",
    "\n",
    "    # reshape grn affine parameters and biases\n",
    "    for k, v in new_ckpt.items():\n",
    "        if k.endswith('bias') and len(v.shape) != 1:\n",
    "            new_ckpt[k] = v.reshape(-1)\n",
    "        elif 'grn' in k:\n",
    "            new_ckpt[k] = v.unsqueeze(0).unsqueeze(1)\n",
    "    return new_ckpt\n",
    "\n",
    "def load_state_dict(model, state_dict, prefix='', ignore_missing=\"relative_position_index\"):\n",
    "    missing_keys = []\n",
    "    unexpected_keys = []\n",
    "    error_msgs = []\n",
    "    # copy state_dict so _load_from_state_dict can modify it\n",
    "    metadata = getattr(state_dict, '_metadata', None)\n",
    "    state_dict = state_dict.copy()\n",
    "    if metadata is not None:\n",
    "        state_dict._metadata = metadata\n",
    "\n",
    "    def load(module, prefix=''):\n",
    "        local_metadata = {} if metadata is None else metadata.get(\n",
    "            prefix[:-1], {})\n",
    "        module._load_from_state_dict(\n",
    "            state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs)\n",
    "        for name, child in module._modules.items():\n",
    "            if child is not None:\n",
    "                load(child, prefix + name + '.')\n",
    "\n",
    "    load(model, prefix=prefix)\n",
    "\n",
    "    warn_missing_keys = []\n",
    "    ignore_missing_keys = []\n",
    "    for key in missing_keys:\n",
    "        keep_flag = True\n",
    "        for ignore_key in ignore_missing.split('|'):\n",
    "            if ignore_key in key:\n",
    "                keep_flag = False\n",
    "                break\n",
    "        if keep_flag:\n",
    "            warn_missing_keys.append(key)\n",
    "        else:\n",
    "            ignore_missing_keys.append(key)\n",
    "\n",
    "    missing_keys = warn_missing_keys\n",
    "\n",
    "    if len(missing_keys) > 0:\n",
    "        print(\"Weights of {} not initialized from pretrained model: {}\".format(\n",
    "            model.__class__.__name__, missing_keys))\n",
    "    if len(unexpected_keys) > 0:\n",
    "        print(\"Weights from pretrained model not used in {}: {}\".format(\n",
    "            model.__class__.__name__, unexpected_keys))\n",
    "    if len(ignore_missing_keys) > 0:\n",
    "        print(\"Ignored weights of {} not initialized from pretrained model: {}\".format(\n",
    "            model.__class__.__name__, ignore_missing_keys))\n",
    "    if len(error_msgs) > 0:\n",
    "        print('\\n'.join(error_msgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing key mask_token from decoder of pretrained checkpoint\n",
      "Removing key proj.weight from decoder of pretrained checkpoint\n",
      "Removing key proj.bias from decoder of pretrained checkpoint\n",
      "Removing key decoder.0.dwconv.weight from decoder of pretrained checkpoint\n",
      "Removing key decoder.0.dwconv.bias from decoder of pretrained checkpoint\n",
      "Removing key decoder.0.layernorm.weight from decoder of pretrained checkpoint\n",
      "Removing key decoder.0.layernorm.bias from decoder of pretrained checkpoint\n",
      "Removing key decoder.0.pwconv1.weight from decoder of pretrained checkpoint\n",
      "Removing key decoder.0.pwconv1.bias from decoder of pretrained checkpoint\n",
      "Removing key decoder.0.grn.gamma from decoder of pretrained checkpoint\n",
      "Removing key decoder.0.grn.beta from decoder of pretrained checkpoint\n",
      "Removing key decoder.0.pwconv2.weight from decoder of pretrained checkpoint\n",
      "Removing key decoder.0.pwconv2.bias from decoder of pretrained checkpoint\n",
      "Removing key pred.weight from decoder of pretrained checkpoint\n",
      "Removing key pred.bias from decoder of pretrained checkpoint\n",
      "Weights of ConvNeXtV2 not initialized from pretrained model: ['downsample_layers.0.stem_ln.weight', 'downsample_layers.0.stem_ln.bias', 'layernorm.weight', 'layernorm.bias', 'fc.weight', 'fc.bias']\n",
      "Weights from pretrained model not used in ConvNeXtV2: ['downsample_layers.0.weight', 'downsample_layers.0.bias']\n"
     ]
    }
   ],
   "source": [
    "pretrain_path = '../../model/convnext/fcmae.pt'\n",
    "checkpoint_model = torch.load(pretrain_path, map_location='cpu')\n",
    "\n",
    "state_dict = model.state_dict()\n",
    "for k in ['head.weight', 'head.bias']:\n",
    "    if k in checkpoint_model and checkpoint_model[k].shape != state_dict[k].shape:\n",
    "        print(f\"Removing key {k} from head of pretrained checkpoint\")\n",
    "        del checkpoint_model[k]\n",
    "\n",
    "# remove decoder weights\n",
    "checkpoint_model_keys = list(checkpoint_model.keys())\n",
    "for k in checkpoint_model_keys:\n",
    "    if 'decoder' in k or 'mask_token'in k or \\\n",
    "        'proj' in k or 'pred' in k:\n",
    "        print(f\"Removing key {k} from decoder of pretrained checkpoint\")\n",
    "        del checkpoint_model[k]\n",
    "\n",
    "checkpoint_model = remap_checkpoint_keys(checkpoint_model)\n",
    "load_state_dict(model, checkpoint_model, prefix='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# manually initialize fc layer\n",
    "nn.init.trunc_normal_(model.fc.weight, std=2e-5)\n",
    "torch.nn.init.constant_(model.fc.bias, 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.2056, -0.8509,  1.1679,  0.6501],\n",
       "         [ 0.1916, -0.6376,  0.5137, -0.1978],\n",
       "         [-0.7029,  0.1940,  0.3492,  0.6778],\n",
       "         [-0.8013,  0.2786,  1.0694, -0.1054]],\n",
       "\n",
       "        [[ 0.4654,  0.1810,  0.9873, -1.6086],\n",
       "         [ 0.7260,  0.8883,  0.5517,  0.1936],\n",
       "         [ 0.6057, -1.0542, -0.8394, -0.0230],\n",
       "         [-0.9260,  0.6637, -1.4884,  0.6571]],\n",
       "\n",
       "        [[ 0.3846,  0.3557,  0.1090,  1.3101],\n",
       "         [-0.3910,  0.5647, -0.3082, -1.5620],\n",
       "         [ 0.5292,  1.0900, -0.2673, -0.2546],\n",
       "         [-0.2218, -0.6494, -0.2747, -0.3344]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['downsample_layers.0.stem_conv.weight'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['downsample_layers.0.stem_ln.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1999,  0.9103, -0.8238, -0.0341,  0.8881,  0.3552, -0.2845],\n",
       "         [-0.0604,  1.1877, -0.9397,  1.6832,  0.0330,  0.5312, -0.1214],\n",
       "         [-0.2699,  0.5054,  0.3215,  0.5711,  0.3631, -1.0114,  0.2572],\n",
       "         [-0.2513, -0.3276, -0.1782,  2.2991,  0.4163,  0.7285, -0.0070],\n",
       "         [-0.0218, -0.6208,  0.7876,  0.3589,  0.3943,  0.6794,  0.7090],\n",
       "         [-0.8632, -0.1470,  0.8258,  0.6889, -0.7541, -0.6836, -0.1466],\n",
       "         [ 0.9184, -0.0388, -0.0988,  0.1967, -0.6746,  0.0904,  0.1652]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['stages.0.0.dwconv.weight'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "Total Parameters: 27,943,396\n",
      "Trainable Parameters: 27,943,396\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 총 파라미터 수 계산\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "# 학습 가능한 파라미터 수 계산\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print('='*80)\n",
    "print(f\"\\nTotal Parameters: {total_params:,}\")\n",
    "print(f\"Trainable Parameters: {trainable_params:,}\\n\")\n",
    "print('='*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms 정의하기\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.TrivialAugmentWide(interpolation=F.InterpolationMode.BICUBIC),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.6,1), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "data_dir = '../../data/sports'\n",
    "batch_size = 320\n",
    "\n",
    "train_path = data_dir+'/train'\n",
    "valid_path = data_dir+'/valid'\n",
    "test_path = data_dir+'/test'\n",
    "\n",
    "# dataset load\n",
    "train_data = ImageFolder(train_path, transform=train_transform)\n",
    "valid_data = ImageFolder(valid_path, transform=test_transform)\n",
    "test_data = ImageFolder(test_path, transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using EMA with decay = 0.998\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:4'\n",
    "max_norm = 3.0 \n",
    "\n",
    "model.to(device)\n",
    "\n",
    "model_ema = None\n",
    "ema_active = True\n",
    "if ema_active:\n",
    "    ema_decay = 0.9999\n",
    "    model_ema = ModelEmaV3(\n",
    "        model,\n",
    "        decay=ema_decay,\n",
    "    )\n",
    "    print(f\"Using EMA with decay = {ema_decay}\")\n",
    "\n",
    "model_path = ''\n",
    "\n",
    "mixup = True\n",
    "if mixup :\n",
    "    mixup_fn = Mixup(mixup_alpha=.8, \n",
    "                    cutmix_alpha=1., \n",
    "                    prob=1., \n",
    "                    switch_prob=0.5, \n",
    "                    mode='batch',\n",
    "                    label_smoothing=.1,\n",
    "                    num_classes=100)\n",
    "    \n",
    "    criterion = SoftTargetCrossEntropy()\n",
    "else :\n",
    "    criterion = LabelSmoothingCrossEntropy(.1)\n",
    "    \n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CASE 01: Without LayerWise Learning Rate Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=8e-3, weight_decay=0.05)\n",
    "warmup_steps = int(len(train_loader)*(epochs)*0.1)\n",
    "train_steps = len(train_loader)*(epochs)\n",
    "scheduler = CosineWarmupScheduler(optimizer, \n",
    "                                num_warmup_steps=warmup_steps, \n",
    "                                num_training_steps=train_steps,\n",
    "                                num_cycles=0.5,\n",
    "                                min_lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 43/43 [01:41<00:00,  2.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.5174, Val_Loss: 3.9555, Total Mean Loss: 4.2364, LR: 0.0008000899999999999, Duration: 103.26 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.3576, Val_Loss: 3.7653, Total Mean Loss: 4.0615, LR: 0.00160008, Duration: 100.78 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.2708, Val_Loss: 3.6170, Total Mean Loss: 3.9439, LR: 0.00240007, Duration: 101.10 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.2401, Val_Loss: 3.4767, Total Mean Loss: 3.8584, LR: 0.00320006, Duration: 100.80 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 43/43 [01:41<00:00,  2.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.1595, Val_Loss: 3.4136, Total Mean Loss: 3.7866, LR: 0.004000050000000001, Duration: 102.41 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 43/43 [01:40<00:00,  2.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.1002, Val_Loss: 3.1582, Total Mean Loss: 3.6292, LR: 0.00480004, Duration: 102.22 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 43/43 [01:41<00:00,  2.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.0583, Val_Loss: 3.2720, Total Mean Loss: 3.6651, LR: 0.005600030000000001, Duration: 102.64 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 43/43 [01:41<00:00,  2.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.0330, Val_Loss: 2.9468, Total Mean Loss: 3.4899, LR: 0.00640002, Duration: 103.30 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 43/43 [01:40<00:00,  2.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.0363, Val_Loss: 3.0413, Total Mean Loss: 3.5388, LR: 0.0072000100000000015, Duration: 102.32 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 43/43 [01:41<00:00,  2.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.9551, Val_Loss: 2.8868, Total Mean Loss: 3.4209, LR: 0.008, Duration: 103.23 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 43/43 [01:42<00:00,  2.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.9574, Val_Loss: 2.7514, Total Mean Loss: 3.3544, LR: 0.007997563338535033, Duration: 104.34 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 43/43 [01:41<00:00,  2.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.8570, Val_Loss: 2.7344, Total Mean Loss: 3.2957, LR: 0.007990256322836784, Duration: 102.60 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 43/43 [01:41<00:00,  2.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.8681, Val_Loss: 2.5248, Total Mean Loss: 3.1965, LR: 0.007978087855378325, Duration: 102.72 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 43/43 [01:42<00:00,  2.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.7876, Val_Loss: 2.5320, Total Mean Loss: 3.1598, LR: 0.007961072761562845, Duration: 103.62 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 43/43 [01:41<00:00,  2.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.7998, Val_Loss: 2.4282, Total Mean Loss: 3.1140, LR: 0.007939231771661183, Duration: 102.80 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 43/43 [01:40<00:00,  2.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.8225, Val_Loss: 2.4126, Total Mean Loss: 3.1176, LR: 0.007912591495555185, Duration: 101.95 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.6847, Val_Loss: 2.4743, Total Mean Loss: 3.0795, LR: 0.007881184390317672, Duration: 101.29 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 43/43 [01:40<00:00,  2.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.7576, Val_Loss: 2.1219, Total Mean Loss: 2.9398, LR: 0.007845048720668478, Duration: 101.73 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 43/43 [01:41<00:00,  2.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.6963, Val_Loss: 2.1244, Total Mean Loss: 2.9104, LR: 0.007804228512354801, Duration: 102.30 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 43/43 [01:42<00:00,  2.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.6600, Val_Loss: 1.9895, Total Mean Loss: 2.8247, LR: 0.007758773498512596, Duration: 103.69 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.7373, Val_Loss: 2.2895, Total Mean Loss: 3.0134, LR: 0.0077087390590744225, Duration: 101.65 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.6895, Val_Loss: 1.9331, Total Mean Loss: 2.8113, LR: 0.007654186153297522, Duration: 100.99 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.5832, Val_Loss: 2.0351, Total Mean Loss: 2.8091, LR: 0.007595181245494354, Duration: 101.03 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.6645, Val_Loss: 1.9636, Total Mean Loss: 2.8141, LR: 0.007531796224056066, Duration: 100.65 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.6152, Val_Loss: 1.8906, Total Mean Loss: 2.7529, LR: 0.007464108313867567, Duration: 100.54 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.5144, Val_Loss: 1.8880, Total Mean Loss: 2.7012, LR: 0.007392199982220897, Duration: 100.78 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.4348, Val_Loss: 1.7945, Total Mean Loss: 2.6147, LR: 0.00731615883834154, Duration: 100.65 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.4844, Val_Loss: 1.7314, Total Mean Loss: 2.6079, LR: 0.007236077526650072, Duration: 100.41 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.4791, Val_Loss: 1.6426, Total Mean Loss: 2.5609, LR: 0.007152053613889208, Duration: 101.06 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.4843, Val_Loss: 1.7159, Total Mean Loss: 2.6001, LR: 0.007064189470253756, Duration: 101.05 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.3647, Val_Loss: 1.6714, Total Mean Loss: 2.5181, LR: 0.006972592144668304, Duration: 100.75 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.4009, Val_Loss: 1.8616, Total Mean Loss: 2.6312, LR: 0.0068773732343645885, Duration: 101.09 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.3194, Val_Loss: 1.4487, Total Mean Loss: 2.3840, LR: 0.006778648748917467, Duration: 101.45 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.3243, Val_Loss: 1.6330, Total Mean Loss: 2.4787, LR: 0.006676538968905116, Duration: 100.81 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.3318, Val_Loss: 1.5348, Total Mean Loss: 2.4333, LR: 0.006571168299365673, Duration: 100.98 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.3523, Val_Loss: 1.5313, Total Mean Loss: 2.4418, LR: 0.006462665118228867, Duration: 100.55 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.2279, Val_Loss: 1.4897, Total Mean Loss: 2.3588, LR: 0.006351161619907278, Duration: 100.72 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.1622, Val_Loss: 1.4437, Total Mean Loss: 2.3030, LR: 0.006236793654237814, Duration: 100.55 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.1753, Val_Loss: 1.4818, Total Mean Loss: 2.3285, LR: 0.006119700560969609, Duration: 100.68 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.3085, Val_Loss: 1.5160, Total Mean Loss: 2.4123, LR: 0.006000025000000001, Duration: 100.76 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.0782, Val_Loss: 1.3151, Total Mean Loss: 2.1966, LR: 0.005877912777565424, Duration: 100.49 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.2108, Val_Loss: 1.2391, Total Mean Loss: 2.2250, LR: 0.005753512668598971, Duration: 100.61 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.2387, Val_Loss: 1.2823, Total Mean Loss: 2.2605, LR: 0.005626976235471049, Duration: 100.50 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.0919, Val_Loss: 1.1246, Total Mean Loss: 2.1083, LR: 0.005498457643333979, Duration: 100.60 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.0948, Val_Loss: 1.2930, Total Mean Loss: 2.1939, LR: 0.00536811347229551, Duration: 100.50 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.8692, Val_Loss: 1.0588, Total Mean Loss: 1.9640, LR: 0.005236102526650072, Duration: 100.50 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.0887, Val_Loss: 1.1858, Total Mean Loss: 2.1373, LR: 0.005102585641400206, Duration: 100.76 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.0582, Val_Loss: 1.2281, Total Mean Loss: 2.1432, LR: 0.004967725486303891, Duration: 100.58 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.9549, Val_Loss: 1.0682, Total Mean Loss: 2.0116, LR: 0.004831686367686497, Duration: 100.42 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.0342, Val_Loss: 1.0445, Total Mean Loss: 2.0393, LR: 0.004694634028258839, Duration: 100.55 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.0304, Val_Loss: 1.0406, Total Mean Loss: 2.0355, LR: 0.004556735445185214, Duration: 100.52 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.9169, Val_Loss: 1.0159, Total Mean Loss: 1.9664, LR: 0.004418158626647451, Duration: 100.40 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.9099, Val_Loss: 1.0923, Total Mean Loss: 2.0011, LR: 0.004279072407152814, Duration: 100.70 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.0031, Val_Loss: 1.1015, Total Mean Loss: 2.0523, LR: 0.00413964624183517, Duration: 100.65 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.8709, Val_Loss: 0.9729, Total Mean Loss: 1.9219, LR: 0.004000050000000001, Duration: 100.43 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.9064, Val_Loss: 1.0415, Total Mean Loss: 1.9739, LR: 0.0038604537581648324, Duration: 100.46 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.7644, Val_Loss: 0.8916, Total Mean Loss: 1.8280, LR: 0.0037210275928471863, Duration: 100.76 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.8485, Val_Loss: 0.9641, Total Mean Loss: 1.9063, LR: 0.00358194137335255, Duration: 101.19 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.7439, Val_Loss: 0.9852, Total Mean Loss: 1.8645, LR: 0.0034433645548147874, Duration: 100.61 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.7590, Val_Loss: 0.9629, Total Mean Loss: 1.8609, LR: 0.0033054659717411624, Duration: 101.03 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.8289, Val_Loss: 0.9579, Total Mean Loss: 1.8934, LR: 0.003168413632313504, Duration: 100.51 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.7437, Val_Loss: 0.9465, Total Mean Loss: 1.8451, LR: 0.00303237451369611, Duration: 100.54 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.7463, Val_Loss: 0.9544, Total Mean Loss: 1.8504, LR: 0.0028975143585997947, Duration: 100.98 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6703, Val_Loss: 0.8787, Total Mean Loss: 1.7745, LR: 0.0027639974733499294, Duration: 100.84 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.7541, Val_Loss: 0.9497, Total Mean Loss: 1.8519, LR: 0.002631986527704492, Duration: 100.80 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5575, Val_Loss: 0.9062, Total Mean Loss: 1.7318, LR: 0.0025016423566660228, Duration: 100.77 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.7459, Val_Loss: 0.8786, Total Mean Loss: 1.8123, LR: 0.0023731237645289536, Duration: 101.30 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5037, Val_Loss: 0.8012, Total Mean Loss: 1.6524, LR: 0.0022465873314010294, Duration: 100.65 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69: 100%|██████████| 43/43 [01:39<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6009, Val_Loss: 0.8504, Total Mean Loss: 1.7257, LR: 0.002122187222434577, Duration: 100.35 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4472, Val_Loss: 0.8363, Total Mean Loss: 1.6417, LR: 0.002000075000000001, Duration: 100.80 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6066, Val_Loss: 0.8907, Total Mean Loss: 1.7486, LR: 0.0018803994390303928, Duration: 100.91 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4942, Val_Loss: 0.8203, Total Mean Loss: 1.6573, LR: 0.001763306345762187, Duration: 100.64 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 73: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2896, Val_Loss: 0.7736, Total Mean Loss: 1.5316, LR: 0.0016489383800927227, Duration: 100.80 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 74: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4167, Val_Loss: 0.8223, Total Mean Loss: 1.6195, LR: 0.0015374348817711334, Duration: 101.16 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5211, Val_Loss: 0.8380, Total Mean Loss: 1.6795, LR: 0.001428931700634327, Duration: 100.65 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 76: 100%|██████████| 43/43 [01:39<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3990, Val_Loss: 0.8152, Total Mean Loss: 1.6071, LR: 0.0013235610310948864, Duration: 100.41 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 77: 100%|██████████| 43/43 [01:38<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3730, Val_Loss: 0.7899, Total Mean Loss: 1.5815, LR: 0.0012214512510825336, Duration: 100.04 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 78: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2933, Val_Loss: 0.7832, Total Mean Loss: 1.5383, LR: 0.0011227267656354132, Duration: 101.12 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 79: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5673, Val_Loss: 0.8515, Total Mean Loss: 1.7094, LR: 0.0010275078553316965, Duration: 100.49 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 80: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4341, Val_Loss: 0.8315, Total Mean Loss: 1.6328, LR: 0.0009359105297462444, Duration: 100.54 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 81: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5285, Val_Loss: 0.7779, Total Mean Loss: 1.6532, LR: 0.0008480463861107927, Duration: 100.93 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 82: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4238, Val_Loss: 0.7848, Total Mean Loss: 1.6043, LR: 0.0007640224733499294, Duration: 100.46 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 83: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1293, Val_Loss: 0.7640, Total Mean Loss: 1.4467, LR: 0.0006839411616584612, Duration: 100.81 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 84: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3635, Val_Loss: 0.7825, Total Mean Loss: 1.5730, LR: 0.000607900017779104, Duration: 100.58 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 85: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5074, Val_Loss: 0.7821, Total Mean Loss: 1.6447, LR: 0.0005359916861324344, Duration: 100.76 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 86: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4612, Val_Loss: 0.7745, Total Mean Loss: 1.6179, LR: 0.000468303775943935, Duration: 100.97 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 87: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3710, Val_Loss: 0.7603, Total Mean Loss: 1.5657, LR: 0.0004049187545056477, Duration: 100.60 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 88: 100%|██████████| 43/43 [01:39<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3455, Val_Loss: 0.7427, Total Mean Loss: 1.5441, LR: 0.00034591384670247825, Duration: 100.33 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 89: 100%|██████████| 43/43 [01:38<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4511, Val_Loss: 0.7847, Total Mean Loss: 1.6179, LR: 0.0002913609409255791, Duration: 99.96 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 90: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3802, Val_Loss: 0.7559, Total Mean Loss: 1.5681, LR: 0.00024132650148740606, Duration: 100.47 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 91: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3893, Val_Loss: 0.7604, Total Mean Loss: 1.5748, LR: 0.00019587148764520066, Duration: 100.73 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 92: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3822, Val_Loss: 0.7569, Total Mean Loss: 1.5695, LR: 0.00015505127933152136, Duration: 101.02 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 93: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4014, Val_Loss: 0.7536, Total Mean Loss: 1.5775, LR: 0.00011891560968232792, Duration: 100.62 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 94: 100%|██████████| 43/43 [01:39<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3272, Val_Loss: 0.7621, Total Mean Loss: 1.5446, LR: 8.750850444481394e-05, Duration: 100.33 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 95: 100%|██████████| 43/43 [01:39<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3300, Val_Loss: 0.7494, Total Mean Loss: 1.5397, LR: 6.0868228338818537e-05, Duration: 100.37 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 96: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3357, Val_Loss: 0.7471, Total Mean Loss: 1.5414, LR: 3.902723843715564e-05, Duration: 100.57 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 97: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4103, Val_Loss: 0.7500, Total Mean Loss: 1.5802, LR: 2.2012144621675257e-05, Duration: 100.54 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 98: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3573, Val_Loss: 0.7515, Total Mean Loss: 1.5544, LR: 9.843677163216202e-06, Duration: 100.81 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4389, Val_Loss: 0.7519, Total Mean Loss: 1.5954, LR: 2.5366614649679064e-06, Duration: 100.53 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 100: 100%|██████████| 43/43 [01:38<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4062, Val_Loss: 0.7520, Total Mean Loss: 1.5791, LR: 1e-07, Duration: 100.26 sec\n",
      "\n",
      "[100 epoch result]\n",
      "       Metric     Value\n",
      "0   Accuracy  0.898000\n",
      "1  Precision  0.913381\n",
      "2     Recall  0.898000\n",
      "3   F1 Score  0.893048\n"
     ]
    }
   ],
   "source": [
    "training_time = 0\n",
    "losses = []\n",
    "val_losses = []\n",
    "lrs = []\n",
    "best_loss = float('inf')\n",
    "model_save = False\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "for i in range(epochs // 100):\n",
    "    for epoch in range(100):\n",
    "        model.train()\n",
    "        start_time = time.time()\n",
    "        running_loss = 0.0\n",
    "        pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch + 1 + i*100}\")\n",
    "        \n",
    "        for _, data in pbar:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            inputs, labels = mixup_fn(inputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "                \n",
    "            loss.backward()\n",
    "            # 그라디언트 클리핑 적용\n",
    "            clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "            optimizer.step()\n",
    "            \n",
    "            # EMA 모델 업데이트, 필요한 경우\n",
    "            if model_ema is not None:\n",
    "                model_ema.update(model)\n",
    "                \n",
    "            scheduler.step()\n",
    "                \n",
    "            lr = optimizer.param_groups[0][\"lr\"]\n",
    "            lrs.append(lr)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        losses.append(epoch_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data in valid_loader:\n",
    "                inputs, labels = data[0].to(device), data[1].to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "        val_loss /= len(valid_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # 모델 저장 조건 수정\n",
    "        total_loss = val_loss + epoch_loss\n",
    "        if total_loss < best_loss:\n",
    "            best_loss = total_loss\n",
    "            # torch.save(model.state_dict(), model_path)\n",
    "            model_save = True\n",
    "            save_text = ' - model saved!'\n",
    "        else:\n",
    "            save_text = ''\n",
    "\n",
    "        epoch_duration = time.time() - start_time\n",
    "        training_time += epoch_duration\n",
    "        \n",
    "        text = f'\\tLoss: {epoch_loss:.4f}, Val_Loss: {val_loss:.4f}, Total Mean Loss: {total_loss/2:.4f}, LR: {lr}, Duration: {epoch_duration:.2f} sec{save_text}'\n",
    "        print(text)\n",
    "\n",
    "    # 예측 수행 및 레이블 저장\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # 혼동 행렬 생성\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    # 예측과 실제 레이블\n",
    "    y_true = all_labels  # 실제 레이블\n",
    "    y_pred = all_preds  # 모델에 의해 예측된 레이블\n",
    "\n",
    "    # 전체 데이터셋에 대한 정확도\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # 평균 정밀도, 리콜, F1-Score ('weighted')\n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "\n",
    "    # 판다스 데이터프레임으로 결과 정리\n",
    "    performance_metrics = pd.DataFrame({\n",
    "        'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "        'Value': [accuracy, precision, recall, f1_score]\n",
    "    })\n",
    "\n",
    "    # 데이터프레임 출력\n",
    "    print(f\"\\n[{i*100+100} epoch result]\\n\", performance_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CASE 02: LLRD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LLRD_ConvNeXt(model, depths=[3,3,9,3], weight_decay=0.05, lr=8e-3, scale=0.9):\n",
    "    \n",
    "    stage = 0\n",
    "    layer_names = []\n",
    "    param_groups = {}\n",
    "    for depth in depths:\n",
    "        if stage == 0:\n",
    "            layer_names.append(f'downsample_layers.{stage}.stem_conv.weight')\n",
    "            layer_names.append(f'downsample_layers.{stage}.stem_conv.bias')\n",
    "            layer_names.append(f'downsample_layers.{stage}.stem_ln.weight')\n",
    "            layer_names.append(f'downsample_layers.{stage}.stem_ln.bias')\n",
    "        else :\n",
    "            layer_names.append(f'downsample_layers.{stage}.ds_ln.weight')\n",
    "            layer_names.append(f'downsample_layers.{stage}.ds_ln.bias')\n",
    "            layer_names.append(f'downsample_layers.{stage}.ds_conv.weight')\n",
    "            layer_names.append(f'downsample_layers.{stage}.ds_conv.bias')        \n",
    "        for i in range(depth):\n",
    "            layer_names.append(f'stages.{stage}.{i}.dwconv.weight')\n",
    "            layer_names.append(f'stages.{stage}.{i}.dwconv.bias')\n",
    "            layer_names.append(f'stages.{stage}.{i}.layernorm.weight')\n",
    "            layer_names.append(f'stages.{stage}.{i}.layernorm.bias')\n",
    "            layer_names.append(f'stages.{stage}.{i}.pwconv1.weight')\n",
    "            layer_names.append(f'stages.{stage}.{i}.pwconv1.bias')\n",
    "            layer_names.append(f'stages.{stage}.{i}.grn.gamma')\n",
    "            layer_names.append(f'stages.{stage}.{i}.grn.beta')            \n",
    "            layer_names.append(f'stages.{stage}.{i}.pwconv2.weight')\n",
    "            layer_names.append(f'stages.{stage}.{i}.pwconv2.bias')\n",
    "        stage += 1\n",
    "    \n",
    "    layer_names.append('layernorm.weight')\n",
    "    layer_names.append('layernorm.bias')\n",
    "    layer_names.append('fc.weight')\n",
    "    layer_names.append('fc.bias')\n",
    "    \n",
    "    # Layer Learning Rate Decay\n",
    "    for name, param in model.named_parameters():\n",
    "        total_depths = sum(depths)\n",
    "        if name.startswith(\"downsample_layers\"):\n",
    "            stage_id = int(name.split('.')[1])\n",
    "            layer_id = sum(depths[:stage_id])\n",
    "            param_groups[name] = {'lr':lr*(scale**(total_depths-layer_id)),\n",
    "                                  'weight_decay':0.}\n",
    "        \n",
    "        elif name.startswith(\"stages\"):\n",
    "            stage_id = int(name.split('.')[1])\n",
    "            block_id = int(name.split('.')[2])\n",
    "            layer_id = sum(depths[:stage_id]) + block_id\n",
    "            if len(param.shape) == 1 or name.endswith(\".bias\") or name.endswith(\".gamma\") or name.endswith(\".beta\"):\n",
    "                param_groups[name] = {'lr':lr*(scale**(total_depths-layer_id)),\n",
    "                                      'weight_decay':0.}\n",
    "            else :\n",
    "                param_groups[name] = {'lr':lr*(scale**(total_depths-layer_id)),\n",
    "                                      'weight_decay':weight_decay}       \n",
    "        else : # head\n",
    "            layer_id = total_depths\n",
    "            if len(param.shape) == 1 or name.endswith(\".bias\"):\n",
    "                param_groups[name] = {'lr':lr*(scale**(total_depths-layer_id)),\n",
    "                                      'weight_decay':0.}\n",
    "            else :\n",
    "                param_groups[name] = {'lr':lr*(scale**(total_depths-layer_id)),\n",
    "                                      'weight_decay':weight_decay}    \n",
    "    return layer_names, param_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: downsample_layers.0.stem_conv.weight, lr: 0.0012007570823759934, weight_decay: 0.0\n",
      "name: downsample_layers.0.stem_conv.bias, lr: 0.0012007570823759934, weight_decay: 0.0\n",
      "name: downsample_layers.0.stem_ln.weight, lr: 0.0012007570823759934, weight_decay: 0.0\n",
      "name: downsample_layers.0.stem_ln.bias, lr: 0.0012007570823759934, weight_decay: 0.0\n",
      "name: stages.0.0.dwconv.weight, lr: 0.0012007570823759934, weight_decay: 0.05\n",
      "name: stages.0.0.dwconv.bias, lr: 0.0012007570823759934, weight_decay: 0.0\n",
      "name: stages.0.0.layernorm.weight, lr: 0.0012007570823759934, weight_decay: 0.0\n",
      "name: stages.0.0.layernorm.bias, lr: 0.0012007570823759934, weight_decay: 0.0\n",
      "name: stages.0.0.pwconv1.weight, lr: 0.0012007570823759934, weight_decay: 0.05\n",
      "name: stages.0.0.pwconv1.bias, lr: 0.0012007570823759934, weight_decay: 0.0\n",
      "name: stages.0.0.grn.gamma, lr: 0.0012007570823759934, weight_decay: 0.0\n",
      "name: stages.0.0.grn.beta, lr: 0.0012007570823759934, weight_decay: 0.0\n",
      "name: stages.0.0.pwconv2.weight, lr: 0.0012007570823759934, weight_decay: 0.05\n",
      "name: stages.0.0.pwconv2.bias, lr: 0.0012007570823759934, weight_decay: 0.0\n",
      "name: stages.0.1.dwconv.weight, lr: 0.001334174535973326, weight_decay: 0.05\n",
      "name: stages.0.1.dwconv.bias, lr: 0.001334174535973326, weight_decay: 0.0\n",
      "name: stages.0.1.layernorm.weight, lr: 0.001334174535973326, weight_decay: 0.0\n",
      "name: stages.0.1.layernorm.bias, lr: 0.001334174535973326, weight_decay: 0.0\n",
      "name: stages.0.1.pwconv1.weight, lr: 0.001334174535973326, weight_decay: 0.05\n",
      "name: stages.0.1.pwconv1.bias, lr: 0.001334174535973326, weight_decay: 0.0\n",
      "name: stages.0.1.grn.gamma, lr: 0.001334174535973326, weight_decay: 0.0\n",
      "name: stages.0.1.grn.beta, lr: 0.001334174535973326, weight_decay: 0.0\n",
      "name: stages.0.1.pwconv2.weight, lr: 0.001334174535973326, weight_decay: 0.05\n",
      "name: stages.0.1.pwconv2.bias, lr: 0.001334174535973326, weight_decay: 0.0\n",
      "name: stages.0.2.dwconv.weight, lr: 0.0014824161510814734, weight_decay: 0.05\n",
      "name: stages.0.2.dwconv.bias, lr: 0.0014824161510814734, weight_decay: 0.0\n",
      "name: stages.0.2.layernorm.weight, lr: 0.0014824161510814734, weight_decay: 0.0\n",
      "name: stages.0.2.layernorm.bias, lr: 0.0014824161510814734, weight_decay: 0.0\n",
      "name: stages.0.2.pwconv1.weight, lr: 0.0014824161510814734, weight_decay: 0.05\n",
      "name: stages.0.2.pwconv1.bias, lr: 0.0014824161510814734, weight_decay: 0.0\n",
      "name: stages.0.2.grn.gamma, lr: 0.0014824161510814734, weight_decay: 0.0\n",
      "name: stages.0.2.grn.beta, lr: 0.0014824161510814734, weight_decay: 0.0\n",
      "name: stages.0.2.pwconv2.weight, lr: 0.0014824161510814734, weight_decay: 0.05\n",
      "name: stages.0.2.pwconv2.bias, lr: 0.0014824161510814734, weight_decay: 0.0\n",
      "name: downsample_layers.1.ds_ln.weight, lr: 0.0016471290567571927, weight_decay: 0.0\n",
      "name: downsample_layers.1.ds_ln.bias, lr: 0.0016471290567571927, weight_decay: 0.0\n",
      "name: downsample_layers.1.ds_conv.weight, lr: 0.0016471290567571927, weight_decay: 0.0\n",
      "name: downsample_layers.1.ds_conv.bias, lr: 0.0016471290567571927, weight_decay: 0.0\n",
      "name: stages.1.0.dwconv.weight, lr: 0.0016471290567571927, weight_decay: 0.05\n",
      "name: stages.1.0.dwconv.bias, lr: 0.0016471290567571927, weight_decay: 0.0\n",
      "name: stages.1.0.layernorm.weight, lr: 0.0016471290567571927, weight_decay: 0.0\n",
      "name: stages.1.0.layernorm.bias, lr: 0.0016471290567571927, weight_decay: 0.0\n",
      "name: stages.1.0.pwconv1.weight, lr: 0.0016471290567571927, weight_decay: 0.05\n",
      "name: stages.1.0.pwconv1.bias, lr: 0.0016471290567571927, weight_decay: 0.0\n",
      "name: stages.1.0.grn.gamma, lr: 0.0016471290567571927, weight_decay: 0.0\n",
      "name: stages.1.0.grn.beta, lr: 0.0016471290567571927, weight_decay: 0.0\n",
      "name: stages.1.0.pwconv2.weight, lr: 0.0016471290567571927, weight_decay: 0.05\n",
      "name: stages.1.0.pwconv2.bias, lr: 0.0016471290567571927, weight_decay: 0.0\n",
      "name: stages.1.1.dwconv.weight, lr: 0.0018301433963968808, weight_decay: 0.05\n",
      "name: stages.1.1.dwconv.bias, lr: 0.0018301433963968808, weight_decay: 0.0\n",
      "name: stages.1.1.layernorm.weight, lr: 0.0018301433963968808, weight_decay: 0.0\n",
      "name: stages.1.1.layernorm.bias, lr: 0.0018301433963968808, weight_decay: 0.0\n",
      "name: stages.1.1.pwconv1.weight, lr: 0.0018301433963968808, weight_decay: 0.05\n",
      "name: stages.1.1.pwconv1.bias, lr: 0.0018301433963968808, weight_decay: 0.0\n",
      "name: stages.1.1.grn.gamma, lr: 0.0018301433963968808, weight_decay: 0.0\n",
      "name: stages.1.1.grn.beta, lr: 0.0018301433963968808, weight_decay: 0.0\n",
      "name: stages.1.1.pwconv2.weight, lr: 0.0018301433963968808, weight_decay: 0.05\n",
      "name: stages.1.1.pwconv2.bias, lr: 0.0018301433963968808, weight_decay: 0.0\n",
      "name: stages.1.2.dwconv.weight, lr: 0.002033492662663201, weight_decay: 0.05\n",
      "name: stages.1.2.dwconv.bias, lr: 0.002033492662663201, weight_decay: 0.0\n",
      "name: stages.1.2.layernorm.weight, lr: 0.002033492662663201, weight_decay: 0.0\n",
      "name: stages.1.2.layernorm.bias, lr: 0.002033492662663201, weight_decay: 0.0\n",
      "name: stages.1.2.pwconv1.weight, lr: 0.002033492662663201, weight_decay: 0.05\n",
      "name: stages.1.2.pwconv1.bias, lr: 0.002033492662663201, weight_decay: 0.0\n",
      "name: stages.1.2.grn.gamma, lr: 0.002033492662663201, weight_decay: 0.0\n",
      "name: stages.1.2.grn.beta, lr: 0.002033492662663201, weight_decay: 0.0\n",
      "name: stages.1.2.pwconv2.weight, lr: 0.002033492662663201, weight_decay: 0.05\n",
      "name: stages.1.2.pwconv2.bias, lr: 0.002033492662663201, weight_decay: 0.0\n",
      "name: downsample_layers.2.ds_ln.weight, lr: 0.0022594362918480007, weight_decay: 0.0\n",
      "name: downsample_layers.2.ds_ln.bias, lr: 0.0022594362918480007, weight_decay: 0.0\n",
      "name: downsample_layers.2.ds_conv.weight, lr: 0.0022594362918480007, weight_decay: 0.0\n",
      "name: downsample_layers.2.ds_conv.bias, lr: 0.0022594362918480007, weight_decay: 0.0\n",
      "name: stages.2.0.dwconv.weight, lr: 0.0022594362918480007, weight_decay: 0.05\n",
      "name: stages.2.0.dwconv.bias, lr: 0.0022594362918480007, weight_decay: 0.0\n",
      "name: stages.2.0.layernorm.weight, lr: 0.0022594362918480007, weight_decay: 0.0\n",
      "name: stages.2.0.layernorm.bias, lr: 0.0022594362918480007, weight_decay: 0.0\n",
      "name: stages.2.0.pwconv1.weight, lr: 0.0022594362918480007, weight_decay: 0.05\n",
      "name: stages.2.0.pwconv1.bias, lr: 0.0022594362918480007, weight_decay: 0.0\n",
      "name: stages.2.0.grn.gamma, lr: 0.0022594362918480007, weight_decay: 0.0\n",
      "name: stages.2.0.grn.beta, lr: 0.0022594362918480007, weight_decay: 0.0\n",
      "name: stages.2.0.pwconv2.weight, lr: 0.0022594362918480007, weight_decay: 0.05\n",
      "name: stages.2.0.pwconv2.bias, lr: 0.0022594362918480007, weight_decay: 0.0\n",
      "name: stages.2.1.dwconv.weight, lr: 0.0025104847687200007, weight_decay: 0.05\n",
      "name: stages.2.1.dwconv.bias, lr: 0.0025104847687200007, weight_decay: 0.0\n",
      "name: stages.2.1.layernorm.weight, lr: 0.0025104847687200007, weight_decay: 0.0\n",
      "name: stages.2.1.layernorm.bias, lr: 0.0025104847687200007, weight_decay: 0.0\n",
      "name: stages.2.1.pwconv1.weight, lr: 0.0025104847687200007, weight_decay: 0.05\n",
      "name: stages.2.1.pwconv1.bias, lr: 0.0025104847687200007, weight_decay: 0.0\n",
      "name: stages.2.1.grn.gamma, lr: 0.0025104847687200007, weight_decay: 0.0\n",
      "name: stages.2.1.grn.beta, lr: 0.0025104847687200007, weight_decay: 0.0\n",
      "name: stages.2.1.pwconv2.weight, lr: 0.0025104847687200007, weight_decay: 0.05\n",
      "name: stages.2.1.pwconv2.bias, lr: 0.0025104847687200007, weight_decay: 0.0\n",
      "name: stages.2.2.dwconv.weight, lr: 0.002789427520800001, weight_decay: 0.05\n",
      "name: stages.2.2.dwconv.bias, lr: 0.002789427520800001, weight_decay: 0.0\n",
      "name: stages.2.2.layernorm.weight, lr: 0.002789427520800001, weight_decay: 0.0\n",
      "name: stages.2.2.layernorm.bias, lr: 0.002789427520800001, weight_decay: 0.0\n",
      "name: stages.2.2.pwconv1.weight, lr: 0.002789427520800001, weight_decay: 0.05\n",
      "name: stages.2.2.pwconv1.bias, lr: 0.002789427520800001, weight_decay: 0.0\n",
      "name: stages.2.2.grn.gamma, lr: 0.002789427520800001, weight_decay: 0.0\n",
      "name: stages.2.2.grn.beta, lr: 0.002789427520800001, weight_decay: 0.0\n",
      "name: stages.2.2.pwconv2.weight, lr: 0.002789427520800001, weight_decay: 0.05\n",
      "name: stages.2.2.pwconv2.bias, lr: 0.002789427520800001, weight_decay: 0.0\n",
      "name: stages.2.3.dwconv.weight, lr: 0.0030993639120000006, weight_decay: 0.05\n",
      "name: stages.2.3.dwconv.bias, lr: 0.0030993639120000006, weight_decay: 0.0\n",
      "name: stages.2.3.layernorm.weight, lr: 0.0030993639120000006, weight_decay: 0.0\n",
      "name: stages.2.3.layernorm.bias, lr: 0.0030993639120000006, weight_decay: 0.0\n",
      "name: stages.2.3.pwconv1.weight, lr: 0.0030993639120000006, weight_decay: 0.05\n",
      "name: stages.2.3.pwconv1.bias, lr: 0.0030993639120000006, weight_decay: 0.0\n",
      "name: stages.2.3.grn.gamma, lr: 0.0030993639120000006, weight_decay: 0.0\n",
      "name: stages.2.3.grn.beta, lr: 0.0030993639120000006, weight_decay: 0.0\n",
      "name: stages.2.3.pwconv2.weight, lr: 0.0030993639120000006, weight_decay: 0.05\n",
      "name: stages.2.3.pwconv2.bias, lr: 0.0030993639120000006, weight_decay: 0.0\n",
      "name: stages.2.4.dwconv.weight, lr: 0.003443737680000001, weight_decay: 0.05\n",
      "name: stages.2.4.dwconv.bias, lr: 0.003443737680000001, weight_decay: 0.0\n",
      "name: stages.2.4.layernorm.weight, lr: 0.003443737680000001, weight_decay: 0.0\n",
      "name: stages.2.4.layernorm.bias, lr: 0.003443737680000001, weight_decay: 0.0\n",
      "name: stages.2.4.pwconv1.weight, lr: 0.003443737680000001, weight_decay: 0.05\n",
      "name: stages.2.4.pwconv1.bias, lr: 0.003443737680000001, weight_decay: 0.0\n",
      "name: stages.2.4.grn.gamma, lr: 0.003443737680000001, weight_decay: 0.0\n",
      "name: stages.2.4.grn.beta, lr: 0.003443737680000001, weight_decay: 0.0\n",
      "name: stages.2.4.pwconv2.weight, lr: 0.003443737680000001, weight_decay: 0.05\n",
      "name: stages.2.4.pwconv2.bias, lr: 0.003443737680000001, weight_decay: 0.0\n",
      "name: stages.2.5.dwconv.weight, lr: 0.003826375200000001, weight_decay: 0.05\n",
      "name: stages.2.5.dwconv.bias, lr: 0.003826375200000001, weight_decay: 0.0\n",
      "name: stages.2.5.layernorm.weight, lr: 0.003826375200000001, weight_decay: 0.0\n",
      "name: stages.2.5.layernorm.bias, lr: 0.003826375200000001, weight_decay: 0.0\n",
      "name: stages.2.5.pwconv1.weight, lr: 0.003826375200000001, weight_decay: 0.05\n",
      "name: stages.2.5.pwconv1.bias, lr: 0.003826375200000001, weight_decay: 0.0\n",
      "name: stages.2.5.grn.gamma, lr: 0.003826375200000001, weight_decay: 0.0\n",
      "name: stages.2.5.grn.beta, lr: 0.003826375200000001, weight_decay: 0.0\n",
      "name: stages.2.5.pwconv2.weight, lr: 0.003826375200000001, weight_decay: 0.05\n",
      "name: stages.2.5.pwconv2.bias, lr: 0.003826375200000001, weight_decay: 0.0\n",
      "name: stages.2.6.dwconv.weight, lr: 0.004251528, weight_decay: 0.05\n",
      "name: stages.2.6.dwconv.bias, lr: 0.004251528, weight_decay: 0.0\n",
      "name: stages.2.6.layernorm.weight, lr: 0.004251528, weight_decay: 0.0\n",
      "name: stages.2.6.layernorm.bias, lr: 0.004251528, weight_decay: 0.0\n",
      "name: stages.2.6.pwconv1.weight, lr: 0.004251528, weight_decay: 0.05\n",
      "name: stages.2.6.pwconv1.bias, lr: 0.004251528, weight_decay: 0.0\n",
      "name: stages.2.6.grn.gamma, lr: 0.004251528, weight_decay: 0.0\n",
      "name: stages.2.6.grn.beta, lr: 0.004251528, weight_decay: 0.0\n",
      "name: stages.2.6.pwconv2.weight, lr: 0.004251528, weight_decay: 0.05\n",
      "name: stages.2.6.pwconv2.bias, lr: 0.004251528, weight_decay: 0.0\n",
      "name: stages.2.7.dwconv.weight, lr: 0.00472392, weight_decay: 0.05\n",
      "name: stages.2.7.dwconv.bias, lr: 0.00472392, weight_decay: 0.0\n",
      "name: stages.2.7.layernorm.weight, lr: 0.00472392, weight_decay: 0.0\n",
      "name: stages.2.7.layernorm.bias, lr: 0.00472392, weight_decay: 0.0\n",
      "name: stages.2.7.pwconv1.weight, lr: 0.00472392, weight_decay: 0.05\n",
      "name: stages.2.7.pwconv1.bias, lr: 0.00472392, weight_decay: 0.0\n",
      "name: stages.2.7.grn.gamma, lr: 0.00472392, weight_decay: 0.0\n",
      "name: stages.2.7.grn.beta, lr: 0.00472392, weight_decay: 0.0\n",
      "name: stages.2.7.pwconv2.weight, lr: 0.00472392, weight_decay: 0.05\n",
      "name: stages.2.7.pwconv2.bias, lr: 0.00472392, weight_decay: 0.0\n",
      "name: stages.2.8.dwconv.weight, lr: 0.0052488000000000005, weight_decay: 0.05\n",
      "name: stages.2.8.dwconv.bias, lr: 0.0052488000000000005, weight_decay: 0.0\n",
      "name: stages.2.8.layernorm.weight, lr: 0.0052488000000000005, weight_decay: 0.0\n",
      "name: stages.2.8.layernorm.bias, lr: 0.0052488000000000005, weight_decay: 0.0\n",
      "name: stages.2.8.pwconv1.weight, lr: 0.0052488000000000005, weight_decay: 0.05\n",
      "name: stages.2.8.pwconv1.bias, lr: 0.0052488000000000005, weight_decay: 0.0\n",
      "name: stages.2.8.grn.gamma, lr: 0.0052488000000000005, weight_decay: 0.0\n",
      "name: stages.2.8.grn.beta, lr: 0.0052488000000000005, weight_decay: 0.0\n",
      "name: stages.2.8.pwconv2.weight, lr: 0.0052488000000000005, weight_decay: 0.05\n",
      "name: stages.2.8.pwconv2.bias, lr: 0.0052488000000000005, weight_decay: 0.0\n",
      "name: downsample_layers.3.ds_ln.weight, lr: 0.005832000000000001, weight_decay: 0.0\n",
      "name: downsample_layers.3.ds_ln.bias, lr: 0.005832000000000001, weight_decay: 0.0\n",
      "name: downsample_layers.3.ds_conv.weight, lr: 0.005832000000000001, weight_decay: 0.0\n",
      "name: downsample_layers.3.ds_conv.bias, lr: 0.005832000000000001, weight_decay: 0.0\n",
      "name: stages.3.0.dwconv.weight, lr: 0.005832000000000001, weight_decay: 0.05\n",
      "name: stages.3.0.dwconv.bias, lr: 0.005832000000000001, weight_decay: 0.0\n",
      "name: stages.3.0.layernorm.weight, lr: 0.005832000000000001, weight_decay: 0.0\n",
      "name: stages.3.0.layernorm.bias, lr: 0.005832000000000001, weight_decay: 0.0\n",
      "name: stages.3.0.pwconv1.weight, lr: 0.005832000000000001, weight_decay: 0.05\n",
      "name: stages.3.0.pwconv1.bias, lr: 0.005832000000000001, weight_decay: 0.0\n",
      "name: stages.3.0.grn.gamma, lr: 0.005832000000000001, weight_decay: 0.0\n",
      "name: stages.3.0.grn.beta, lr: 0.005832000000000001, weight_decay: 0.0\n",
      "name: stages.3.0.pwconv2.weight, lr: 0.005832000000000001, weight_decay: 0.05\n",
      "name: stages.3.0.pwconv2.bias, lr: 0.005832000000000001, weight_decay: 0.0\n",
      "name: stages.3.1.dwconv.weight, lr: 0.0064800000000000005, weight_decay: 0.05\n",
      "name: stages.3.1.dwconv.bias, lr: 0.0064800000000000005, weight_decay: 0.0\n",
      "name: stages.3.1.layernorm.weight, lr: 0.0064800000000000005, weight_decay: 0.0\n",
      "name: stages.3.1.layernorm.bias, lr: 0.0064800000000000005, weight_decay: 0.0\n",
      "name: stages.3.1.pwconv1.weight, lr: 0.0064800000000000005, weight_decay: 0.05\n",
      "name: stages.3.1.pwconv1.bias, lr: 0.0064800000000000005, weight_decay: 0.0\n",
      "name: stages.3.1.grn.gamma, lr: 0.0064800000000000005, weight_decay: 0.0\n",
      "name: stages.3.1.grn.beta, lr: 0.0064800000000000005, weight_decay: 0.0\n",
      "name: stages.3.1.pwconv2.weight, lr: 0.0064800000000000005, weight_decay: 0.05\n",
      "name: stages.3.1.pwconv2.bias, lr: 0.0064800000000000005, weight_decay: 0.0\n",
      "name: stages.3.2.dwconv.weight, lr: 0.007200000000000001, weight_decay: 0.05\n",
      "name: stages.3.2.dwconv.bias, lr: 0.007200000000000001, weight_decay: 0.0\n",
      "name: stages.3.2.layernorm.weight, lr: 0.007200000000000001, weight_decay: 0.0\n",
      "name: stages.3.2.layernorm.bias, lr: 0.007200000000000001, weight_decay: 0.0\n",
      "name: stages.3.2.pwconv1.weight, lr: 0.007200000000000001, weight_decay: 0.05\n",
      "name: stages.3.2.pwconv1.bias, lr: 0.007200000000000001, weight_decay: 0.0\n",
      "name: stages.3.2.grn.gamma, lr: 0.007200000000000001, weight_decay: 0.0\n",
      "name: stages.3.2.grn.beta, lr: 0.007200000000000001, weight_decay: 0.0\n",
      "name: stages.3.2.pwconv2.weight, lr: 0.007200000000000001, weight_decay: 0.05\n",
      "name: stages.3.2.pwconv2.bias, lr: 0.007200000000000001, weight_decay: 0.0\n",
      "name: layernorm.weight, lr: 0.008, weight_decay: 0.0\n",
      "name: layernorm.bias, lr: 0.008, weight_decay: 0.0\n",
      "name: fc.weight, lr: 0.008, weight_decay: 0.05\n",
      "name: fc.bias, lr: 0.008, weight_decay: 0.0\n"
     ]
    }
   ],
   "source": [
    "layer_names, param_groups = LLRD_ConvNeXt(model)\n",
    "for name in layer_names:\n",
    "    print(f\"name: {name}, lr: {param_groups[name]['lr']}, weight_decay: {param_groups[name]['weight_decay']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = [{'params': param,\n",
    "            'lr' : param_groups[name]['lr'],\n",
    "            'weight_decay': param_groups[name]['weight_decay']} for name, param in model.named_parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "\n",
    "optimizer = optim.AdamW(groups)\n",
    "warmup_steps = int(len(train_loader)*(epochs)*0.1)\n",
    "train_steps = len(train_loader)*(epochs)\n",
    "scheduler = CosineWarmupScheduler(optimizer, \n",
    "                                num_warmup_steps=warmup_steps, \n",
    "                                num_training_steps=train_steps,\n",
    "                                num_cycles=0.5,\n",
    "                                min_lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/43 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.44 GiB. GPU 4 has a total capacity of 79.15 GiB of which 746.62 MiB is free. Process 1669451 has 24.98 GiB memory in use. Process 1817861 has 53.42 GiB memory in use. Of the allocated memory 23.40 GiB is allocated by PyTorch, and 1.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m mixup_fn(inputs, labels)\n\u001b[1;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 21\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     24\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/study/algorithm/06.ConvNext/v2/model/convnextv2.py:128\u001b[0m, in \u001b[0;36mConvNeXtV2.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 128\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)     \u001b[38;5;66;03m# (N, C, H, W) -> (N, C, 1, 1)\u001b[39;00m\n\u001b[1;32m    131\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# (N, C, 1, 1) -> (N, C)\u001b[39;00m\n",
      "File \u001b[0;32m~/study/algorithm/06.ConvNext/v2/model/convnextv2.py:124\u001b[0m, in \u001b[0;36mConvNeXtV2.forward_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4\u001b[39m):        \n\u001b[1;32m    123\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample_layers[i](x)    \n\u001b[0;32m--> 124\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstages\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m        \n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/study/algorithm/06.ConvNext/v2/model/convnextv2.py:69\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     67\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpwconv1(x)\n\u001b[1;32m     68\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(x)\n\u001b[0;32m---> 69\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpwconv2(x)\n\u001b[1;32m     71\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m# (N, H, W, C) -> (N, C, H, W)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/study/algorithm/06.ConvNext/v2/model/convnextv2.py:22\u001b[0m, in \u001b[0;36mGRN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m Gx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnorm(x, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m), keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     21\u001b[0m Nx \u001b[38;5;241m=\u001b[39m Gx \u001b[38;5;241m/\u001b[39m (Gx\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-6\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m (\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mNx\u001b[49m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta \u001b[38;5;241m+\u001b[39m x\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.44 GiB. GPU 4 has a total capacity of 79.15 GiB of which 746.62 MiB is free. Process 1669451 has 24.98 GiB memory in use. Process 1817861 has 53.42 GiB memory in use. Of the allocated memory 23.40 GiB is allocated by PyTorch, and 1.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "training_time = 0\n",
    "losses = []\n",
    "val_losses = []\n",
    "lrs = []\n",
    "best_loss = float('inf')\n",
    "model_save = False\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "for i in range(epochs // 100):\n",
    "    for epoch in range(100):\n",
    "        model.train()\n",
    "        start_time = time.time()\n",
    "        running_loss = 0.0\n",
    "        pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch + 1 + i*100}\")\n",
    "        \n",
    "        for _, data in pbar:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            inputs, labels = mixup_fn(inputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "                \n",
    "            loss.backward()\n",
    "            # 그라디언트 클리핑 적용\n",
    "            clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "            optimizer.step()\n",
    "            \n",
    "            # EMA 모델 업데이트, 필요한 경우\n",
    "            if model_ema is not None:\n",
    "                model_ema.update(model)\n",
    "                \n",
    "            scheduler.step()\n",
    "                \n",
    "            lr = optimizer.param_groups[0][\"lr\"]\n",
    "            lrs.append(lr)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        losses.append(epoch_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data in valid_loader:\n",
    "                inputs, labels = data[0].to(device), data[1].to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "        val_loss /= len(valid_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # 모델 저장 조건 수정\n",
    "        total_loss = val_loss + epoch_loss\n",
    "        if total_loss < best_loss:\n",
    "            best_loss = total_loss\n",
    "            # torch.save(model.state_dict(), model_path)\n",
    "            model_save = True\n",
    "            save_text = ' - model saved!'\n",
    "        else:\n",
    "            save_text = ''\n",
    "\n",
    "        epoch_duration = time.time() - start_time\n",
    "        training_time += epoch_duration\n",
    "        \n",
    "        text = f'\\tLoss: {epoch_loss:.4f}, Val_Loss: {val_loss:.4f}, Total Mean Loss: {total_loss/2:.4f}, LR: {lr}, Duration: {epoch_duration:.2f} sec{save_text}'\n",
    "        print(text)\n",
    "\n",
    "    # 예측 수행 및 레이블 저장\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # 혼동 행렬 생성\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    # 예측과 실제 레이블\n",
    "    y_true = all_labels  # 실제 레이블\n",
    "    y_pred = all_preds  # 모델에 의해 예측된 레이블\n",
    "\n",
    "    # 전체 데이터셋에 대한 정확도\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # 평균 정밀도, 리콜, F1-Score ('weighted')\n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "\n",
    "    # 판다스 데이터프레임으로 결과 정리\n",
    "    performance_metrics = pd.DataFrame({\n",
    "        'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "        'Value': [accuracy, precision, recall, f1_score]\n",
    "    })\n",
    "\n",
    "    # 데이터프레임 출력\n",
    "    print(f\"\\n[{i*100+100} epoch result]\\n\", performance_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 43/43 [01:41<00:00,  2.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.5287, Val_Loss: 3.9159, Total Mean Loss: 4.2223, LR: 0.0003178614547665746, Duration: 103.17 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.2734, Val_Loss: 3.4585, Total Mean Loss: 3.8659, LR: 0.0006356229095331492, Duration: 100.82 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.1719, Val_Loss: 3.2927, Total Mean Loss: 3.7323, LR: 0.0009533843642997237, Duration: 101.20 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.0386, Val_Loss: 2.9514, Total Mean Loss: 3.4950, LR: 0.0012711458190662985, Duration: 100.93 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.0751, Val_Loss: 2.8893, Total Mean Loss: 3.4822, LR: 0.001588907273832873, Duration: 101.43 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.9660, Val_Loss: 2.8268, Total Mean Loss: 3.3964, LR: 0.0019066687285994475, Duration: 101.31 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.9046, Val_Loss: 2.8021, Total Mean Loss: 3.3534, LR: 0.002224430183366022, Duration: 101.10 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.9231, Val_Loss: 2.4574, Total Mean Loss: 3.1903, LR: 0.0025421916381325967, Duration: 101.21 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.8459, Val_Loss: 2.5138, Total Mean Loss: 3.1799, LR: 0.002859953092899171, Duration: 101.39 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.8035, Val_Loss: 2.3973, Total Mean Loss: 3.1004, LR: 0.0031777145476657455, Duration: 101.67 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 43/43 [01:40<00:00,  2.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.6747, Val_Loss: 2.1981, Total Mean Loss: 2.9364, LR: 0.0031767466892026626, Duration: 101.77 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.7028, Val_Loss: 2.3046, Total Mean Loss: 3.0037, LR: 0.003173844292999863, Duration: 101.57 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.6989, Val_Loss: 2.1315, Total Mean Loss: 2.9152, LR: 0.0031690108951800407, Duration: 101.33 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.5692, Val_Loss: 1.9906, Total Mean Loss: 2.7799, LR: 0.003162252384493911, Duration: 101.39 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.6127, Val_Loss: 2.0173, Total Mean Loss: 2.8150, LR: 0.0031535769951456763, Duration: 101.15 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.4291, Val_Loss: 1.7500, Total Mean Loss: 2.5896, LR: 0.0031429952967609157, Duration: 101.64 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 43/43 [01:40<00:00,  2.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.4419, Val_Loss: 1.8246, Total Mean Loss: 2.6333, LR: 0.003130520181509126, Duration: 101.74 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.4182, Val_Loss: 1.5813, Total Mean Loss: 2.4997, LR: 0.003116166848396597, Duration: 101.67 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.2623, Val_Loss: 1.5283, Total Mean Loss: 2.3953, LR: 0.003099952784748765, Duration: 101.35 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.3023, Val_Loss: 1.5513, Total Mean Loss: 2.4268, LR: 0.0030818977449045995, Duration: 101.47 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 43/43 [01:40<00:00,  2.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.3689, Val_Loss: 1.4606, Total Mean Loss: 2.4147, LR: 0.003062023726148985, Duration: 101.96 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.2842, Val_Loss: 1.4506, Total Mean Loss: 2.3674, LR: 0.0030403549419124173, Duration: 101.39 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.2748, Val_Loss: 1.4586, Total Mean Loss: 2.3667, LR: 0.0030169177922706692, Duration: 101.51 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.2875, Val_Loss: 1.3987, Total Mean Loss: 2.3431, LR: 0.0029917408317803617, Duration: 101.20 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.2779, Val_Loss: 1.2844, Total Mean Loss: 2.2811, LR: 0.0029648547346896397, Duration: 101.70 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.3447, Val_Loss: 1.3311, Total Mean Loss: 2.3379, LR: 0.0029362922575663216, Duration: 101.59 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 43/43 [01:39<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.1405, Val_Loss: 1.2528, Total Mean Loss: 2.1966, LR: 0.002906088199389071, Duration: 101.26 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.1310, Val_Loss: 1.2650, Total Mean Loss: 2.1980, LR: 0.0028742793591501974, Duration: 101.36 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.1289, Val_Loss: 1.1390, Total Mean Loss: 2.1340, LR: 0.002840904491021756, Duration: 101.08 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.1529, Val_Loss: 1.3275, Total Mean Loss: 2.2402, LR: 0.002806004257139557, Duration: 101.12 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.2226, Val_Loss: 1.3311, Total Mean Loss: 2.2768, LR: 0.0027696211780626175, Duration: 100.98 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.1082, Val_Loss: 1.1390, Total Mean Loss: 2.1236, LR: 0.002731799580968408, Duration: 100.81 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.9851, Val_Loss: 1.1876, Total Mean Loss: 2.0863, LR: 0.002692585545647018, Duration: 101.10 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.0585, Val_Loss: 1.0951, Total Mean Loss: 2.0768, LR: 0.0026520268483600272, Duration: 101.29 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.9912, Val_Loss: 1.1544, Total Mean Loss: 2.0728, LR: 0.0026101729036324918, Duration: 101.16 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.9583, Val_Loss: 0.9371, Total Mean Loss: 1.9477, LR: 0.002567074704048956, Duration: 101.35 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.9369, Val_Loss: 1.1084, Total Mean Loss: 2.0227, LR: 0.0025227847581268444, Duration: 101.65 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.9988, Val_Loss: 1.0546, Total Mean Loss: 2.0267, LR: 0.0024773570263429186, Duration: 101.04 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.9389, Val_Loss: 1.0804, Total Mean Loss: 2.0097, LR: 0.002430846855390753, Duration: 100.87 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.9318, Val_Loss: 0.9029, Total Mean Loss: 1.9174, LR: 0.0023833109107493093, Duration: 101.10 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.9433, Val_Loss: 1.0889, Total Mean Loss: 2.0161, LR: 0.0023348071076447824, Duration: 101.41 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.9525, Val_Loss: 0.9664, Total Mean Loss: 1.9595, LR: 0.002285394540489817, Duration: 101.13 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.8303, Val_Loss: 1.0099, Total Mean Loss: 1.9201, LR: 0.0022351334108860694, Duration: 101.32 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.8993, Val_Loss: 0.9599, Total Mean Loss: 1.9296, LR: 0.0021840849542778275, Duration: 101.43 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6934, Val_Loss: 0.8701, Total Mean Loss: 1.7818, LR: 0.0021323113653460572, Duration: 101.28 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.8386, Val_Loss: 0.9527, Total Mean Loss: 1.8956, LR: 0.002079875722233761, Duration: 101.45 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6570, Val_Loss: 0.9809, Total Mean Loss: 1.8190, LR: 0.0020268419096949805, Duration: 101.11 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48: 100%|██████████| 43/43 [01:39<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.8723, Val_Loss: 0.8841, Total Mean Loss: 1.8782, LR: 0.0019732745412610613, Duration: 101.26 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.8212, Val_Loss: 0.8553, Total Mean Loss: 1.8383, LR: 0.00191923888051902, Duration: 101.22 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50: 100%|██████████| 43/43 [01:40<00:00,  2.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6832, Val_Loss: 0.8335, Total Mean Loss: 1.7583, LR: 0.0018648007615979152, Duration: 101.73 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6253, Val_Loss: 0.8351, Total Mean Loss: 1.7302, LR: 0.0018100265089601017, Duration: 101.67 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6523, Val_Loss: 0.8278, Total Mean Loss: 1.7401, LR: 0.001754982856595093, Duration: 101.62 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.7655, Val_Loss: 0.8404, Total Mean Loss: 1.8030, LR: 0.001699736866714471, Duration: 101.44 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54: 100%|██████████| 43/43 [01:39<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5825, Val_Loss: 0.8886, Total Mean Loss: 1.7356, LR: 0.001644355848046913, Duration: 101.28 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55: 100%|██████████| 43/43 [01:40<00:00,  2.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.7330, Val_Loss: 0.9120, Total Mean Loss: 1.8225, LR: 0.001588907273832873, Duration: 101.77 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6799, Val_Loss: 0.7814, Total Mean Loss: 1.7306, LR: 0.0015334586996188335, Duration: 101.23 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5000, Val_Loss: 0.8507, Total Mean Loss: 1.6753, LR: 0.001478077680951275, Duration: 101.62 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5685, Val_Loss: 0.8521, Total Mean Loss: 1.7103, LR: 0.001422831691070653, Duration: 101.10 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5161, Val_Loss: 0.8062, Total Mean Loss: 1.6611, LR: 0.0013677880387056446, Duration: 101.16 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.7035, Val_Loss: 0.8373, Total Mean Loss: 1.7704, LR: 0.001313013786067831, Duration: 101.25 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61: 100%|██████████| 43/43 [01:40<00:00,  2.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4608, Val_Loss: 0.8281, Total Mean Loss: 1.6445, LR: 0.0012585756671467257, Duration: 101.76 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62: 100%|██████████| 43/43 [01:40<00:00,  2.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5962, Val_Loss: 0.8003, Total Mean Loss: 1.6982, LR: 0.0012045400064046841, Duration: 101.89 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4581, Val_Loss: 0.7837, Total Mean Loss: 1.6209, LR: 0.001150972637970765, Duration: 101.23 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5356, Val_Loss: 0.8106, Total Mean Loss: 1.6731, LR: 0.0010979388254319846, Duration: 101.30 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6427, Val_Loss: 0.8175, Total Mean Loss: 1.7301, LR: 0.0010455031823196888, Duration: 101.43 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6342, Val_Loss: 0.7670, Total Mean Loss: 1.7006, LR: 0.0009937295933879183, Duration: 101.19 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67: 100%|██████████| 43/43 [01:39<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5225, Val_Loss: 0.7328, Total Mean Loss: 1.6277, LR: 0.0009426811367796767, Duration: 101.27 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6396, Val_Loss: 0.8011, Total Mean Loss: 1.7203, LR: 0.0008924200071759285, Duration: 101.36 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3965, Val_Loss: 0.7741, Total Mean Loss: 1.5853, LR: 0.0008430074400209637, Duration: 101.42 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70: 100%|██████████| 43/43 [01:40<00:00,  2.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6007, Val_Loss: 0.7678, Total Mean Loss: 1.6842, LR: 0.0007945036369164367, Duration: 101.68 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5420, Val_Loss: 0.8167, Total Mean Loss: 1.6794, LR: 0.0007469676922749929, Duration: 101.53 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4630, Val_Loss: 0.7867, Total Mean Loss: 1.6248, LR: 0.000700457521322827, Duration: 101.53 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 73: 100%|██████████| 43/43 [01:40<00:00,  2.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3901, Val_Loss: 0.7535, Total Mean Loss: 1.5718, LR: 0.0006550297895389014, Duration: 101.86 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 74: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5295, Val_Loss: 0.7820, Total Mean Loss: 1.6557, LR: 0.0006107398436167892, Duration: 101.31 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75: 100%|██████████| 43/43 [01:39<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4729, Val_Loss: 0.7846, Total Mean Loss: 1.6287, LR: 0.0005676416440332535, Duration: 101.26 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 76: 100%|██████████| 43/43 [01:40<00:00,  2.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4584, Val_Loss: 0.7611, Total Mean Loss: 1.6098, LR: 0.0005257876993057186, Duration: 101.70 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 77: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4153, Val_Loss: 0.7854, Total Mean Loss: 1.6003, LR: 0.0004852290020187274, Duration: 101.60 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 78: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6781, Val_Loss: 0.7827, Total Mean Loss: 1.7304, LR: 0.00044601496669733774, Duration: 101.14 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 79: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3445, Val_Loss: 0.7552, Total Mean Loss: 1.5498, LR: 0.00040819336960312783, Duration: 101.57 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 80: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4209, Val_Loss: 0.7498, Total Mean Loss: 1.5854, LR: 0.0003718102905261883, Duration: 101.39 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 81: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3658, Val_Loss: 0.7305, Total Mean Loss: 1.5481, LR: 0.00033691005664398936, Duration: 101.03 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 82: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5719, Val_Loss: 0.7732, Total Mean Loss: 1.6726, LR: 0.00030353518851554813, Duration: 101.40 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 83: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4414, Val_Loss: 0.8011, Total Mean Loss: 1.6213, LR: 0.00027172634827667466, Duration: 101.19 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 84: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4517, Val_Loss: 0.7666, Total Mean Loss: 1.6092, LR: 0.00024152229009942371, Duration: 101.43 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 85: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5137, Val_Loss: 0.8130, Total Mean Loss: 1.6634, LR: 0.00021295981297610587, Duration: 101.11 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 86: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5777, Val_Loss: 0.7862, Total Mean Loss: 1.6819, LR: 0.00018607371588538372, Duration: 101.35 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 87: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4618, Val_Loss: 0.7659, Total Mean Loss: 1.6139, LR: 0.00016089675539507673, Duration: 101.18 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 88: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3832, Val_Loss: 0.7664, Total Mean Loss: 1.5748, LR: 0.00013745960575332777, Duration: 101.24 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 89: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3641, Val_Loss: 0.7842, Total Mean Loss: 1.5742, LR: 0.00011579082151676064, Duration: 100.95 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 90: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4012, Val_Loss: 0.7698, Total Mean Loss: 1.5855, LR: 9.591680276114627e-05, Duration: 101.15 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 91: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4258, Val_Loss: 0.7620, Total Mean Loss: 1.5939, LR: 7.786176291698076e-05, Duration: 101.11 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 92: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4686, Val_Loss: 0.7689, Total Mean Loss: 1.6188, LR: 6.164769926914846e-05, Duration: 100.99 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 93: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5160, Val_Loss: 0.7657, Total Mean Loss: 1.6409, LR: 4.729436615661948e-05, Duration: 101.14 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 94: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6158, Val_Loss: 0.7725, Total Mean Loss: 1.6941, LR: 3.4819250904829655e-05, Duration: 101.35 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 95: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3931, Val_Loss: 0.7669, Total Mean Loss: 1.5800, LR: 2.423755252006945e-05, Duration: 101.22 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 96: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4339, Val_Loss: 0.7722, Total Mean Loss: 1.6030, LR: 1.556216317183451e-05, Duration: 101.30 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 97: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4339, Val_Loss: 0.7740, Total Mean Loss: 1.6039, LR: 8.803652485704949e-06, Duration: 101.56 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 98: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3417, Val_Loss: 0.7718, Total Mean Loss: 1.5567, LR: 3.970254665882611e-06, Duration: 101.59 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 43/43 [01:40<00:00,  2.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4796, Val_Loss: 0.7715, Total Mean Loss: 1.6256, LR: 1.0678584630831068e-06, Duration: 101.69 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 100: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4305, Val_Loss: 0.7713, Total Mean Loss: 1.6009, LR: 1e-07, Duration: 101.19 sec\n",
      "\n",
      "[100 epoch result]\n",
      "       Metric     Value\n",
      "0   Accuracy  0.900000\n",
      "1  Precision  0.913560\n",
      "2     Recall  0.900000\n",
      "3   F1 Score  0.897192\n"
     ]
    }
   ],
   "source": [
    "training_time = 0\n",
    "losses = []\n",
    "val_losses = []\n",
    "lrs = []\n",
    "best_loss = float('inf')\n",
    "model_save = False\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "for i in range(epochs // 100):\n",
    "    for epoch in range(100):\n",
    "        model.train()\n",
    "        start_time = time.time()\n",
    "        running_loss = 0.0\n",
    "        pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch + 1 + i*100}\")\n",
    "        \n",
    "        for _, data in pbar:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            inputs, labels = mixup_fn(inputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "                \n",
    "            loss.backward()\n",
    "            # 그라디언트 클리핑 적용\n",
    "            clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "            optimizer.step()\n",
    "            \n",
    "            # EMA 모델 업데이트, 필요한 경우\n",
    "            if model_ema is not None:\n",
    "                model_ema.update(model)\n",
    "                \n",
    "            scheduler.step()\n",
    "                \n",
    "            lr = optimizer.param_groups[0][\"lr\"]\n",
    "            lrs.append(lr)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        losses.append(epoch_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data in valid_loader:\n",
    "                inputs, labels = data[0].to(device), data[1].to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "        val_loss /= len(valid_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # 모델 저장 조건 수정\n",
    "        total_loss = val_loss + epoch_loss\n",
    "        if total_loss < best_loss:\n",
    "            best_loss = total_loss\n",
    "            # torch.save(model.state_dict(), model_path)\n",
    "            model_save = True\n",
    "            save_text = ' - model saved!'\n",
    "        else:\n",
    "            save_text = ''\n",
    "\n",
    "        epoch_duration = time.time() - start_time\n",
    "        training_time += epoch_duration\n",
    "        \n",
    "        text = f'\\tLoss: {epoch_loss:.4f}, Val_Loss: {val_loss:.4f}, Total Mean Loss: {total_loss/2:.4f}, LR: {lr}, Duration: {epoch_duration:.2f} sec{save_text}'\n",
    "        print(text)\n",
    "\n",
    "    # 예측 수행 및 레이블 저장\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # 혼동 행렬 생성\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    # 예측과 실제 레이블\n",
    "    y_true = all_labels  # 실제 레이블\n",
    "    y_pred = all_preds  # 모델에 의해 예측된 레이블\n",
    "\n",
    "    # 전체 데이터셋에 대한 정확도\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # 평균 정밀도, 리콜, F1-Score ('weighted')\n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "\n",
    "    # 판다스 데이터프레임으로 결과 정리\n",
    "    performance_metrics = pd.DataFrame({\n",
    "        'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "        'Value': [accuracy, precision, recall, f1_score]\n",
    "    })\n",
    "\n",
    "    # 데이터프레임 출력\n",
    "    print(f\"\\n[{i*100+100} epoch result]\\n\", performance_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CASE 03 : Grouped Learning Rate Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LLRD_ConvNeXt(model, depths=[3,3,9,3], weight_decay=1e-5, lr=8e-3, scale=0.85):\n",
    "    \n",
    "    stage = 0\n",
    "    layer_names = []\n",
    "    param_groups = {}\n",
    "    for depth in depths:\n",
    "        if stage == 0:\n",
    "            layer_names.append(f'downsample_layers.{stage}.stem_conv.weight')\n",
    "            layer_names.append(f'downsample_layers.{stage}.stem_conv.bias')\n",
    "            layer_names.append(f'downsample_layers.{stage}.stem_ln.weight')\n",
    "            layer_names.append(f'downsample_layers.{stage}.stem_ln.bias')\n",
    "        else :\n",
    "            layer_names.append(f'downsample_layers.{stage}.ds_ln.weight')\n",
    "            layer_names.append(f'downsample_layers.{stage}.ds_ln.bias')\n",
    "            layer_names.append(f'downsample_layers.{stage}.ds_conv.weight')\n",
    "            layer_names.append(f'downsample_layers.{stage}.ds_conv.bias')        \n",
    "        for i in range(depth):\n",
    "            layer_names.append(f'stages.{stage}.{i}.dwconv.weight')\n",
    "            layer_names.append(f'stages.{stage}.{i}.dwconv.bias')\n",
    "            layer_names.append(f'stages.{stage}.{i}.layernorm.weight')\n",
    "            layer_names.append(f'stages.{stage}.{i}.layernorm.bias')\n",
    "            layer_names.append(f'stages.{stage}.{i}.pwconv1.weight')\n",
    "            layer_names.append(f'stages.{stage}.{i}.pwconv1.bias')\n",
    "            layer_names.append(f'stages.{stage}.{i}.grn.gamma')\n",
    "            layer_names.append(f'stages.{stage}.{i}.grn.beta')            \n",
    "            layer_names.append(f'stages.{stage}.{i}.pwconv2.weight')\n",
    "            layer_names.append(f'stages.{stage}.{i}.pwconv2.bias')\n",
    "        stage += 1\n",
    "    \n",
    "    layer_names.append('layernorm.weight')\n",
    "    layer_names.append('layernorm.bias')\n",
    "    layer_names.append('fc.weight')\n",
    "    layer_names.append('fc.bias')\n",
    "    \n",
    "    # Layer Learning Rate Decay\n",
    "    for name, param in model.named_parameters():\n",
    "        total_depths = sum(depths)\n",
    "        if name.startswith(\"downsample_layers\"):\n",
    "            stage_id = int(name.split('.')[1])\n",
    "            layer_id = sum(depths[:stage_id]) + 1\n",
    "            param_groups[name] = {'lr':lr*(scale**((total_depths-layer_id)//3+1)),\n",
    "                                  'weight_decay':0.}\n",
    "        \n",
    "        elif name.startswith(\"stages\"):\n",
    "            stage_id = int(name.split('.')[1])\n",
    "            block_id = int(name.split('.')[2])\n",
    "            layer_id = sum(depths[:stage_id]) + block_id + 1\n",
    "            if len(param.shape) == 1 or name.endswith(\".bias\") or name.endswith(\".gamma\") or name.endswith(\".beta\"):\n",
    "                param_groups[name] = {'lr':lr*(scale**((total_depths-layer_id)//3+1)),\n",
    "                                      'weight_decay':0.}\n",
    "            else :\n",
    "                param_groups[name] = {'lr':lr*(scale**((total_depths-layer_id)//3+1)),\n",
    "                                      'weight_decay':weight_decay}       \n",
    "        else : # head\n",
    "            if len(param.shape) == 1 or name.endswith(\".bias\"):\n",
    "                param_groups[name] = {'lr':lr,\n",
    "                                      'weight_decay':0.}\n",
    "            else :\n",
    "                param_groups[name] = {'lr':lr,\n",
    "                                      'weight_decay':weight_decay}    \n",
    "    return layer_names, param_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: downsample_layers.0.stem_conv.weight, lr: 0.0030171961249999997, weight_decay: 0.0\n",
      "name: downsample_layers.0.stem_conv.bias, lr: 0.0030171961249999997, weight_decay: 0.0\n",
      "name: downsample_layers.0.stem_ln.weight, lr: 0.0030171961249999997, weight_decay: 0.0\n",
      "name: downsample_layers.0.stem_ln.bias, lr: 0.0030171961249999997, weight_decay: 0.0\n",
      "name: stages.0.0.dwconv.weight, lr: 0.0030171961249999997, weight_decay: 1e-05\n",
      "name: stages.0.0.dwconv.bias, lr: 0.0030171961249999997, weight_decay: 0.0\n",
      "name: stages.0.0.layernorm.weight, lr: 0.0030171961249999997, weight_decay: 0.0\n",
      "name: stages.0.0.layernorm.bias, lr: 0.0030171961249999997, weight_decay: 0.0\n",
      "name: stages.0.0.pwconv1.weight, lr: 0.0030171961249999997, weight_decay: 1e-05\n",
      "name: stages.0.0.pwconv1.bias, lr: 0.0030171961249999997, weight_decay: 0.0\n",
      "name: stages.0.0.grn.gamma, lr: 0.0030171961249999997, weight_decay: 0.0\n",
      "name: stages.0.0.grn.beta, lr: 0.0030171961249999997, weight_decay: 0.0\n",
      "name: stages.0.0.pwconv2.weight, lr: 0.0030171961249999997, weight_decay: 1e-05\n",
      "name: stages.0.0.pwconv2.bias, lr: 0.0030171961249999997, weight_decay: 0.0\n",
      "name: stages.0.1.dwconv.weight, lr: 0.0030171961249999997, weight_decay: 1e-05\n",
      "name: stages.0.1.dwconv.bias, lr: 0.0030171961249999997, weight_decay: 0.0\n",
      "name: stages.0.1.layernorm.weight, lr: 0.0030171961249999997, weight_decay: 0.0\n",
      "name: stages.0.1.layernorm.bias, lr: 0.0030171961249999997, weight_decay: 0.0\n",
      "name: stages.0.1.pwconv1.weight, lr: 0.0030171961249999997, weight_decay: 1e-05\n",
      "name: stages.0.1.pwconv1.bias, lr: 0.0030171961249999997, weight_decay: 0.0\n",
      "name: stages.0.1.grn.gamma, lr: 0.0030171961249999997, weight_decay: 0.0\n",
      "name: stages.0.1.grn.beta, lr: 0.0030171961249999997, weight_decay: 0.0\n",
      "name: stages.0.1.pwconv2.weight, lr: 0.0030171961249999997, weight_decay: 1e-05\n",
      "name: stages.0.1.pwconv2.bias, lr: 0.0030171961249999997, weight_decay: 0.0\n",
      "name: stages.0.2.dwconv.weight, lr: 0.0030171961249999997, weight_decay: 1e-05\n",
      "name: stages.0.2.dwconv.bias, lr: 0.0030171961249999997, weight_decay: 0.0\n",
      "name: stages.0.2.layernorm.weight, lr: 0.0030171961249999997, weight_decay: 0.0\n",
      "name: stages.0.2.layernorm.bias, lr: 0.0030171961249999997, weight_decay: 0.0\n",
      "name: stages.0.2.pwconv1.weight, lr: 0.0030171961249999997, weight_decay: 1e-05\n",
      "name: stages.0.2.pwconv1.bias, lr: 0.0030171961249999997, weight_decay: 0.0\n",
      "name: stages.0.2.grn.gamma, lr: 0.0030171961249999997, weight_decay: 0.0\n",
      "name: stages.0.2.grn.beta, lr: 0.0030171961249999997, weight_decay: 0.0\n",
      "name: stages.0.2.pwconv2.weight, lr: 0.0030171961249999997, weight_decay: 1e-05\n",
      "name: stages.0.2.pwconv2.bias, lr: 0.0030171961249999997, weight_decay: 0.0\n",
      "name: downsample_layers.1.ds_ln.weight, lr: 0.0035496424999999997, weight_decay: 0.0\n",
      "name: downsample_layers.1.ds_ln.bias, lr: 0.0035496424999999997, weight_decay: 0.0\n",
      "name: downsample_layers.1.ds_conv.weight, lr: 0.0035496424999999997, weight_decay: 0.0\n",
      "name: downsample_layers.1.ds_conv.bias, lr: 0.0035496424999999997, weight_decay: 0.0\n",
      "name: stages.1.0.dwconv.weight, lr: 0.0035496424999999997, weight_decay: 1e-05\n",
      "name: stages.1.0.dwconv.bias, lr: 0.0035496424999999997, weight_decay: 0.0\n",
      "name: stages.1.0.layernorm.weight, lr: 0.0035496424999999997, weight_decay: 0.0\n",
      "name: stages.1.0.layernorm.bias, lr: 0.0035496424999999997, weight_decay: 0.0\n",
      "name: stages.1.0.pwconv1.weight, lr: 0.0035496424999999997, weight_decay: 1e-05\n",
      "name: stages.1.0.pwconv1.bias, lr: 0.0035496424999999997, weight_decay: 0.0\n",
      "name: stages.1.0.grn.gamma, lr: 0.0035496424999999997, weight_decay: 0.0\n",
      "name: stages.1.0.grn.beta, lr: 0.0035496424999999997, weight_decay: 0.0\n",
      "name: stages.1.0.pwconv2.weight, lr: 0.0035496424999999997, weight_decay: 1e-05\n",
      "name: stages.1.0.pwconv2.bias, lr: 0.0035496424999999997, weight_decay: 0.0\n",
      "name: stages.1.1.dwconv.weight, lr: 0.0035496424999999997, weight_decay: 1e-05\n",
      "name: stages.1.1.dwconv.bias, lr: 0.0035496424999999997, weight_decay: 0.0\n",
      "name: stages.1.1.layernorm.weight, lr: 0.0035496424999999997, weight_decay: 0.0\n",
      "name: stages.1.1.layernorm.bias, lr: 0.0035496424999999997, weight_decay: 0.0\n",
      "name: stages.1.1.pwconv1.weight, lr: 0.0035496424999999997, weight_decay: 1e-05\n",
      "name: stages.1.1.pwconv1.bias, lr: 0.0035496424999999997, weight_decay: 0.0\n",
      "name: stages.1.1.grn.gamma, lr: 0.0035496424999999997, weight_decay: 0.0\n",
      "name: stages.1.1.grn.beta, lr: 0.0035496424999999997, weight_decay: 0.0\n",
      "name: stages.1.1.pwconv2.weight, lr: 0.0035496424999999997, weight_decay: 1e-05\n",
      "name: stages.1.1.pwconv2.bias, lr: 0.0035496424999999997, weight_decay: 0.0\n",
      "name: stages.1.2.dwconv.weight, lr: 0.0035496424999999997, weight_decay: 1e-05\n",
      "name: stages.1.2.dwconv.bias, lr: 0.0035496424999999997, weight_decay: 0.0\n",
      "name: stages.1.2.layernorm.weight, lr: 0.0035496424999999997, weight_decay: 0.0\n",
      "name: stages.1.2.layernorm.bias, lr: 0.0035496424999999997, weight_decay: 0.0\n",
      "name: stages.1.2.pwconv1.weight, lr: 0.0035496424999999997, weight_decay: 1e-05\n",
      "name: stages.1.2.pwconv1.bias, lr: 0.0035496424999999997, weight_decay: 0.0\n",
      "name: stages.1.2.grn.gamma, lr: 0.0035496424999999997, weight_decay: 0.0\n",
      "name: stages.1.2.grn.beta, lr: 0.0035496424999999997, weight_decay: 0.0\n",
      "name: stages.1.2.pwconv2.weight, lr: 0.0035496424999999997, weight_decay: 1e-05\n",
      "name: stages.1.2.pwconv2.bias, lr: 0.0035496424999999997, weight_decay: 0.0\n",
      "name: downsample_layers.2.ds_ln.weight, lr: 0.004176049999999999, weight_decay: 0.0\n",
      "name: downsample_layers.2.ds_ln.bias, lr: 0.004176049999999999, weight_decay: 0.0\n",
      "name: downsample_layers.2.ds_conv.weight, lr: 0.004176049999999999, weight_decay: 0.0\n",
      "name: downsample_layers.2.ds_conv.bias, lr: 0.004176049999999999, weight_decay: 0.0\n",
      "name: stages.2.0.dwconv.weight, lr: 0.004176049999999999, weight_decay: 1e-05\n",
      "name: stages.2.0.dwconv.bias, lr: 0.004176049999999999, weight_decay: 0.0\n",
      "name: stages.2.0.layernorm.weight, lr: 0.004176049999999999, weight_decay: 0.0\n",
      "name: stages.2.0.layernorm.bias, lr: 0.004176049999999999, weight_decay: 0.0\n",
      "name: stages.2.0.pwconv1.weight, lr: 0.004176049999999999, weight_decay: 1e-05\n",
      "name: stages.2.0.pwconv1.bias, lr: 0.004176049999999999, weight_decay: 0.0\n",
      "name: stages.2.0.grn.gamma, lr: 0.004176049999999999, weight_decay: 0.0\n",
      "name: stages.2.0.grn.beta, lr: 0.004176049999999999, weight_decay: 0.0\n",
      "name: stages.2.0.pwconv2.weight, lr: 0.004176049999999999, weight_decay: 1e-05\n",
      "name: stages.2.0.pwconv2.bias, lr: 0.004176049999999999, weight_decay: 0.0\n",
      "name: stages.2.1.dwconv.weight, lr: 0.004176049999999999, weight_decay: 1e-05\n",
      "name: stages.2.1.dwconv.bias, lr: 0.004176049999999999, weight_decay: 0.0\n",
      "name: stages.2.1.layernorm.weight, lr: 0.004176049999999999, weight_decay: 0.0\n",
      "name: stages.2.1.layernorm.bias, lr: 0.004176049999999999, weight_decay: 0.0\n",
      "name: stages.2.1.pwconv1.weight, lr: 0.004176049999999999, weight_decay: 1e-05\n",
      "name: stages.2.1.pwconv1.bias, lr: 0.004176049999999999, weight_decay: 0.0\n",
      "name: stages.2.1.grn.gamma, lr: 0.004176049999999999, weight_decay: 0.0\n",
      "name: stages.2.1.grn.beta, lr: 0.004176049999999999, weight_decay: 0.0\n",
      "name: stages.2.1.pwconv2.weight, lr: 0.004176049999999999, weight_decay: 1e-05\n",
      "name: stages.2.1.pwconv2.bias, lr: 0.004176049999999999, weight_decay: 0.0\n",
      "name: stages.2.2.dwconv.weight, lr: 0.004176049999999999, weight_decay: 1e-05\n",
      "name: stages.2.2.dwconv.bias, lr: 0.004176049999999999, weight_decay: 0.0\n",
      "name: stages.2.2.layernorm.weight, lr: 0.004176049999999999, weight_decay: 0.0\n",
      "name: stages.2.2.layernorm.bias, lr: 0.004176049999999999, weight_decay: 0.0\n",
      "name: stages.2.2.pwconv1.weight, lr: 0.004176049999999999, weight_decay: 1e-05\n",
      "name: stages.2.2.pwconv1.bias, lr: 0.004176049999999999, weight_decay: 0.0\n",
      "name: stages.2.2.grn.gamma, lr: 0.004176049999999999, weight_decay: 0.0\n",
      "name: stages.2.2.grn.beta, lr: 0.004176049999999999, weight_decay: 0.0\n",
      "name: stages.2.2.pwconv2.weight, lr: 0.004176049999999999, weight_decay: 1e-05\n",
      "name: stages.2.2.pwconv2.bias, lr: 0.004176049999999999, weight_decay: 0.0\n",
      "name: stages.2.3.dwconv.weight, lr: 0.004913, weight_decay: 1e-05\n",
      "name: stages.2.3.dwconv.bias, lr: 0.004913, weight_decay: 0.0\n",
      "name: stages.2.3.layernorm.weight, lr: 0.004913, weight_decay: 0.0\n",
      "name: stages.2.3.layernorm.bias, lr: 0.004913, weight_decay: 0.0\n",
      "name: stages.2.3.pwconv1.weight, lr: 0.004913, weight_decay: 1e-05\n",
      "name: stages.2.3.pwconv1.bias, lr: 0.004913, weight_decay: 0.0\n",
      "name: stages.2.3.grn.gamma, lr: 0.004913, weight_decay: 0.0\n",
      "name: stages.2.3.grn.beta, lr: 0.004913, weight_decay: 0.0\n",
      "name: stages.2.3.pwconv2.weight, lr: 0.004913, weight_decay: 1e-05\n",
      "name: stages.2.3.pwconv2.bias, lr: 0.004913, weight_decay: 0.0\n",
      "name: stages.2.4.dwconv.weight, lr: 0.004913, weight_decay: 1e-05\n",
      "name: stages.2.4.dwconv.bias, lr: 0.004913, weight_decay: 0.0\n",
      "name: stages.2.4.layernorm.weight, lr: 0.004913, weight_decay: 0.0\n",
      "name: stages.2.4.layernorm.bias, lr: 0.004913, weight_decay: 0.0\n",
      "name: stages.2.4.pwconv1.weight, lr: 0.004913, weight_decay: 1e-05\n",
      "name: stages.2.4.pwconv1.bias, lr: 0.004913, weight_decay: 0.0\n",
      "name: stages.2.4.grn.gamma, lr: 0.004913, weight_decay: 0.0\n",
      "name: stages.2.4.grn.beta, lr: 0.004913, weight_decay: 0.0\n",
      "name: stages.2.4.pwconv2.weight, lr: 0.004913, weight_decay: 1e-05\n",
      "name: stages.2.4.pwconv2.bias, lr: 0.004913, weight_decay: 0.0\n",
      "name: stages.2.5.dwconv.weight, lr: 0.004913, weight_decay: 1e-05\n",
      "name: stages.2.5.dwconv.bias, lr: 0.004913, weight_decay: 0.0\n",
      "name: stages.2.5.layernorm.weight, lr: 0.004913, weight_decay: 0.0\n",
      "name: stages.2.5.layernorm.bias, lr: 0.004913, weight_decay: 0.0\n",
      "name: stages.2.5.pwconv1.weight, lr: 0.004913, weight_decay: 1e-05\n",
      "name: stages.2.5.pwconv1.bias, lr: 0.004913, weight_decay: 0.0\n",
      "name: stages.2.5.grn.gamma, lr: 0.004913, weight_decay: 0.0\n",
      "name: stages.2.5.grn.beta, lr: 0.004913, weight_decay: 0.0\n",
      "name: stages.2.5.pwconv2.weight, lr: 0.004913, weight_decay: 1e-05\n",
      "name: stages.2.5.pwconv2.bias, lr: 0.004913, weight_decay: 0.0\n",
      "name: stages.2.6.dwconv.weight, lr: 0.0057799999999999995, weight_decay: 1e-05\n",
      "name: stages.2.6.dwconv.bias, lr: 0.0057799999999999995, weight_decay: 0.0\n",
      "name: stages.2.6.layernorm.weight, lr: 0.0057799999999999995, weight_decay: 0.0\n",
      "name: stages.2.6.layernorm.bias, lr: 0.0057799999999999995, weight_decay: 0.0\n",
      "name: stages.2.6.pwconv1.weight, lr: 0.0057799999999999995, weight_decay: 1e-05\n",
      "name: stages.2.6.pwconv1.bias, lr: 0.0057799999999999995, weight_decay: 0.0\n",
      "name: stages.2.6.grn.gamma, lr: 0.0057799999999999995, weight_decay: 0.0\n",
      "name: stages.2.6.grn.beta, lr: 0.0057799999999999995, weight_decay: 0.0\n",
      "name: stages.2.6.pwconv2.weight, lr: 0.0057799999999999995, weight_decay: 1e-05\n",
      "name: stages.2.6.pwconv2.bias, lr: 0.0057799999999999995, weight_decay: 0.0\n",
      "name: stages.2.7.dwconv.weight, lr: 0.0057799999999999995, weight_decay: 1e-05\n",
      "name: stages.2.7.dwconv.bias, lr: 0.0057799999999999995, weight_decay: 0.0\n",
      "name: stages.2.7.layernorm.weight, lr: 0.0057799999999999995, weight_decay: 0.0\n",
      "name: stages.2.7.layernorm.bias, lr: 0.0057799999999999995, weight_decay: 0.0\n",
      "name: stages.2.7.pwconv1.weight, lr: 0.0057799999999999995, weight_decay: 1e-05\n",
      "name: stages.2.7.pwconv1.bias, lr: 0.0057799999999999995, weight_decay: 0.0\n",
      "name: stages.2.7.grn.gamma, lr: 0.0057799999999999995, weight_decay: 0.0\n",
      "name: stages.2.7.grn.beta, lr: 0.0057799999999999995, weight_decay: 0.0\n",
      "name: stages.2.7.pwconv2.weight, lr: 0.0057799999999999995, weight_decay: 1e-05\n",
      "name: stages.2.7.pwconv2.bias, lr: 0.0057799999999999995, weight_decay: 0.0\n",
      "name: stages.2.8.dwconv.weight, lr: 0.0057799999999999995, weight_decay: 1e-05\n",
      "name: stages.2.8.dwconv.bias, lr: 0.0057799999999999995, weight_decay: 0.0\n",
      "name: stages.2.8.layernorm.weight, lr: 0.0057799999999999995, weight_decay: 0.0\n",
      "name: stages.2.8.layernorm.bias, lr: 0.0057799999999999995, weight_decay: 0.0\n",
      "name: stages.2.8.pwconv1.weight, lr: 0.0057799999999999995, weight_decay: 1e-05\n",
      "name: stages.2.8.pwconv1.bias, lr: 0.0057799999999999995, weight_decay: 0.0\n",
      "name: stages.2.8.grn.gamma, lr: 0.0057799999999999995, weight_decay: 0.0\n",
      "name: stages.2.8.grn.beta, lr: 0.0057799999999999995, weight_decay: 0.0\n",
      "name: stages.2.8.pwconv2.weight, lr: 0.0057799999999999995, weight_decay: 1e-05\n",
      "name: stages.2.8.pwconv2.bias, lr: 0.0057799999999999995, weight_decay: 0.0\n",
      "name: downsample_layers.3.ds_ln.weight, lr: 0.0068, weight_decay: 0.0\n",
      "name: downsample_layers.3.ds_ln.bias, lr: 0.0068, weight_decay: 0.0\n",
      "name: downsample_layers.3.ds_conv.weight, lr: 0.0068, weight_decay: 0.0\n",
      "name: downsample_layers.3.ds_conv.bias, lr: 0.0068, weight_decay: 0.0\n",
      "name: stages.3.0.dwconv.weight, lr: 0.0068, weight_decay: 1e-05\n",
      "name: stages.3.0.dwconv.bias, lr: 0.0068, weight_decay: 0.0\n",
      "name: stages.3.0.layernorm.weight, lr: 0.0068, weight_decay: 0.0\n",
      "name: stages.3.0.layernorm.bias, lr: 0.0068, weight_decay: 0.0\n",
      "name: stages.3.0.pwconv1.weight, lr: 0.0068, weight_decay: 1e-05\n",
      "name: stages.3.0.pwconv1.bias, lr: 0.0068, weight_decay: 0.0\n",
      "name: stages.3.0.grn.gamma, lr: 0.0068, weight_decay: 0.0\n",
      "name: stages.3.0.grn.beta, lr: 0.0068, weight_decay: 0.0\n",
      "name: stages.3.0.pwconv2.weight, lr: 0.0068, weight_decay: 1e-05\n",
      "name: stages.3.0.pwconv2.bias, lr: 0.0068, weight_decay: 0.0\n",
      "name: stages.3.1.dwconv.weight, lr: 0.0068, weight_decay: 1e-05\n",
      "name: stages.3.1.dwconv.bias, lr: 0.0068, weight_decay: 0.0\n",
      "name: stages.3.1.layernorm.weight, lr: 0.0068, weight_decay: 0.0\n",
      "name: stages.3.1.layernorm.bias, lr: 0.0068, weight_decay: 0.0\n",
      "name: stages.3.1.pwconv1.weight, lr: 0.0068, weight_decay: 1e-05\n",
      "name: stages.3.1.pwconv1.bias, lr: 0.0068, weight_decay: 0.0\n",
      "name: stages.3.1.grn.gamma, lr: 0.0068, weight_decay: 0.0\n",
      "name: stages.3.1.grn.beta, lr: 0.0068, weight_decay: 0.0\n",
      "name: stages.3.1.pwconv2.weight, lr: 0.0068, weight_decay: 1e-05\n",
      "name: stages.3.1.pwconv2.bias, lr: 0.0068, weight_decay: 0.0\n",
      "name: stages.3.2.dwconv.weight, lr: 0.0068, weight_decay: 1e-05\n",
      "name: stages.3.2.dwconv.bias, lr: 0.0068, weight_decay: 0.0\n",
      "name: stages.3.2.layernorm.weight, lr: 0.0068, weight_decay: 0.0\n",
      "name: stages.3.2.layernorm.bias, lr: 0.0068, weight_decay: 0.0\n",
      "name: stages.3.2.pwconv1.weight, lr: 0.0068, weight_decay: 1e-05\n",
      "name: stages.3.2.pwconv1.bias, lr: 0.0068, weight_decay: 0.0\n",
      "name: stages.3.2.grn.gamma, lr: 0.0068, weight_decay: 0.0\n",
      "name: stages.3.2.grn.beta, lr: 0.0068, weight_decay: 0.0\n",
      "name: stages.3.2.pwconv2.weight, lr: 0.0068, weight_decay: 1e-05\n",
      "name: stages.3.2.pwconv2.bias, lr: 0.0068, weight_decay: 0.0\n",
      "name: layernorm.weight, lr: 0.008, weight_decay: 0.0\n",
      "name: layernorm.bias, lr: 0.008, weight_decay: 0.0\n",
      "name: fc.weight, lr: 0.008, weight_decay: 1e-05\n",
      "name: fc.bias, lr: 0.008, weight_decay: 0.0\n"
     ]
    }
   ],
   "source": [
    "layer_names, param_groups = LLRD_ConvNeXt(model)\n",
    "for name in layer_names:\n",
    "    print(f\"name: {name}, lr: {param_groups[name]['lr']}, weight_decay: {param_groups[name]['weight_decay']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = [{'params': param,\n",
    "            'lr' : param_groups[name]['lr'],\n",
    "            'weight_decay': param_groups[name]['weight_decay']} for name, param in model.named_parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "\n",
    "optimizer = optim.AdamW(groups)\n",
    "warmup_steps = int(len(train_loader)*(epochs)*0.1)\n",
    "train_steps = len(train_loader)*(epochs)\n",
    "scheduler = CosineWarmupScheduler(optimizer, \n",
    "                                num_warmup_steps=warmup_steps, \n",
    "                                num_training_steps=train_steps,\n",
    "                                num_cycles=0.5,\n",
    "                                min_lr=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 43/43 [01:40<00:00,  2.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.4857, Val_Loss: 3.8331, Total Mean Loss: 4.1594, LR: 0.00030180961250000004, Duration: 102.14 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.2767, Val_Loss: 3.6028, Total Mean Loss: 3.9398, LR: 0.000603519225, Duration: 101.11 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.1784, Val_Loss: 3.2906, Total Mean Loss: 3.7345, LR: 0.0009052288374999999, Duration: 100.95 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.0981, Val_Loss: 3.1178, Total Mean Loss: 3.6079, LR: 0.0012069384500000002, Duration: 100.96 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.9585, Val_Loss: 2.8514, Total Mean Loss: 3.4049, LR: 0.0015086480625, Duration: 100.89 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.9852, Val_Loss: 2.6817, Total Mean Loss: 3.3334, LR: 0.001810357675, Duration: 101.37 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.9467, Val_Loss: 2.6600, Total Mean Loss: 3.3033, LR: 0.0021120672874999995, Duration: 101.11 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.8676, Val_Loss: 2.5285, Total Mean Loss: 3.1981, LR: 0.0024137769, Duration: 101.15 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.8309, Val_Loss: 2.5251, Total Mean Loss: 3.1780, LR: 0.0027154865124999995, Duration: 101.19 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.7421, Val_Loss: 2.3874, Total Mean Loss: 3.0648, LR: 0.0030171961249999997, Duration: 101.10 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.8283, Val_Loss: 2.2168, Total Mean Loss: 3.0225, LR: 0.0030162771582799295, Duration: 101.42 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.7297, Val_Loss: 2.3591, Total Mean Loss: 3.0444, LR: 0.0030135213777391103, Duration: 101.21 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.6912, Val_Loss: 1.9646, Total Mean Loss: 2.8279, LR: 0.003008932140871636, Duration: 101.17 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.6529, Val_Loss: 1.8340, Total Mean Loss: 2.7434, LR: 0.0030025150389557127, Duration: 101.32 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.6479, Val_Loss: 1.8491, Total Mean Loss: 2.7485, LR: 0.002994277890241545, Duration: 100.71 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 43/43 [01:39<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.6033, Val_Loss: 1.7758, Total Mean Loss: 2.6895, LR: 0.0029842307304260057, Duration: 101.26 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.4704, Val_Loss: 1.6802, Total Mean Loss: 2.5753, LR: 0.002972385800425684, Duration: 101.40 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.4921, Val_Loss: 1.6557, Total Mean Loss: 2.5739, LR: 0.002958757531463215, Duration: 101.17 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.3856, Val_Loss: 1.6062, Total Mean Loss: 2.4959, LR: 0.0029433625274850536, Duration: 101.61 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.4014, Val_Loss: 1.6125, Total Mean Loss: 2.5070, LR: 0.0029262195449321292, Duration: 101.10 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.3419, Val_Loss: 1.7016, Total Mean Loss: 2.5217, LR: 0.0029073494698880088, Duration: 101.16 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.3871, Val_Loss: 1.2839, Total Mean Loss: 2.3355, LR: 0.0028867752926324207, Duration: 100.94 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.2332, Val_Loss: 1.4280, Total Mean Loss: 2.3306, LR: 0.0028645220796311432, Duration: 101.11 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.2622, Val_Loss: 1.2784, Total Mean Loss: 2.2703, LR: 0.002840616942996373, Duration: 100.95 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.1698, Val_Loss: 1.1751, Total Mean Loss: 2.1724, LR: 0.0028150890074547952, Duration: 101.24 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.1769, Val_Loss: 1.4946, Total Mean Loss: 2.3358, LR: 0.0027879693748635898, Duration: 100.93 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.2564, Val_Loss: 1.2450, Total Mean Loss: 2.2507, LR: 0.002759291086317611, Duration: 100.94 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.1565, Val_Loss: 1.2095, Total Mean Loss: 2.1830, LR: 0.0027290890818939, Duration: 100.85 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.2302, Val_Loss: 1.1644, Total Mean Loss: 2.1973, LR: 0.0026974001580825848, Duration: 100.96 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.2072, Val_Loss: 1.1382, Total Mean Loss: 2.1727, LR: 0.002664262922956025, Duration: 101.67 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.0303, Val_Loss: 1.1534, Total Mean Loss: 2.0919, LR: 0.0026297177491308235, Duration: 100.85 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.0060, Val_Loss: 1.1256, Total Mean Loss: 2.0658, LR: 0.0025938067245800086, Duration: 100.62 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.0838, Val_Loss: 1.2027, Total Mean Loss: 2.1432, LR: 0.0025565736013553274, Duration: 100.76 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.8839, Val_Loss: 0.9721, Total Mean Loss: 1.9280, LR: 0.0025180637422821056, Duration: 101.10 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.0039, Val_Loss: 1.1629, Total Mean Loss: 2.0834, LR: 0.002478324065691635, Duration: 101.50 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.0878, Val_Loss: 1.1015, Total Mean Loss: 2.0947, LR: 0.002437402988258413, Duration: 101.01 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.0100, Val_Loss: 0.9653, Total Mean Loss: 1.9876, LR: 0.0023953503660118836, Duration: 101.24 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.8891, Val_Loss: 1.1417, Total Mean Loss: 2.0154, LR: 0.002352217433594544, Duration: 101.25 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.0130, Val_Loss: 1.0053, Total Mean Loss: 2.0091, LR: 0.0023080567418404265, Duration: 101.47 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.9046, Val_Loss: 0.9875, Total Mean Loss: 1.9461, LR: 0.0022629220937499996, Duration: 101.51 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.8942, Val_Loss: 0.9617, Total Mean Loss: 1.9279, LR: 0.0022168684789395025, Duration: 101.12 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.8620, Val_Loss: 0.9971, Total Mean Loss: 1.9295, LR: 0.0021699520066445657, Duration: 100.94 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.9190, Val_Loss: 0.9129, Total Mean Loss: 1.9160, LR: 0.0021222298373597525, Duration: 100.83 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.7311, Val_Loss: 0.9670, Total Mean Loss: 1.8491, LR: 0.002073760113197299, Duration: 100.96 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.8629, Val_Loss: 1.0207, Total Mean Loss: 1.9418, LR: 0.0020246018870499097, Duration: 100.90 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.8401, Val_Loss: 1.0051, Total Mean Loss: 1.9226, LR: 0.0019748150506439, Duration: 101.16 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.8495, Val_Loss: 0.9294, Total Mean Loss: 1.8895, LR: 0.001924460261570357, Duration: 100.86 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.8946, Val_Loss: 0.9120, Total Mean Loss: 1.9033, LR: 0.0018735988693832058, Duration: 100.92 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6630, Val_Loss: 0.9833, Total Mean Loss: 1.8231, LR: 0.0018222928408542297, Duration: 101.02 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.7948, Val_Loss: 0.8593, Total Mean Loss: 1.8271, LR: 0.001770604684476104, Duration: 100.75 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.8985, Val_Loss: 0.9468, Total Mean Loss: 1.9227, LR: 0.0017185973743054236, Duration: 100.95 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6911, Val_Loss: 0.9069, Total Mean Loss: 1.7990, LR: 0.0016663342732385212, Duration: 100.97 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.8352, Val_Loss: 0.8474, Total Mean Loss: 1.8413, LR: 0.001613879055813532, Duration: 101.10 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6636, Val_Loss: 0.8349, Total Mean Loss: 1.7492, LR: 0.0015612956306327833, Duration: 100.90 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.7926, Val_Loss: 0.9490, Total Mean Loss: 1.8708, LR: 0.0015086480625, Duration: 100.81 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6866, Val_Loss: 0.8673, Total Mean Loss: 1.7770, LR: 0.0014560004943672173, Duration: 100.97 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6546, Val_Loss: 0.8251, Total Mean Loss: 1.7399, LR: 0.0014034170691864677, Duration: 100.94 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.7557, Val_Loss: 0.8476, Total Mean Loss: 1.8016, LR: 0.0013509618517614792, Duration: 101.20 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5744, Val_Loss: 0.8548, Total Mean Loss: 1.7146, LR: 0.0012986987506945769, Duration: 100.95 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6487, Val_Loss: 0.8826, Total Mean Loss: 1.7656, LR: 0.0012466914405238965, Duration: 101.03 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5643, Val_Loss: 0.8381, Total Mean Loss: 1.7012, LR: 0.0011950032841457703, Duration: 100.89 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6995, Val_Loss: 0.8630, Total Mean Loss: 1.7812, LR: 0.0011436972556167942, Duration: 101.17 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6018, Val_Loss: 0.8151, Total Mean Loss: 1.7084, LR: 0.001092835863429643, Duration: 100.87 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6726, Val_Loss: 0.8560, Total Mean Loss: 1.7643, LR: 0.0010424810743560999, Duration: 101.03 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5349, Val_Loss: 0.8104, Total Mean Loss: 1.6727, LR: 0.0009926942379500903, Duration: 100.96 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6217, Val_Loss: 0.8652, Total Mean Loss: 1.7434, LR: 0.0009435360118027004, Duration: 101.15 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6546, Val_Loss: 0.8654, Total Mean Loss: 1.7600, LR: 0.0008950662876402477, Duration: 101.02 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.7741, Val_Loss: 0.8238, Total Mean Loss: 1.7989, LR: 0.0008473441183554339, Duration: 101.07 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4989, Val_Loss: 0.8279, Total Mean Loss: 1.6634, LR: 0.0008004276460604977, Duration: 101.21 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4742, Val_Loss: 0.8026, Total Mean Loss: 1.6384, LR: 0.0007543740312500002, Duration: 100.71 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4015, Val_Loss: 0.8228, Total Mean Loss: 1.6121, LR: 0.0007092393831595733, Duration: 100.98 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4517, Val_Loss: 0.8172, Total Mean Loss: 1.6344, LR: 0.0006650786914054555, Duration: 101.30 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 73: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3721, Val_Loss: 0.7730, Total Mean Loss: 1.5726, LR: 0.000621945758988116, Duration: 101.20 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 74: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5126, Val_Loss: 0.8010, Total Mean Loss: 1.6568, LR: 0.0005798931367415865, Duration: 101.12 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.7072, Val_Loss: 0.8150, Total Mean Loss: 1.7611, LR: 0.0005389720593083647, Duration: 101.10 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 76: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4851, Val_Loss: 0.8070, Total Mean Loss: 1.6460, LR: 0.0004992323827178947, Duration: 101.53 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 77: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4803, Val_Loss: 0.8138, Total Mean Loss: 1.6470, LR: 0.00046072252364467227, Duration: 101.10 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 78: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4835, Val_Loss: 0.7834, Total Mean Loss: 1.6335, LR: 0.0004234894004199912, Duration: 101.36 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 79: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5316, Val_Loss: 0.7916, Total Mean Loss: 1.6616, LR: 0.0003875783758691761, Duration: 101.50 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 80: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5390, Val_Loss: 0.7889, Total Mean Loss: 1.6639, LR: 0.0003530332020439744, Duration: 101.46 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 81: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5888, Val_Loss: 0.7982, Total Mean Loss: 1.6935, LR: 0.0003198959669174148, Duration: 101.19 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 82: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4856, Val_Loss: 0.7652, Total Mean Loss: 1.6254, LR: 0.00028820704310609976, Duration: 101.45 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 83: 100%|██████████| 43/43 [01:40<00:00,  2.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4215, Val_Loss: 0.7697, Total Mean Loss: 1.5956, LR: 0.0002580050386823888, Duration: 101.82 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 84: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4588, Val_Loss: 0.7848, Total Mean Loss: 1.6218, LR: 0.00022932675013640993, Duration: 100.91 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 85: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4698, Val_Loss: 0.7607, Total Mean Loss: 1.6153, LR: 0.0002022071175452048, Duration: 100.76 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 86: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4794, Val_Loss: 0.7690, Total Mean Loss: 1.6242, LR: 0.00017667918200362686, Duration: 101.07 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 87: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2995, Val_Loss: 0.7477, Total Mean Loss: 1.5236, LR: 0.0001527740453688566, Duration: 101.23 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 88: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4708, Val_Loss: 0.7596, Total Mean Loss: 1.6152, LR: 0.00013052083236757847, Duration: 101.13 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 89: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3472, Val_Loss: 0.7680, Total Mean Loss: 1.5576, LR: 0.00010994665511199122, Duration: 100.87 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 90: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2858, Val_Loss: 0.7600, Total Mean Loss: 1.5229, LR: 9.107658006787078e-05, Duration: 101.10 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 91: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2685, Val_Loss: 0.7563, Total Mean Loss: 1.5124, LR: 7.393359751494645e-05, Duration: 101.01 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 92: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5000, Val_Loss: 0.7729, Total Mean Loss: 1.6365, LR: 5.8538593536784914e-05, Duration: 100.97 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 93: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3398, Val_Loss: 0.7726, Total Mean Loss: 1.5562, LR: 4.491032457431518e-05, Duration: 101.19 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 94: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3352, Val_Loss: 0.7700, Total Mean Loss: 1.5526, LR: 3.306539457399385e-05, Duration: 101.19 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 95: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4751, Val_Loss: 0.7759, Total Mean Loss: 1.6255, LR: 2.3018234758455052e-05, Duration: 100.90 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 96: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5151, Val_Loss: 0.7847, Total Mean Loss: 1.6499, LR: 1.4781086044287217e-05, Duration: 101.00 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 97: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2507, Val_Loss: 0.7785, Total Mean Loss: 1.5146, LR: 8.363984128363606e-06, Duration: 101.08 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 98: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3460, Val_Loss: 0.7753, Total Mean Loss: 1.5607, LR: 3.774747260889585e-06, Duration: 100.90 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3444, Val_Loss: 0.7751, Total Mean Loss: 1.5597, LR: 1.0189667200704374e-06, Duration: 101.22 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 100: 100%|██████████| 43/43 [01:40<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3700, Val_Loss: 0.7749, Total Mean Loss: 1.5725, LR: 1e-07, Duration: 101.69 sec\n",
      "\n",
      "[100 epoch result]\n",
      "       Metric     Value\n",
      "0   Accuracy  0.900000\n",
      "1  Precision  0.912020\n",
      "2     Recall  0.900000\n",
      "3   F1 Score  0.892759\n"
     ]
    }
   ],
   "source": [
    "training_time = 0\n",
    "losses = []\n",
    "val_losses = []\n",
    "lrs = []\n",
    "best_loss = float('inf')\n",
    "model_save = False\n",
    "\n",
    "for i in range(epochs // 100):\n",
    "    for epoch in range(100):\n",
    "        model.train()\n",
    "        start_time = time.time()\n",
    "        running_loss = 0.0\n",
    "        pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch + 1 + i*100}\")\n",
    "        \n",
    "        for _, data in pbar:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            inputs, labels = mixup_fn(inputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "                \n",
    "            loss.backward()\n",
    "            # 그라디언트 클리핑 적용\n",
    "            clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "            optimizer.step()\n",
    "            \n",
    "            # EMA 모델 업데이트, 필요한 경우\n",
    "            if model_ema is not None:\n",
    "                model_ema.update(model)\n",
    "                \n",
    "            scheduler.step()\n",
    "                \n",
    "            lr = optimizer.param_groups[0][\"lr\"]\n",
    "            lrs.append(lr)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        losses.append(epoch_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data in valid_loader:\n",
    "                inputs, labels = data[0].to(device), data[1].to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "        val_loss /= len(valid_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # 모델 저장 조건 수정\n",
    "        total_loss = val_loss + epoch_loss\n",
    "        if total_loss < best_loss:\n",
    "            best_loss = total_loss\n",
    "            # torch.save(model.state_dict(), model_path)\n",
    "            model_save = True\n",
    "            save_text = ' - model saved!'\n",
    "        else:\n",
    "            save_text = ''\n",
    "\n",
    "        epoch_duration = time.time() - start_time\n",
    "        training_time += epoch_duration\n",
    "        \n",
    "        text = f'\\tLoss: {epoch_loss:.4f}, Val_Loss: {val_loss:.4f}, Total Mean Loss: {total_loss/2:.4f}, LR: {lr}, Duration: {epoch_duration:.2f} sec{save_text}'\n",
    "        print(text)\n",
    "\n",
    "    # 예측 수행 및 레이블 저장\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # 혼동 행렬 생성\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    # 예측과 실제 레이블\n",
    "    y_true = all_labels  # 실제 레이블\n",
    "    y_pred = all_preds  # 모델에 의해 예측된 레이블\n",
    "\n",
    "    # 전체 데이터셋에 대한 정확도\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # 평균 정밀도, 리콜, F1-Score ('weighted')\n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "\n",
    "    # 판다스 데이터프레임으로 결과 정리\n",
    "    performance_metrics = pd.DataFrame({\n",
    "        'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "        'Value': [accuracy, precision, recall, f1_score]\n",
    "    })\n",
    "\n",
    "    # 데이터프레임 출력\n",
    "    print(f\"\\n[{i*100+100} epoch result]\\n\", performance_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
