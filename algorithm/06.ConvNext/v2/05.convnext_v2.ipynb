{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 00 - Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "from timm.data import Mixup\n",
    "from timm.utils import ModelEmaV3\n",
    "from timm.loss import LabelSmoothingCrossEntropy, SoftTargetCrossEntropy\n",
    "import transformers\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from torchsummary import summary\n",
    "\n",
    "from convnext_v1 import load_convNext_v1\n",
    "from convnext_v2 import load_convNext\n",
    "import math\n",
    "import warnings\n",
    "from torch.optim.lr_scheduler import _LRScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineWarmupScheduler(_LRScheduler):\n",
    "    def __init__(self, optimizer, num_warmup_steps, num_training_steps, num_cycles=0.5, min_lr=1e-6, last_epoch=-1, verbose=False):\n",
    "        self.num_warmup_steps = num_warmup_steps\n",
    "        self.num_training_steps = num_training_steps\n",
    "        self.num_cycles = num_cycles\n",
    "        self.min_lr = min_lr\n",
    "        self.base_lrs = [group['lr'] for group in optimizer.param_groups]\n",
    "        super().__init__(optimizer, last_epoch, verbose)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if not self._get_lr_called_within_step:\n",
    "            warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
    "                          \"please use `get_last_lr()`.\", UserWarning)\n",
    "        \n",
    "        lrs = []\n",
    "        for base_lr in self.base_lrs:\n",
    "            if self.last_epoch < self.num_warmup_steps:\n",
    "                # Linear warmup\n",
    "                lr = (base_lr - self.min_lr) * self.last_epoch / max(1, self.num_warmup_steps) + self.min_lr\n",
    "            else:\n",
    "                # Cosine annealing\n",
    "                progress = (self.last_epoch - self.num_warmup_steps) / max(1, self.num_training_steps - self.num_warmup_steps)\n",
    "                lr = self.min_lr + (base_lr - self.min_lr) * 0.5 * (1 + math.cos(math.pi * self.num_cycles * 2.0 * progress))\n",
    "            lrs.append(lr)\n",
    "        return lrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 01 - Test ConvNeXt V1(sup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_v1 = load_convNext_v1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "Total Parameters: 27,897,028\n",
      "Trainable Parameters: 27,897,028\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 총 파라미터 수 계산\n",
    "total_params = sum(p.numel() for p in model_v1.parameters())\n",
    "\n",
    "# 학습 가능한 파라미터 수 계산\n",
    "trainable_params = sum(p.numel() for p in model_v1.parameters() if p.requires_grad)\n",
    "\n",
    "print('='*80)\n",
    "print(f\"\\nTotal Parameters: {total_params:,}\")\n",
    "print(f\"Trainable Parameters: {trainable_params:,}\\n\")\n",
    "print('='*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using EMA with decay = 0.9998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "# Transforms 정의하기\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.6,1), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    transforms.RandomErasing(p=1., scale=(0.02, 0.33)),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "data_dir = '../../data/sports'\n",
    "batch_size = 512\n",
    "\n",
    "train_path = data_dir+'/train'\n",
    "valid_path = data_dir+'/valid'\n",
    "test_path = data_dir+'/test'\n",
    "\n",
    "# dataset load\n",
    "train_data = ImageFolder(train_path, transform=train_transform)\n",
    "valid_data = ImageFolder(valid_path, transform=test_transform)\n",
    "test_data = ImageFolder(test_path, transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "device = 'cuda:3'\n",
    "max_norm = 3.0 \n",
    "\n",
    "model_v1.to(device)\n",
    "\n",
    "model_ema = None\n",
    "ema_active = True\n",
    "if ema_active:\n",
    "    ema_decay = 0.9998\n",
    "    model_ema = ModelEmaV3(\n",
    "        model_v1,\n",
    "        decay=ema_decay,\n",
    "    )\n",
    "    print(f\"Using EMA with decay = {ema_decay}\")\n",
    "\n",
    "model_path = ''\n",
    "\n",
    "mixup = True\n",
    "if mixup :\n",
    "    mixup_fn = Mixup(mixup_alpha=.8, \n",
    "                    cutmix_alpha=1., \n",
    "                    prob=1., \n",
    "                    switch_prob=0.5, \n",
    "                    mode='batch',\n",
    "                    label_smoothing=.1,\n",
    "                    num_classes=100)\n",
    "    \n",
    "    criterion = SoftTargetCrossEntropy()\n",
    "else :\n",
    "    criterion = LabelSmoothingCrossEntropy(.1)\n",
    "    \n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "optimizer = optim.AdamW(model_v1.parameters(), lr=4e-3, weight_decay=0.05)\n",
    "warmup_steps = int(len(train_loader)*(epochs)*0.1)\n",
    "train_steps = len(train_loader)*(epochs)\n",
    "scheduler = CosineWarmupScheduler(optimizer, \n",
    "                                num_warmup_steps=warmup_steps, \n",
    "                                num_training_steps=train_steps,\n",
    "                                num_cycles=0.5,\n",
    "                                min_lr=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 27/27 [00:56<00:00,  2.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.4774, Val_Loss: 4.0328, Total Mean Loss: 4.2551, LR: 0.00040008999999999997, Duration: 57.10 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 27/27 [00:55<00:00,  2.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.3636, Val_Loss: 3.9938, Total Mean Loss: 4.1787, LR: 0.0008000799999999999, Duration: 56.24 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 27/27 [00:55<00:00,  2.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.3047, Val_Loss: 3.7656, Total Mean Loss: 4.0352, LR: 0.0012000700000000001, Duration: 56.54 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 27/27 [00:55<00:00,  2.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.2676, Val_Loss: 3.7741, Total Mean Loss: 4.0209, LR: 0.00160006, Duration: 56.30 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.1701, Val_Loss: 3.5921, Total Mean Loss: 3.8811, LR: 0.0020000499999999997, Duration: 56.13 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.1979, Val_Loss: 3.5395, Total Mean Loss: 3.8687, LR: 0.00240004, Duration: 56.20 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.1892, Val_Loss: 3.5515, Total Mean Loss: 3.8703, LR: 0.00280003, Duration: 56.03 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.0846, Val_Loss: 3.3567, Total Mean Loss: 3.7207, LR: 0.0032000199999999996, Duration: 56.08 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 27/27 [00:55<00:00,  2.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.0832, Val_Loss: 3.2658, Total Mean Loss: 3.6745, LR: 0.00360001, Duration: 56.34 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.9733, Val_Loss: 3.2181, Total Mean Loss: 3.5957, LR: 0.004, Duration: 56.18 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.9585, Val_Loss: 3.1206, Total Mean Loss: 3.5395, LR: 0.003998781684496841, Duration: 56.13 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.9332, Val_Loss: 2.9625, Total Mean Loss: 3.4478, LR: 0.003995128222317136, Duration: 56.06 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 27/27 [00:54<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.9000, Val_Loss: 2.9625, Total Mean Loss: 3.4313, LR: 0.003989044064641779, Duration: 55.92 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.8427, Val_Loss: 2.8141, Total Mean Loss: 3.3284, LR: 0.0039805366240797035, Duration: 56.11 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 27/27 [00:54<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.7609, Val_Loss: 2.6553, Total Mean Loss: 3.2081, LR: 0.003969616265636766, Duration: 55.97 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 27/27 [00:54<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.8194, Val_Loss: 2.6457, Total Mean Loss: 3.2325, LR: 0.003956296294087574, Duration: 55.85 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 27/27 [00:54<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.6426, Val_Loss: 2.5182, Total Mean Loss: 3.0804, LR: 0.003940592937765679, Duration: 55.96 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.6091, Val_Loss: 2.3043, Total Mean Loss: 2.9567, LR: 0.003922525328791841, Duration: 56.04 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 27/27 [00:54<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.7503, Val_Loss: 2.3364, Total Mean Loss: 3.0433, LR: 0.0039021154797644923, Duration: 55.73 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 27/27 [00:54<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.5655, Val_Loss: 2.2997, Total Mean Loss: 2.9326, LR: 0.0038793882569407774, Duration: 55.96 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 27/27 [00:54<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.5359, Val_Loss: 2.1852, Total Mean Loss: 2.8605, LR: 0.0038543713499408464, Duration: 55.90 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.5495, Val_Loss: 2.2021, Total Mean Loss: 2.8758, LR: 0.003827095238012319, Duration: 56.02 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 27/27 [00:54<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.4314, Val_Loss: 1.8701, Total Mean Loss: 2.6508, LR: 0.003797593152896019, Duration: 55.81 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 27/27 [00:54<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.5395, Val_Loss: 2.0334, Total Mean Loss: 2.7864, LR: 0.0037659010383382105, Duration: 55.96 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 27/27 [00:54<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.3701, Val_Loss: 1.9876, Total Mean Loss: 2.6788, LR: 0.003732057506298688, Duration: 55.82 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 27/27 [00:54<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.3378, Val_Loss: 1.7445, Total Mean Loss: 2.5411, LR: 0.0036961037899080436, Duration: 55.90 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 27/27 [00:54<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.2249, Val_Loss: 1.6983, Total Mean Loss: 2.4616, LR: 0.0036580836932314552, Duration: 55.84 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 27/27 [00:55<00:00,  2.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.3712, Val_Loss: 1.7954, Total Mean Loss: 2.5833, LR: 0.003618043537900176, Duration: 56.32 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 27/27 [00:54<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.2627, Val_Loss: 1.6444, Total Mean Loss: 2.4536, LR: 0.003576032106675763, Duration: 55.88 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 27/27 [00:54<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.2453, Val_Loss: 1.6847, Total Mean Loss: 2.4650, LR: 0.0035321005840157995, Duration: 55.90 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|██████████| 27/27 [00:54<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.0739, Val_Loss: 1.4796, Total Mean Loss: 2.2767, LR: 0.0034863024937135142, Duration: 55.85 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.2991, Val_Loss: 1.4953, Total Mean Loss: 2.3972, LR: 0.003438693633687285, Duration: 56.03 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|██████████| 27/27 [00:54<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.0333, Val_Loss: 1.4960, Total Mean Loss: 2.2647, LR: 0.0033893320079994714, Duration: 55.97 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.1110, Val_Loss: 1.4444, Total Mean Loss: 2.2777, LR: 0.003338277756187398, Duration: 56.15 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.1195, Val_Loss: 1.5030, Total Mean Loss: 2.3113, LR: 0.003285593079992594, Duration: 56.06 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.1100, Val_Loss: 1.3832, Total Mean Loss: 2.2466, LR: 0.00323134216757755, Duration: 56.12 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.0795, Val_Loss: 1.4472, Total Mean Loss: 2.2634, LR: 0.0031755911153223313, Duration: 56.11 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.9890, Val_Loss: 1.2568, Total Mean Loss: 2.1229, LR: 0.0031184078472963196, Duration: 56.06 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|██████████| 27/27 [00:54<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.1263, Val_Loss: 1.3073, Total Mean Loss: 2.2168, LR: 0.003059862032503198, Duration: 55.97 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.0529, Val_Loss: 1.2554, Total Mean Loss: 2.1542, LR: 0.0030000249999999995, Duration: 56.05 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.7628, Val_Loss: 1.1780, Total Mean Loss: 1.9704, LR: 0.002938969651993642, Duration: 56.07 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42: 100%|██████████| 27/27 [00:54<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.8932, Val_Loss: 1.1685, Total Mean Loss: 2.0308, LR: 0.002876770375020815, Duration: 55.86 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.8992, Val_Loss: 1.2290, Total Mean Loss: 2.0641, LR: 0.0028135029493194467, Duration: 56.12 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44: 100%|██████████| 27/27 [00:55<00:00,  2.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.9412, Val_Loss: 1.2404, Total Mean Loss: 2.0908, LR: 0.0027492444565021534, Duration: 56.27 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45: 100%|██████████| 27/27 [00:54<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.9296, Val_Loss: 1.1521, Total Mean Loss: 2.0409, LR: 0.0026840731856441714, Duration: 55.90 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6970, Val_Loss: 1.0895, Total Mean Loss: 1.8932, LR: 0.0026180685379001757, Duration: 56.01 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47: 100%|██████████| 27/27 [00:54<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5881, Val_Loss: 0.9863, Total Mean Loss: 1.7872, LR: 0.002551310929766207, Duration: 55.95 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48: 100%|██████████| 27/27 [00:54<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.9589, Val_Loss: 1.0798, Total Mean Loss: 2.0193, LR: 0.002483881695104555, Duration: 55.89 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 27/27 [00:54<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.7937, Val_Loss: 1.0734, Total Mean Loss: 1.9336, LR: 0.0024158629860509774, Duration: 55.96 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.8662, Val_Loss: 1.0298, Total Mean Loss: 1.9480, LR: 0.0023473376729249776, Duration: 56.12 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51: 100%|██████████| 27/27 [00:54<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6298, Val_Loss: 1.0176, Total Mean Loss: 1.8237, LR: 0.0022783892432650826, Duration: 55.98 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2934, Val_Loss: 0.9682, Total Mean Loss: 1.6308, LR: 0.0022091017001121434, Duration: 56.07 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53: 100%|██████████| 27/27 [00:54<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.7997, Val_Loss: 1.0152, Total Mean Loss: 1.9075, LR: 0.002139559459664563, Duration: 55.89 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5031, Val_Loss: 0.9415, Total Mean Loss: 1.7223, LR: 0.0020698472484301667, Duration: 56.06 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55: 100%|██████████| 27/27 [00:54<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3984, Val_Loss: 0.9141, Total Mean Loss: 1.6562, LR: 0.0020000499999999997, Duration: 55.90 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56: 100%|██████████| 27/27 [00:54<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5740, Val_Loss: 0.8883, Total Mean Loss: 1.7312, LR: 0.0019302527515698336, Duration: 55.99 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57: 100%|██████████| 27/27 [00:54<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6690, Val_Loss: 0.9551, Total Mean Loss: 1.8120, LR: 0.0018605405403354365, Duration: 55.98 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6134, Val_Loss: 0.9461, Total Mean Loss: 1.7797, LR: 0.0017909982998878568, Duration: 56.05 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5852, Val_Loss: 0.9271, Total Mean Loss: 1.7561, LR: 0.0017217107567349176, Duration: 56.02 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60: 100%|██████████| 27/27 [00:55<00:00,  2.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5558, Val_Loss: 0.8173, Total Mean Loss: 1.6865, LR: 0.0016527623270750228, Duration: 56.21 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61: 100%|██████████| 27/27 [00:54<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5551, Val_Loss: 0.8222, Total Mean Loss: 1.6887, LR: 0.0015842370139490226, Duration: 55.97 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62: 100%|██████████| 27/27 [00:54<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5135, Val_Loss: 0.8065, Total Mean Loss: 1.6600, LR: 0.0015162183048954448, Duration: 55.95 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4218, Val_Loss: 0.8387, Total Mean Loss: 1.6302, LR: 0.0014487890702337925, Duration: 56.10 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3691, Val_Loss: 0.8183, Total Mean Loss: 1.5937, LR: 0.001382031462099824, Duration: 56.03 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4415, Val_Loss: 0.8186, Total Mean Loss: 1.6301, LR: 0.001316026814355829, Duration: 56.01 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4425, Val_Loss: 0.8346, Total Mean Loss: 1.6386, LR: 0.0012508555434978467, Duration: 56.06 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3497, Val_Loss: 0.7826, Total Mean Loss: 1.5661, LR: 0.0011865970506805537, Duration: 56.03 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68: 100%|██████████| 27/27 [00:54<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3265, Val_Loss: 0.7799, Total Mean Loss: 1.5532, LR: 0.0011233296249791845, Duration: 55.93 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5047, Val_Loss: 0.8032, Total Mean Loss: 1.6540, LR: 0.0010611303480063583, Duration: 56.02 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70: 100%|██████████| 27/27 [00:55<00:00,  2.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0608, Val_Loss: 0.7569, Total Mean Loss: 1.4088, LR: 0.0010000750000000004, Duration: 56.27 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2896, Val_Loss: 0.7690, Total Mean Loss: 1.5293, LR: 0.000940237967496802, Duration: 56.04 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72: 100%|██████████| 27/27 [00:54<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3211, Val_Loss: 0.7482, Total Mean Loss: 1.5347, LR: 0.0008816921527036801, Duration: 55.96 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 73: 100%|██████████| 27/27 [00:54<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3094, Val_Loss: 0.7497, Total Mean Loss: 1.5295, LR: 0.0008245088846776685, Duration: 55.97 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 74: 100%|██████████| 27/27 [00:54<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3828, Val_Loss: 0.7911, Total Mean Loss: 1.5869, LR: 0.0007687578324224496, Duration: 55.95 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3892, Val_Loss: 0.7802, Total Mean Loss: 1.5847, LR: 0.0007145069200074055, Duration: 56.00 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 76: 100%|██████████| 27/27 [00:55<00:00,  2.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3461, Val_Loss: 0.7231, Total Mean Loss: 1.5346, LR: 0.000661822243812602, Duration: 56.24 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 77: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1545, Val_Loss: 0.7566, Total Mean Loss: 1.4556, LR: 0.0006107679920005282, Duration: 56.03 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 78: 100%|██████████| 27/27 [00:55<00:00,  2.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2703, Val_Loss: 0.7705, Total Mean Loss: 1.5204, LR: 0.0005614063663127149, Duration: 56.34 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 79: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3561, Val_Loss: 0.7499, Total Mean Loss: 1.5530, LR: 0.000513797506286485, Duration: 56.12 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 80: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2656, Val_Loss: 0.7683, Total Mean Loss: 1.5169, LR: 0.00046799941598420013, Duration: 56.14 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 81: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2729, Val_Loss: 0.7432, Total Mean Loss: 1.5081, LR: 0.0004240678933242365, Duration: 56.04 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 82: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2517, Val_Loss: 0.7783, Total Mean Loss: 1.5150, LR: 0.00038205646209982404, Duration: 56.18 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 83: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2930, Val_Loss: 0.7482, Total Mean Loss: 1.5206, LR: 0.0003420163067685445, Duration: 56.11 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 84: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1917, Val_Loss: 0.7575, Total Mean Loss: 1.4746, LR: 0.0003039962100919559, Duration: 56.06 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 85: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2999, Val_Loss: 0.7637, Total Mean Loss: 1.5318, LR: 0.0002680424937013118, Duration: 56.07 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 86: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1889, Val_Loss: 0.7282, Total Mean Loss: 1.4586, LR: 0.00023419896166178896, Duration: 56.07 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 87: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1473, Val_Loss: 0.7745, Total Mean Loss: 1.4609, LR: 0.0002025068471039813, Duration: 56.04 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 88: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1531, Val_Loss: 0.8005, Total Mean Loss: 1.4768, LR: 0.00017300476198768016, Duration: 56.04 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 89: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2986, Val_Loss: 0.7652, Total Mean Loss: 1.5319, LR: 0.00014572865005915372, Duration: 56.19 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 90: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3740, Val_Loss: 0.7677, Total Mean Loss: 1.5708, LR: 0.00012071174305922266, Duration: 56.13 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 91: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2788, Val_Loss: 0.7759, Total Mean Loss: 1.5273, LR: 9.79845202355077e-05, Duration: 56.12 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 92: 100%|██████████| 27/27 [00:55<00:00,  2.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1289, Val_Loss: 0.7813, Total Mean Loss: 1.4551, LR: 7.757467120815912e-05, Duration: 56.22 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 93: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0349, Val_Loss: 0.7657, Total Mean Loss: 1.4003, LR: 5.950706223432085e-05, Duration: 56.15 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 94: 100%|██████████| 27/27 [00:54<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0171, Val_Loss: 0.7645, Total Mean Loss: 1.3908, LR: 4.3803705912425316e-05, Duration: 55.99 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 95: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0323, Val_Loss: 0.7693, Total Mean Loss: 1.4008, LR: 3.0483734363234566e-05, Duration: 56.10 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 96: 100%|██████████| 27/27 [00:54<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2488, Val_Loss: 0.7582, Total Mean Loss: 1.5035, LR: 1.9563375920296352e-05, Duration: 55.99 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 97: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.8898, Val_Loss: 0.7615, Total Mean Loss: 1.3256, LR: 1.1055935358221834e-05, Duration: 56.10 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 98: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1196, Val_Loss: 0.7607, Total Mean Loss: 1.4401, LR: 4.971777682864596e-06, Duration: 56.02 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0866, Val_Loss: 0.7598, Total Mean Loss: 1.4232, LR: 1.3183155031594304e-06, Duration: 56.12 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 100: 100%|██████████| 27/27 [00:55<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1089, Val_Loss: 0.7597, Total Mean Loss: 1.4343, LR: 1e-07, Duration: 56.03 sec\n",
      "\n",
      "[100 epoch result]\n",
      "       Metric     Value\n",
      "0   Accuracy  0.876000\n",
      "1  Precision  0.896143\n",
      "2     Recall  0.876000\n",
      "3   F1 Score  0.872440\n"
     ]
    }
   ],
   "source": [
    "training_time = 0\n",
    "losses = []\n",
    "val_losses = []\n",
    "lrs = []\n",
    "best_loss = float('inf')\n",
    "\n",
    "# GradScaler 초기화\n",
    "scaler = GradScaler()\n",
    "\n",
    "for i in range(epochs // 100):\n",
    "    for epoch in range(100):\n",
    "        model_v1.train()\n",
    "        start_time = time.time()\n",
    "        running_loss = 0.0\n",
    "        pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch + 1 + i*100}\")\n",
    "        \n",
    "        for _, data in pbar:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            inputs, labels = mixup_fn(inputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # AutoCast 적용\n",
    "            with autocast():\n",
    "                outputs = model_v1(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "            # 스케일링된 그라디언트 계산\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # 그라디언트 클리핑 전에 스케일링 제거\n",
    "            scaler.unscale_(optimizer)\n",
    "            clip_grad_norm_(model_v1.parameters(), max_norm=max_norm)\n",
    "\n",
    "            # 옵티마이저 스텝 및 스케일러 업데이트\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            # EMA 모델 업데이트\n",
    "            if model_ema is not None:\n",
    "                model_ema.update(model_v1)\n",
    "                \n",
    "            scheduler.step()\n",
    "                \n",
    "            lr = optimizer.param_groups[0][\"lr\"]\n",
    "            lrs.append(lr)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        losses.append(epoch_loss)        \n",
    "\n",
    "        model_v1.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data in valid_loader:\n",
    "                inputs, labels = data[0].to(device), data[1].to(device)\n",
    "                outputs = model_v1(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "        val_loss /= len(valid_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # 모델 저장\n",
    "        total_loss = val_loss + epoch_loss\n",
    "        if total_loss < best_loss:\n",
    "            best_loss = total_loss\n",
    "            model_save = False\n",
    "            if model_save:\n",
    "                torch.save(model_v1.state_dict(), model_path)\n",
    "\n",
    "        epoch_duration = time.time() - start_time\n",
    "        training_time += epoch_duration\n",
    "        \n",
    "        text = f'\\tLoss: {epoch_loss:.4f}, Val_Loss: {val_loss:.4f}, Total Mean Loss: {total_loss/2:.4f}, LR: {lr}, Duration: {epoch_duration:.2f} sec'\n",
    "        \n",
    "        if model_save:\n",
    "            text += f' - model saved!'\n",
    "            model_save = False\n",
    "\n",
    "        print(text)\n",
    "\n",
    "    # 예측 수행 및 레이블 저장\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model_v1(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # 혼동 행렬 생성\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    # 예측과 실제 레이블\n",
    "    y_true = all_labels  # 실제 레이블\n",
    "    y_pred = all_preds  # 모델에 의해 예측된 레이블\n",
    "\n",
    "    # 전체 데이터셋에 대한 정확도\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # 평균 정밀도, 리콜, F1-Score ('weighted')\n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "\n",
    "    # 판다스 데이터프레임으로 결과 정리\n",
    "    performance_metrics = pd.DataFrame({\n",
    "        'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "        'Value': [accuracy, precision, recall, f1_score]\n",
    "    })\n",
    "\n",
    "    # 데이터프레임 출력\n",
    "    print(f\"\\n[{i*100+100} epoch result]\\n\", performance_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_v1.cpu()\n",
    "del model_v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 02 - Test ConvNeXt V2(sup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "Total Parameters: 27,943,396\n",
      "Trainable Parameters: 27,943,396\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "model_v2 = load_convNext()\n",
    "\n",
    "# 총 파라미터 수 계산\n",
    "total_params = sum(p.numel() for p in model_v2.parameters())\n",
    "\n",
    "# 학습 가능한 파라미터 수 계산\n",
    "trainable_params = sum(p.numel() for p in model_v2.parameters() if p.requires_grad)\n",
    "\n",
    "print('='*80)\n",
    "print(f\"\\nTotal Parameters: {total_params:,}\")\n",
    "print(f\"Trainable Parameters: {trainable_params:,}\\n\")\n",
    "print('='*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using EMA with decay = 0.9998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "model_v2.to(device)\n",
    "\n",
    "model_ema = None\n",
    "ema_active = True\n",
    "if ema_active:\n",
    "    ema_decay = 0.9998\n",
    "    model_ema = ModelEmaV3(\n",
    "        model_v2,\n",
    "        decay=ema_decay,\n",
    "    )\n",
    "    print(f\"Using EMA with decay = {ema_decay}\")\n",
    "\n",
    "model_path = ''\n",
    "\n",
    "mixup = True\n",
    "if mixup :\n",
    "    mixup_fn = Mixup(mixup_alpha=.8, \n",
    "                    cutmix_alpha=1., \n",
    "                    prob=1., \n",
    "                    switch_prob=0.5, \n",
    "                    mode='batch',\n",
    "                    label_smoothing=.1,\n",
    "                    num_classes=100)\n",
    "    \n",
    "    criterion = SoftTargetCrossEntropy()\n",
    "else :\n",
    "    criterion = LabelSmoothingCrossEntropy(.1)\n",
    "    \n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "optimizer = optim.AdamW(model_v2.parameters(), lr=4e-3, weight_decay=0.05)\n",
    "warmup_steps = int(len(train_loader)*(epochs)*0.1)\n",
    "train_steps = len(train_loader)*(epochs)\n",
    "scheduler = CosineWarmupScheduler(optimizer, \n",
    "                                num_warmup_steps=warmup_steps, \n",
    "                                num_training_steps=train_steps,\n",
    "                                num_cycles=0.5,\n",
    "                                min_lr=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.4953, Val_Loss: 3.9957, Total Mean Loss: 4.2455, LR: 0.00040008999999999997, Duration: 67.57 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.3383, Val_Loss: 3.8066, Total Mean Loss: 4.0724, LR: 0.0008000799999999999, Duration: 67.57 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 27/27 [01:06<00:00,  2.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.2863, Val_Loss: 3.7261, Total Mean Loss: 4.0062, LR: 0.0012000700000000001, Duration: 67.70 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 27/27 [01:06<00:00,  2.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.2008, Val_Loss: 3.6685, Total Mean Loss: 3.9346, LR: 0.00160006, Duration: 67.98 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 27/27 [01:06<00:00,  2.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.2399, Val_Loss: 3.5638, Total Mean Loss: 3.9018, LR: 0.0020000499999999997, Duration: 67.65 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 27/27 [01:06<00:00,  2.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.1107, Val_Loss: 3.3985, Total Mean Loss: 3.7546, LR: 0.00240004, Duration: 67.89 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 27/27 [01:06<00:00,  2.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.0405, Val_Loss: 3.2016, Total Mean Loss: 3.6211, LR: 0.00280003, Duration: 67.75 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 27/27 [01:06<00:00,  2.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.0470, Val_Loss: 3.0241, Total Mean Loss: 3.5355, LR: 0.0032000199999999996, Duration: 67.74 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 27/27 [01:07<00:00,  2.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.0337, Val_Loss: 3.0783, Total Mean Loss: 3.5560, LR: 0.00360001, Duration: 68.12 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 27/27 [01:06<00:00,  2.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.9008, Val_Loss: 2.8049, Total Mean Loss: 3.3528, LR: 0.004, Duration: 67.85 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 27/27 [01:06<00:00,  2.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.7319, Val_Loss: 2.6172, Total Mean Loss: 3.1745, LR: 0.003998781684496841, Duration: 67.93 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.7359, Val_Loss: 2.5972, Total Mean Loss: 3.1665, LR: 0.003995128222317136, Duration: 67.60 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.7365, Val_Loss: 2.3938, Total Mean Loss: 3.0652, LR: 0.003989044064641779, Duration: 67.60 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.5635, Val_Loss: 2.3816, Total Mean Loss: 2.9726, LR: 0.0039805366240797035, Duration: 67.60 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.7393, Val_Loss: 2.2984, Total Mean Loss: 3.0189, LR: 0.003969616265636766, Duration: 67.59 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.5639, Val_Loss: 2.1167, Total Mean Loss: 2.8403, LR: 0.003956296294087574, Duration: 67.48 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.6771, Val_Loss: 2.0762, Total Mean Loss: 2.8767, LR: 0.003940592937765679, Duration: 67.58 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.4355, Val_Loss: 2.0092, Total Mean Loss: 2.7224, LR: 0.003922525328791841, Duration: 67.61 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.4700, Val_Loss: 2.1298, Total Mean Loss: 2.7999, LR: 0.0039021154797644923, Duration: 67.51 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.5844, Val_Loss: 2.1278, Total Mean Loss: 2.8561, LR: 0.0038793882569407774, Duration: 67.55 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.4391, Val_Loss: 1.9167, Total Mean Loss: 2.6779, LR: 0.0038543713499408464, Duration: 67.53 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.3674, Val_Loss: 1.7117, Total Mean Loss: 2.5396, LR: 0.003827095238012319, Duration: 67.48 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.1984, Val_Loss: 1.6721, Total Mean Loss: 2.4352, LR: 0.003797593152896019, Duration: 67.56 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.3351, Val_Loss: 1.7236, Total Mean Loss: 2.5294, LR: 0.0037659010383382105, Duration: 67.45 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 27/27 [01:06<00:00,  2.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.2980, Val_Loss: 1.5397, Total Mean Loss: 2.4188, LR: 0.003732057506298688, Duration: 67.72 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 27/27 [01:06<00:00,  2.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.2775, Val_Loss: 1.6634, Total Mean Loss: 2.4704, LR: 0.0036961037899080436, Duration: 67.67 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.1404, Val_Loss: 1.6914, Total Mean Loss: 2.4159, LR: 0.0036580836932314552, Duration: 67.52 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.2671, Val_Loss: 1.5354, Total Mean Loss: 2.4012, LR: 0.003618043537900176, Duration: 67.54 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.9860, Val_Loss: 1.4390, Total Mean Loss: 2.2125, LR: 0.003576032106675763, Duration: 67.41 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.1470, Val_Loss: 1.3110, Total Mean Loss: 2.2290, LR: 0.0035321005840157995, Duration: 67.41 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.8700, Val_Loss: 1.2069, Total Mean Loss: 2.0385, LR: 0.0034863024937135142, Duration: 67.52 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.0219, Val_Loss: 1.3896, Total Mean Loss: 2.2057, LR: 0.003438693633687285, Duration: 67.56 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.0175, Val_Loss: 1.3374, Total Mean Loss: 2.1774, LR: 0.0033893320079994714, Duration: 67.50 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.8511, Val_Loss: 1.1852, Total Mean Loss: 2.0182, LR: 0.003338277756187398, Duration: 67.57 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.0255, Val_Loss: 1.2675, Total Mean Loss: 2.1465, LR: 0.003285593079992594, Duration: 67.41 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.9926, Val_Loss: 1.1701, Total Mean Loss: 2.0814, LR: 0.00323134216757755, Duration: 67.49 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.8710, Val_Loss: 1.1276, Total Mean Loss: 1.9993, LR: 0.0031755911153223313, Duration: 67.41 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.8884, Val_Loss: 1.0406, Total Mean Loss: 1.9645, LR: 0.0031184078472963196, Duration: 67.40 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|██████████| 27/27 [01:06<00:00,  2.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6145, Val_Loss: 1.1127, Total Mean Loss: 1.8636, LR: 0.003059862032503198, Duration: 67.70 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.9169, Val_Loss: 1.0917, Total Mean Loss: 2.0043, LR: 0.0030000249999999995, Duration: 67.52 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.9894, Val_Loss: 1.0460, Total Mean Loss: 2.0177, LR: 0.002938969651993642, Duration: 67.55 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.7877, Val_Loss: 1.0938, Total Mean Loss: 1.9408, LR: 0.002876770375020815, Duration: 67.37 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5092, Val_Loss: 0.9956, Total Mean Loss: 1.7524, LR: 0.0028135029493194467, Duration: 67.48 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6443, Val_Loss: 1.0065, Total Mean Loss: 1.8254, LR: 0.0027492444565021534, Duration: 67.49 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45: 100%|██████████| 27/27 [01:06<00:00,  2.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4927, Val_Loss: 0.9553, Total Mean Loss: 1.7240, LR: 0.0026840731856441714, Duration: 67.69 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46: 100%|██████████| 27/27 [01:07<00:00,  2.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5427, Val_Loss: 0.9239, Total Mean Loss: 1.7333, LR: 0.0026180685379001757, Duration: 68.13 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47: 100%|██████████| 27/27 [01:06<00:00,  2.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6331, Val_Loss: 0.9464, Total Mean Loss: 1.7898, LR: 0.002551310929766207, Duration: 67.36 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6876, Val_Loss: 0.8948, Total Mean Loss: 1.7912, LR: 0.002483881695104555, Duration: 67.42 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6114, Val_Loss: 0.9237, Total Mean Loss: 1.7676, LR: 0.0024158629860509774, Duration: 67.55 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50: 100%|██████████| 27/27 [01:06<00:00,  2.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6508, Val_Loss: 0.9648, Total Mean Loss: 1.8078, LR: 0.0023473376729249776, Duration: 67.33 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5695, Val_Loss: 0.9561, Total Mean Loss: 1.7628, LR: 0.0022783892432650826, Duration: 67.48 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4391, Val_Loss: 0.8177, Total Mean Loss: 1.6284, LR: 0.0022091017001121434, Duration: 67.44 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4974, Val_Loss: 0.8427, Total Mean Loss: 1.6701, LR: 0.002139559459664563, Duration: 67.52 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4575, Val_Loss: 0.7762, Total Mean Loss: 1.6168, LR: 0.0020698472484301667, Duration: 67.46 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5227, Val_Loss: 0.8147, Total Mean Loss: 1.6687, LR: 0.0020000499999999997, Duration: 67.56 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56: 100%|██████████| 27/27 [01:06<00:00,  2.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4267, Val_Loss: 0.8300, Total Mean Loss: 1.6284, LR: 0.0019302527515698336, Duration: 67.35 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4398, Val_Loss: 0.7255, Total Mean Loss: 1.5827, LR: 0.0018605405403354365, Duration: 67.56 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2488, Val_Loss: 0.8539, Total Mean Loss: 1.5514, LR: 0.0017909982998878568, Duration: 67.61 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1985, Val_Loss: 0.7956, Total Mean Loss: 1.4970, LR: 0.0017217107567349176, Duration: 67.54 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3759, Val_Loss: 0.8497, Total Mean Loss: 1.6128, LR: 0.0016527623270750228, Duration: 67.58 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61: 100%|██████████| 27/27 [01:06<00:00,  2.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2043, Val_Loss: 0.8578, Total Mean Loss: 1.5311, LR: 0.0015842370139490226, Duration: 67.73 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3755, Val_Loss: 0.8482, Total Mean Loss: 1.6119, LR: 0.0015162183048954448, Duration: 67.44 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1355, Val_Loss: 0.8108, Total Mean Loss: 1.4731, LR: 0.0014487890702337925, Duration: 67.54 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1350, Val_Loss: 0.7789, Total Mean Loss: 1.4570, LR: 0.001382031462099824, Duration: 67.42 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1932, Val_Loss: 0.8184, Total Mean Loss: 1.5058, LR: 0.001316026814355829, Duration: 67.60 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1584, Val_Loss: 0.7591, Total Mean Loss: 1.4588, LR: 0.0012508555434978467, Duration: 67.54 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67: 100%|██████████| 27/27 [01:06<00:00,  2.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2821, Val_Loss: 0.8223, Total Mean Loss: 1.5522, LR: 0.0011865970506805537, Duration: 67.65 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2484, Val_Loss: 0.8220, Total Mean Loss: 1.5352, LR: 0.0011233296249791845, Duration: 67.50 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1439, Val_Loss: 0.7533, Total Mean Loss: 1.4486, LR: 0.0010611303480063583, Duration: 67.41 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2082, Val_Loss: 0.7790, Total Mean Loss: 1.4936, LR: 0.0010000750000000004, Duration: 67.47 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2692, Val_Loss: 0.7999, Total Mean Loss: 1.5346, LR: 0.000940237967496802, Duration: 67.54 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0755, Val_Loss: 0.7652, Total Mean Loss: 1.4203, LR: 0.0008816921527036801, Duration: 67.51 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 73: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1475, Val_Loss: 0.7764, Total Mean Loss: 1.4620, LR: 0.0008245088846776685, Duration: 67.45 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 74: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1947, Val_Loss: 0.7709, Total Mean Loss: 1.4828, LR: 0.0007687578324224496, Duration: 67.51 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1150, Val_Loss: 0.7763, Total Mean Loss: 1.4456, LR: 0.0007145069200074055, Duration: 67.46 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 76: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2568, Val_Loss: 0.7770, Total Mean Loss: 1.5169, LR: 0.000661822243812602, Duration: 67.61 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 77: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0956, Val_Loss: 0.8004, Total Mean Loss: 1.4480, LR: 0.0006107679920005282, Duration: 67.49 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 78: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2704, Val_Loss: 0.8184, Total Mean Loss: 1.5444, LR: 0.0005614063663127149, Duration: 67.54 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 79: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1208, Val_Loss: 0.7864, Total Mean Loss: 1.4536, LR: 0.000513797506286485, Duration: 67.39 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 80: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2827, Val_Loss: 0.7837, Total Mean Loss: 1.5332, LR: 0.00046799941598420013, Duration: 67.39 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 81: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.9240, Val_Loss: 0.7633, Total Mean Loss: 1.3437, LR: 0.0004240678933242365, Duration: 67.58 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 82: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1949, Val_Loss: 0.7407, Total Mean Loss: 1.4678, LR: 0.00038205646209982404, Duration: 67.43 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 83: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.9439, Val_Loss: 0.7818, Total Mean Loss: 1.3629, LR: 0.0003420163067685445, Duration: 67.42 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 84: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0535, Val_Loss: 0.7407, Total Mean Loss: 1.3971, LR: 0.0003039962100919559, Duration: 67.52 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 85: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0102, Val_Loss: 0.7809, Total Mean Loss: 1.3955, LR: 0.0002680424937013118, Duration: 67.37 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 86: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3759, Val_Loss: 0.7563, Total Mean Loss: 1.5661, LR: 0.00023419896166178896, Duration: 67.55 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 87: 100%|██████████| 27/27 [01:06<00:00,  2.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2072, Val_Loss: 0.7671, Total Mean Loss: 1.4871, LR: 0.0002025068471039813, Duration: 67.28 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 88: 100%|██████████| 27/27 [01:06<00:00,  2.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0811, Val_Loss: 0.8004, Total Mean Loss: 1.4408, LR: 0.00017300476198768016, Duration: 67.27 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 89: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0743, Val_Loss: 0.7554, Total Mean Loss: 1.4149, LR: 0.00014572865005915372, Duration: 67.54 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 90: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0216, Val_Loss: 0.7178, Total Mean Loss: 1.3697, LR: 0.00012071174305922266, Duration: 67.37 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 91: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.9036, Val_Loss: 0.7793, Total Mean Loss: 1.3415, LR: 9.79845202355077e-05, Duration: 67.45 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 92: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1194, Val_Loss: 0.7585, Total Mean Loss: 1.4390, LR: 7.757467120815912e-05, Duration: 67.51 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 93: 100%|██████████| 27/27 [01:06<00:00,  2.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1681, Val_Loss: 0.7490, Total Mean Loss: 1.4586, LR: 5.950706223432085e-05, Duration: 67.35 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 94: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0356, Val_Loss: 0.7410, Total Mean Loss: 1.3883, LR: 4.3803705912425316e-05, Duration: 67.52 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 95: 100%|██████████| 27/27 [01:06<00:00,  2.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.9751, Val_Loss: 0.7690, Total Mean Loss: 1.3720, LR: 3.0483734363234566e-05, Duration: 67.76 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 96: 100%|██████████| 27/27 [01:06<00:00,  2.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0435, Val_Loss: 0.7558, Total Mean Loss: 1.3996, LR: 1.9563375920296352e-05, Duration: 67.69 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 97: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0720, Val_Loss: 0.7562, Total Mean Loss: 1.4141, LR: 1.1055935358221834e-05, Duration: 67.62 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 98: 100%|██████████| 27/27 [01:06<00:00,  2.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1632, Val_Loss: 0.7543, Total Mean Loss: 1.4588, LR: 4.971777682864596e-06, Duration: 67.64 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1685, Val_Loss: 0.7549, Total Mean Loss: 1.4617, LR: 1.3183155031594304e-06, Duration: 67.57 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 100: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0983, Val_Loss: 0.7549, Total Mean Loss: 1.4266, LR: 1e-07, Duration: 67.38 sec\n",
      "\n",
      "[100 epoch result]\n",
      "       Metric     Value\n",
      "0   Accuracy  0.894000\n",
      "1  Precision  0.912238\n",
      "2     Recall  0.894000\n",
      "3   F1 Score  0.891306\n"
     ]
    }
   ],
   "source": [
    "training_time = 0\n",
    "losses = []\n",
    "val_losses = []\n",
    "lrs = []\n",
    "best_loss = float('inf')\n",
    "\n",
    "# GradScaler 초기화\n",
    "scaler = GradScaler()\n",
    "\n",
    "for i in range(epochs // 100):\n",
    "    for epoch in range(100):\n",
    "        model_v2.train()\n",
    "        start_time = time.time()\n",
    "        running_loss = 0.0\n",
    "        pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch + 1 + i*100}\")\n",
    "        \n",
    "        for _, data in pbar:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            inputs, labels = mixup_fn(inputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # AutoCast 적용\n",
    "            with autocast():\n",
    "                outputs = model_v2(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "            # 스케일링된 그라디언트 계산\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # 그라디언트 클리핑 전에 스케일링 제거\n",
    "            scaler.unscale_(optimizer)\n",
    "            clip_grad_norm_(model_v2.parameters(), max_norm=max_norm)\n",
    "\n",
    "            # 옵티마이저 스텝 및 스케일러 업데이트\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            # EMA 모델 업데이트\n",
    "            if model_ema is not None:\n",
    "                model_ema.update(model_v2)\n",
    "                \n",
    "            scheduler.step()\n",
    "                \n",
    "            lr = optimizer.param_groups[0][\"lr\"]\n",
    "            lrs.append(lr)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        losses.append(epoch_loss)        \n",
    "\n",
    "        model_v2.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data in valid_loader:\n",
    "                inputs, labels = data[0].to(device), data[1].to(device)\n",
    "                outputs = model_v2(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "        val_loss /= len(valid_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # 모델 저장\n",
    "        total_loss = val_loss + epoch_loss\n",
    "        if total_loss < best_loss:\n",
    "            best_loss = total_loss\n",
    "            model_save = False\n",
    "            if model_save:\n",
    "                torch.save(model_v2.state_dict(), model_path)\n",
    "\n",
    "        epoch_duration = time.time() - start_time\n",
    "        training_time += epoch_duration\n",
    "        \n",
    "        text = f'\\tLoss: {epoch_loss:.4f}, Val_Loss: {val_loss:.4f}, Total Mean Loss: {total_loss/2:.4f}, LR: {lr}, Duration: {epoch_duration:.2f} sec'\n",
    "        \n",
    "        if model_save:\n",
    "            text += f' - model saved!'\n",
    "            model_save = False\n",
    "\n",
    "        print(text)\n",
    "\n",
    "    # 예측 수행 및 레이블 저장\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model_v2(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # 혼동 행렬 생성\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    # 예측과 실제 레이블\n",
    "    y_true = all_labels  # 실제 레이블\n",
    "    y_pred = all_preds  # 모델에 의해 예측된 레이블\n",
    "\n",
    "    # 전체 데이터셋에 대한 정확도\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # 평균 정밀도, 리콜, F1-Score ('weighted')\n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "\n",
    "    # 판다스 데이터프레임으로 결과 정리\n",
    "    performance_metrics = pd.DataFrame({\n",
    "        'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "        'Value': [accuracy, precision, recall, f1_score]\n",
    "    })\n",
    "\n",
    "    # 데이터프레임 출력\n",
    "    print(f\"\\n[{i*100+100} epoch result]\\n\", performance_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 03 - Test ConvNeXt V2 with FCMAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 00.MASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "원본 데이터\n",
      "tensor([[[[ 1.,  2.,  3.,  4.],\n",
      "          [ 5.,  6.,  7.,  8.],\n",
      "          [ 9., 10., 11., 12.],\n",
      "          [13., 14., 15., 16.]]]])\n",
      "\n",
      "마스크\n",
      "tensor([[[[0, 0, 1, 1],\n",
      "          [0, 0, 1, 1],\n",
      "          [0, 0, 1, 1],\n",
      "          [0, 0, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 0, 0]]]])\n",
      "\n",
      "마스크 된 데이터\n",
      "tensor([[[[ 1.,  2.,  0.,  0.],\n",
      "          [ 5.,  6.,  0.,  0.],\n",
      "          [ 9., 10.,  0.,  0.],\n",
      "          [13., 14.,  0.,  0.]]],\n",
      "\n",
      "\n",
      "        [[[ 0.,  0.,  3.,  4.],\n",
      "          [ 0.,  0.,  7.,  8.],\n",
      "          [ 0.,  0., 11., 12.],\n",
      "          [ 0.,  0., 15., 16.]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define the upsample_mask and a simple example\n",
    "def upsample_mask(mask, scale):\n",
    "    assert len(mask.shape) == 2\n",
    "    p = int(mask.shape[1] ** .5)\n",
    "    return mask.reshape(-1, p, p).repeat_interleave(scale, axis=1).repeat_interleave(scale, axis=2)\n",
    "\n",
    "# 예시 마스크와 데이터\n",
    "mask = torch.tensor([[0, 1, 0, 1],\n",
    "                     [1, 0, 1, 0]])\n",
    "x = torch.arange(1., 17.).reshape(1, 1, 4, 4)  # 4x4 데이터, 채널과 배치 차원 포함\n",
    "\n",
    "# 마스크 업샘플링\n",
    "upsampled_mask = upsample_mask(mask, 2)  # 스케일을 2로 설정\n",
    "upsampled_mask = upsampled_mask.unsqueeze(1)  # 채널 차원 추가, 올바른 변수 이름 사용\n",
    "\n",
    "# 데이터에 마스크 적용\n",
    "x_orig = x.clone()  # 원본 데이터 복사\n",
    "x_masked = x * (1. - upsampled_mask.type_as(x))  # 마스크 적용, 타입 일치 시키기\n",
    "\n",
    "print(f\"\\n원본 데이터\\n{x_orig}\")\n",
    "print(f\"\\n마스크\\n{upsampled_mask}\")\n",
    "print(f\"\\n마스크 된 데이터\\n{x_masked}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01.TRAIN FCMAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/MinkowskiEngine-0.5.4-py3.10-linux-x86_64.egg/MinkowskiEngine/__init__.py:36: UserWarning: The environment variable `OMP_NUM_THREADS` not set. MinkowskiEngine will automatically set `OMP_NUM_THREADS=16`. If you want to set `OMP_NUM_THREADS` manually, please export it on the command line before running a python script. e.g. `export OMP_NUM_THREADS=12; python your_program.py`. It is recommended to set it below 24.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from model.fcmae import convnextv2_fcmae_tiny\n",
    "\n",
    "model = convnextv2_fcmae_tiny()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "# Transforms 정의하기\n",
    "train_transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224, scale=(0.2, 1.0), interpolation=3),  # 3 is bicubic\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "data_dir = '../../data/sports/'\n",
    "batch_size = 800\n",
    "train_path = data_dir\n",
    "\n",
    "# dataset load\n",
    "train_data = ImageFolder(train_path, transform=train_transform)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "len(train_loader)\n",
    "device = 'cuda:0'\n",
    "model.to(device)\n",
    "\n",
    "model_path = '../../model/convnext/fcmae.pt'\n",
    "\n",
    "epochs = 500\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1.5e-4, weight_decay=0.05, betas=(0.9, 0.95))\n",
    "\n",
    "warmup_steps = int(len(train_loader)*(epochs)*0.1)\n",
    "train_steps = len(train_loader)*(epochs)\n",
    "scheduler = CosineWarmupScheduler(optimizer, \n",
    "                                num_warmup_steps=50, \n",
    "                                num_training_steps=500,\n",
    "                                num_cycles=0.5,\n",
    "                                min_lr=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_time = 0\n",
    "losses = []\n",
    "val_losses = []\n",
    "lrs = []\n",
    "best_loss = float('inf')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch + 1}\")\n",
    "    \n",
    "    for _, data in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        samples= data[0].to(device)\n",
    "        loss, _, _ = model(samples, mask_ratio=0.6)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        lrs.append(lr)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)        \n",
    "    \n",
    "    # 모델 저장 로직 조정\n",
    "    if epoch_loss < best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        if epoch > (epochs // 2) :\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            model_saved_text = ' - model saved!'\n",
    "        else :\n",
    "            model_saved_text = ' - model save pass'\n",
    "    else:\n",
    "        model_saved_text = ''\n",
    "    \n",
    "    epoch_duration = time.time() - start_time\n",
    "    training_time += epoch_duration\n",
    "    \n",
    "    text = f'\\tLoss: {epoch_loss:,.4f}, LR: {lr}, Duration: {epoch_duration:.2f} sec{model_saved_text}'\n",
    "    print(text)\n",
    "\n",
    "    # 에폭마다 스케줄러 업데이트\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02.SUP after FCMAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model & pretrained Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.convnextv2 import load_convNext\n",
    "\n",
    "model = load_convNext(droppath=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['stem.stem_conv.weight', 'stem.stem_conv.bias', 'stem.stem_ln.weight', 'stem.stem_ln.bias', 'downsample_layers.0.stem_conv.weight', 'downsample_layers.0.stem_conv.bias', 'downsample_layers.0.stem_ln.weight', 'downsample_layers.0.stem_ln.bias', 'downsample_layers.1.ds_ln0.weight', 'downsample_layers.1.ds_ln0.bias', 'downsample_layers.1.ds_conv1.weight', 'downsample_layers.1.ds_conv1.bias', 'downsample_layers.2.ds_ln1.weight', 'downsample_layers.2.ds_ln1.bias', 'downsample_layers.2.ds_conv2.weight', 'downsample_layers.2.ds_conv2.bias', 'downsample_layers.3.ds_ln2.weight', 'downsample_layers.3.ds_ln2.bias', 'downsample_layers.3.ds_conv3.weight', 'downsample_layers.3.ds_conv3.bias', 'stages.0.0.dwconv.weight', 'stages.0.0.dwconv.bias', 'stages.0.0.layernorm.weight', 'stages.0.0.layernorm.bias', 'stages.0.0.pwconv1.weight', 'stages.0.0.pwconv1.bias', 'stages.0.0.grn.gamma', 'stages.0.0.grn.beta', 'stages.0.0.pwconv2.weight', 'stages.0.0.pwconv2.bias', 'stages.0.1.dwconv.weight', 'stages.0.1.dwconv.bias', 'stages.0.1.layernorm.weight', 'stages.0.1.layernorm.bias', 'stages.0.1.pwconv1.weight', 'stages.0.1.pwconv1.bias', 'stages.0.1.grn.gamma', 'stages.0.1.grn.beta', 'stages.0.1.pwconv2.weight', 'stages.0.1.pwconv2.bias', 'stages.0.2.dwconv.weight', 'stages.0.2.dwconv.bias', 'stages.0.2.layernorm.weight', 'stages.0.2.layernorm.bias', 'stages.0.2.pwconv1.weight', 'stages.0.2.pwconv1.bias', 'stages.0.2.grn.gamma', 'stages.0.2.grn.beta', 'stages.0.2.pwconv2.weight', 'stages.0.2.pwconv2.bias', 'stages.1.0.dwconv.weight', 'stages.1.0.dwconv.bias', 'stages.1.0.layernorm.weight', 'stages.1.0.layernorm.bias', 'stages.1.0.pwconv1.weight', 'stages.1.0.pwconv1.bias', 'stages.1.0.grn.gamma', 'stages.1.0.grn.beta', 'stages.1.0.pwconv2.weight', 'stages.1.0.pwconv2.bias', 'stages.1.1.dwconv.weight', 'stages.1.1.dwconv.bias', 'stages.1.1.layernorm.weight', 'stages.1.1.layernorm.bias', 'stages.1.1.pwconv1.weight', 'stages.1.1.pwconv1.bias', 'stages.1.1.grn.gamma', 'stages.1.1.grn.beta', 'stages.1.1.pwconv2.weight', 'stages.1.1.pwconv2.bias', 'stages.1.2.dwconv.weight', 'stages.1.2.dwconv.bias', 'stages.1.2.layernorm.weight', 'stages.1.2.layernorm.bias', 'stages.1.2.pwconv1.weight', 'stages.1.2.pwconv1.bias', 'stages.1.2.grn.gamma', 'stages.1.2.grn.beta', 'stages.1.2.pwconv2.weight', 'stages.1.2.pwconv2.bias', 'stages.2.0.dwconv.weight', 'stages.2.0.dwconv.bias', 'stages.2.0.layernorm.weight', 'stages.2.0.layernorm.bias', 'stages.2.0.pwconv1.weight', 'stages.2.0.pwconv1.bias', 'stages.2.0.grn.gamma', 'stages.2.0.grn.beta', 'stages.2.0.pwconv2.weight', 'stages.2.0.pwconv2.bias', 'stages.2.1.dwconv.weight', 'stages.2.1.dwconv.bias', 'stages.2.1.layernorm.weight', 'stages.2.1.layernorm.bias', 'stages.2.1.pwconv1.weight', 'stages.2.1.pwconv1.bias', 'stages.2.1.grn.gamma', 'stages.2.1.grn.beta', 'stages.2.1.pwconv2.weight', 'stages.2.1.pwconv2.bias', 'stages.2.2.dwconv.weight', 'stages.2.2.dwconv.bias', 'stages.2.2.layernorm.weight', 'stages.2.2.layernorm.bias', 'stages.2.2.pwconv1.weight', 'stages.2.2.pwconv1.bias', 'stages.2.2.grn.gamma', 'stages.2.2.grn.beta', 'stages.2.2.pwconv2.weight', 'stages.2.2.pwconv2.bias', 'stages.2.3.dwconv.weight', 'stages.2.3.dwconv.bias', 'stages.2.3.layernorm.weight', 'stages.2.3.layernorm.bias', 'stages.2.3.pwconv1.weight', 'stages.2.3.pwconv1.bias', 'stages.2.3.grn.gamma', 'stages.2.3.grn.beta', 'stages.2.3.pwconv2.weight', 'stages.2.3.pwconv2.bias', 'stages.2.4.dwconv.weight', 'stages.2.4.dwconv.bias', 'stages.2.4.layernorm.weight', 'stages.2.4.layernorm.bias', 'stages.2.4.pwconv1.weight', 'stages.2.4.pwconv1.bias', 'stages.2.4.grn.gamma', 'stages.2.4.grn.beta', 'stages.2.4.pwconv2.weight', 'stages.2.4.pwconv2.bias', 'stages.2.5.dwconv.weight', 'stages.2.5.dwconv.bias', 'stages.2.5.layernorm.weight', 'stages.2.5.layernorm.bias', 'stages.2.5.pwconv1.weight', 'stages.2.5.pwconv1.bias', 'stages.2.5.grn.gamma', 'stages.2.5.grn.beta', 'stages.2.5.pwconv2.weight', 'stages.2.5.pwconv2.bias', 'stages.2.6.dwconv.weight', 'stages.2.6.dwconv.bias', 'stages.2.6.layernorm.weight', 'stages.2.6.layernorm.bias', 'stages.2.6.pwconv1.weight', 'stages.2.6.pwconv1.bias', 'stages.2.6.grn.gamma', 'stages.2.6.grn.beta', 'stages.2.6.pwconv2.weight', 'stages.2.6.pwconv2.bias', 'stages.2.7.dwconv.weight', 'stages.2.7.dwconv.bias', 'stages.2.7.layernorm.weight', 'stages.2.7.layernorm.bias', 'stages.2.7.pwconv1.weight', 'stages.2.7.pwconv1.bias', 'stages.2.7.grn.gamma', 'stages.2.7.grn.beta', 'stages.2.7.pwconv2.weight', 'stages.2.7.pwconv2.bias', 'stages.2.8.dwconv.weight', 'stages.2.8.dwconv.bias', 'stages.2.8.layernorm.weight', 'stages.2.8.layernorm.bias', 'stages.2.8.pwconv1.weight', 'stages.2.8.pwconv1.bias', 'stages.2.8.grn.gamma', 'stages.2.8.grn.beta', 'stages.2.8.pwconv2.weight', 'stages.2.8.pwconv2.bias', 'stages.3.0.dwconv.weight', 'stages.3.0.dwconv.bias', 'stages.3.0.layernorm.weight', 'stages.3.0.layernorm.bias', 'stages.3.0.pwconv1.weight', 'stages.3.0.pwconv1.bias', 'stages.3.0.grn.gamma', 'stages.3.0.grn.beta', 'stages.3.0.pwconv2.weight', 'stages.3.0.pwconv2.bias', 'stages.3.1.dwconv.weight', 'stages.3.1.dwconv.bias', 'stages.3.1.layernorm.weight', 'stages.3.1.layernorm.bias', 'stages.3.1.pwconv1.weight', 'stages.3.1.pwconv1.bias', 'stages.3.1.grn.gamma', 'stages.3.1.grn.beta', 'stages.3.1.pwconv2.weight', 'stages.3.1.pwconv2.bias', 'stages.3.2.dwconv.weight', 'stages.3.2.dwconv.bias', 'stages.3.2.layernorm.weight', 'stages.3.2.layernorm.bias', 'stages.3.2.pwconv1.weight', 'stages.3.2.pwconv1.bias', 'stages.3.2.grn.gamma', 'stages.3.2.grn.beta', 'stages.3.2.pwconv2.weight', 'stages.3.2.pwconv2.bias', 'layernorm.weight', 'layernorm.bias', 'fc.weight', 'fc.bias'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0629,  0.0952,  0.0748,  0.0902],\n",
       "         [ 0.1215,  0.0440,  0.0270,  0.1279],\n",
       "         [-0.0605, -0.0492,  0.0668,  0.0991],\n",
       "         [-0.1112, -0.0794, -0.0212, -0.1278]],\n",
       "\n",
       "        [[ 0.1158,  0.1106, -0.0207,  0.1087],\n",
       "         [-0.1291, -0.1180,  0.0860, -0.1328],\n",
       "         [-0.0183,  0.0993,  0.0566,  0.0913],\n",
       "         [-0.1206,  0.1105, -0.0288,  0.0171]],\n",
       "\n",
       "        [[-0.0108, -0.0132,  0.1078, -0.0028],\n",
       "         [ 0.1139,  0.0600,  0.0073, -0.0734],\n",
       "         [-0.1233, -0.0571, -0.0904,  0.1424],\n",
       "         [ 0.1076,  0.1441, -0.0413, -0.1250]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['downsample_layers.0.stem_conv.weight'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0157, -0.1240, -0.0387,  0.1137, -0.0658,  0.0407,  0.0408],\n",
       "         [-0.0975, -0.0339,  0.0239,  0.0722, -0.0470, -0.0437, -0.0265],\n",
       "         [ 0.1292,  0.0394,  0.0845,  0.1158, -0.0432,  0.0255,  0.0569],\n",
       "         [-0.0611, -0.0005,  0.1062,  0.0153,  0.0700, -0.1000,  0.1077],\n",
       "         [ 0.1302,  0.1213, -0.1228,  0.1067,  0.1099,  0.0903,  0.0400],\n",
       "         [-0.0856,  0.1377,  0.1230,  0.0098, -0.0291, -0.0246,  0.1001],\n",
       "         [-0.0116,  0.0964,  0.0121,  0.0938, -0.1285, -0.0569, -0.0039]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['stages.0.0.dwconv.weight'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remap Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def remap_checkpoint_keys(ckpt):\n",
    "    new_ckpt = OrderedDict()\n",
    "    for k, v in ckpt.items():\n",
    "        if k.startswith('encoder'):\n",
    "            k = '.'.join(k.split('.')[1:]) # remove encoder in the name\n",
    "        if k.endswith('kernel'):\n",
    "            k = '.'.join(k.split('.')[:-1]) # remove kernel in the name\n",
    "            new_k = k + '.weight'\n",
    "            if len(v.shape) == 3: # resahpe standard convolution\n",
    "                kv, in_dim, out_dim = v.shape\n",
    "                ks = int(math.sqrt(kv))\n",
    "                new_ckpt[new_k] = v.permute(2, 1, 0).\\\n",
    "                    reshape(out_dim, in_dim, ks, ks).transpose(3, 2)\n",
    "            elif len(v.shape) == 2: # reshape depthwise convolution\n",
    "                kv, dim = v.shape\n",
    "                ks = int(math.sqrt(kv))\n",
    "                new_ckpt[new_k] = v.permute(1, 0).\\\n",
    "                    reshape(dim, 1, ks, ks).transpose(3, 2)\n",
    "            continue\n",
    "        elif 'ln' in k or 'linear' in k:\n",
    "            k = k.split('.')\n",
    "            k.pop(-2) # remove ln and linear in the name\n",
    "            new_k = '.'.join(k)\n",
    "        else:\n",
    "            new_k = k\n",
    "        new_ckpt[new_k] = v\n",
    "\n",
    "    # reshape grn affine parameters and biases\n",
    "    for k, v in new_ckpt.items():\n",
    "        if k.endswith('bias') and len(v.shape) != 1:\n",
    "            new_ckpt[k] = v.reshape(-1)\n",
    "        elif 'grn' in k:\n",
    "            new_ckpt[k] = v.unsqueeze(0).unsqueeze(1)\n",
    "    return new_ckpt\n",
    "\n",
    "def load_state_dict(model, state_dict, prefix='', ignore_missing=\"relative_position_index\"):\n",
    "    missing_keys = []\n",
    "    unexpected_keys = []\n",
    "    error_msgs = []\n",
    "    # copy state_dict so _load_from_state_dict can modify it\n",
    "    metadata = getattr(state_dict, '_metadata', None)\n",
    "    state_dict = state_dict.copy()\n",
    "    if metadata is not None:\n",
    "        state_dict._metadata = metadata\n",
    "\n",
    "    def load(module, prefix=''):\n",
    "        local_metadata = {} if metadata is None else metadata.get(\n",
    "            prefix[:-1], {})\n",
    "        module._load_from_state_dict(\n",
    "            state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs)\n",
    "        for name, child in module._modules.items():\n",
    "            if child is not None:\n",
    "                load(child, prefix + name + '.')\n",
    "\n",
    "    load(model, prefix=prefix)\n",
    "\n",
    "    warn_missing_keys = []\n",
    "    ignore_missing_keys = []\n",
    "    for key in missing_keys:\n",
    "        keep_flag = True\n",
    "        for ignore_key in ignore_missing.split('|'):\n",
    "            if ignore_key in key:\n",
    "                keep_flag = False\n",
    "                break\n",
    "        if keep_flag:\n",
    "            warn_missing_keys.append(key)\n",
    "        else:\n",
    "            ignore_missing_keys.append(key)\n",
    "\n",
    "    missing_keys = warn_missing_keys\n",
    "\n",
    "    if len(missing_keys) > 0:\n",
    "        print(\"Weights of {} not initialized from pretrained model: {}\".format(\n",
    "            model.__class__.__name__, missing_keys))\n",
    "    if len(unexpected_keys) > 0:\n",
    "        print(\"Weights from pretrained model not used in {}: {}\".format(\n",
    "            model.__class__.__name__, unexpected_keys))\n",
    "    if len(ignore_missing_keys) > 0:\n",
    "        print(\"Ignored weights of {} not initialized from pretrained model: {}\".format(\n",
    "            model.__class__.__name__, ignore_missing_keys))\n",
    "    if len(error_msgs) > 0:\n",
    "        print('\\n'.join(error_msgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing key mask_token from decoder of pretrained checkpoint\n",
      "Removing key proj.weight from decoder of pretrained checkpoint\n",
      "Removing key proj.bias from decoder of pretrained checkpoint\n",
      "Removing key decoder.0.dwconv.weight from decoder of pretrained checkpoint\n",
      "Removing key decoder.0.dwconv.bias from decoder of pretrained checkpoint\n",
      "Removing key decoder.0.layernorm.weight from decoder of pretrained checkpoint\n",
      "Removing key decoder.0.layernorm.bias from decoder of pretrained checkpoint\n",
      "Removing key decoder.0.pwconv1.weight from decoder of pretrained checkpoint\n",
      "Removing key decoder.0.pwconv1.bias from decoder of pretrained checkpoint\n",
      "Removing key decoder.0.grn.gamma from decoder of pretrained checkpoint\n",
      "Removing key decoder.0.grn.beta from decoder of pretrained checkpoint\n",
      "Removing key decoder.0.pwconv2.weight from decoder of pretrained checkpoint\n",
      "Removing key decoder.0.pwconv2.bias from decoder of pretrained checkpoint\n",
      "Removing key pred.weight from decoder of pretrained checkpoint\n",
      "Removing key pred.bias from decoder of pretrained checkpoint\n",
      "Weights of ConvNeXtV2 not initialized from pretrained model: ['stem.stem_ln.weight', 'stem.stem_ln.bias', 'downsample_layers.0.stem_ln.weight', 'downsample_layers.0.stem_ln.bias', 'layernorm.weight', 'layernorm.bias', 'fc.weight', 'fc.bias']\n",
      "Weights from pretrained model not used in ConvNeXtV2: ['stem.weight', 'stem.bias', 'downsample_layers.0.weight', 'downsample_layers.0.bias']\n"
     ]
    }
   ],
   "source": [
    "pretrain_path = '../../model/convnext/fcmae.pt'\n",
    "checkpoint_model = torch.load(pretrain_path, map_location='cpu')\n",
    "\n",
    "state_dict = model.state_dict()\n",
    "for k in ['head.weight', 'head.bias']:\n",
    "    if k in checkpoint_model and checkpoint_model[k].shape != state_dict[k].shape:\n",
    "        print(f\"Removing key {k} from head of pretrained checkpoint\")\n",
    "        del checkpoint_model[k]\n",
    "\n",
    "# remove decoder weights\n",
    "checkpoint_model_keys = list(checkpoint_model.keys())\n",
    "for k in checkpoint_model_keys:\n",
    "    if 'decoder' in k or 'mask_token'in k or \\\n",
    "        'proj' in k or 'pred' in k:\n",
    "        print(f\"Removing key {k} from decoder of pretrained checkpoint\")\n",
    "        del checkpoint_model[k]\n",
    "\n",
    "checkpoint_model = remap_checkpoint_keys(checkpoint_model)\n",
    "load_state_dict(model, checkpoint_model, prefix='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# manually initialize fc layer\n",
    "nn.init.trunc_normal_(model.fc.weight, std=2e-5)\n",
    "torch.nn.init.constant_(model.fc.bias, 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2945,  0.7202, -0.3333, -0.2289],\n",
       "         [-0.5944,  0.4452,  0.9049, -0.2540],\n",
       "         [ 0.4562, -0.8116,  1.4202,  0.4324],\n",
       "         [ 0.1777,  0.3165, -1.3547,  0.2427]],\n",
       "\n",
       "        [[-1.5330, -1.2280, -0.4023, -0.5692],\n",
       "         [-0.5322, -0.0219,  0.3301, -1.0079],\n",
       "         [ 0.4587, -0.0214, -0.6747, -0.7620],\n",
       "         [ 0.1962, -0.4949, -0.0375, -0.0354]],\n",
       "\n",
       "        [[-0.8695,  0.9264, -0.2321, -1.5825],\n",
       "         [-0.4034, -0.4807,  0.2547, -0.3684],\n",
       "         [-0.6111, -1.2094,  0.8702,  0.9287],\n",
       "         [ 1.1989,  0.0221, -0.5370,  0.5360]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['downsample_layers.0.stem_conv.weight'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['downsample_layers.0.stem_ln.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.2615, -0.2950, -0.1866,  0.1731,  0.0968, -0.2890, -0.9205],\n",
       "         [-0.5179,  0.9794, -0.8361,  0.5204, -0.4569,  0.2246, -0.1680],\n",
       "         [-0.5284, -0.1686, -0.2800,  1.1796, -1.3214,  0.7117, -1.5333],\n",
       "         [-0.2485, -0.1683,  1.0382,  0.2789,  0.3089,  0.2259, -0.1799],\n",
       "         [ 1.6295,  0.7848,  0.8198, -1.2768, -0.3535, -1.3434,  0.1347],\n",
       "         [-0.9390,  0.4803,  0.1530,  0.2759, -0.5772, -0.9058,  0.6663],\n",
       "         [ 0.7074,  0.3372, -0.4517, -0.3804,  0.4552,  0.9987,  0.3588]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['stages.0.0.dwconv.weight'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "Total Parameters: 27,943,396\n",
      "Trainable Parameters: 27,943,396\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 총 파라미터 수 계산\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "# 학습 가능한 파라미터 수 계산\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print('='*80)\n",
    "print(f\"\\nTotal Parameters: {total_params:,}\")\n",
    "print(f\"Trainable Parameters: {trainable_params:,}\\n\")\n",
    "print('='*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms 정의하기\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.TrivialAugmentWide(interpolation=F.InterpolationMode.BICUBIC),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.6,1), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "data_dir = '../../data/sports'\n",
    "batch_size = 320\n",
    "\n",
    "train_path = data_dir+'/train'\n",
    "valid_path = data_dir+'/valid'\n",
    "test_path = data_dir+'/test'\n",
    "\n",
    "# dataset load\n",
    "train_data = ImageFolder(train_path, transform=train_transform)\n",
    "valid_data = ImageFolder(valid_path, transform=test_transform)\n",
    "test_data = ImageFolder(test_path, transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using EMA with decay = 0.9998\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:4'\n",
    "max_norm = 3.0 \n",
    "\n",
    "model.to(device)\n",
    "\n",
    "model_ema = None\n",
    "ema_active = True\n",
    "if ema_active:\n",
    "    ema_decay = 0.9998\n",
    "    model_ema = ModelEmaV3(\n",
    "        model,\n",
    "        decay=ema_decay,\n",
    "    )\n",
    "    print(f\"Using EMA with decay = {ema_decay}\")\n",
    "\n",
    "model_path = ''\n",
    "\n",
    "mixup = True\n",
    "if mixup :\n",
    "    mixup_fn = Mixup(mixup_alpha=.8, \n",
    "                    cutmix_alpha=1., \n",
    "                    prob=1., \n",
    "                    switch_prob=0.5, \n",
    "                    mode='batch',\n",
    "                    label_smoothing=.1,\n",
    "                    num_classes=100)\n",
    "    \n",
    "    criterion = SoftTargetCrossEntropy()\n",
    "else :\n",
    "    criterion = LabelSmoothingCrossEntropy(.1)\n",
    "    \n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without LayerWise Learning Rate Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=4e-3, weight_decay=0.05)\n",
    "warmup_steps = int(len(train_loader)*(epochs)*0.1)\n",
    "train_steps = len(train_loader)*(epochs)\n",
    "scheduler = CosineWarmupScheduler(optimizer, \n",
    "                                num_warmup_steps=warmup_steps, \n",
    "                                num_training_steps=train_steps,\n",
    "                                num_cycles=0.5,\n",
    "                                min_lr=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/43 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 43/43 [01:41<00:00,  2.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.5177, Val_Loss: 4.1077, Total Mean Loss: 4.3127, LR: 0.00040009, Duration: 102.63 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.4140, Val_Loss: 4.0270, Total Mean Loss: 4.2205, LR: 0.00080008, Duration: 100.71 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.3534, Val_Loss: 3.9134, Total Mean Loss: 4.1334, LR: 0.00120007, Duration: 100.47 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.3314, Val_Loss: 3.7517, Total Mean Loss: 4.0416, LR: 0.0016000600000000002, Duration: 101.10 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 43/43 [01:40<00:00,  2.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.2681, Val_Loss: 3.6038, Total Mean Loss: 3.9359, LR: 0.0020000499999999997, Duration: 102.34 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 43/43 [01:41<00:00,  2.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.2788, Val_Loss: 3.5822, Total Mean Loss: 3.9305, LR: 0.0024000399999999996, Duration: 102.91 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  23%|██▎       | 10/43 [00:25<01:23,  2.53s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m---> 24\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# 그라디언트 클리핑 적용\u001b[39;00m\n\u001b[1;32m     26\u001b[0m clip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39mmax_norm)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_time = 0\n",
    "losses = []\n",
    "val_losses = []\n",
    "lrs = []\n",
    "best_loss = float('inf')\n",
    "model_save = False\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "for i in range(epochs // 100):\n",
    "    for epoch in range(100):\n",
    "        model.train()\n",
    "        start_time = time.time()\n",
    "        running_loss = 0.0\n",
    "        pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch + 1 + i*100}\")\n",
    "        \n",
    "        for _, data in pbar:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            inputs, labels = mixup_fn(inputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "                \n",
    "            loss.backward()\n",
    "            # 그라디언트 클리핑 적용\n",
    "            clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "            optimizer.step()\n",
    "            \n",
    "            # EMA 모델 업데이트, 필요한 경우\n",
    "            if model_ema is not None:\n",
    "                model_ema.update(model)\n",
    "                \n",
    "            scheduler.step()\n",
    "                \n",
    "            lr = optimizer.param_groups[0][\"lr\"]\n",
    "            lrs.append(lr)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        losses.append(epoch_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data in valid_loader:\n",
    "                inputs, labels = data[0].to(device), data[1].to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "        val_loss /= len(valid_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # 모델 저장 조건 수정\n",
    "        total_loss = val_loss + epoch_loss\n",
    "        if total_loss < best_loss:\n",
    "            best_loss = total_loss\n",
    "            # torch.save(model.state_dict(), model_path)\n",
    "            model_save = True\n",
    "            save_text = ' - model saved!'\n",
    "        else:\n",
    "            save_text = ''\n",
    "\n",
    "        epoch_duration = time.time() - start_time\n",
    "        training_time += epoch_duration\n",
    "        \n",
    "        text = f'\\tLoss: {epoch_loss:.4f}, Val_Loss: {val_loss:.4f}, Total Mean Loss: {total_loss/2:.4f}, LR: {lr}, Duration: {epoch_duration:.2f} sec{save_text}'\n",
    "        print(text)\n",
    "\n",
    "    # 예측 수행 및 레이블 저장\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # 혼동 행렬 생성\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    # 예측과 실제 레이블\n",
    "    y_true = all_labels  # 실제 레이블\n",
    "    y_pred = all_preds  # 모델에 의해 예측된 레이블\n",
    "\n",
    "    # 전체 데이터셋에 대한 정확도\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # 평균 정밀도, 리콜, F1-Score ('weighted')\n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "\n",
    "    # 판다스 데이터프레임으로 결과 정리\n",
    "    performance_metrics = pd.DataFrame({\n",
    "        'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "        'Value': [accuracy, precision, recall, f1_score]\n",
    "    })\n",
    "\n",
    "    # 데이터프레임 출력\n",
    "    print(f\"\\n[{i*100+100} epoch result]\\n\", performance_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LayerWise Learning Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LLRD_ConvNeXt(depths=[3,3,9,3]):\n",
    "    stage = 0\n",
    "    layer_names = []\n",
    "    for depth in depths:\n",
    "        if stage == 0:\n",
    "            layer_names.append(f'downsampling_layers.{stage}.stem_conv.weight')\n",
    "            layer_names.append(f'downsampling_layers.{stage}.stem_conv.bias')\n",
    "            layer_names.append(f'downsampling_layers.{stage}.stem_ln.weight')\n",
    "            layer_names.append(f'downsampling_layers.{stage}.stem_ln.bias')\n",
    "        else :\n",
    "            layer_names.append(f'downsampling_layers.{stage}.ds_ln.weight')\n",
    "            layer_names.append(f'downsampling_layers.{stage}.ds_ln.bias')\n",
    "            layer_names.append(f'downsampling_layers.{stage}.ds_conv.weight')\n",
    "            layer_names.append(f'downsampling_layers.{stage}.ds_conv.bias')\n",
    "        \n",
    "        for i in range(depth):\n",
    "            layer_names.append(f'stages.{stage}.{i}.dwconv.weight')\n",
    "            layer_names.append(f'stages.{stage}.{i}.dwconv.bias')\n",
    "            layer_names.append(f'stages.{stage}.{i}.layernorm.weight')\n",
    "            layer_names.append(f'stages.{stage}.{i}.layernorm.bias')\n",
    "            layer_names.append(f'stages.{stage}.{i}.pwconv1.weight')\n",
    "            layer_names.append(f'stages.{stage}.{i}.pwconv1.bias')\n",
    "            layer_names.append(f'stages.{stage}.{i}.grn.gamma')\n",
    "            layer_names.append(f'stages.{stage}.{i}.grn.beta')            \n",
    "            layer_names.append(f'stages.{stage}.{i}.pwconv2.weight')\n",
    "            layer_names.append(f'stages.{stage}.{i}.pwconv2.bias')\n",
    "        stage += 1\n",
    "    \n",
    "    layer_names.append('layernorm.weight')\n",
    "    layer_names.append('layernorm.bias')\n",
    "    layer_names.append('fc.weight')\n",
    "    layer_names.append('fc.bias')\n",
    "    return layer_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: fc.bias's lr=0.008\n",
      "1: fc.weight's lr=0.008\n",
      "2: layernorm.bias's lr=0.007200000000000001\n",
      "3: layernorm.weight's lr=0.007200000000000001\n",
      "4: stages.3.2.pwconv2.bias's lr=0.0064800000000000005\n",
      "5: stages.3.2.pwconv2.weight's lr=0.0064800000000000005\n",
      "6: stages.3.2.grn.beta's lr=0.0064800000000000005\n",
      "7: stages.3.2.grn.gamma's lr=0.0064800000000000005\n",
      "8: stages.3.2.pwconv1.bias's lr=0.0064800000000000005\n",
      "9: stages.3.2.pwconv1.weight's lr=0.0064800000000000005\n",
      "10: stages.3.2.layernorm.bias's lr=0.0064800000000000005\n",
      "11: stages.3.2.layernorm.weight's lr=0.0064800000000000005\n",
      "12: stages.3.2.dwconv.bias's lr=0.0064800000000000005\n",
      "13: stages.3.2.dwconv.weight's lr=0.0064800000000000005\n",
      "14: stages.3.1.pwconv2.bias's lr=0.0064800000000000005\n",
      "15: stages.3.1.pwconv2.weight's lr=0.0064800000000000005\n",
      "16: stages.3.1.grn.beta's lr=0.0064800000000000005\n",
      "17: stages.3.1.grn.gamma's lr=0.0064800000000000005\n",
      "18: stages.3.1.pwconv1.bias's lr=0.0064800000000000005\n",
      "19: stages.3.1.pwconv1.weight's lr=0.0064800000000000005\n",
      "20: stages.3.1.layernorm.bias's lr=0.0064800000000000005\n",
      "21: stages.3.1.layernorm.weight's lr=0.0064800000000000005\n",
      "22: stages.3.1.dwconv.bias's lr=0.0064800000000000005\n",
      "23: stages.3.1.dwconv.weight's lr=0.0064800000000000005\n",
      "24: stages.3.0.pwconv2.bias's lr=0.0064800000000000005\n",
      "25: stages.3.0.pwconv2.weight's lr=0.0064800000000000005\n",
      "26: stages.3.0.grn.beta's lr=0.0064800000000000005\n",
      "27: stages.3.0.grn.gamma's lr=0.0064800000000000005\n",
      "28: stages.3.0.pwconv1.bias's lr=0.0064800000000000005\n",
      "29: stages.3.0.pwconv1.weight's lr=0.0064800000000000005\n",
      "30: stages.3.0.layernorm.bias's lr=0.0064800000000000005\n",
      "31: stages.3.0.layernorm.weight's lr=0.0064800000000000005\n",
      "32: stages.3.0.dwconv.bias's lr=0.0064800000000000005\n",
      "33: stages.3.0.dwconv.weight's lr=0.0064800000000000005\n",
      "34: stages.2.8.pwconv2.bias's lr=0.0064800000000000005\n",
      "35: stages.2.8.pwconv2.weight's lr=0.0064800000000000005\n",
      "36: stages.2.8.grn.beta's lr=0.0064800000000000005\n",
      "37: stages.2.8.grn.gamma's lr=0.0064800000000000005\n",
      "38: stages.2.8.pwconv1.bias's lr=0.0064800000000000005\n",
      "39: stages.2.8.pwconv1.weight's lr=0.0064800000000000005\n",
      "40: stages.2.8.layernorm.bias's lr=0.0064800000000000005\n",
      "41: stages.2.8.layernorm.weight's lr=0.0064800000000000005\n",
      "42: stages.2.8.dwconv.bias's lr=0.0064800000000000005\n",
      "43: stages.2.8.dwconv.weight's lr=0.0064800000000000005\n",
      "44: stages.2.7.pwconv2.bias's lr=0.0064800000000000005\n",
      "45: stages.2.7.pwconv2.weight's lr=0.0064800000000000005\n",
      "46: stages.2.7.grn.beta's lr=0.0064800000000000005\n",
      "47: stages.2.7.grn.gamma's lr=0.0064800000000000005\n",
      "48: stages.2.7.pwconv1.bias's lr=0.0064800000000000005\n",
      "49: stages.2.7.pwconv1.weight's lr=0.0064800000000000005\n",
      "50: stages.2.7.layernorm.bias's lr=0.0064800000000000005\n",
      "51: stages.2.7.layernorm.weight's lr=0.0064800000000000005\n",
      "52: stages.2.7.dwconv.bias's lr=0.0064800000000000005\n",
      "53: stages.2.7.dwconv.weight's lr=0.0064800000000000005\n",
      "54: stages.2.6.pwconv2.bias's lr=0.0064800000000000005\n",
      "55: stages.2.6.pwconv2.weight's lr=0.0064800000000000005\n",
      "56: stages.2.6.grn.beta's lr=0.0064800000000000005\n",
      "57: stages.2.6.grn.gamma's lr=0.0064800000000000005\n",
      "58: stages.2.6.pwconv1.bias's lr=0.0064800000000000005\n",
      "59: stages.2.6.pwconv1.weight's lr=0.0064800000000000005\n",
      "60: stages.2.6.layernorm.bias's lr=0.0064800000000000005\n",
      "61: stages.2.6.layernorm.weight's lr=0.0064800000000000005\n",
      "62: stages.2.6.dwconv.bias's lr=0.0064800000000000005\n",
      "63: stages.2.6.dwconv.weight's lr=0.0064800000000000005\n",
      "64: stages.2.5.pwconv2.bias's lr=0.0064800000000000005\n",
      "65: stages.2.5.pwconv2.weight's lr=0.0064800000000000005\n",
      "66: stages.2.5.grn.beta's lr=0.0064800000000000005\n",
      "67: stages.2.5.grn.gamma's lr=0.0064800000000000005\n",
      "68: stages.2.5.pwconv1.bias's lr=0.0064800000000000005\n",
      "69: stages.2.5.pwconv1.weight's lr=0.0064800000000000005\n",
      "70: stages.2.5.layernorm.bias's lr=0.0064800000000000005\n",
      "71: stages.2.5.layernorm.weight's lr=0.0064800000000000005\n",
      "72: stages.2.5.dwconv.bias's lr=0.0064800000000000005\n",
      "73: stages.2.5.dwconv.weight's lr=0.0064800000000000005\n",
      "74: stages.2.4.pwconv2.bias's lr=0.0064800000000000005\n",
      "75: stages.2.4.pwconv2.weight's lr=0.0064800000000000005\n",
      "76: stages.2.4.grn.beta's lr=0.0064800000000000005\n",
      "77: stages.2.4.grn.gamma's lr=0.0064800000000000005\n",
      "78: stages.2.4.pwconv1.bias's lr=0.0064800000000000005\n",
      "79: stages.2.4.pwconv1.weight's lr=0.0064800000000000005\n",
      "80: stages.2.4.layernorm.bias's lr=0.0064800000000000005\n",
      "81: stages.2.4.layernorm.weight's lr=0.0064800000000000005\n",
      "82: stages.2.4.dwconv.bias's lr=0.0064800000000000005\n",
      "83: stages.2.4.dwconv.weight's lr=0.0064800000000000005\n",
      "84: stages.2.3.pwconv2.bias's lr=0.0064800000000000005\n",
      "85: stages.2.3.pwconv2.weight's lr=0.0064800000000000005\n",
      "86: stages.2.3.grn.beta's lr=0.0064800000000000005\n",
      "87: stages.2.3.grn.gamma's lr=0.0064800000000000005\n",
      "88: stages.2.3.pwconv1.bias's lr=0.0064800000000000005\n",
      "89: stages.2.3.pwconv1.weight's lr=0.0064800000000000005\n",
      "90: stages.2.3.layernorm.bias's lr=0.0064800000000000005\n",
      "91: stages.2.3.layernorm.weight's lr=0.0064800000000000005\n",
      "92: stages.2.3.dwconv.bias's lr=0.0064800000000000005\n",
      "93: stages.2.3.dwconv.weight's lr=0.0064800000000000005\n",
      "94: stages.2.2.pwconv2.bias's lr=0.0064800000000000005\n",
      "95: stages.2.2.pwconv2.weight's lr=0.0064800000000000005\n",
      "96: stages.2.2.grn.beta's lr=0.0064800000000000005\n",
      "97: stages.2.2.grn.gamma's lr=0.0064800000000000005\n",
      "98: stages.2.2.pwconv1.bias's lr=0.0064800000000000005\n",
      "99: stages.2.2.pwconv1.weight's lr=0.0064800000000000005\n",
      "100: stages.2.2.layernorm.bias's lr=0.0064800000000000005\n",
      "101: stages.2.2.layernorm.weight's lr=0.0064800000000000005\n",
      "102: stages.2.2.dwconv.bias's lr=0.0064800000000000005\n",
      "103: stages.2.2.dwconv.weight's lr=0.0064800000000000005\n",
      "104: stages.2.1.pwconv2.bias's lr=0.0064800000000000005\n",
      "105: stages.2.1.pwconv2.weight's lr=0.0064800000000000005\n",
      "106: stages.2.1.grn.beta's lr=0.0064800000000000005\n",
      "107: stages.2.1.grn.gamma's lr=0.0064800000000000005\n",
      "108: stages.2.1.pwconv1.bias's lr=0.0064800000000000005\n",
      "109: stages.2.1.pwconv1.weight's lr=0.0064800000000000005\n",
      "110: stages.2.1.layernorm.bias's lr=0.0064800000000000005\n",
      "111: stages.2.1.layernorm.weight's lr=0.0064800000000000005\n",
      "112: stages.2.1.dwconv.bias's lr=0.0064800000000000005\n",
      "113: stages.2.1.dwconv.weight's lr=0.0064800000000000005\n",
      "114: stages.2.0.pwconv2.bias's lr=0.0064800000000000005\n",
      "115: stages.2.0.pwconv2.weight's lr=0.0064800000000000005\n",
      "116: stages.2.0.grn.beta's lr=0.0064800000000000005\n",
      "117: stages.2.0.grn.gamma's lr=0.0064800000000000005\n",
      "118: stages.2.0.pwconv1.bias's lr=0.0064800000000000005\n",
      "119: stages.2.0.pwconv1.weight's lr=0.0064800000000000005\n",
      "120: stages.2.0.layernorm.bias's lr=0.0064800000000000005\n",
      "121: stages.2.0.layernorm.weight's lr=0.0064800000000000005\n",
      "122: stages.2.0.dwconv.bias's lr=0.0064800000000000005\n",
      "123: stages.2.0.dwconv.weight's lr=0.0064800000000000005\n",
      "124: stages.1.2.pwconv2.bias's lr=0.0064800000000000005\n",
      "125: stages.1.2.pwconv2.weight's lr=0.0064800000000000005\n",
      "126: stages.1.2.grn.beta's lr=0.0064800000000000005\n",
      "127: stages.1.2.grn.gamma's lr=0.0064800000000000005\n",
      "128: stages.1.2.pwconv1.bias's lr=0.0064800000000000005\n",
      "129: stages.1.2.pwconv1.weight's lr=0.0064800000000000005\n",
      "130: stages.1.2.layernorm.bias's lr=0.0064800000000000005\n",
      "131: stages.1.2.layernorm.weight's lr=0.0064800000000000005\n",
      "132: stages.1.2.dwconv.bias's lr=0.0064800000000000005\n",
      "133: stages.1.2.dwconv.weight's lr=0.0064800000000000005\n",
      "134: stages.1.1.pwconv2.bias's lr=0.0064800000000000005\n",
      "135: stages.1.1.pwconv2.weight's lr=0.0064800000000000005\n",
      "136: stages.1.1.grn.beta's lr=0.0064800000000000005\n",
      "137: stages.1.1.grn.gamma's lr=0.0064800000000000005\n",
      "138: stages.1.1.pwconv1.bias's lr=0.0064800000000000005\n",
      "139: stages.1.1.pwconv1.weight's lr=0.0064800000000000005\n",
      "140: stages.1.1.layernorm.bias's lr=0.0064800000000000005\n",
      "141: stages.1.1.layernorm.weight's lr=0.0064800000000000005\n",
      "142: stages.1.1.dwconv.bias's lr=0.0064800000000000005\n",
      "143: stages.1.1.dwconv.weight's lr=0.0064800000000000005\n",
      "144: stages.1.0.pwconv2.bias's lr=0.0064800000000000005\n",
      "145: stages.1.0.pwconv2.weight's lr=0.0064800000000000005\n",
      "146: stages.1.0.grn.beta's lr=0.0064800000000000005\n",
      "147: stages.1.0.grn.gamma's lr=0.0064800000000000005\n",
      "148: stages.1.0.pwconv1.bias's lr=0.0064800000000000005\n",
      "149: stages.1.0.pwconv1.weight's lr=0.0064800000000000005\n",
      "150: stages.1.0.layernorm.bias's lr=0.0064800000000000005\n",
      "151: stages.1.0.layernorm.weight's lr=0.0064800000000000005\n",
      "152: stages.1.0.dwconv.bias's lr=0.0064800000000000005\n",
      "153: stages.1.0.dwconv.weight's lr=0.0064800000000000005\n",
      "154: stages.0.2.pwconv2.bias's lr=0.0064800000000000005\n",
      "155: stages.0.2.pwconv2.weight's lr=0.0064800000000000005\n",
      "156: stages.0.2.grn.beta's lr=0.0064800000000000005\n",
      "157: stages.0.2.grn.gamma's lr=0.0064800000000000005\n",
      "158: stages.0.2.pwconv1.bias's lr=0.0064800000000000005\n",
      "159: stages.0.2.pwconv1.weight's lr=0.0064800000000000005\n",
      "160: stages.0.2.layernorm.bias's lr=0.0064800000000000005\n",
      "161: stages.0.2.layernorm.weight's lr=0.0064800000000000005\n",
      "162: stages.0.2.dwconv.bias's lr=0.0064800000000000005\n",
      "163: stages.0.2.dwconv.weight's lr=0.0064800000000000005\n",
      "164: stages.0.1.pwconv2.bias's lr=0.0064800000000000005\n",
      "165: stages.0.1.pwconv2.weight's lr=0.0064800000000000005\n",
      "166: stages.0.1.grn.beta's lr=0.0064800000000000005\n",
      "167: stages.0.1.grn.gamma's lr=0.0064800000000000005\n",
      "168: stages.0.1.pwconv1.bias's lr=0.0064800000000000005\n",
      "169: stages.0.1.pwconv1.weight's lr=0.0064800000000000005\n",
      "170: stages.0.1.layernorm.bias's lr=0.0064800000000000005\n",
      "171: stages.0.1.layernorm.weight's lr=0.0064800000000000005\n",
      "172: stages.0.1.dwconv.bias's lr=0.0064800000000000005\n",
      "173: stages.0.1.dwconv.weight's lr=0.0064800000000000005\n",
      "174: stages.0.0.pwconv2.bias's lr=0.0064800000000000005\n",
      "175: stages.0.0.pwconv2.weight's lr=0.0064800000000000005\n",
      "176: stages.0.0.grn.beta's lr=0.0064800000000000005\n",
      "177: stages.0.0.grn.gamma's lr=0.0064800000000000005\n",
      "178: stages.0.0.pwconv1.bias's lr=0.0064800000000000005\n",
      "179: stages.0.0.pwconv1.weight's lr=0.0064800000000000005\n",
      "180: stages.0.0.layernorm.bias's lr=0.0064800000000000005\n",
      "181: stages.0.0.layernorm.weight's lr=0.0064800000000000005\n",
      "182: stages.0.0.dwconv.bias's lr=0.0064800000000000005\n",
      "183: stages.0.0.dwconv.weight's lr=0.0064800000000000005\n",
      "184: downsample_layers.3.ds_conv.bias's lr=0.005832\n",
      "185: downsample_layers.3.ds_conv.weight's lr=0.005832\n",
      "186: downsample_layers.3.ds_ln.bias's lr=0.005832\n",
      "187: downsample_layers.3.ds_ln.weight's lr=0.005832\n",
      "188: downsample_layers.2.ds_conv.bias's lr=0.005832\n",
      "189: downsample_layers.2.ds_conv.weight's lr=0.005832\n",
      "190: downsample_layers.2.ds_ln.bias's lr=0.005832\n",
      "191: downsample_layers.2.ds_ln.weight's lr=0.005832\n",
      "192: downsample_layers.1.ds_conv.bias's lr=0.005832\n",
      "193: downsample_layers.1.ds_conv.weight's lr=0.005832\n",
      "194: downsample_layers.1.ds_ln.bias's lr=0.005832\n",
      "195: downsample_layers.1.ds_ln.weight's lr=0.005832\n",
      "196: downsample_layers.0.stem_ln.bias's lr=0.005832\n",
      "197: downsample_layers.0.stem_ln.weight's lr=0.005832\n",
      "198: downsample_layers.0.stem_conv.bias's lr=0.005832\n",
      "199: downsample_layers.0.stem_conv.weight's lr=0.005832\n"
     ]
    }
   ],
   "source": [
    "# LLRD\n",
    "layer_names = []\n",
    "for i, (name, params) in enumerate(model.named_parameters()):\n",
    "    layer_names.append(name)\n",
    "    \n",
    "layer_names.reverse()\n",
    "\n",
    "lr      = 8e-3  \n",
    "lr_mult = 0.9  \n",
    "weight_decay = 0.05 \n",
    "\n",
    "param_groups = []\n",
    "prev_group_name = layer_names[0].split('.')[0]\n",
    "\n",
    "for idx, name in enumerate(layer_names):\n",
    "    \n",
    "    cur_group_name = name.split('.')[0]\n",
    "    \n",
    "    if cur_group_name != prev_group_name:\n",
    "        lr *= lr_mult\n",
    "    prev_group_name = cur_group_name\n",
    "    \n",
    "    print(f\"{idx}: {name}'s lr={lr}\")\n",
    "    \n",
    "    param_groups += [{'params': [ p for n, p in model.named_parameters() if n == name and p.requires_grad],\n",
    "                      'lr' : lr,\n",
    "                      'weight_decay': weight_decay}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: fc.bias | lr=0.008\n",
      "1: fc.weight | lr=0.008, weight decay=0.05\n",
      "2: layernorm.bias | lr=0.008\n",
      "3: layernorm.weight | lr=0.008\n",
      "4: stages.3.2.pwconv2.bias | lr=0.008\n",
      "5: stages.3.2.pwconv2.weight | lr=0.007200000000000001, weight decay=0.05\n",
      "6: stages.3.2.grn.beta | lr=0.008\n",
      "7: stages.3.2.grn.gamma | lr=0.008\n",
      "8: stages.3.2.pwconv1.bias | lr=0.008\n",
      "9: stages.3.2.pwconv1.weight | lr=0.007200000000000001, weight decay=0.05\n",
      "10: stages.3.2.layernorm.bias | lr=0.008\n",
      "11: stages.3.2.layernorm.weight | lr=0.008\n",
      "12: stages.3.2.dwconv.bias | lr=0.008\n",
      "13: stages.3.2.dwconv.weight | lr=0.007200000000000001, weight decay=0.05\n",
      "14: stages.3.1.pwconv2.bias | lr=0.008\n",
      "15: stages.3.1.pwconv2.weight | lr=0.007200000000000001, weight decay=0.05\n",
      "16: stages.3.1.grn.beta | lr=0.008\n",
      "17: stages.3.1.grn.gamma | lr=0.008\n",
      "18: stages.3.1.pwconv1.bias | lr=0.008\n",
      "19: stages.3.1.pwconv1.weight | lr=0.007200000000000001, weight decay=0.05\n",
      "20: stages.3.1.layernorm.bias | lr=0.008\n",
      "21: stages.3.1.layernorm.weight | lr=0.008\n",
      "22: stages.3.1.dwconv.bias | lr=0.008\n",
      "23: stages.3.1.dwconv.weight | lr=0.007200000000000001, weight decay=0.05\n",
      "24: stages.3.0.pwconv2.bias | lr=0.008\n",
      "25: stages.3.0.pwconv2.weight | lr=0.007200000000000001, weight decay=0.05\n",
      "26: stages.3.0.grn.beta | lr=0.008\n",
      "27: stages.3.0.grn.gamma | lr=0.008\n",
      "28: stages.3.0.pwconv1.bias | lr=0.008\n",
      "29: stages.3.0.pwconv1.weight | lr=0.007200000000000001, weight decay=0.05\n",
      "30: stages.3.0.layernorm.bias | lr=0.008\n",
      "31: stages.3.0.layernorm.weight | lr=0.008\n",
      "32: stages.3.0.dwconv.bias | lr=0.008\n",
      "33: stages.3.0.dwconv.weight | lr=0.007200000000000001, weight decay=0.05\n",
      "34: downsampling_layers.3.ds_conv.bias | lr=0.008\n",
      "35: downsampling_layers.3.ds_conv.weight | lr=0.0064800000000000005, weight decay=0.05\n",
      "36: downsampling_layers.3.ds_ln.bias | lr=0.008\n",
      "37: downsampling_layers.3.ds_ln.weight | lr=0.008\n",
      "38: stages.2.8.pwconv2.bias | lr=0.008\n",
      "39: stages.2.8.pwconv2.weight | lr=0.005832, weight decay=0.05\n",
      "40: stages.2.8.grn.beta | lr=0.008\n",
      "41: stages.2.8.grn.gamma | lr=0.008\n",
      "42: stages.2.8.pwconv1.bias | lr=0.008\n",
      "43: stages.2.8.pwconv1.weight | lr=0.005832, weight decay=0.05\n",
      "44: stages.2.8.layernorm.bias | lr=0.008\n",
      "45: stages.2.8.layernorm.weight | lr=0.008\n",
      "46: stages.2.8.dwconv.bias | lr=0.008\n",
      "47: stages.2.8.dwconv.weight | lr=0.005832, weight decay=0.05\n",
      "48: stages.2.7.pwconv2.bias | lr=0.008\n",
      "49: stages.2.7.pwconv2.weight | lr=0.005832, weight decay=0.05\n",
      "50: stages.2.7.grn.beta | lr=0.008\n",
      "51: stages.2.7.grn.gamma | lr=0.008\n",
      "52: stages.2.7.pwconv1.bias | lr=0.008\n",
      "53: stages.2.7.pwconv1.weight | lr=0.005832, weight decay=0.05\n",
      "54: stages.2.7.layernorm.bias | lr=0.008\n",
      "55: stages.2.7.layernorm.weight | lr=0.008\n",
      "56: stages.2.7.dwconv.bias | lr=0.008\n",
      "57: stages.2.7.dwconv.weight | lr=0.005832, weight decay=0.05\n",
      "58: stages.2.6.pwconv2.bias | lr=0.008\n",
      "59: stages.2.6.pwconv2.weight | lr=0.005832, weight decay=0.05\n",
      "60: stages.2.6.grn.beta | lr=0.008\n",
      "61: stages.2.6.grn.gamma | lr=0.008\n",
      "62: stages.2.6.pwconv1.bias | lr=0.008\n",
      "63: stages.2.6.pwconv1.weight | lr=0.005832, weight decay=0.05\n",
      "64: stages.2.6.layernorm.bias | lr=0.008\n",
      "65: stages.2.6.layernorm.weight | lr=0.008\n",
      "66: stages.2.6.dwconv.bias | lr=0.008\n",
      "67: stages.2.6.dwconv.weight | lr=0.005832, weight decay=0.05\n",
      "68: stages.2.5.pwconv2.bias | lr=0.008\n",
      "69: stages.2.5.pwconv2.weight | lr=0.005832, weight decay=0.05\n",
      "70: stages.2.5.grn.beta | lr=0.008\n",
      "71: stages.2.5.grn.gamma | lr=0.008\n",
      "72: stages.2.5.pwconv1.bias | lr=0.008\n",
      "73: stages.2.5.pwconv1.weight | lr=0.005832, weight decay=0.05\n",
      "74: stages.2.5.layernorm.bias | lr=0.008\n",
      "75: stages.2.5.layernorm.weight | lr=0.008\n",
      "76: stages.2.5.dwconv.bias | lr=0.008\n",
      "77: stages.2.5.dwconv.weight | lr=0.005832, weight decay=0.05\n",
      "78: stages.2.4.pwconv2.bias | lr=0.008\n",
      "79: stages.2.4.pwconv2.weight | lr=0.005832, weight decay=0.05\n",
      "80: stages.2.4.grn.beta | lr=0.008\n",
      "81: stages.2.4.grn.gamma | lr=0.008\n",
      "82: stages.2.4.pwconv1.bias | lr=0.008\n",
      "83: stages.2.4.pwconv1.weight | lr=0.005832, weight decay=0.05\n",
      "84: stages.2.4.layernorm.bias | lr=0.008\n",
      "85: stages.2.4.layernorm.weight | lr=0.008\n",
      "86: stages.2.4.dwconv.bias | lr=0.008\n",
      "87: stages.2.4.dwconv.weight | lr=0.005832, weight decay=0.05\n",
      "88: stages.2.3.pwconv2.bias | lr=0.008\n",
      "89: stages.2.3.pwconv2.weight | lr=0.005832, weight decay=0.05\n",
      "90: stages.2.3.grn.beta | lr=0.008\n",
      "91: stages.2.3.grn.gamma | lr=0.008\n",
      "92: stages.2.3.pwconv1.bias | lr=0.008\n",
      "93: stages.2.3.pwconv1.weight | lr=0.005832, weight decay=0.05\n",
      "94: stages.2.3.layernorm.bias | lr=0.008\n",
      "95: stages.2.3.layernorm.weight | lr=0.008\n",
      "96: stages.2.3.dwconv.bias | lr=0.008\n",
      "97: stages.2.3.dwconv.weight | lr=0.005832, weight decay=0.05\n",
      "98: stages.2.2.pwconv2.bias | lr=0.008\n",
      "99: stages.2.2.pwconv2.weight | lr=0.005832, weight decay=0.05\n",
      "100: stages.2.2.grn.beta | lr=0.008\n",
      "101: stages.2.2.grn.gamma | lr=0.008\n",
      "102: stages.2.2.pwconv1.bias | lr=0.008\n",
      "103: stages.2.2.pwconv1.weight | lr=0.005832, weight decay=0.05\n",
      "104: stages.2.2.layernorm.bias | lr=0.008\n",
      "105: stages.2.2.layernorm.weight | lr=0.008\n",
      "106: stages.2.2.dwconv.bias | lr=0.008\n",
      "107: stages.2.2.dwconv.weight | lr=0.005832, weight decay=0.05\n",
      "108: stages.2.1.pwconv2.bias | lr=0.008\n",
      "109: stages.2.1.pwconv2.weight | lr=0.005832, weight decay=0.05\n",
      "110: stages.2.1.grn.beta | lr=0.008\n",
      "111: stages.2.1.grn.gamma | lr=0.008\n",
      "112: stages.2.1.pwconv1.bias | lr=0.008\n",
      "113: stages.2.1.pwconv1.weight | lr=0.005832, weight decay=0.05\n",
      "114: stages.2.1.layernorm.bias | lr=0.008\n",
      "115: stages.2.1.layernorm.weight | lr=0.008\n",
      "116: stages.2.1.dwconv.bias | lr=0.008\n",
      "117: stages.2.1.dwconv.weight | lr=0.005832, weight decay=0.05\n",
      "118: stages.2.0.pwconv2.bias | lr=0.008\n",
      "119: stages.2.0.pwconv2.weight | lr=0.005832, weight decay=0.05\n",
      "120: stages.2.0.grn.beta | lr=0.008\n",
      "121: stages.2.0.grn.gamma | lr=0.008\n",
      "122: stages.2.0.pwconv1.bias | lr=0.008\n",
      "123: stages.2.0.pwconv1.weight | lr=0.005832, weight decay=0.05\n",
      "124: stages.2.0.layernorm.bias | lr=0.008\n",
      "125: stages.2.0.layernorm.weight | lr=0.008\n",
      "126: stages.2.0.dwconv.bias | lr=0.008\n",
      "127: stages.2.0.dwconv.weight | lr=0.005832, weight decay=0.05\n",
      "128: downsampling_layers.2.ds_conv.bias | lr=0.008\n",
      "129: downsampling_layers.2.ds_conv.weight | lr=0.0052488000000000005, weight decay=0.05\n",
      "130: downsampling_layers.2.ds_ln.bias | lr=0.008\n",
      "131: downsampling_layers.2.ds_ln.weight | lr=0.008\n",
      "132: stages.1.2.pwconv2.bias | lr=0.008\n",
      "133: stages.1.2.pwconv2.weight | lr=0.00472392, weight decay=0.05\n",
      "134: stages.1.2.grn.beta | lr=0.008\n",
      "135: stages.1.2.grn.gamma | lr=0.008\n",
      "136: stages.1.2.pwconv1.bias | lr=0.008\n",
      "137: stages.1.2.pwconv1.weight | lr=0.00472392, weight decay=0.05\n",
      "138: stages.1.2.layernorm.bias | lr=0.008\n",
      "139: stages.1.2.layernorm.weight | lr=0.008\n",
      "140: stages.1.2.dwconv.bias | lr=0.008\n",
      "141: stages.1.2.dwconv.weight | lr=0.00472392, weight decay=0.05\n",
      "142: stages.1.1.pwconv2.bias | lr=0.008\n",
      "143: stages.1.1.pwconv2.weight | lr=0.00472392, weight decay=0.05\n",
      "144: stages.1.1.grn.beta | lr=0.008\n",
      "145: stages.1.1.grn.gamma | lr=0.008\n",
      "146: stages.1.1.pwconv1.bias | lr=0.008\n",
      "147: stages.1.1.pwconv1.weight | lr=0.00472392, weight decay=0.05\n",
      "148: stages.1.1.layernorm.bias | lr=0.008\n",
      "149: stages.1.1.layernorm.weight | lr=0.008\n",
      "150: stages.1.1.dwconv.bias | lr=0.008\n",
      "151: stages.1.1.dwconv.weight | lr=0.00472392, weight decay=0.05\n",
      "152: stages.1.0.pwconv2.bias | lr=0.008\n",
      "153: stages.1.0.pwconv2.weight | lr=0.00472392, weight decay=0.05\n",
      "154: stages.1.0.grn.beta | lr=0.008\n",
      "155: stages.1.0.grn.gamma | lr=0.008\n",
      "156: stages.1.0.pwconv1.bias | lr=0.008\n",
      "157: stages.1.0.pwconv1.weight | lr=0.00472392, weight decay=0.05\n",
      "158: stages.1.0.layernorm.bias | lr=0.008\n",
      "159: stages.1.0.layernorm.weight | lr=0.008\n",
      "160: stages.1.0.dwconv.bias | lr=0.008\n",
      "161: stages.1.0.dwconv.weight | lr=0.00472392, weight decay=0.05\n",
      "162: downsampling_layers.1.ds_conv.bias | lr=0.008\n",
      "163: downsampling_layers.1.ds_conv.weight | lr=0.004251528, weight decay=0.05\n",
      "164: downsampling_layers.1.ds_ln.bias | lr=0.008\n",
      "165: downsampling_layers.1.ds_ln.weight | lr=0.008\n",
      "166: stages.0.2.pwconv2.bias | lr=0.008\n",
      "167: stages.0.2.pwconv2.weight | lr=0.0038263752, weight decay=0.05\n",
      "168: stages.0.2.grn.beta | lr=0.008\n",
      "169: stages.0.2.grn.gamma | lr=0.008\n",
      "170: stages.0.2.pwconv1.bias | lr=0.008\n",
      "171: stages.0.2.pwconv1.weight | lr=0.0038263752, weight decay=0.05\n",
      "172: stages.0.2.layernorm.bias | lr=0.008\n",
      "173: stages.0.2.layernorm.weight | lr=0.008\n",
      "174: stages.0.2.dwconv.bias | lr=0.008\n",
      "175: stages.0.2.dwconv.weight | lr=0.0038263752, weight decay=0.05\n",
      "176: stages.0.1.pwconv2.bias | lr=0.008\n",
      "177: stages.0.1.pwconv2.weight | lr=0.0038263752, weight decay=0.05\n",
      "178: stages.0.1.grn.beta | lr=0.008\n",
      "179: stages.0.1.grn.gamma | lr=0.008\n",
      "180: stages.0.1.pwconv1.bias | lr=0.008\n",
      "181: stages.0.1.pwconv1.weight | lr=0.0038263752, weight decay=0.05\n",
      "182: stages.0.1.layernorm.bias | lr=0.008\n",
      "183: stages.0.1.layernorm.weight | lr=0.008\n",
      "184: stages.0.1.dwconv.bias | lr=0.008\n",
      "185: stages.0.1.dwconv.weight | lr=0.0038263752, weight decay=0.05\n",
      "186: stages.0.0.pwconv2.bias | lr=0.008\n",
      "187: stages.0.0.pwconv2.weight | lr=0.0038263752, weight decay=0.05\n",
      "188: stages.0.0.grn.beta | lr=0.008\n",
      "189: stages.0.0.grn.gamma | lr=0.008\n",
      "190: stages.0.0.pwconv1.bias | lr=0.008\n",
      "191: stages.0.0.pwconv1.weight | lr=0.0038263752, weight decay=0.05\n",
      "192: stages.0.0.layernorm.bias | lr=0.008\n",
      "193: stages.0.0.layernorm.weight | lr=0.008\n",
      "194: stages.0.0.dwconv.bias | lr=0.008\n",
      "195: stages.0.0.dwconv.weight | lr=0.0038263752, weight decay=0.05\n",
      "196: downsampling_layers.0.stem_ln.bias | lr=0.008\n",
      "197: downsampling_layers.0.stem_ln.weight | lr=0.008\n",
      "198: downsampling_layers.0.stem_conv.bias | lr=0.008\n",
      "199: downsampling_layers.0.stem_conv.weight | lr=0.0038263752, weight decay=0.05\n"
     ]
    }
   ],
   "source": [
    "# LLRD\n",
    "layer_names = LLRD_ConvNeXt()\n",
    "    \n",
    "layer_names.reverse()\n",
    "\n",
    "lr0     = 8e-3  \n",
    "lr_mult = 0.9  \n",
    "weight_decay = 0.05 \n",
    "\n",
    "param_groups = []\n",
    "prev_group_name = layer_names[0].split('.')[0]\n",
    "\n",
    "lr = lr0\n",
    "for idx, name in enumerate(layer_names):\n",
    "    \n",
    "    cur_group_name = name.split('.')[0] \n",
    "    \n",
    "    if cur_group_name != prev_group_name:\n",
    "        if (\"ln\" in name) | (\"layernorm\" in name) | ('grn' in name):\n",
    "            pass\n",
    "        else:\n",
    "            lr *= lr_mult\n",
    "    prev_group_name = cur_group_name\n",
    "    \n",
    "    if \"bias\" in name :\n",
    "        print(f\"{idx}: {name} | lr={lr0}\")\n",
    "        param_groups += [{'params': [ p for n, p in model.named_parameters() if n == name and p.requires_grad],\n",
    "                        'lr' : lr0}]\n",
    "    elif (\"ln\" in name) | (\"layernorm\" in name) | ('grn' in name):\n",
    "        param_groups += [{'params': [ p for n, p in model.named_parameters() if n == name and p.requires_grad],\n",
    "                        'lr' : lr0}]   \n",
    "        print(f\"{idx}: {name} | lr={lr0}\")\n",
    "    else :        \n",
    "        param_groups += [{'params': [ p for n, p in model.named_parameters() if n == name and p.requires_grad],\n",
    "                        'lr' : lr,\n",
    "                        'weight_decay': weight_decay}]\n",
    "        print(f\"{idx}: {name} | lr={lr}, weight decay={weight_decay}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "\n",
    "optimizer = optim.AdamW(param_groups, betas=(0.9,0.999))\n",
    "warmup_steps = int(len(train_loader)*(epochs)*0.1)\n",
    "train_steps = len(train_loader)*(epochs)\n",
    "scheduler = CosineWarmupScheduler(optimizer, \n",
    "                                num_warmup_steps=warmup_steps, \n",
    "                                num_training_steps=train_steps,\n",
    "                                num_cycles=0.5,\n",
    "                                min_lr=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/43 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 43/43 [01:42<00:00,  2.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.6701, Val_Loss: 4.6486, Total Mean Loss: 4.6594, LR: 0.0008000899999999999, Duration: 104.06 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.6242, Val_Loss: 4.3898, Total Mean Loss: 4.5070, LR: 0.00160008, Duration: 100.48 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.5296, Val_Loss: 4.1583, Total Mean Loss: 4.3439, LR: 0.00240007, Duration: 100.78 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 43/43 [01:39<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.4511, Val_Loss: 3.9385, Total Mean Loss: 4.1948, LR: 0.00320006, Duration: 100.96 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 43/43 [01:39<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.4286, Val_Loss: 3.8703, Total Mean Loss: 4.1494, LR: 0.004000050000000001, Duration: 100.65 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6:  81%|████████▏ | 35/43 [01:22<00:18,  2.37s/it]"
     ]
    }
   ],
   "source": [
    "training_time = 0\n",
    "losses = []\n",
    "val_losses = []\n",
    "lrs = []\n",
    "best_loss = float('inf')\n",
    "model_save = False\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "for i in range(epochs // 100):\n",
    "    for epoch in range(100):\n",
    "        model.train()\n",
    "        start_time = time.time()\n",
    "        running_loss = 0.0\n",
    "        pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch + 1 + i*100}\")\n",
    "        \n",
    "        for _, data in pbar:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            inputs, labels = mixup_fn(inputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "                \n",
    "            loss.backward()\n",
    "            # 그라디언트 클리핑 적용\n",
    "            clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "            optimizer.step()\n",
    "            \n",
    "            # EMA 모델 업데이트, 필요한 경우\n",
    "            if model_ema is not None:\n",
    "                model_ema.update(model)\n",
    "                \n",
    "            scheduler.step()\n",
    "                \n",
    "            lr = optimizer.param_groups[0][\"lr\"]\n",
    "            lrs.append(lr)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        losses.append(epoch_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data in valid_loader:\n",
    "                inputs, labels = data[0].to(device), data[1].to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "        val_loss /= len(valid_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # 모델 저장 조건 수정\n",
    "        total_loss = val_loss + epoch_loss\n",
    "        if total_loss < best_loss:\n",
    "            best_loss = total_loss\n",
    "            # torch.save(model.state_dict(), model_path)\n",
    "            model_save = True\n",
    "            save_text = ' - model saved!'\n",
    "        else:\n",
    "            save_text = ''\n",
    "\n",
    "        epoch_duration = time.time() - start_time\n",
    "        training_time += epoch_duration\n",
    "        \n",
    "        text = f'\\tLoss: {epoch_loss:.4f}, Val_Loss: {val_loss:.4f}, Total Mean Loss: {total_loss/2:.4f}, LR: {lr}, Duration: {epoch_duration:.2f} sec{save_text}'\n",
    "        print(text)\n",
    "\n",
    "    # 예측 수행 및 레이블 저장\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # 혼동 행렬 생성\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    # 예측과 실제 레이블\n",
    "    y_true = all_labels  # 실제 레이블\n",
    "    y_pred = all_preds  # 모델에 의해 예측된 레이블\n",
    "\n",
    "    # 전체 데이터셋에 대한 정확도\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # 평균 정밀도, 리콜, F1-Score ('weighted')\n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "\n",
    "    # 판다스 데이터프레임으로 결과 정리\n",
    "    performance_metrics = pd.DataFrame({\n",
    "        'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "        'Value': [accuracy, precision, recall, f1_score]\n",
    "    })\n",
    "\n",
    "    # 데이터프레임 출력\n",
    "    print(f\"\\n[{i*100+100} epoch result]\\n\", performance_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 53/53 [01:41<00:00,  1.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.5950, Val_Loss: 4.1453, Total Mean Loss: 4.3702, LR: 0.00080009, Duration: 103.15 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 53/53 [01:39<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.4743, Val_Loss: 4.0018, Total Mean Loss: 4.2381, LR: 0.0016000800000000003, Duration: 100.79 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 53/53 [01:40<00:00,  1.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.4219, Val_Loss: 3.7958, Total Mean Loss: 4.1088, LR: 0.00240007, Duration: 101.36 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 53/53 [01:39<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.3900, Val_Loss: 3.7397, Total Mean Loss: 4.0649, LR: 0.0032000600000000002, Duration: 100.97 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 53/53 [01:39<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.3341, Val_Loss: 3.6376, Total Mean Loss: 3.9859, LR: 0.004000050000000001, Duration: 100.68 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 53/53 [01:39<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.2782, Val_Loss: 3.5548, Total Mean Loss: 3.9165, LR: 0.00480004, Duration: 100.57 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 53/53 [01:39<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.2267, Val_Loss: 3.5453, Total Mean Loss: 3.8860, LR: 0.005600030000000001, Duration: 100.35 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 53/53 [01:39<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.1982, Val_Loss: 3.4035, Total Mean Loss: 3.8008, LR: 0.006400020000000001, Duration: 100.46 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 53/53 [01:39<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.1818, Val_Loss: 3.3875, Total Mean Loss: 3.7846, LR: 0.007200010000000001, Duration: 100.41 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 53/53 [01:38<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.1136, Val_Loss: 3.3307, Total Mean Loss: 3.7221, LR: 0.008, Duration: 100.29 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 53/53 [01:39<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.1009, Val_Loss: 3.2129, Total Mean Loss: 3.6569, LR: 0.007997563338535033, Duration: 100.33 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 53/53 [01:39<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.1097, Val_Loss: 3.1263, Total Mean Loss: 3.6180, LR: 0.007990256322836784, Duration: 100.57 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 53/53 [01:39<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.0599, Val_Loss: 3.1223, Total Mean Loss: 3.5911, LR: 0.007978087855378325, Duration: 100.44 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 53/53 [01:39<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.0101, Val_Loss: 2.9449, Total Mean Loss: 3.4775, LR: 0.007961072761562845, Duration: 100.31 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 53/53 [01:39<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.9389, Val_Loss: 2.8412, Total Mean Loss: 3.3900, LR: 0.007939231771661183, Duration: 100.35 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 53/53 [01:38<00:00,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.9606, Val_Loss: 2.7031, Total Mean Loss: 3.3318, LR: 0.007912591495555185, Duration: 100.07 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 53/53 [01:39<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.9554, Val_Loss: 2.6873, Total Mean Loss: 3.3214, LR: 0.007881184390317672, Duration: 100.29 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 53/53 [01:39<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.8809, Val_Loss: 2.6832, Total Mean Loss: 3.2821, LR: 0.007845048720668478, Duration: 100.26 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 53/53 [01:39<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.8607, Val_Loss: 2.6046, Total Mean Loss: 3.2326, LR: 0.007804228512354801, Duration: 100.28 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 53/53 [01:39<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.9065, Val_Loss: 2.6144, Total Mean Loss: 3.2604, LR: 0.007758773498512596, Duration: 100.74 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 53/53 [01:39<00:00,  1.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.8473, Val_Loss: 2.4817, Total Mean Loss: 3.1645, LR: 0.0077087390590744225, Duration: 101.22 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 53/53 [01:39<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.7878, Val_Loss: 2.4968, Total Mean Loss: 3.1423, LR: 0.007654186153297522, Duration: 100.28 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 53/53 [01:39<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.8438, Val_Loss: 2.4143, Total Mean Loss: 3.1290, LR: 0.007595181245494354, Duration: 100.72 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 53/53 [01:38<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.7342, Val_Loss: 2.1994, Total Mean Loss: 2.9668, LR: 0.007531796224056066, Duration: 100.29 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 53/53 [01:39<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.7472, Val_Loss: 2.0719, Total Mean Loss: 2.9096, LR: 0.007464108313867567, Duration: 100.27 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 53/53 [01:38<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.6859, Val_Loss: 2.3841, Total Mean Loss: 3.0350, LR: 0.007392199982220897, Duration: 100.18 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 53/53 [01:38<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.7741, Val_Loss: 2.3366, Total Mean Loss: 3.0553, LR: 0.00731615883834154, Duration: 100.17 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 53/53 [01:38<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.5131, Val_Loss: 1.9265, Total Mean Loss: 2.7198, LR: 0.007236077526650072, Duration: 100.15 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 53/53 [01:38<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.6372, Val_Loss: 2.2424, Total Mean Loss: 2.9398, LR: 0.007152053613889208, Duration: 100.14 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 53/53 [01:39<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.7462, Val_Loss: 2.1536, Total Mean Loss: 2.9499, LR: 0.007064189470253756, Duration: 100.26 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|██████████| 53/53 [01:39<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.4918, Val_Loss: 1.6969, Total Mean Loss: 2.5943, LR: 0.006972592144668304, Duration: 100.28 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|██████████| 53/53 [01:38<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.5671, Val_Loss: 1.8910, Total Mean Loss: 2.7291, LR: 0.0068773732343645885, Duration: 100.24 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|██████████| 53/53 [01:39<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.5183, Val_Loss: 1.7305, Total Mean Loss: 2.6244, LR: 0.006778648748917467, Duration: 100.40 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|██████████| 53/53 [01:39<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.4650, Val_Loss: 1.8143, Total Mean Loss: 2.6396, LR: 0.006676538968905116, Duration: 100.35 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|██████████| 53/53 [01:39<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.5158, Val_Loss: 1.9125, Total Mean Loss: 2.7142, LR: 0.006571168299365673, Duration: 100.57 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|██████████| 53/53 [01:39<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.4353, Val_Loss: 1.7001, Total Mean Loss: 2.5677, LR: 0.006462665118228867, Duration: 100.66 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37: 100%|██████████| 53/53 [01:39<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.4231, Val_Loss: 1.6558, Total Mean Loss: 2.5395, LR: 0.006351161619907278, Duration: 100.42 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|██████████| 53/53 [01:39<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.4246, Val_Loss: 1.6772, Total Mean Loss: 2.5509, LR: 0.006236793654237814, Duration: 100.34 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|██████████| 53/53 [01:38<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.3955, Val_Loss: 1.6819, Total Mean Loss: 2.5387, LR: 0.006119700560969609, Duration: 100.21 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40: 100%|██████████| 53/53 [01:38<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.3652, Val_Loss: 1.4796, Total Mean Loss: 2.4224, LR: 0.006000025000000001, Duration: 100.18 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41: 100%|██████████| 53/53 [01:39<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.3364, Val_Loss: 1.4906, Total Mean Loss: 2.4135, LR: 0.005877912777565424, Duration: 100.48 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42: 100%|██████████| 53/53 [01:39<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.3491, Val_Loss: 1.5221, Total Mean Loss: 2.4356, LR: 0.005753512668598971, Duration: 100.56 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43: 100%|██████████| 53/53 [01:39<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.2630, Val_Loss: 1.3132, Total Mean Loss: 2.2881, LR: 0.005626976235471049, Duration: 100.36 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44: 100%|██████████| 53/53 [01:39<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.2669, Val_Loss: 1.4940, Total Mean Loss: 2.3805, LR: 0.005498457643333979, Duration: 100.54 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45: 100%|██████████| 53/53 [01:39<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.2238, Val_Loss: 1.2956, Total Mean Loss: 2.2597, LR: 0.00536811347229551, Duration: 100.38 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46: 100%|██████████| 53/53 [01:39<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.2199, Val_Loss: 1.3076, Total Mean Loss: 2.2638, LR: 0.005236102526650072, Duration: 100.38 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47: 100%|██████████| 53/53 [01:39<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.0790, Val_Loss: 1.3395, Total Mean Loss: 2.2093, LR: 0.005102585641400206, Duration: 100.55 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48: 100%|██████████| 53/53 [01:39<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.2223, Val_Loss: 1.4247, Total Mean Loss: 2.3235, LR: 0.004967725486303891, Duration: 100.48 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 53/53 [01:39<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.0647, Val_Loss: 1.2575, Total Mean Loss: 2.1611, LR: 0.004831686367686497, Duration: 100.35 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50: 100%|██████████| 53/53 [01:39<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.1002, Val_Loss: 1.2536, Total Mean Loss: 2.1769, LR: 0.004694634028258839, Duration: 100.68 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51: 100%|██████████| 53/53 [01:39<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.1481, Val_Loss: 1.1780, Total Mean Loss: 2.1631, LR: 0.004556735445185214, Duration: 100.47 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52: 100%|██████████| 53/53 [01:39<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.9954, Val_Loss: 1.2351, Total Mean Loss: 2.1153, LR: 0.004418158626647451, Duration: 100.29 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53: 100%|██████████| 53/53 [01:39<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.9960, Val_Loss: 1.2575, Total Mean Loss: 2.1268, LR: 0.004279072407152814, Duration: 100.52 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54: 100%|██████████| 53/53 [01:39<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.1454, Val_Loss: 1.2170, Total Mean Loss: 2.1812, LR: 0.00413964624183517, Duration: 100.39 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55: 100%|██████████| 53/53 [01:39<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.0753, Val_Loss: 1.1802, Total Mean Loss: 2.1278, LR: 0.004000050000000001, Duration: 100.43 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56: 100%|██████████| 53/53 [01:39<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.9854, Val_Loss: 1.1241, Total Mean Loss: 2.0548, LR: 0.0038604537581648324, Duration: 100.51 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57: 100%|██████████| 53/53 [01:39<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.0592, Val_Loss: 1.2451, Total Mean Loss: 2.1521, LR: 0.0037210275928471863, Duration: 100.32 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58: 100%|██████████| 53/53 [01:39<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.8498, Val_Loss: 1.0461, Total Mean Loss: 1.9479, LR: 0.00358194137335255, Duration: 100.52 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59: 100%|██████████| 53/53 [01:39<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.0221, Val_Loss: 1.1233, Total Mean Loss: 2.0727, LR: 0.0034433645548147874, Duration: 100.73 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60: 100%|██████████| 53/53 [01:39<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.0172, Val_Loss: 1.0411, Total Mean Loss: 2.0291, LR: 0.0033054659717411624, Duration: 100.79 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61: 100%|██████████| 53/53 [01:39<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.7804, Val_Loss: 1.0907, Total Mean Loss: 1.9356, LR: 0.003168413632313504, Duration: 100.34 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62: 100%|██████████| 53/53 [01:38<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.9019, Val_Loss: 1.1806, Total Mean Loss: 2.0412, LR: 0.00303237451369611, Duration: 100.27 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63: 100%|██████████| 53/53 [01:39<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.7794, Val_Loss: 0.9896, Total Mean Loss: 1.8845, LR: 0.0028975143585997947, Duration: 100.63 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64: 100%|██████████| 53/53 [01:39<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.8134, Val_Loss: 1.0241, Total Mean Loss: 1.9187, LR: 0.0027639974733499294, Duration: 100.72 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65: 100%|██████████| 53/53 [01:39<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.7360, Val_Loss: 0.9448, Total Mean Loss: 1.8404, LR: 0.002631986527704492, Duration: 100.32 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66: 100%|██████████| 53/53 [01:39<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.8561, Val_Loss: 1.0950, Total Mean Loss: 1.9755, LR: 0.0025016423566660228, Duration: 100.30 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67: 100%|██████████| 53/53 [01:39<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5907, Val_Loss: 0.9677, Total Mean Loss: 1.7792, LR: 0.0023731237645289536, Duration: 100.32 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68: 100%|██████████| 53/53 [01:39<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6671, Val_Loss: 0.9903, Total Mean Loss: 1.8287, LR: 0.0022465873314010294, Duration: 100.53 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69: 100%|██████████| 53/53 [01:38<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6060, Val_Loss: 0.9529, Total Mean Loss: 1.7795, LR: 0.002122187222434577, Duration: 100.21 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70: 100%|██████████| 53/53 [01:38<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6892, Val_Loss: 1.0041, Total Mean Loss: 1.8466, LR: 0.002000075000000001, Duration: 100.25 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71: 100%|██████████| 53/53 [01:41<00:00,  1.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6691, Val_Loss: 0.9944, Total Mean Loss: 1.8318, LR: 0.0018803994390303928, Duration: 102.34 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72: 100%|██████████| 53/53 [01:39<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5781, Val_Loss: 0.9373, Total Mean Loss: 1.7577, LR: 0.001763306345762187, Duration: 100.83 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 73: 100%|██████████| 53/53 [01:39<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.7129, Val_Loss: 0.9540, Total Mean Loss: 1.8334, LR: 0.0016489383800927227, Duration: 100.65 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 74: 100%|██████████| 53/53 [01:39<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6384, Val_Loss: 0.9378, Total Mean Loss: 1.7881, LR: 0.0015374348817711334, Duration: 100.51 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75: 100%|██████████| 53/53 [01:39<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4799, Val_Loss: 0.9027, Total Mean Loss: 1.6913, LR: 0.001428931700634327, Duration: 100.69 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 76: 100%|██████████| 53/53 [01:39<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5822, Val_Loss: 0.8696, Total Mean Loss: 1.7259, LR: 0.0013235610310948864, Duration: 100.49 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 77: 100%|██████████| 53/53 [01:39<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5972, Val_Loss: 0.9180, Total Mean Loss: 1.7576, LR: 0.0012214512510825336, Duration: 100.58 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 78: 100%|██████████| 53/53 [01:39<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6590, Val_Loss: 0.9583, Total Mean Loss: 1.8086, LR: 0.0011227267656354132, Duration: 100.57 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 79: 100%|██████████| 53/53 [01:39<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5161, Val_Loss: 0.9193, Total Mean Loss: 1.7177, LR: 0.0010275078553316965, Duration: 100.40 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 80: 100%|██████████| 53/53 [01:39<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6772, Val_Loss: 0.9182, Total Mean Loss: 1.7977, LR: 0.0009359105297462444, Duration: 100.55 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 81: 100%|██████████| 53/53 [01:38<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4884, Val_Loss: 0.8812, Total Mean Loss: 1.6848, LR: 0.0008480463861107927, Duration: 100.15 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 82: 100%|██████████| 53/53 [01:39<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5202, Val_Loss: 0.8777, Total Mean Loss: 1.6989, LR: 0.0007640224733499294, Duration: 100.48 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 83: 100%|██████████| 53/53 [01:39<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.7413, Val_Loss: 0.9471, Total Mean Loss: 1.8442, LR: 0.0006839411616584612, Duration: 100.54 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 84: 100%|██████████| 53/53 [01:39<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5241, Val_Loss: 0.8664, Total Mean Loss: 1.6953, LR: 0.000607900017779104, Duration: 100.56 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 85: 100%|██████████| 53/53 [01:39<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5300, Val_Loss: 0.8870, Total Mean Loss: 1.7085, LR: 0.0005359916861324344, Duration: 100.55 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 86: 100%|██████████| 53/53 [01:39<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4429, Val_Loss: 0.8650, Total Mean Loss: 1.6540, LR: 0.000468303775943935, Duration: 100.62 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 87: 100%|██████████| 53/53 [01:39<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6560, Val_Loss: 0.8964, Total Mean Loss: 1.7762, LR: 0.0004049187545056477, Duration: 100.85 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 88: 100%|██████████| 53/53 [01:39<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5887, Val_Loss: 0.8557, Total Mean Loss: 1.7222, LR: 0.00034591384670247825, Duration: 100.54 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 89: 100%|██████████| 53/53 [01:39<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4711, Val_Loss: 0.8516, Total Mean Loss: 1.6613, LR: 0.0002913609409255791, Duration: 100.97 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 90: 100%|██████████| 53/53 [01:40<00:00,  1.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5234, Val_Loss: 0.8694, Total Mean Loss: 1.6964, LR: 0.00024132650148740606, Duration: 101.34 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 91: 100%|██████████| 53/53 [01:39<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5279, Val_Loss: 0.8560, Total Mean Loss: 1.6920, LR: 0.00019587148764520066, Duration: 100.64 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 92: 100%|██████████| 53/53 [01:40<00:00,  1.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5953, Val_Loss: 0.8657, Total Mean Loss: 1.7305, LR: 0.00015505127933152136, Duration: 101.71 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 93: 100%|██████████| 53/53 [01:41<00:00,  1.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6174, Val_Loss: 0.8608, Total Mean Loss: 1.7391, LR: 0.00011891560968232792, Duration: 102.95 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 94: 100%|██████████| 53/53 [01:41<00:00,  1.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4460, Val_Loss: 0.8459, Total Mean Loss: 1.6459, LR: 8.750850444481394e-05, Duration: 102.68 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 95: 100%|██████████| 53/53 [01:41<00:00,  1.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5132, Val_Loss: 0.8560, Total Mean Loss: 1.6846, LR: 6.0868228338818537e-05, Duration: 102.48 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 96: 100%|██████████| 53/53 [01:41<00:00,  1.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4481, Val_Loss: 0.8532, Total Mean Loss: 1.6506, LR: 3.902723843715564e-05, Duration: 102.59 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 97: 100%|██████████| 53/53 [01:41<00:00,  1.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4170, Val_Loss: 0.8491, Total Mean Loss: 1.6330, LR: 2.2012144621675257e-05, Duration: 102.89 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 98: 100%|██████████| 53/53 [01:41<00:00,  1.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5952, Val_Loss: 0.8477, Total Mean Loss: 1.7215, LR: 9.843677163216202e-06, Duration: 103.13 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 53/53 [01:40<00:00,  1.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5665, Val_Loss: 0.8481, Total Mean Loss: 1.7073, LR: 2.5366614649679064e-06, Duration: 102.18 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 100: 100%|██████████| 53/53 [01:40<00:00,  1.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5940, Val_Loss: 0.8485, Total Mean Loss: 1.7212, LR: 1e-07, Duration: 101.85 sec\n",
      "\n",
      "[100 epoch result]\n",
      "       Metric     Value\n",
      "0   Accuracy  0.858000\n",
      "1  Precision  0.885246\n",
      "2     Recall  0.858000\n",
      "3   F1 Score  0.852777\n"
     ]
    }
   ],
   "source": [
    "training_time = 0\n",
    "losses = []\n",
    "val_losses = []\n",
    "lrs = []\n",
    "best_loss = float('inf')\n",
    "model_save = False\n",
    "\n",
    "for i in range(epochs // 100):\n",
    "    for epoch in range(100):\n",
    "        model.train()\n",
    "        start_time = time.time()\n",
    "        running_loss = 0.0\n",
    "        pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch + 1 + i*100}\")\n",
    "        \n",
    "        for _, data in pbar:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            inputs, labels = mixup_fn(inputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "                \n",
    "            loss.backward()\n",
    "            # 그라디언트 클리핑 적용\n",
    "            clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "            optimizer.step()\n",
    "            \n",
    "            # EMA 모델 업데이트, 필요한 경우\n",
    "            if model_ema is not None:\n",
    "                model_ema.update(model)\n",
    "                \n",
    "            scheduler.step()\n",
    "                \n",
    "            lr = optimizer.param_groups[0][\"lr\"]\n",
    "            lrs.append(lr)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        losses.append(epoch_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data in valid_loader:\n",
    "                inputs, labels = data[0].to(device), data[1].to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "        val_loss /= len(valid_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # 모델 저장 조건 수정\n",
    "        total_loss = val_loss + epoch_loss\n",
    "        if total_loss < best_loss:\n",
    "            best_loss = total_loss\n",
    "            # torch.save(model.state_dict(), model_path)\n",
    "            model_save = True\n",
    "            save_text = ' - model saved!'\n",
    "        else:\n",
    "            save_text = ''\n",
    "\n",
    "        epoch_duration = time.time() - start_time\n",
    "        training_time += epoch_duration\n",
    "        \n",
    "        text = f'\\tLoss: {epoch_loss:.4f}, Val_Loss: {val_loss:.4f}, Total Mean Loss: {total_loss/2:.4f}, LR: {lr}, Duration: {epoch_duration:.2f} sec{save_text}'\n",
    "        print(text)\n",
    "\n",
    "    # 예측 수행 및 레이블 저장\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # 혼동 행렬 생성\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    # 예측과 실제 레이블\n",
    "    y_true = all_labels  # 실제 레이블\n",
    "    y_pred = all_preds  # 모델에 의해 예측된 레이블\n",
    "\n",
    "    # 전체 데이터셋에 대한 정확도\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # 평균 정밀도, 리콜, F1-Score ('weighted')\n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "\n",
    "    # 판다스 데이터프레임으로 결과 정리\n",
    "    performance_metrics = pd.DataFrame({\n",
    "        'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "        'Value': [accuracy, precision, recall, f1_score]\n",
    "    })\n",
    "\n",
    "    # 데이터프레임 출력\n",
    "    print(f\"\\n[{i*100+100} epoch result]\\n\", performance_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
