{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "from timm.data import Mixup\n",
    "import transformers\n",
    "\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "from timm.models.layers import DropPath, trunc_normal_, to_2tuple\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01.Convolutional Token Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvEmbed(nn.Module):\n",
    "    '''\n",
    "    img/token map to Conv Embedding\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,\n",
    "                 patch_size=11, # [11, 7, 7, 3]\n",
    "                 in_chans=3,   # [3, dim of stage1, dim of stage2]\n",
    "                 embed_dim=64, # [32, 64, 192, 384]\n",
    "                 stride=4,     # [6, 4, 4, 2]\n",
    "                 padding=2,    # [3, 2, 2, 1]\n",
    "                 norm_layer=None):\n",
    "        super().__init__()\n",
    "        self.patch_size = to_2tuple(patch_size)\n",
    "        \n",
    "        self.proj = nn.Conv2d(\n",
    "            in_channels=in_chans,\n",
    "            out_channels=embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=stride,\n",
    "            padding=padding\n",
    "        )\n",
    "        \n",
    "        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        \n",
    "        _, _, H, W = x.shape\n",
    "        x = rearrange(x, 'b c h w -> b (h w) c')\n",
    "        x = self.norm(x)\n",
    "        x = rearrange(x, 'b (h w) c -> b c h w', h=H, w=W)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    r\"\"\" LayerNorm that supports two data formats: channels_last (default) or channels_first. \n",
    "    The ordering of the dimensions in the inputs. channels_last corresponds to inputs with \n",
    "    shape (batch_size, height, width, channels) while channels_first corresponds to inputs \n",
    "    with shape (batch_size, channels, height, width).\n",
    "    \"\"\"\n",
    "    def __init__(self, normalized_shape, eps=1e-6, data_format=\"channels_last\"):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
    "        self.eps = eps\n",
    "        self.data_format = data_format\n",
    "        if self.data_format not in [\"channels_last\", \"channels_first\"]:\n",
    "            raise NotImplementedError \n",
    "        self.normalized_shape = (normalized_shape, )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.data_format == \"channels_last\":\n",
    "            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
    "        elif self.data_format == \"channels_first\":\n",
    "            u = x.mean(1, keepdim=True)\n",
    "            s = (x - u).pow(2).mean(1, keepdim=True)\n",
    "            x = (x - u) / torch.sqrt(s + self.eps)\n",
    "            x = self.weight[:, None, None] * x + self.bias[:, None, None]\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionConv(nn.Module):\n",
    "    def __init__(self,\n",
    "                 dim=64,        # [32,64,192,384]\n",
    "                 num_heads=4,   # [1,3,6,9]\n",
    "                 qkv_bias=False,\n",
    "                 attn_drop=0.,\n",
    "                 proj_drop=0.,\n",
    "                 kernel_size=3,\n",
    "                 padding_q=1,\n",
    "                 padding_kv=1,\n",
    "                 stride_q=1,\n",
    "                 stride_kv=2,\n",
    "                 act_layer=nn.GELU,\n",
    "                 **kwargs\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.qkv_bias = qkv_bias\n",
    "        self.stride_q = stride_q\n",
    "        self.stride_kv = stride_kv\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads        \n",
    "        self.scale = dim ** -0.5\n",
    "        self.act_layer = act_layer()\n",
    "        \n",
    "        self.conv_proj_q = self._build_projection(dim,\n",
    "                                                  kernel_size,\n",
    "                                                  padding_q,\n",
    "                                                  stride_q,\n",
    "                                                  )\n",
    "        self.conv_proj_k = self._build_projection(dim,\n",
    "                                                  kernel_size,\n",
    "                                                  padding_kv,\n",
    "                                                  stride_kv,\n",
    "                                                  )\n",
    "        \n",
    "        self.conv_proj_v = self._build_projection(dim,\n",
    "                                                  kernel_size,\n",
    "                                                  padding_kv,\n",
    "                                                  stride_kv,\n",
    "                                                  )\n",
    "        \n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.linear_proj_last = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)        \n",
    "        \n",
    "    def _build_projection(self,\n",
    "                          dim,\n",
    "                          kernel_size,\n",
    "                          padding,\n",
    "                          stride,\n",
    "                          ):\n",
    "        \n",
    "        proj = nn.Sequential(OrderedDict([\n",
    "            ('depthwise', nn.Conv2d(\n",
    "                dim,\n",
    "                dim,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=padding,\n",
    "                stride=stride,\n",
    "                bias=self.qkv_bias,\n",
    "                groups=dim)),\n",
    "            ('rearrange1', Rearrange('b c h w -> b h w c')),\n",
    "            ('ln', nn.LayerNorm(dim)),\n",
    "            ('rearrange2', Rearrange('b h w c -> b c h w')),\n",
    "            ('pointwise1', nn.Conv2d(\n",
    "                dim,\n",
    "                dim*4,\n",
    "                kernel_size=1,\n",
    "                bias=self.qkv_bias)),\n",
    "            ('activation', self.act_layer),\n",
    "            ('pointwise2', nn.Conv2d(\n",
    "                dim*4,\n",
    "                dim,\n",
    "                kernel_size=1,\n",
    "                bias=self.qkv_bias)),\n",
    "            ('rearrange3', Rearrange('b c h w -> b (h w) c')),\n",
    "\n",
    "        ]))\n",
    "        \n",
    "        return proj\n",
    "    \n",
    "    def forward(self, x, h, w):\n",
    "        x = rearrange(x, 'b (h w) c -> b c h w', h=h, w=w)\n",
    "        \n",
    "        q = F.normalize(self.conv_proj_q(x), dim=-1)\n",
    "        k = F.normalize(self.conv_proj_k(x), dim=-1)\n",
    "        v = self.conv_proj_v(x)\n",
    "        \n",
    "        q = rearrange(q, 'b t (h d) -> b h t d', h=self.num_heads)\n",
    "        k = rearrange(k, 'b t (h d) -> b h t d', h=self.num_heads)\n",
    "        v = rearrange(v, 'b t (h d) -> b h t d', h=self.num_heads)\n",
    "        \n",
    "        attn_score = torch.einsum('bhlk,bhtk->bhlt', [q, k]) * self.scale\n",
    "        attn = self.attn_drop(F.softmax(attn_score, dim=-1))\n",
    "        \n",
    "        x = torch.matmul(attn, v)\n",
    "        batch_size, num_heads, seq_length, depth = x.size()\n",
    "        x = x.view(batch_size, seq_length, num_heads * depth)\n",
    "        \n",
    "        x = self.proj_drop(self.linear_proj_last(x))\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer block에 작은 스케일 인자 곱하기\n",
    "class LayerScale(nn.Module):\n",
    "    def __init__(self, dim, init_values=1e-5):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(init_values * torch.ones((dim)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gamma * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 num_heads,\n",
    "                 mlp_ratio=4.,\n",
    "                 qkv_bias=False,\n",
    "                 drop=0.,\n",
    "                 attn_drop=0.,\n",
    "                 drop_path=0.,\n",
    "                 act_layer=nn.GELU,\n",
    "                 norm_layer=nn.LayerNorm,\n",
    "                 **kwargs\n",
    "                ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.ls1 = LayerScale(dim)\n",
    "        self.attn = AttentionConv(dim=dim,\n",
    "                                  num_heads=num_heads,\n",
    "                                  qkv_bias=qkv_bias,\n",
    "                                  attn_drop=attn_drop,\n",
    "                                  proj_drop=drop,\n",
    "                                  act_layer=act_layer,\n",
    "                                  **kwargs)        \n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        self.ls2 = LayerScale(dim)\n",
    "        mlp_hidden_dim = int(dim*mlp_ratio)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, mlp_hidden_dim),\n",
    "            act_layer(),\n",
    "            nn.Linear(mlp_hidden_dim, dim),\n",
    "            nn.Dropout(drop),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, h, w):\n",
    "        res = x\n",
    "        x = self.norm1(x)\n",
    "        attn = self.attn(x, h, w)\n",
    "        x = res + self.drop_path(self.ls1(attn))\n",
    "        x = x + self.drop_path(self.ls2(self.mlp(self.norm2(x))))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = torch.Tensor(np.zeros((2,3,224,224))) # B, C, H, W\n",
    "\n",
    "block = Block(dim=64,\n",
    "              num_heads=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stage 1 | img shape: torch.Size([2, 3, 224, 224]) → Conv Embed Shape: torch.Size([2, 64, 56, 56])\n"
     ]
    }
   ],
   "source": [
    "# Stage 1 \n",
    "\n",
    "## Patch Embedding\n",
    "convembed = ConvEmbed(patch_size=7, stride=4, padding=2)\n",
    "stage1_img = convembed(test_img)\n",
    "\n",
    "## Attention with Convolution\n",
    "b, c, h, w = stage1_img.shape\n",
    "stage1_img = rearrange(stage1_img, 'b c h w -> b (h w) c')\n",
    "stage1_img = block(stage1_img, h=h, w=w)\n",
    "stage1_img = rearrange(stage1_img, 'b (h w) c -> b c h w', h=h, w=w)\n",
    "\n",
    "## Check Result\n",
    "print(f'stage 1 | img shape: {test_img.shape} → Conv Embed Shape: {stage1_img.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stage 2 | img shape: torch.Size([2, 64, 56, 56]) → Conv Embed Shape: torch.Size([2, 64, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# Stage 2 \n",
    "\n",
    "## Patch Embedding\n",
    "convembed = ConvEmbed(patch_size=7, in_chans=64, stride=2, padding=3)\n",
    "stage2_img = convembed(stage1_img)\n",
    "\n",
    "## Attention with Convolution\n",
    "b, c, h, w = stage2_img.shape\n",
    "stage2_img = rearrange(stage2_img, 'b c h w -> b (h w) c')\n",
    "stage2_img = block(stage2_img, h=h, w=w)\n",
    "stage2_img = rearrange(stage2_img, 'b (h w) c -> b c h w', h=h, w=w)\n",
    "\n",
    "## Check Result\n",
    "print(f'stage 2 | img shape: {stage1_img.shape} → Conv Embed Shape: {stage2_img.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stage 3 | img shape: torch.Size([2, 64, 28, 28]) → Conv Embed Shape: torch.Size([2, 64, 14, 14])\n"
     ]
    }
   ],
   "source": [
    "# Stage 3 \n",
    "\n",
    "## Patch Embedding\n",
    "convembed = ConvEmbed(patch_size=7, in_chans=64, stride=2, padding=3)\n",
    "stage3_img = convembed(stage2_img)\n",
    "\n",
    "## Attention with Convolution\n",
    "b, c, h, w = stage3_img.shape\n",
    "stage3_img = rearrange(stage3_img, 'b c h w -> b (h w) c')\n",
    "stage3_img = block(stage3_img, h=h, w=w)\n",
    "stage3_img = rearrange(stage3_img, 'b (h w) c -> b c h w', h=h, w=w)\n",
    "\n",
    "## Check Result\n",
    "print(f'stage 3 | img shape: {stage2_img.shape} → Conv Embed Shape: {stage3_img.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stage 4 | img shape: torch.Size([2, 64, 14, 14]) → Conv Embed Shape: torch.Size([2, 64, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "# Stage 4 \n",
    "\n",
    "## Patch Embedding\n",
    "convembed = ConvEmbed(patch_size=7, in_chans=64, stride=2, padding=3)\n",
    "stage4_img = convembed(stage3_img)\n",
    "\n",
    "## Attention with Convolution\n",
    "b, c, h, w = stage4_img.shape\n",
    "stage4_img = rearrange(stage4_img, 'b c h w -> b (h w) c')\n",
    "stage4_img = block(stage4_img, h=h, w=w)\n",
    "stage4_img = rearrange(stage4_img, 'b (h w) c -> b c h w', h=h, w=w)\n",
    "\n",
    "## Check Result\n",
    "print(f'stage 4 | img shape: {stage3_img.shape} → Conv Embed Shape: {stage4_img.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 patch_size=16,\n",
    "                 patch_stride=16,\n",
    "                 patch_padding=0,\n",
    "                 in_chans=3,\n",
    "                 embed_dim=768,\n",
    "                 depth=12,\n",
    "                 num_heads=12,\n",
    "                 mlp_ratio=4.,\n",
    "                 qkv_bias=False,\n",
    "                 drop_rate=0.,\n",
    "                 attn_drop_rate=0.,\n",
    "                 drop_path_rate=0.,\n",
    "                 act_layer=nn.GELU,\n",
    "                 norm_layer=nn.LayerNorm,\n",
    "                 init='trunc_norm',\n",
    "                 **kwargs\n",
    "                 ):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_embed = ConvEmbed(\n",
    "            patch_size=patch_size,\n",
    "            in_chans=in_chans,\n",
    "            stride=patch_stride,\n",
    "            padding=patch_padding,\n",
    "            embed_dim=embed_dim,\n",
    "            norm_layer=norm_layer\n",
    "        )\n",
    "\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                drop=drop_rate,\n",
    "                attn_drop=attn_drop_rate,\n",
    "                drop_path=drop_path_rate,\n",
    "                act_layer=act_layer,\n",
    "                norm_layer=norm_layer,\n",
    "                **kwargs\n",
    "            ) for _ in range(depth)\n",
    "        ])\n",
    "\n",
    "        if init == 'xavier':\n",
    "            self.apply(self._init_weights_xavier)\n",
    "        else:\n",
    "            self.apply(self._init_weights_trunc_normal)\n",
    "\n",
    "    def _init_weights_trunc_normal(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, (nn.LayerNorm, nn.BatchNorm2d)):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def _init_weights_xavier(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, (nn.LayerNorm, nn.BatchNorm2d)):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        _, _, H, W = x.size()\n",
    "\n",
    "        x = rearrange(x, 'b c h w -> b (h w) c')\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for _, blk in enumerate(self.blocks):\n",
    "            x = blk(x, H, W)\n",
    "        x = rearrange(x, 'b (h w) c -> b c h w', h=H, w=W)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalVisionTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_chans=3,\n",
    "                 num_classes=100,\n",
    "                 act_layer=nn.GELU,\n",
    "                 norm_layer=nn.LayerNorm,\n",
    "                 init='trunc_norm',\n",
    "                 spec=None):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.num_stages = spec['NUM_STAGES']\n",
    "        self.stages = nn.ModuleList()\n",
    "        for i in range(self.num_stages):\n",
    "            kwargs = {\n",
    "                'patch_size': spec['PATCH_SIZE'][i],\n",
    "                'patch_stride': spec['PATCH_STRIDE'][i],\n",
    "                'patch_padding': spec['PATCH_PADDING'][i],\n",
    "                'embed_dim': spec['DIM_EMBED'][i],\n",
    "                'depth': spec['DEPTH'][i],\n",
    "                'num_heads': spec['NUM_HEADS'][i],\n",
    "                'mlp_ratio': spec['MLP_RATIO'][i],\n",
    "                'qkv_bias': spec['QKV_BIAS'][i],\n",
    "                'drop_rate': spec['DROP_RATE'][i],\n",
    "                'attn_drop_rate': spec['ATTN_DROP_RATE'][i],\n",
    "                'drop_path_rate': spec['DROP_PATH_RATE'][i],\n",
    "                'kernel_size': spec['KERNEL_QKV'][i],\n",
    "                'padding_q': spec['PADDING_Q'][i],\n",
    "                'padding_kv': spec['PADDING_KV'][i],\n",
    "                'stride_q': spec['STRIDE_Q'][i],\n",
    "                'stride_kv': spec['STRIDE_KV'][i],\n",
    "            }\n",
    "\n",
    "            stage = VisionTransformer(\n",
    "                in_chans=in_chans,\n",
    "                init=init,\n",
    "                act_layer=act_layer,\n",
    "                norm_layer=norm_layer,\n",
    "                **kwargs\n",
    "            )\n",
    "            \n",
    "            self.stages.append(stage)\n",
    "\n",
    "            in_chans = spec['DIM_EMBED'][i]\n",
    "\n",
    "        dim_embed = spec['DIM_EMBED'][-1]\n",
    "        self.norm = norm_layer(dim_embed)\n",
    "        self.pooler = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "        # Classifier head\n",
    "        self.head = nn.Linear(dim_embed, num_classes) if num_classes > 0 else nn.Identity()\n",
    "        trunc_normal_(self.head.weight, std=0.02)\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        for stage in self.stages:\n",
    "            x = stage(x)\n",
    "\n",
    "        x = rearrange(x, 'b c h w -> b (h w) c') # (B, L, C)\n",
    "        x = self.norm(x)                         # (B, L, C)\n",
    "        x = self.pooler(x.transpose(1,2))        # (B, C, 1)\n",
    "        x = torch.flatten(x, 1)                  # (B, C)\n",
    "        # x = torch.mean(x, dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.head(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuickGELU(nn.Module):\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return x * torch.sigmoid(1.702 * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NUM_STAGES': 4,\n",
       " 'PATCH_SIZE': [7, 7, 7, 7],\n",
       " 'PATCH_STRIDE': [4, 2, 2, 2],\n",
       " 'PATCH_PADDING': [2, 3, 3, 3],\n",
       " 'DIM_EMBED': [64, 128, 192, 256],\n",
       " 'DEPTH': [2, 2, 6, 2],\n",
       " 'NUM_HEADS': [4, 8, 8, 16],\n",
       " 'MLP_RATIO': [4.0, 4.0, 4.0, 4.0],\n",
       " 'QKV_BIAS': [True, True, True, True],\n",
       " 'DROP_RATE': [0.0, 0.0, 0.0, 0.0],\n",
       " 'ATTN_DROP_RATE': [0.0, 0.0, 0.0, 0.0],\n",
       " 'DROP_PATH_RATE': [0.0, 0.0, 0.1, 0.1],\n",
       " 'KERNEL_QKV': [3, 3, 3, 3],\n",
       " 'PADDING_Q': [1, 1, 1, 1],\n",
       " 'PADDING_KV': [1, 1, 1, 1],\n",
       " 'STRIDE_Q': [1, 1, 1, 1],\n",
       " 'STRIDE_KV': [2, 2, 2, 2]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec = {\n",
    "    'NUM_STAGES': 4,\n",
    "    'PATCH_SIZE': [7,7,7,7],\n",
    "    'PATCH_STRIDE': [4,2,2,2],\n",
    "    'PATCH_PADDING': [2,3,3,3],\n",
    "    'DIM_EMBED': [64,128,192,256],\n",
    "    'DEPTH': [2,2,6,2],\n",
    "    'NUM_HEADS': [4,8,8,16],   # original : [1,3,6]\n",
    "    'MLP_RATIO': [4.,4.,4.,4.],\n",
    "    'QKV_BIAS': [True, True, True, True],\n",
    "    'DROP_RATE': [0.,0.,0.,0.],\n",
    "    'ATTN_DROP_RATE': [0.,0.,0.,0.],\n",
    "    'DROP_PATH_RATE': [0.,0.,0.1,0.1],\n",
    "    'KERNEL_QKV': [3,3,3,3],\n",
    "    'PADDING_Q': [1,1,1,1],\n",
    "    'PADDING_KV': [1,1,1,1],\n",
    "    'STRIDE_Q': [1,1,1,1],\n",
    "    'STRIDE_KV': [2,2,2,2],\n",
    "}\n",
    "\n",
    "spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 224, 224]), torch.Size([2, 100]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ConvolutionalVisionTransformer(act_layer=QuickGELU, spec=spec)\n",
    "\n",
    "test_result = model(test_img)\n",
    "test_img.shape, test_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 56, 56]           9,472\n",
      "         LayerNorm-2             [-1, 3136, 64]             128\n",
      "         ConvEmbed-3           [-1, 64, 56, 56]               0\n",
      "           Dropout-4             [-1, 3136, 64]               0\n",
      "         LayerNorm-5             [-1, 3136, 64]             128\n",
      "            Conv2d-6           [-1, 64, 56, 56]             640\n",
      "         Rearrange-7           [-1, 56, 56, 64]               0\n",
      "         LayerNorm-8           [-1, 56, 56, 64]             128\n",
      "         Rearrange-9           [-1, 64, 56, 56]               0\n",
      "           Conv2d-10          [-1, 256, 56, 56]          16,640\n",
      "        QuickGELU-11          [-1, 256, 56, 56]               0\n",
      "        QuickGELU-12          [-1, 256, 56, 56]               0\n",
      "        QuickGELU-13          [-1, 256, 56, 56]               0\n",
      "        QuickGELU-14          [-1, 256, 56, 56]               0\n",
      "           Conv2d-15           [-1, 64, 56, 56]          16,448\n",
      "        Rearrange-16             [-1, 3136, 64]               0\n",
      "           Conv2d-17           [-1, 64, 28, 28]             640\n",
      "        Rearrange-18           [-1, 28, 28, 64]               0\n",
      "        LayerNorm-19           [-1, 28, 28, 64]             128\n",
      "        Rearrange-20           [-1, 64, 28, 28]               0\n",
      "           Conv2d-21          [-1, 256, 28, 28]          16,640\n",
      "        QuickGELU-22          [-1, 256, 28, 28]               0\n",
      "        QuickGELU-23          [-1, 256, 28, 28]               0\n",
      "        QuickGELU-24          [-1, 256, 28, 28]               0\n",
      "        QuickGELU-25          [-1, 256, 28, 28]               0\n",
      "           Conv2d-26           [-1, 64, 28, 28]          16,448\n",
      "        Rearrange-27              [-1, 784, 64]               0\n",
      "           Conv2d-28           [-1, 64, 28, 28]             640\n",
      "        Rearrange-29           [-1, 28, 28, 64]               0\n",
      "        LayerNorm-30           [-1, 28, 28, 64]             128\n",
      "        Rearrange-31           [-1, 64, 28, 28]               0\n",
      "           Conv2d-32          [-1, 256, 28, 28]          16,640\n",
      "        QuickGELU-33          [-1, 256, 28, 28]               0\n",
      "        QuickGELU-34          [-1, 256, 28, 28]               0\n",
      "        QuickGELU-35          [-1, 256, 28, 28]               0\n",
      "        QuickGELU-36          [-1, 256, 28, 28]               0\n",
      "           Conv2d-37           [-1, 64, 28, 28]          16,448\n",
      "        Rearrange-38              [-1, 784, 64]               0\n",
      "          Dropout-39         [-1, 4, 3136, 784]               0\n",
      "           Linear-40             [-1, 3136, 64]           4,160\n",
      "          Dropout-41             [-1, 3136, 64]               0\n",
      "    AttentionConv-42             [-1, 3136, 64]               0\n",
      "       LayerScale-43             [-1, 3136, 64]               0\n",
      "         Identity-44             [-1, 3136, 64]               0\n",
      "        LayerNorm-45             [-1, 3136, 64]             128\n",
      "           Linear-46            [-1, 3136, 256]          16,640\n",
      "        QuickGELU-47            [-1, 3136, 256]               0\n",
      "           Linear-48             [-1, 3136, 64]          16,448\n",
      "          Dropout-49             [-1, 3136, 64]               0\n",
      "       LayerScale-50             [-1, 3136, 64]               0\n",
      "         Identity-51             [-1, 3136, 64]               0\n",
      "            Block-52             [-1, 3136, 64]               0\n",
      "        LayerNorm-53             [-1, 3136, 64]             128\n",
      "           Conv2d-54           [-1, 64, 56, 56]             640\n",
      "        Rearrange-55           [-1, 56, 56, 64]               0\n",
      "        LayerNorm-56           [-1, 56, 56, 64]             128\n",
      "        Rearrange-57           [-1, 64, 56, 56]               0\n",
      "           Conv2d-58          [-1, 256, 56, 56]          16,640\n",
      "        QuickGELU-59          [-1, 256, 56, 56]               0\n",
      "        QuickGELU-60          [-1, 256, 56, 56]               0\n",
      "        QuickGELU-61          [-1, 256, 56, 56]               0\n",
      "        QuickGELU-62          [-1, 256, 56, 56]               0\n",
      "           Conv2d-63           [-1, 64, 56, 56]          16,448\n",
      "        Rearrange-64             [-1, 3136, 64]               0\n",
      "           Conv2d-65           [-1, 64, 28, 28]             640\n",
      "        Rearrange-66           [-1, 28, 28, 64]               0\n",
      "        LayerNorm-67           [-1, 28, 28, 64]             128\n",
      "        Rearrange-68           [-1, 64, 28, 28]               0\n",
      "           Conv2d-69          [-1, 256, 28, 28]          16,640\n",
      "        QuickGELU-70          [-1, 256, 28, 28]               0\n",
      "        QuickGELU-71          [-1, 256, 28, 28]               0\n",
      "        QuickGELU-72          [-1, 256, 28, 28]               0\n",
      "        QuickGELU-73          [-1, 256, 28, 28]               0\n",
      "           Conv2d-74           [-1, 64, 28, 28]          16,448\n",
      "        Rearrange-75              [-1, 784, 64]               0\n",
      "           Conv2d-76           [-1, 64, 28, 28]             640\n",
      "        Rearrange-77           [-1, 28, 28, 64]               0\n",
      "        LayerNorm-78           [-1, 28, 28, 64]             128\n",
      "        Rearrange-79           [-1, 64, 28, 28]               0\n",
      "           Conv2d-80          [-1, 256, 28, 28]          16,640\n",
      "        QuickGELU-81          [-1, 256, 28, 28]               0\n",
      "        QuickGELU-82          [-1, 256, 28, 28]               0\n",
      "        QuickGELU-83          [-1, 256, 28, 28]               0\n",
      "        QuickGELU-84          [-1, 256, 28, 28]               0\n",
      "           Conv2d-85           [-1, 64, 28, 28]          16,448\n",
      "        Rearrange-86              [-1, 784, 64]               0\n",
      "          Dropout-87         [-1, 4, 3136, 784]               0\n",
      "           Linear-88             [-1, 3136, 64]           4,160\n",
      "          Dropout-89             [-1, 3136, 64]               0\n",
      "    AttentionConv-90             [-1, 3136, 64]               0\n",
      "       LayerScale-91             [-1, 3136, 64]               0\n",
      "         Identity-92             [-1, 3136, 64]               0\n",
      "        LayerNorm-93             [-1, 3136, 64]             128\n",
      "           Linear-94            [-1, 3136, 256]          16,640\n",
      "        QuickGELU-95            [-1, 3136, 256]               0\n",
      "           Linear-96             [-1, 3136, 64]          16,448\n",
      "          Dropout-97             [-1, 3136, 64]               0\n",
      "       LayerScale-98             [-1, 3136, 64]               0\n",
      "         Identity-99             [-1, 3136, 64]               0\n",
      "           Block-100             [-1, 3136, 64]               0\n",
      "VisionTransformer-101           [-1, 64, 56, 56]               0\n",
      "          Conv2d-102          [-1, 128, 28, 28]         401,536\n",
      "       LayerNorm-103             [-1, 784, 128]             256\n",
      "       ConvEmbed-104          [-1, 128, 28, 28]               0\n",
      "         Dropout-105             [-1, 784, 128]               0\n",
      "       LayerNorm-106             [-1, 784, 128]             256\n",
      "          Conv2d-107          [-1, 128, 28, 28]           1,280\n",
      "       Rearrange-108          [-1, 28, 28, 128]               0\n",
      "       LayerNorm-109          [-1, 28, 28, 128]             256\n",
      "       Rearrange-110          [-1, 128, 28, 28]               0\n",
      "          Conv2d-111          [-1, 512, 28, 28]          66,048\n",
      "       QuickGELU-112          [-1, 512, 28, 28]               0\n",
      "       QuickGELU-113          [-1, 512, 28, 28]               0\n",
      "       QuickGELU-114          [-1, 512, 28, 28]               0\n",
      "       QuickGELU-115          [-1, 512, 28, 28]               0\n",
      "          Conv2d-116          [-1, 128, 28, 28]          65,664\n",
      "       Rearrange-117             [-1, 784, 128]               0\n",
      "          Conv2d-118          [-1, 128, 14, 14]           1,280\n",
      "       Rearrange-119          [-1, 14, 14, 128]               0\n",
      "       LayerNorm-120          [-1, 14, 14, 128]             256\n",
      "       Rearrange-121          [-1, 128, 14, 14]               0\n",
      "          Conv2d-122          [-1, 512, 14, 14]          66,048\n",
      "       QuickGELU-123          [-1, 512, 14, 14]               0\n",
      "       QuickGELU-124          [-1, 512, 14, 14]               0\n",
      "       QuickGELU-125          [-1, 512, 14, 14]               0\n",
      "       QuickGELU-126          [-1, 512, 14, 14]               0\n",
      "          Conv2d-127          [-1, 128, 14, 14]          65,664\n",
      "       Rearrange-128             [-1, 196, 128]               0\n",
      "          Conv2d-129          [-1, 128, 14, 14]           1,280\n",
      "       Rearrange-130          [-1, 14, 14, 128]               0\n",
      "       LayerNorm-131          [-1, 14, 14, 128]             256\n",
      "       Rearrange-132          [-1, 128, 14, 14]               0\n",
      "          Conv2d-133          [-1, 512, 14, 14]          66,048\n",
      "       QuickGELU-134          [-1, 512, 14, 14]               0\n",
      "       QuickGELU-135          [-1, 512, 14, 14]               0\n",
      "       QuickGELU-136          [-1, 512, 14, 14]               0\n",
      "       QuickGELU-137          [-1, 512, 14, 14]               0\n",
      "          Conv2d-138          [-1, 128, 14, 14]          65,664\n",
      "       Rearrange-139             [-1, 196, 128]               0\n",
      "         Dropout-140          [-1, 8, 784, 196]               0\n",
      "          Linear-141             [-1, 784, 128]          16,512\n",
      "         Dropout-142             [-1, 784, 128]               0\n",
      "   AttentionConv-143             [-1, 784, 128]               0\n",
      "      LayerScale-144             [-1, 784, 128]               0\n",
      "        Identity-145             [-1, 784, 128]               0\n",
      "       LayerNorm-146             [-1, 784, 128]             256\n",
      "          Linear-147             [-1, 784, 512]          66,048\n",
      "       QuickGELU-148             [-1, 784, 512]               0\n",
      "          Linear-149             [-1, 784, 128]          65,664\n",
      "         Dropout-150             [-1, 784, 128]               0\n",
      "      LayerScale-151             [-1, 784, 128]               0\n",
      "        Identity-152             [-1, 784, 128]               0\n",
      "           Block-153             [-1, 784, 128]               0\n",
      "       LayerNorm-154             [-1, 784, 128]             256\n",
      "          Conv2d-155          [-1, 128, 28, 28]           1,280\n",
      "       Rearrange-156          [-1, 28, 28, 128]               0\n",
      "       LayerNorm-157          [-1, 28, 28, 128]             256\n",
      "       Rearrange-158          [-1, 128, 28, 28]               0\n",
      "          Conv2d-159          [-1, 512, 28, 28]          66,048\n",
      "       QuickGELU-160          [-1, 512, 28, 28]               0\n",
      "       QuickGELU-161          [-1, 512, 28, 28]               0\n",
      "       QuickGELU-162          [-1, 512, 28, 28]               0\n",
      "       QuickGELU-163          [-1, 512, 28, 28]               0\n",
      "          Conv2d-164          [-1, 128, 28, 28]          65,664\n",
      "       Rearrange-165             [-1, 784, 128]               0\n",
      "          Conv2d-166          [-1, 128, 14, 14]           1,280\n",
      "       Rearrange-167          [-1, 14, 14, 128]               0\n",
      "       LayerNorm-168          [-1, 14, 14, 128]             256\n",
      "       Rearrange-169          [-1, 128, 14, 14]               0\n",
      "          Conv2d-170          [-1, 512, 14, 14]          66,048\n",
      "       QuickGELU-171          [-1, 512, 14, 14]               0\n",
      "       QuickGELU-172          [-1, 512, 14, 14]               0\n",
      "       QuickGELU-173          [-1, 512, 14, 14]               0\n",
      "       QuickGELU-174          [-1, 512, 14, 14]               0\n",
      "          Conv2d-175          [-1, 128, 14, 14]          65,664\n",
      "       Rearrange-176             [-1, 196, 128]               0\n",
      "          Conv2d-177          [-1, 128, 14, 14]           1,280\n",
      "       Rearrange-178          [-1, 14, 14, 128]               0\n",
      "       LayerNorm-179          [-1, 14, 14, 128]             256\n",
      "       Rearrange-180          [-1, 128, 14, 14]               0\n",
      "          Conv2d-181          [-1, 512, 14, 14]          66,048\n",
      "       QuickGELU-182          [-1, 512, 14, 14]               0\n",
      "       QuickGELU-183          [-1, 512, 14, 14]               0\n",
      "       QuickGELU-184          [-1, 512, 14, 14]               0\n",
      "       QuickGELU-185          [-1, 512, 14, 14]               0\n",
      "          Conv2d-186          [-1, 128, 14, 14]          65,664\n",
      "       Rearrange-187             [-1, 196, 128]               0\n",
      "         Dropout-188          [-1, 8, 784, 196]               0\n",
      "          Linear-189             [-1, 784, 128]          16,512\n",
      "         Dropout-190             [-1, 784, 128]               0\n",
      "   AttentionConv-191             [-1, 784, 128]               0\n",
      "      LayerScale-192             [-1, 784, 128]               0\n",
      "        Identity-193             [-1, 784, 128]               0\n",
      "       LayerNorm-194             [-1, 784, 128]             256\n",
      "          Linear-195             [-1, 784, 512]          66,048\n",
      "       QuickGELU-196             [-1, 784, 512]               0\n",
      "          Linear-197             [-1, 784, 128]          65,664\n",
      "         Dropout-198             [-1, 784, 128]               0\n",
      "      LayerScale-199             [-1, 784, 128]               0\n",
      "        Identity-200             [-1, 784, 128]               0\n",
      "           Block-201             [-1, 784, 128]               0\n",
      "VisionTransformer-202          [-1, 128, 28, 28]               0\n",
      "          Conv2d-203          [-1, 192, 14, 14]       1,204,416\n",
      "       LayerNorm-204             [-1, 196, 192]             384\n",
      "       ConvEmbed-205          [-1, 192, 14, 14]               0\n",
      "         Dropout-206             [-1, 196, 192]               0\n",
      "       LayerNorm-207             [-1, 196, 192]             384\n",
      "          Conv2d-208          [-1, 192, 14, 14]           1,920\n",
      "       Rearrange-209          [-1, 14, 14, 192]               0\n",
      "       LayerNorm-210          [-1, 14, 14, 192]             384\n",
      "       Rearrange-211          [-1, 192, 14, 14]               0\n",
      "          Conv2d-212          [-1, 768, 14, 14]         148,224\n",
      "       QuickGELU-213          [-1, 768, 14, 14]               0\n",
      "       QuickGELU-214          [-1, 768, 14, 14]               0\n",
      "       QuickGELU-215          [-1, 768, 14, 14]               0\n",
      "       QuickGELU-216          [-1, 768, 14, 14]               0\n",
      "          Conv2d-217          [-1, 192, 14, 14]         147,648\n",
      "       Rearrange-218             [-1, 196, 192]               0\n",
      "          Conv2d-219            [-1, 192, 7, 7]           1,920\n",
      "       Rearrange-220            [-1, 7, 7, 192]               0\n",
      "       LayerNorm-221            [-1, 7, 7, 192]             384\n",
      "       Rearrange-222            [-1, 192, 7, 7]               0\n",
      "          Conv2d-223            [-1, 768, 7, 7]         148,224\n",
      "       QuickGELU-224            [-1, 768, 7, 7]               0\n",
      "       QuickGELU-225            [-1, 768, 7, 7]               0\n",
      "       QuickGELU-226            [-1, 768, 7, 7]               0\n",
      "       QuickGELU-227            [-1, 768, 7, 7]               0\n",
      "          Conv2d-228            [-1, 192, 7, 7]         147,648\n",
      "       Rearrange-229              [-1, 49, 192]               0\n",
      "          Conv2d-230            [-1, 192, 7, 7]           1,920\n",
      "       Rearrange-231            [-1, 7, 7, 192]               0\n",
      "       LayerNorm-232            [-1, 7, 7, 192]             384\n",
      "       Rearrange-233            [-1, 192, 7, 7]               0\n",
      "          Conv2d-234            [-1, 768, 7, 7]         148,224\n",
      "       QuickGELU-235            [-1, 768, 7, 7]               0\n",
      "       QuickGELU-236            [-1, 768, 7, 7]               0\n",
      "       QuickGELU-237            [-1, 768, 7, 7]               0\n",
      "       QuickGELU-238            [-1, 768, 7, 7]               0\n",
      "          Conv2d-239            [-1, 192, 7, 7]         147,648\n",
      "       Rearrange-240              [-1, 49, 192]               0\n",
      "         Dropout-241           [-1, 8, 196, 49]               0\n",
      "          Linear-242             [-1, 196, 192]          37,056\n",
      "         Dropout-243             [-1, 196, 192]               0\n",
      "   AttentionConv-244             [-1, 196, 192]               0\n",
      "      LayerScale-245             [-1, 196, 192]               0\n",
      "        DropPath-246             [-1, 196, 192]               0\n",
      "       LayerNorm-247             [-1, 196, 192]             384\n",
      "          Linear-248             [-1, 196, 768]         148,224\n",
      "       QuickGELU-249             [-1, 196, 768]               0\n",
      "          Linear-250             [-1, 196, 192]         147,648\n",
      "         Dropout-251             [-1, 196, 192]               0\n",
      "      LayerScale-252             [-1, 196, 192]               0\n",
      "        DropPath-253             [-1, 196, 192]               0\n",
      "           Block-254             [-1, 196, 192]               0\n",
      "       LayerNorm-255             [-1, 196, 192]             384\n",
      "          Conv2d-256          [-1, 192, 14, 14]           1,920\n",
      "       Rearrange-257          [-1, 14, 14, 192]               0\n",
      "       LayerNorm-258          [-1, 14, 14, 192]             384\n",
      "       Rearrange-259          [-1, 192, 14, 14]               0\n",
      "          Conv2d-260          [-1, 768, 14, 14]         148,224\n",
      "       QuickGELU-261          [-1, 768, 14, 14]               0\n",
      "       QuickGELU-262          [-1, 768, 14, 14]               0\n",
      "       QuickGELU-263          [-1, 768, 14, 14]               0\n",
      "       QuickGELU-264          [-1, 768, 14, 14]               0\n",
      "          Conv2d-265          [-1, 192, 14, 14]         147,648\n",
      "       Rearrange-266             [-1, 196, 192]               0\n",
      "          Conv2d-267            [-1, 192, 7, 7]           1,920\n",
      "       Rearrange-268            [-1, 7, 7, 192]               0\n",
      "       LayerNorm-269            [-1, 7, 7, 192]             384\n",
      "       Rearrange-270            [-1, 192, 7, 7]               0\n",
      "          Conv2d-271            [-1, 768, 7, 7]         148,224\n",
      "       QuickGELU-272            [-1, 768, 7, 7]               0\n",
      "       QuickGELU-273            [-1, 768, 7, 7]               0\n",
      "       QuickGELU-274            [-1, 768, 7, 7]               0\n",
      "       QuickGELU-275            [-1, 768, 7, 7]               0\n",
      "          Conv2d-276            [-1, 192, 7, 7]         147,648\n",
      "       Rearrange-277              [-1, 49, 192]               0\n",
      "          Conv2d-278            [-1, 192, 7, 7]           1,920\n",
      "       Rearrange-279            [-1, 7, 7, 192]               0\n",
      "       LayerNorm-280            [-1, 7, 7, 192]             384\n",
      "       Rearrange-281            [-1, 192, 7, 7]               0\n",
      "          Conv2d-282            [-1, 768, 7, 7]         148,224\n",
      "       QuickGELU-283            [-1, 768, 7, 7]               0\n",
      "       QuickGELU-284            [-1, 768, 7, 7]               0\n",
      "       QuickGELU-285            [-1, 768, 7, 7]               0\n",
      "       QuickGELU-286            [-1, 768, 7, 7]               0\n",
      "          Conv2d-287            [-1, 192, 7, 7]         147,648\n",
      "       Rearrange-288              [-1, 49, 192]               0\n",
      "         Dropout-289           [-1, 8, 196, 49]               0\n",
      "          Linear-290             [-1, 196, 192]          37,056\n",
      "         Dropout-291             [-1, 196, 192]               0\n",
      "   AttentionConv-292             [-1, 196, 192]               0\n",
      "      LayerScale-293             [-1, 196, 192]               0\n",
      "        DropPath-294             [-1, 196, 192]               0\n",
      "       LayerNorm-295             [-1, 196, 192]             384\n",
      "          Linear-296             [-1, 196, 768]         148,224\n",
      "       QuickGELU-297             [-1, 196, 768]               0\n",
      "          Linear-298             [-1, 196, 192]         147,648\n",
      "         Dropout-299             [-1, 196, 192]               0\n",
      "      LayerScale-300             [-1, 196, 192]               0\n",
      "        DropPath-301             [-1, 196, 192]               0\n",
      "           Block-302             [-1, 196, 192]               0\n",
      "       LayerNorm-303             [-1, 196, 192]             384\n",
      "          Conv2d-304          [-1, 192, 14, 14]           1,920\n",
      "       Rearrange-305          [-1, 14, 14, 192]               0\n",
      "       LayerNorm-306          [-1, 14, 14, 192]             384\n",
      "       Rearrange-307          [-1, 192, 14, 14]               0\n",
      "          Conv2d-308          [-1, 768, 14, 14]         148,224\n",
      "       QuickGELU-309          [-1, 768, 14, 14]               0\n",
      "       QuickGELU-310          [-1, 768, 14, 14]               0\n",
      "       QuickGELU-311          [-1, 768, 14, 14]               0\n",
      "       QuickGELU-312          [-1, 768, 14, 14]               0\n",
      "          Conv2d-313          [-1, 192, 14, 14]         147,648\n",
      "       Rearrange-314             [-1, 196, 192]               0\n",
      "          Conv2d-315            [-1, 192, 7, 7]           1,920\n",
      "       Rearrange-316            [-1, 7, 7, 192]               0\n",
      "       LayerNorm-317            [-1, 7, 7, 192]             384\n",
      "       Rearrange-318            [-1, 192, 7, 7]               0\n",
      "          Conv2d-319            [-1, 768, 7, 7]         148,224\n",
      "       QuickGELU-320            [-1, 768, 7, 7]               0\n",
      "       QuickGELU-321            [-1, 768, 7, 7]               0\n",
      "       QuickGELU-322            [-1, 768, 7, 7]               0\n",
      "       QuickGELU-323            [-1, 768, 7, 7]               0\n",
      "          Conv2d-324            [-1, 192, 7, 7]         147,648\n",
      "       Rearrange-325              [-1, 49, 192]               0\n",
      "          Conv2d-326            [-1, 192, 7, 7]           1,920\n",
      "       Rearrange-327            [-1, 7, 7, 192]               0\n",
      "       LayerNorm-328            [-1, 7, 7, 192]             384\n",
      "       Rearrange-329            [-1, 192, 7, 7]               0\n",
      "          Conv2d-330            [-1, 768, 7, 7]         148,224\n",
      "       QuickGELU-331            [-1, 768, 7, 7]               0\n",
      "       QuickGELU-332            [-1, 768, 7, 7]               0\n",
      "       QuickGELU-333            [-1, 768, 7, 7]               0\n",
      "       QuickGELU-334            [-1, 768, 7, 7]               0\n",
      "          Conv2d-335            [-1, 192, 7, 7]         147,648\n",
      "       Rearrange-336              [-1, 49, 192]               0\n",
      "         Dropout-337           [-1, 8, 196, 49]               0\n",
      "          Linear-338             [-1, 196, 192]          37,056\n",
      "         Dropout-339             [-1, 196, 192]               0\n",
      "   AttentionConv-340             [-1, 196, 192]               0\n",
      "      LayerScale-341             [-1, 196, 192]               0\n",
      "        DropPath-342             [-1, 196, 192]               0\n",
      "       LayerNorm-343             [-1, 196, 192]             384\n",
      "          Linear-344             [-1, 196, 768]         148,224\n",
      "       QuickGELU-345             [-1, 196, 768]               0\n",
      "          Linear-346             [-1, 196, 192]         147,648\n",
      "         Dropout-347             [-1, 196, 192]               0\n",
      "      LayerScale-348             [-1, 196, 192]               0\n",
      "        DropPath-349             [-1, 196, 192]               0\n",
      "           Block-350             [-1, 196, 192]               0\n",
      "       LayerNorm-351             [-1, 196, 192]             384\n",
      "          Conv2d-352          [-1, 192, 14, 14]           1,920\n",
      "       Rearrange-353          [-1, 14, 14, 192]               0\n",
      "       LayerNorm-354          [-1, 14, 14, 192]             384\n",
      "       Rearrange-355          [-1, 192, 14, 14]               0\n",
      "          Conv2d-356          [-1, 768, 14, 14]         148,224\n",
      "       QuickGELU-357          [-1, 768, 14, 14]               0\n",
      "       QuickGELU-358          [-1, 768, 14, 14]               0\n",
      "       QuickGELU-359          [-1, 768, 14, 14]               0\n",
      "       QuickGELU-360          [-1, 768, 14, 14]               0\n",
      "          Conv2d-361          [-1, 192, 14, 14]         147,648\n",
      "       Rearrange-362             [-1, 196, 192]               0\n",
      "          Conv2d-363            [-1, 192, 7, 7]           1,920\n",
      "       Rearrange-364            [-1, 7, 7, 192]               0\n",
      "       LayerNorm-365            [-1, 7, 7, 192]             384\n",
      "       Rearrange-366            [-1, 192, 7, 7]               0\n",
      "          Conv2d-367            [-1, 768, 7, 7]         148,224\n",
      "       QuickGELU-368            [-1, 768, 7, 7]               0\n",
      "       QuickGELU-369            [-1, 768, 7, 7]               0\n",
      "       QuickGELU-370            [-1, 768, 7, 7]               0\n",
      "       QuickGELU-371            [-1, 768, 7, 7]               0\n",
      "          Conv2d-372            [-1, 192, 7, 7]         147,648\n",
      "       Rearrange-373              [-1, 49, 192]               0\n",
      "          Conv2d-374            [-1, 192, 7, 7]           1,920\n",
      "       Rearrange-375            [-1, 7, 7, 192]               0\n",
      "       LayerNorm-376            [-1, 7, 7, 192]             384\n",
      "       Rearrange-377            [-1, 192, 7, 7]               0\n",
      "          Conv2d-378            [-1, 768, 7, 7]         148,224\n",
      "       QuickGELU-379            [-1, 768, 7, 7]               0\n",
      "       QuickGELU-380            [-1, 768, 7, 7]               0\n",
      "       QuickGELU-381            [-1, 768, 7, 7]               0\n",
      "       QuickGELU-382            [-1, 768, 7, 7]               0\n",
      "          Conv2d-383            [-1, 192, 7, 7]         147,648\n",
      "       Rearrange-384              [-1, 49, 192]               0\n",
      "         Dropout-385           [-1, 8, 196, 49]               0\n",
      "          Linear-386             [-1, 196, 192]          37,056\n",
      "         Dropout-387             [-1, 196, 192]               0\n",
      "   AttentionConv-388             [-1, 196, 192]               0\n",
      "      LayerScale-389             [-1, 196, 192]               0\n",
      "        DropPath-390             [-1, 196, 192]               0\n",
      "       LayerNorm-391             [-1, 196, 192]             384\n",
      "          Linear-392             [-1, 196, 768]         148,224\n",
      "       QuickGELU-393             [-1, 196, 768]               0\n",
      "          Linear-394             [-1, 196, 192]         147,648\n",
      "         Dropout-395             [-1, 196, 192]               0\n",
      "      LayerScale-396             [-1, 196, 192]               0\n",
      "        DropPath-397             [-1, 196, 192]               0\n",
      "           Block-398             [-1, 196, 192]               0\n",
      "       LayerNorm-399             [-1, 196, 192]             384\n",
      "          Conv2d-400          [-1, 192, 14, 14]           1,920\n",
      "       Rearrange-401          [-1, 14, 14, 192]               0\n",
      "       LayerNorm-402          [-1, 14, 14, 192]             384\n",
      "       Rearrange-403          [-1, 192, 14, 14]               0\n",
      "          Conv2d-404          [-1, 768, 14, 14]         148,224\n",
      "       QuickGELU-405          [-1, 768, 14, 14]               0\n",
      "       QuickGELU-406          [-1, 768, 14, 14]               0\n",
      "       QuickGELU-407          [-1, 768, 14, 14]               0\n",
      "       QuickGELU-408          [-1, 768, 14, 14]               0\n",
      "          Conv2d-409          [-1, 192, 14, 14]         147,648\n",
      "       Rearrange-410             [-1, 196, 192]               0\n",
      "          Conv2d-411            [-1, 192, 7, 7]           1,920\n",
      "       Rearrange-412            [-1, 7, 7, 192]               0\n",
      "       LayerNorm-413            [-1, 7, 7, 192]             384\n",
      "       Rearrange-414            [-1, 192, 7, 7]               0\n",
      "          Conv2d-415            [-1, 768, 7, 7]         148,224\n",
      "       QuickGELU-416            [-1, 768, 7, 7]               0\n",
      "       QuickGELU-417            [-1, 768, 7, 7]               0\n",
      "       QuickGELU-418            [-1, 768, 7, 7]               0\n",
      "       QuickGELU-419            [-1, 768, 7, 7]               0\n",
      "          Conv2d-420            [-1, 192, 7, 7]         147,648\n",
      "       Rearrange-421              [-1, 49, 192]               0\n",
      "          Conv2d-422            [-1, 192, 7, 7]           1,920\n",
      "       Rearrange-423            [-1, 7, 7, 192]               0\n",
      "       LayerNorm-424            [-1, 7, 7, 192]             384\n",
      "       Rearrange-425            [-1, 192, 7, 7]               0\n",
      "          Conv2d-426            [-1, 768, 7, 7]         148,224\n",
      "       QuickGELU-427            [-1, 768, 7, 7]               0\n",
      "       QuickGELU-428            [-1, 768, 7, 7]               0\n",
      "       QuickGELU-429            [-1, 768, 7, 7]               0\n",
      "       QuickGELU-430            [-1, 768, 7, 7]               0\n",
      "          Conv2d-431            [-1, 192, 7, 7]         147,648\n",
      "       Rearrange-432              [-1, 49, 192]               0\n",
      "         Dropout-433           [-1, 8, 196, 49]               0\n",
      "          Linear-434             [-1, 196, 192]          37,056\n",
      "         Dropout-435             [-1, 196, 192]               0\n",
      "   AttentionConv-436             [-1, 196, 192]               0\n",
      "      LayerScale-437             [-1, 196, 192]               0\n",
      "        DropPath-438             [-1, 196, 192]               0\n",
      "       LayerNorm-439             [-1, 196, 192]             384\n",
      "          Linear-440             [-1, 196, 768]         148,224\n",
      "       QuickGELU-441             [-1, 196, 768]               0\n",
      "          Linear-442             [-1, 196, 192]         147,648\n",
      "         Dropout-443             [-1, 196, 192]               0\n",
      "      LayerScale-444             [-1, 196, 192]               0\n",
      "        DropPath-445             [-1, 196, 192]               0\n",
      "           Block-446             [-1, 196, 192]               0\n",
      "       LayerNorm-447             [-1, 196, 192]             384\n",
      "          Conv2d-448          [-1, 192, 14, 14]           1,920\n",
      "       Rearrange-449          [-1, 14, 14, 192]               0\n",
      "       LayerNorm-450          [-1, 14, 14, 192]             384\n",
      "       Rearrange-451          [-1, 192, 14, 14]               0\n",
      "          Conv2d-452          [-1, 768, 14, 14]         148,224\n",
      "       QuickGELU-453          [-1, 768, 14, 14]               0\n",
      "       QuickGELU-454          [-1, 768, 14, 14]               0\n",
      "       QuickGELU-455          [-1, 768, 14, 14]               0\n",
      "       QuickGELU-456          [-1, 768, 14, 14]               0\n",
      "          Conv2d-457          [-1, 192, 14, 14]         147,648\n",
      "       Rearrange-458             [-1, 196, 192]               0\n",
      "          Conv2d-459            [-1, 192, 7, 7]           1,920\n",
      "       Rearrange-460            [-1, 7, 7, 192]               0\n",
      "       LayerNorm-461            [-1, 7, 7, 192]             384\n",
      "       Rearrange-462            [-1, 192, 7, 7]               0\n",
      "          Conv2d-463            [-1, 768, 7, 7]         148,224\n",
      "       QuickGELU-464            [-1, 768, 7, 7]               0\n",
      "       QuickGELU-465            [-1, 768, 7, 7]               0\n",
      "       QuickGELU-466            [-1, 768, 7, 7]               0\n",
      "       QuickGELU-467            [-1, 768, 7, 7]               0\n",
      "          Conv2d-468            [-1, 192, 7, 7]         147,648\n",
      "       Rearrange-469              [-1, 49, 192]               0\n",
      "          Conv2d-470            [-1, 192, 7, 7]           1,920\n",
      "       Rearrange-471            [-1, 7, 7, 192]               0\n",
      "       LayerNorm-472            [-1, 7, 7, 192]             384\n",
      "       Rearrange-473            [-1, 192, 7, 7]               0\n",
      "          Conv2d-474            [-1, 768, 7, 7]         148,224\n",
      "       QuickGELU-475            [-1, 768, 7, 7]               0\n",
      "       QuickGELU-476            [-1, 768, 7, 7]               0\n",
      "       QuickGELU-477            [-1, 768, 7, 7]               0\n",
      "       QuickGELU-478            [-1, 768, 7, 7]               0\n",
      "          Conv2d-479            [-1, 192, 7, 7]         147,648\n",
      "       Rearrange-480              [-1, 49, 192]               0\n",
      "         Dropout-481           [-1, 8, 196, 49]               0\n",
      "          Linear-482             [-1, 196, 192]          37,056\n",
      "         Dropout-483             [-1, 196, 192]               0\n",
      "   AttentionConv-484             [-1, 196, 192]               0\n",
      "      LayerScale-485             [-1, 196, 192]               0\n",
      "        DropPath-486             [-1, 196, 192]               0\n",
      "       LayerNorm-487             [-1, 196, 192]             384\n",
      "          Linear-488             [-1, 196, 768]         148,224\n",
      "       QuickGELU-489             [-1, 196, 768]               0\n",
      "          Linear-490             [-1, 196, 192]         147,648\n",
      "         Dropout-491             [-1, 196, 192]               0\n",
      "      LayerScale-492             [-1, 196, 192]               0\n",
      "        DropPath-493             [-1, 196, 192]               0\n",
      "           Block-494             [-1, 196, 192]               0\n",
      "VisionTransformer-495          [-1, 192, 14, 14]               0\n",
      "          Conv2d-496            [-1, 256, 7, 7]       2,408,704\n",
      "       LayerNorm-497              [-1, 49, 256]             512\n",
      "       ConvEmbed-498            [-1, 256, 7, 7]               0\n",
      "         Dropout-499              [-1, 49, 256]               0\n",
      "       LayerNorm-500              [-1, 49, 256]             512\n",
      "          Conv2d-501            [-1, 256, 7, 7]           2,560\n",
      "       Rearrange-502            [-1, 7, 7, 256]               0\n",
      "       LayerNorm-503            [-1, 7, 7, 256]             512\n",
      "       Rearrange-504            [-1, 256, 7, 7]               0\n",
      "          Conv2d-505           [-1, 1024, 7, 7]         263,168\n",
      "       QuickGELU-506           [-1, 1024, 7, 7]               0\n",
      "       QuickGELU-507           [-1, 1024, 7, 7]               0\n",
      "       QuickGELU-508           [-1, 1024, 7, 7]               0\n",
      "       QuickGELU-509           [-1, 1024, 7, 7]               0\n",
      "          Conv2d-510            [-1, 256, 7, 7]         262,400\n",
      "       Rearrange-511              [-1, 49, 256]               0\n",
      "          Conv2d-512            [-1, 256, 4, 4]           2,560\n",
      "       Rearrange-513            [-1, 4, 4, 256]               0\n",
      "       LayerNorm-514            [-1, 4, 4, 256]             512\n",
      "       Rearrange-515            [-1, 256, 4, 4]               0\n",
      "          Conv2d-516           [-1, 1024, 4, 4]         263,168\n",
      "       QuickGELU-517           [-1, 1024, 4, 4]               0\n",
      "       QuickGELU-518           [-1, 1024, 4, 4]               0\n",
      "       QuickGELU-519           [-1, 1024, 4, 4]               0\n",
      "       QuickGELU-520           [-1, 1024, 4, 4]               0\n",
      "          Conv2d-521            [-1, 256, 4, 4]         262,400\n",
      "       Rearrange-522              [-1, 16, 256]               0\n",
      "          Conv2d-523            [-1, 256, 4, 4]           2,560\n",
      "       Rearrange-524            [-1, 4, 4, 256]               0\n",
      "       LayerNorm-525            [-1, 4, 4, 256]             512\n",
      "       Rearrange-526            [-1, 256, 4, 4]               0\n",
      "          Conv2d-527           [-1, 1024, 4, 4]         263,168\n",
      "       QuickGELU-528           [-1, 1024, 4, 4]               0\n",
      "       QuickGELU-529           [-1, 1024, 4, 4]               0\n",
      "       QuickGELU-530           [-1, 1024, 4, 4]               0\n",
      "       QuickGELU-531           [-1, 1024, 4, 4]               0\n",
      "          Conv2d-532            [-1, 256, 4, 4]         262,400\n",
      "       Rearrange-533              [-1, 16, 256]               0\n",
      "         Dropout-534           [-1, 16, 49, 16]               0\n",
      "          Linear-535              [-1, 49, 256]          65,792\n",
      "         Dropout-536              [-1, 49, 256]               0\n",
      "   AttentionConv-537              [-1, 49, 256]               0\n",
      "      LayerScale-538              [-1, 49, 256]               0\n",
      "        DropPath-539              [-1, 49, 256]               0\n",
      "       LayerNorm-540              [-1, 49, 256]             512\n",
      "          Linear-541             [-1, 49, 1024]         263,168\n",
      "       QuickGELU-542             [-1, 49, 1024]               0\n",
      "          Linear-543              [-1, 49, 256]         262,400\n",
      "         Dropout-544              [-1, 49, 256]               0\n",
      "      LayerScale-545              [-1, 49, 256]               0\n",
      "        DropPath-546              [-1, 49, 256]               0\n",
      "           Block-547              [-1, 49, 256]               0\n",
      "       LayerNorm-548              [-1, 49, 256]             512\n",
      "          Conv2d-549            [-1, 256, 7, 7]           2,560\n",
      "       Rearrange-550            [-1, 7, 7, 256]               0\n",
      "       LayerNorm-551            [-1, 7, 7, 256]             512\n",
      "       Rearrange-552            [-1, 256, 7, 7]               0\n",
      "          Conv2d-553           [-1, 1024, 7, 7]         263,168\n",
      "       QuickGELU-554           [-1, 1024, 7, 7]               0\n",
      "       QuickGELU-555           [-1, 1024, 7, 7]               0\n",
      "       QuickGELU-556           [-1, 1024, 7, 7]               0\n",
      "       QuickGELU-557           [-1, 1024, 7, 7]               0\n",
      "          Conv2d-558            [-1, 256, 7, 7]         262,400\n",
      "       Rearrange-559              [-1, 49, 256]               0\n",
      "          Conv2d-560            [-1, 256, 4, 4]           2,560\n",
      "       Rearrange-561            [-1, 4, 4, 256]               0\n",
      "       LayerNorm-562            [-1, 4, 4, 256]             512\n",
      "       Rearrange-563            [-1, 256, 4, 4]               0\n",
      "          Conv2d-564           [-1, 1024, 4, 4]         263,168\n",
      "       QuickGELU-565           [-1, 1024, 4, 4]               0\n",
      "       QuickGELU-566           [-1, 1024, 4, 4]               0\n",
      "       QuickGELU-567           [-1, 1024, 4, 4]               0\n",
      "       QuickGELU-568           [-1, 1024, 4, 4]               0\n",
      "          Conv2d-569            [-1, 256, 4, 4]         262,400\n",
      "       Rearrange-570              [-1, 16, 256]               0\n",
      "          Conv2d-571            [-1, 256, 4, 4]           2,560\n",
      "       Rearrange-572            [-1, 4, 4, 256]               0\n",
      "       LayerNorm-573            [-1, 4, 4, 256]             512\n",
      "       Rearrange-574            [-1, 256, 4, 4]               0\n",
      "          Conv2d-575           [-1, 1024, 4, 4]         263,168\n",
      "       QuickGELU-576           [-1, 1024, 4, 4]               0\n",
      "       QuickGELU-577           [-1, 1024, 4, 4]               0\n",
      "       QuickGELU-578           [-1, 1024, 4, 4]               0\n",
      "       QuickGELU-579           [-1, 1024, 4, 4]               0\n",
      "          Conv2d-580            [-1, 256, 4, 4]         262,400\n",
      "       Rearrange-581              [-1, 16, 256]               0\n",
      "         Dropout-582           [-1, 16, 49, 16]               0\n",
      "          Linear-583              [-1, 49, 256]          65,792\n",
      "         Dropout-584              [-1, 49, 256]               0\n",
      "   AttentionConv-585              [-1, 49, 256]               0\n",
      "      LayerScale-586              [-1, 49, 256]               0\n",
      "        DropPath-587              [-1, 49, 256]               0\n",
      "       LayerNorm-588              [-1, 49, 256]             512\n",
      "          Linear-589             [-1, 49, 1024]         263,168\n",
      "       QuickGELU-590             [-1, 49, 1024]               0\n",
      "          Linear-591              [-1, 49, 256]         262,400\n",
      "         Dropout-592              [-1, 49, 256]               0\n",
      "      LayerScale-593              [-1, 49, 256]               0\n",
      "        DropPath-594              [-1, 49, 256]               0\n",
      "           Block-595              [-1, 49, 256]               0\n",
      "VisionTransformer-596            [-1, 256, 7, 7]               0\n",
      "       LayerNorm-597              [-1, 49, 256]             512\n",
      "AdaptiveAvgPool1d-598               [-1, 256, 1]               0\n",
      "          Linear-599                  [-1, 100]          25,700\n",
      "================================================================\n",
      "Total params: 17,152,676\n",
      "Trainable params: 17,152,676\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 570.75\n",
      "Params size (MB): 65.43\n",
      "Estimated Total Size (MB): 636.75\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(model.cuda(), (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms 정의하기\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8,1), interpolation=transforms.InterpolationMode.LANCZOS),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    transforms.RandomErasing(p=0.9, scale=(0.02, 0.33)),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "data_dir = '../data/sports'\n",
    "batch_size = 256\n",
    "\n",
    "train_path = data_dir+'/train'\n",
    "valid_path = data_dir+'/valid'\n",
    "test_path = data_dir+'/test'\n",
    "\n",
    "# dataset load\n",
    "train_data = ImageFolder(train_path, transform=train_transform)\n",
    "valid_data = ImageFolder(valid_path, transform=test_transform)\n",
    "test_data = ImageFolder(test_path, transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:3'\n",
    "max_norm = 5.0 \n",
    "\n",
    "model.to(device)\n",
    "model_path = '../models/cvt/model_revision.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixup_fn = Mixup(mixup_alpha=.8, \n",
    "                cutmix_alpha=1., \n",
    "                prob=1., \n",
    "                switch_prob=0.5, \n",
    "                mode='batch',\n",
    "                label_smoothing=.1,\n",
    "                num_classes=100)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-26 12:59:14.331887: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-26 12:59:14.331979: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-26 12:59:14.332981: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-26 12:59:14.339260: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-26 12:59:15.296105: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.AdamW(model.parameters())\n",
    "warmup_steps = int(len(train_loader)*(epochs)*0.1)\n",
    "train_steps = len(train_loader)*(epochs)\n",
    "scheduler = transformers.get_cosine_schedule_with_warmup(optimizer, \n",
    "                                                        num_warmup_steps=warmup_steps, \n",
    "                                                        num_training_steps=train_steps,\n",
    "                                                        num_cycles=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 53/53 [01:01<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.491039788947915, Val Loss: 4.250633955001831, LR: 0.0001, Duration: 63.13 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 53/53 [01:00<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.3790437950278225, Val Loss: 4.119809627532959, LR: 0.0002, Duration: 62.00 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 53/53 [01:00<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.324519427317493, Val Loss: 4.009176731109619, LR: 0.0003, Duration: 62.01 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.257045457947929, Val Loss: 3.897426724433899, LR: 0.0004, Duration: 61.91 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.192792883459127, Val Loss: 3.7397795915603638, LR: 0.0005, Duration: 61.84 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.111031361345975, Val Loss: 3.644646167755127, LR: 0.0006, Duration: 61.82 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.131182625608624, Val Loss: 3.570503830909729, LR: 0.0007, Duration: 61.73 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.064160005101618, Val Loss: 3.4499846696853638, LR: 0.0008, Duration: 61.80 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.027986166612157, Val Loss: 3.4108489751815796, LR: 0.0009000000000000001, Duration: 61.67 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.9973911339381956, Val Loss: 3.235844135284424, LR: 0.001, Duration: 61.76 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.9543190047426044, Val Loss: 3.2171614170074463, LR: 0.0009996954135095479, Duration: 61.86 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.827674334903933, Val Loss: 3.1099013090133667, LR: 0.0009987820251299122, Duration: 61.83 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.8569642822697476, Val Loss: 3.0267852544784546, LR: 0.0009972609476841367, Duration: 61.76 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.8774879563529536, Val Loss: 2.954328179359436, LR: 0.0009951340343707852, Duration: 61.73 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.766154001343925, Val Loss: 2.8296825885772705, LR: 0.000992403876506104, Duration: 61.71 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.773716112352767, Val Loss: 2.747504949569702, LR: 0.0009890738003669028, Duration: 61.59 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.7036735291750924, Val Loss: 2.7663460969924927, LR: 0.0009851478631379982, Duration: 61.46 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.6905135388644235, Val Loss: 2.620993494987488, LR: 0.0009806308479691594, Duration: 61.65 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.6245140399572984, Val Loss: 2.608414649963379, LR: 0.0009755282581475768, Duration: 61.66 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.5727882385253906, Val Loss: 2.471630334854126, LR: 0.0009698463103929542, Duration: 61.66 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.565060273656305, Val Loss: 2.5728055238723755, LR: 0.0009635919272833937, Duration: 61.49 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.5276912518267363, Val Loss: 2.3284218311309814, LR: 0.0009567727288213005, Duration: 61.77 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.556080894650153, Val Loss: 2.2296935319900513, LR: 0.0009493970231495835, Duration: 61.63 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.4078589385410525, Val Loss: 2.1222803592681885, LR: 0.0009414737964294635, Duration: 61.63 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.334725209002225, Val Loss: 2.0900317430496216, LR: 0.0009330127018922195, Duration: 61.85 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.4933261961307167, Val Loss: 2.074242115020752, LR: 0.0009240240480782129, Duration: 61.60 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.4045373988601395, Val Loss: 2.0458030700683594, LR: 0.0009145187862775209, Duration: 61.77 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.3715593634911305, Val Loss: 1.9632411003112793, LR: 0.0009045084971874737, Duration: 61.66 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.266687555133172, Val Loss: 1.9416547417640686, LR: 0.0008940053768033609, Duration: 61.67 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.3741801819711363, Val Loss: 1.9517342448234558, LR: 0.000883022221559489, Duration: 61.47 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.317072521965459, Val Loss: 1.845496118068695, LR: 0.0008715724127386971, Duration: 61.60 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.2580979275253585, Val Loss: 1.7860190868377686, LR: 0.0008596699001693256, Duration: 61.76 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.258915028482113, Val Loss: 1.7462166547775269, LR: 0.0008473291852294987, Duration: 61.66 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.0867727477595492, Val Loss: 1.6886086463928223, LR: 0.0008345653031794292, Duration: 61.75 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.1998259616347977, Val Loss: 1.6266695261001587, LR: 0.0008213938048432696, Duration: 61.84 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.133610765889006, Val Loss: 1.598767876625061, LR: 0.0008078307376628291, Duration: 61.77 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37: 100%|██████████| 53/53 [01:00<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.0084940847360864, Val Loss: 1.5904669761657715, LR: 0.0007938926261462366, Duration: 61.92 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.0751553436495223, Val Loss: 1.617292582988739, LR: 0.0007795964517353734, Duration: 61.59 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.9540500370961316, Val Loss: 1.4375624060630798, LR: 0.0007649596321166025, Duration: 61.77 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.982711270170392, Val Loss: 1.4386451244354248, LR: 0.00075, Duration: 61.67 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.024764580546685, Val Loss: 1.4747841358184814, LR: 0.0007347357813929454, Duration: 61.46 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.988191496651128, Val Loss: 1.418253481388092, LR: 0.0007191855733945387, Duration: 61.69 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.0399051477324286, Val Loss: 1.3937439322471619, LR: 0.0007033683215379002, Duration: 61.75 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.9420273821308927, Val Loss: 1.391537070274353, LR: 0.0006873032967079561, Duration: 61.76 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.9671568240759507, Val Loss: 1.406368374824524, LR: 0.0006710100716628344, Duration: 61.53 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.7893805121475794, Val Loss: 1.3200807571411133, LR: 0.0006545084971874737, Duration: 61.73 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.8643419832553505, Val Loss: 1.3233751058578491, LR: 0.0006378186779084996, Duration: 61.57 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.8003722834137252, Val Loss: 1.2586259841918945, LR: 0.0006209609477998338, Duration: 61.69 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.8297858215727896, Val Loss: 1.2884414196014404, LR: 0.0006039558454088796, Duration: 61.51 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.7412464753636776, Val Loss: 1.2197287678718567, LR: 0.0005868240888334653, Duration: 61.59 sec - model saved!\n",
      "Epoch 당 평균 소요시간 : 30.87초\n"
     ]
    }
   ],
   "source": [
    "training_time = 0\n",
    "losses = []\n",
    "val_losses = []\n",
    "lrs = []\n",
    "best_loss = float('inf')\n",
    "\n",
    "# GradScaler 초기화\n",
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch + 1}\")\n",
    "    \n",
    "    for _, data in pbar:\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        inputs, labels = mixup_fn(inputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # AutoCast 적용\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "        # 스케일링된 그라디언트 계산\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # 그라디언트 클리핑 전에 스케일링 제거\n",
    "        scaler.unscale_(optimizer)\n",
    "        clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "\n",
    "        # 옵티마이저 스텝 및 스케일러 업데이트\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "            \n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        lrs.append(lr)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)        \n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in valid_loader:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    val_loss /= len(valid_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # 모델 저장\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        vit_save = True\n",
    "        if vit_save:\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_duration = time.time() - start_time\n",
    "    training_time += epoch_duration\n",
    "    \n",
    "    text = f'\\tLoss: {epoch_loss}, Val Loss: {val_loss}, LR: {lr}, Duration: {epoch_duration:.2f} sec'\n",
    "    \n",
    "    if vit_save:\n",
    "        text += f' - model saved!'\n",
    "        vit_save = False\n",
    "\n",
    "    print(text)\n",
    "        \n",
    "text = f\"Epoch 당 평균 소요시간 : {training_time / epochs:.2f}초\"      \n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.748000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.776603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Recall</td>\n",
       "      <td>0.748000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F1 Score</td>\n",
       "      <td>0.736155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Metric     Value\n",
       "0   Accuracy  0.748000\n",
       "1  Precision  0.776603\n",
       "2     Recall  0.748000\n",
       "3   F1 Score  0.736155"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예측 수행 및 레이블 저장\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# 혼동 행렬 생성\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# 예측과 실제 레이블\n",
    "y_true = all_labels  # 실제 레이블\n",
    "y_pred = all_preds  # 모델에 의해 예측된 레이블\n",
    "\n",
    "# 전체 데이터셋에 대한 정확도\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# 평균 정밀도, 리콜, F1-Score ('weighted')\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "\n",
    "# 판다스 데이터프레임으로 결과 정리\n",
    "performance_metrics = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "    'Value': [accuracy, precision, recall, f1_score]\n",
    "})\n",
    "\n",
    "# 데이터프레임 출력\n",
    "performance_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.664388946767123, Val Loss: 1.182432472705841, LR: 0.0005695865504800327, Duration: 61.71 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.690109801742266, Val Loss: 1.1874754428863525, LR: 0.0005522642316338268, Duration: 61.66 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 53/53 [01:00<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.658997306283915, Val Loss: 1.1869313716888428, LR: 0.0005348782368720626, Duration: 61.93 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.725780673746793, Val Loss: 1.1693055033683777, LR: 0.0005174497483512506, Duration: 61.68 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.645304999261532, Val Loss: 1.1657801866531372, LR: 0.0005, Duration: 61.57 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.7765888672954633, Val Loss: 1.1800063848495483, LR: 0.0004825502516487497, Duration: 61.48 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.665741443634033, Val Loss: 1.1219471096992493, LR: 0.00046512176312793734, Duration: 61.76 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.7277145565680736, Val Loss: 1.1381598114967346, LR: 0.00044773576836617336, Duration: 61.61 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6650882891888887, Val Loss: 1.1050522923469543, LR: 0.0004304134495199674, Duration: 61.65 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.552897235132613, Val Loss: 1.0660218596458435, LR: 0.00041317591116653486, Duration: 61.70 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6931786424708815, Val Loss: 1.0880324840545654, LR: 0.0003960441545911204, Duration: 61.42 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.7023172693432502, Val Loss: 1.067952811717987, LR: 0.0003790390522001662, Duration: 61.52 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5251943975124718, Val Loss: 1.1014899015426636, LR: 0.00036218132209150044, Duration: 61.50 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6650103735473922, Val Loss: 1.0900723338127136, LR: 0.00034549150281252633, Duration: 61.58 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6865972955271884, Val Loss: 1.0965940356254578, LR: 0.0003289899283371657, Duration: 61.50 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6102457136478066, Val Loss: 1.0710596442222595, LR: 0.00031269670329204396, Duration: 61.60 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 53/53 [01:00<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5886582698462144, Val Loss: 1.059012770652771, LR: 0.0002966316784621, Duration: 61.90 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 53/53 [01:00<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.481731151634792, Val Loss: 1.075021207332611, LR: 0.00028081442660546124, Duration: 61.88 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5424877000304886, Val Loss: 1.0477690696716309, LR: 0.00026526421860705474, Duration: 61.75 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6523905115307502, Val Loss: 1.0448976159095764, LR: 0.0002500000000000001, Duration: 61.75 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6889889780080543, Val Loss: 1.0668753385543823, LR: 0.0002350403678833976, Duration: 61.59 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 53/53 [01:00<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.658509171233987, Val Loss: 1.0499722957611084, LR: 0.00022040354826462666, Duration: 61.84 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.584170638390307, Val Loss: 1.0469989776611328, LR: 0.00020610737385376348, Duration: 61.66 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5876458793316246, Val Loss: 1.0145525932312012, LR: 0.00019216926233717085, Duration: 61.89 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 53/53 [01:01<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4392318005831735, Val Loss: 1.0258703827857971, LR: 0.0001786061951567303, Duration: 62.20 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.537560026600676, Val Loss: 1.024184763431549, LR: 0.00016543469682057105, Duration: 61.74 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 53/53 [01:00<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6047259434214176, Val Loss: 0.9980037212371826, LR: 0.00015267081477050133, Duration: 62.05 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 53/53 [01:00<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6459678074098982, Val Loss: 1.0099011957645416, LR: 0.00014033009983067452, Duration: 61.91 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.296959751057175, Val Loss: 1.0075197219848633, LR: 0.00012842758726130281, Duration: 61.68 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6155606463270367, Val Loss: 0.9911544919013977, LR: 0.00011697777844051105, Duration: 61.87 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5283864421664544, Val Loss: 1.0100283920764923, LR: 0.00010599462319663906, Duration: 61.72 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|██████████| 53/53 [01:00<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.529347172323263, Val Loss: 1.0094934701919556, LR: 9.549150281252633e-05, Duration: 61.78 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.513541131649377, Val Loss: 1.0003241896629333, LR: 8.548121372247918e-05, Duration: 61.67 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|██████████| 53/53 [01:00<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.495615358622569, Val Loss: 0.9906017184257507, LR: 7.597595192178702e-05, Duration: 61.94 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5116015357791253, Val Loss: 0.977340579032898, LR: 6.698729810778065e-05, Duration: 61.89 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5669742305323764, Val Loss: 0.9889179170131683, LR: 5.852620357053651e-05, Duration: 61.72 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.490295691310235, Val Loss: 0.9839414656162262, LR: 5.060297685041659e-05, Duration: 61.63 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.444623843678888, Val Loss: 0.9805052280426025, LR: 4.322727117869951e-05, Duration: 61.58 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.442868743302687, Val Loss: 0.968334436416626, LR: 3.6408072716606344e-05, Duration: 61.84 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4938369741979636, Val Loss: 0.9714963138103485, LR: 3.0153689607045842e-05, Duration: 61.61 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.341384620036719, Val Loss: 0.968377411365509, LR: 2.4471741852423235e-05, Duration: 61.71 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5525424233022727, Val Loss: 0.9791654944419861, LR: 1.9369152030840554e-05, Duration: 61.70 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.561695065138475, Val Loss: 0.9694730341434479, LR: 1.4852136862001764e-05, Duration: 61.62 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.567573884748063, Val Loss: 0.9660724103450775, LR: 1.0926199633097156e-05, Duration: 61.79 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3567223323965973, Val Loss: 0.9649331271648407, LR: 7.59612349389599e-06, Duration: 61.76 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.498915487865232, Val Loss: 0.9672039151191711, LR: 4.865965629214819e-06, Duration: 61.71 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47: 100%|██████████| 53/53 [01:00<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.486452577249059, Val Loss: 0.9657709002494812, LR: 2.739052315863355e-06, Duration: 61.83 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3929346237542495, Val Loss: 0.9653393924236298, LR: 1.2179748700879012e-06, Duration: 61.72 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.475441665019629, Val Loss: 0.9657445549964905, LR: 3.0458649045211895e-07, Duration: 61.59 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4677765841753976, Val Loss: 0.9656921625137329, LR: 0.0, Duration: 61.56 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.808000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.816901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Recall</td>\n",
       "      <td>0.808000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F1 Score</td>\n",
       "      <td>0.797948</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Metric     Value\n",
       "0   Accuracy  0.808000\n",
       "1  Precision  0.816901\n",
       "2     Recall  0.808000\n",
       "3   F1 Score  0.797948"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_time = 0\n",
    "losses = []\n",
    "val_losses = []\n",
    "lrs = []\n",
    "best_loss = float('inf')\n",
    "\n",
    "# GradScaler 초기화\n",
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch + 1}\")\n",
    "    \n",
    "    for _, data in pbar:\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        inputs, labels = mixup_fn(inputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # AutoCast 적용\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "        # 스케일링된 그라디언트 계산\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # 그라디언트 클리핑 전에 스케일링 제거\n",
    "        scaler.unscale_(optimizer)\n",
    "        clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "\n",
    "        # 옵티마이저 스텝 및 스케일러 업데이트\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "            \n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        lrs.append(lr)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)        \n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in valid_loader:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    val_loss /= len(valid_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # 모델 저장\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        vit_save = True\n",
    "        if vit_save:\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_duration = time.time() - start_time\n",
    "    training_time += epoch_duration\n",
    "    \n",
    "    text = f'\\tLoss: {epoch_loss}, Val Loss: {val_loss}, LR: {lr}, Duration: {epoch_duration:.2f} sec'\n",
    "    \n",
    "    if vit_save:\n",
    "        text += f' - model saved!'\n",
    "        vit_save = False\n",
    "\n",
    "    print(text)\n",
    "        \n",
    "# 예측 수행 및 레이블 저장\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# 혼동 행렬 생성\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# 예측과 실제 레이블\n",
    "y_true = all_labels  # 실제 레이블\n",
    "y_pred = all_preds  # 모델에 의해 예측된 레이블\n",
    "\n",
    "# 전체 데이터셋에 대한 정확도\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# 평균 정밀도, 리콜, F1-Score ('weighted')\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "\n",
    "# 판다스 데이터프레임으로 결과 정리\n",
    "performance_metrics = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "    'Value': [accuracy, precision, recall, f1_score]\n",
    "})\n",
    "\n",
    "# 데이터프레임 출력\n",
    "performance_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
