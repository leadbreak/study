{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Token Embedding\n",
    "- ViT에서는 이미지를 고정된 크기의 패치로 분할해, 이를 flatten해 사용\n",
    "- CvT에서는 이미지 혹은 이전 스테이지의 2D-shape token map에 대해 convolution을 적용한 뒤, 이를 flatten해 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 196, 768]), torch.Size([1, 50176, 768]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# ViT의 패치 임베딩 레이어\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)  # [B, E, H, W]\n",
    "        x = x.flatten(2)  # [B, E, N]\n",
    "        x = x.transpose(1, 2)  # [B, N, E]\n",
    "        return x\n",
    "\n",
    "# CvT의 컨볼루션 토큰 임베딩 레이어\n",
    "class ConvTokenEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=768, kernel_size=3, stride=1, padding=1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)  # [B, E, H, W]\n",
    "        x = x.flatten(2)  # [B, E, N]\n",
    "        x = x.transpose(1, 2)  # [B, N, E]\n",
    "        return x\n",
    "\n",
    "# 더미 이미지 데이터 생성\n",
    "dummy_img = torch.randn(1, 3, 224, 224)  # [B, C, H, W]\n",
    "\n",
    "# 모델 초기화\n",
    "vit_patch_embedding = PatchEmbedding()\n",
    "cvt_conv_embedding = ConvTokenEmbedding()\n",
    "\n",
    "# 더미 데이터를 통과시키기\n",
    "vit_output = vit_patch_embedding(dummy_img)\n",
    "cvt_output = cvt_conv_embedding(dummy_img)\n",
    "\n",
    "vit_output.shape, cvt_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Projection\n",
    "- ViT에서는 전달된 토큰에 대해 선형 레이어(Linear Layer)를 이용해 qkv를 투사(projection)하고,\n",
    "- CvT에서는 전달된 토큰에 대해 컨볼루션(Convolution Layer)을 이용해 qkv를 투사(projection)함.\n",
    " - 정확히는 Linear Projection하기 전에 Convolution Layer를 한 번 더 통과하는 게 맞음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((torch.Size([1, 12, 196, 64]),\n",
       "  torch.Size([1, 12, 196, 64]),\n",
       "  torch.Size([1, 12, 196, 64])),\n",
       " (torch.Size([1, 12, 64, 28, 28]),\n",
       "  torch.Size([1, 12, 64, 28, 28]),\n",
       "  torch.Size([1, 12, 64, 28, 28])))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ViT의 Q, K, V 투사 레이어\n",
    "class LinearProjection(nn.Module):\n",
    "    def __init__(self, dim, heads):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.scale = (dim // heads) ** -0.5\n",
    "\n",
    "        # Q, K, V에 대한 선형 투사\n",
    "        self.qkv = nn.Linear(dim, dim * 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, n, _ = x.shape\n",
    "        qkv = self.qkv(x).reshape(b, n, 3, self.heads, -1).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        return q, k, v\n",
    "\n",
    "# CvT의 Convolutional Projection 레이어 (수정된 버전)\n",
    "class ConvProjectionCorrected(nn.Module):\n",
    "    def __init__(self, dim, heads, kernel_size=3, stride=1, padding=1):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.scale = (dim // heads) ** -0.5\n",
    "\n",
    "        # Q, K, V에 대한 컨볼루션 레이어\n",
    "        self.qkv = nn.Conv2d(dim, dim * 3, kernel_size, stride, padding, groups=heads)\n",
    "        self.reshape = nn.Unflatten(2, (heads, dim // heads))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, _, h, w = x.shape\n",
    "        x = self.qkv(x).view(b, self.heads, 3, -1, h, w).permute(2, 0, 1, 3, 4, 5)\n",
    "        q, k, v = x[0], x[1], x[2]\n",
    "        return q, k, v\n",
    "\n",
    "# 더미 데이터 초기화 및 모델 정의\n",
    "dummy_img = torch.randn(1, 768, 28, 28)  # CvT의 더미 데이터\n",
    "dummy_seq = torch.randn(1, 196, 768)  # ViT의 더미 데이터 (패치 임베딩 후)\n",
    "\n",
    "linear_projection = LinearProjection(dim=768, heads=12)\n",
    "conv_projection_corrected = ConvProjectionCorrected(dim=768, heads=12)\n",
    "\n",
    "# 더미 데이터를 통과시키기\n",
    "vit_q, vit_k, vit_v = linear_projection(dummy_seq)\n",
    "cvt_q, cvt_k, cvt_v = conv_projection_corrected(dummy_img)\n",
    "\n",
    "(vit_q.shape, vit_k.shape, vit_v.shape), (cvt_q.shape, cvt_k.shape, cvt_v.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((torch.Size([1, 12, 196, 64]),\n",
       "  torch.Size([1, 12, 196, 64]),\n",
       "  torch.Size([1, 12, 196, 64])),\n",
       " (torch.Size([1, 196, 768]),\n",
       "  torch.Size([1, 49, 768]),\n",
       "  torch.Size([1, 49, 768])))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from einops import rearrange\n",
    "\n",
    "# CvT의 멀티헤드 어텐션 레이어 수정\n",
    "class CvTattention(nn.Module):\n",
    "    def __init__(self, dim, num_heads, kernel_size=3, stride_kv=2, stride_q=1, padding_kv=1, padding_q=1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.dim = dim\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        # 컨볼루션 프로젝션\n",
    "        self.conv_proj_q = nn.Conv2d(dim, dim, kernel_size=kernel_size, padding=padding_q, stride=stride_q, groups=dim)\n",
    "        self.conv_proj_k = nn.Conv2d(dim, dim, kernel_size=kernel_size, padding=padding_kv, stride=stride_kv, groups=dim)\n",
    "        self.conv_proj_v = nn.Conv2d(dim, dim, kernel_size=kernel_size, padding=padding_kv, stride=stride_kv, groups=dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        q = self.conv_proj_q(x)\n",
    "        k = self.conv_proj_k(x)\n",
    "        v = self.conv_proj_v(x)\n",
    "\n",
    "        q = rearrange(q, 'b c h w -> b (h w) c')\n",
    "        k = rearrange(k, 'b c h w -> b (h w) c')\n",
    "        v = rearrange(v, 'b c h w -> b (h w) c')\n",
    "\n",
    "        return q, k, v\n",
    "    \n",
    "# 더미 데이터 초기화 및 모델 정의\n",
    "dummy_img_vit = torch.randn(1, 196, 768)  # ViT의 더미 데이터 (패치 임베딩 후)\n",
    "dummy_img_cvt = torch.randn(1, 768, 14, 14)  # CvT의 더미 데이터 (컨볼루션 후)\n",
    "\n",
    "# CvT의 멀티헤드 어텐션 모델 정의 및 더미 데이터 통과\n",
    "cvt_attention = CvTattention(dim=768, num_heads=12, kernel_size=3, stride_kv=2, stride_q=1)\n",
    "cvt_q, cvt_k, cvt_v = cvt_attention(dummy_img_cvt)\n",
    "\n",
    "# 각 모델의 출력 형태\n",
    "(vit_q.shape, vit_k.shape, vit_v.shape), (cvt_q.shape, cvt_k.shape, cvt_v.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
