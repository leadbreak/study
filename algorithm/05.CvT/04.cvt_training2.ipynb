{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "from timm.data import Mixup\n",
    "import transformers\n",
    "\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "from timm.models.layers import DropPath, trunc_normal_, to_2tuple\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01.Convolutional Token Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvEmbed(nn.Module):\n",
    "    '''\n",
    "    img/token map to Conv Embedding\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,\n",
    "                 patch_size=7, # [7, 3, 3]\n",
    "                 in_chans=3,   # [3, dim of stage1, dim of stage2]\n",
    "                 embed_dim=64, # [64, 192, 384]\n",
    "                 stride=4,     # [4, 2, 2]\n",
    "                 padding=2,    # [2, 1, 1]\n",
    "                 norm_layer=None):\n",
    "        super().__init__()\n",
    "        self.patch_size = to_2tuple(patch_size)\n",
    "        \n",
    "        self.proj = nn.Conv2d(\n",
    "            in_channels=in_chans,\n",
    "            out_channels=embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=stride,\n",
    "            padding=padding\n",
    "        )\n",
    "        \n",
    "        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        \n",
    "        _, _, H, W = x.shape\n",
    "        x = rearrange(x, 'b c h w -> b (h w) c')\n",
    "        x = self.norm(x)\n",
    "        x = rearrange(x, 'b (h w) c -> b c h w', h=H, w=W)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionConv(nn.Module):\n",
    "    def __init__(self,\n",
    "                 dim=64,        # [64,192,384]\n",
    "                 num_heads=4,   # paper: [1,3,6], me: [4,8,16]\n",
    "                 qkv_bias=False,\n",
    "                 attn_drop=0.,\n",
    "                 proj_drop=0.,\n",
    "                 kernel_size=3,\n",
    "                 padding_q=1,\n",
    "                 padding_kv=1,\n",
    "                 stride_q=1,\n",
    "                 stride_kv=2,\n",
    "                 **kwargs\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.stride_q = stride_q\n",
    "        self.stride_kv = stride_kv\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads        \n",
    "        self.scale = dim ** -0.5\n",
    "        \n",
    "        self.conv_proj_q = self._build_projection(dim,\n",
    "                                                  kernel_size,\n",
    "                                                  padding_q,\n",
    "                                                  stride_q,\n",
    "                                                  )\n",
    "        self.conv_proj_k = self._build_projection(dim,\n",
    "                                                  kernel_size,\n",
    "                                                  padding_kv,\n",
    "                                                  stride_kv,\n",
    "                                                  )\n",
    "        \n",
    "        self.conv_proj_v = self._build_projection(dim,\n",
    "                                                  kernel_size,\n",
    "                                                  padding_kv,\n",
    "                                                  stride_kv,\n",
    "                                                  )\n",
    "        \n",
    "        self.linear_proj_q = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.linear_proj_k = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.linear_proj_v = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        \n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.linear_proj_last = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)        \n",
    "        \n",
    "    def _build_projection(self,\n",
    "                          dim,\n",
    "                          kernel_size,\n",
    "                          padding,\n",
    "                          stride,\n",
    "                          ):\n",
    "        \n",
    "        proj = nn.Sequential(OrderedDict([\n",
    "            ('depthwise', nn.Conv2d(\n",
    "                dim,\n",
    "                dim,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=padding,\n",
    "                stride=stride,\n",
    "                bias=False,\n",
    "                groups=dim)),\n",
    "            ('bn', nn.BatchNorm2d(dim)),\n",
    "            ('pointwise', nn.Conv2d(\n",
    "                dim,\n",
    "                dim,\n",
    "                kernel_size=1)),\n",
    "            ('rearrange', Rearrange('b c h w -> b (h w) c'))\n",
    "        ]))\n",
    "        \n",
    "        return proj\n",
    "    \n",
    "    def forward(self, x, h, w):\n",
    "        x = rearrange(x, 'b (h w) c -> b c h w', h=h, w=w)\n",
    "        \n",
    "        q = F.normalize(self.conv_proj_q(x), dim=-1)\n",
    "        k = F.normalize(self.conv_proj_k(x), dim=-1)\n",
    "        v = self.conv_proj_v(x)\n",
    "        \n",
    "        q = rearrange(self.linear_proj_q(q), 'b t (h d) -> b h t d', h=self.num_heads)\n",
    "        k = rearrange(self.linear_proj_k(k), 'b t (h d) -> b h t d', h=self.num_heads)\n",
    "        v = rearrange(self.linear_proj_v(v), 'b t (h d) -> b h t d', h=self.num_heads)\n",
    "        \n",
    "        attn_score = torch.einsum('bhlk,bhtk->bhlt', [q, k]) * self.scale\n",
    "        attn = self.attn_drop(F.softmax(attn_score, dim=-1))\n",
    "        \n",
    "        x = torch.matmul(attn, v)\n",
    "        batch_size, num_heads, seq_length, depth = x.size()\n",
    "        x = x.view(batch_size, seq_length, num_heads * depth)\n",
    "        \n",
    "        x = self.proj_drop(self.linear_proj_last(x))\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer block에 작은 스케일 인자 곱하기\n",
    "class LayerScale(nn.Module):\n",
    "    def __init__(self, dim, init_values=1e-5):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(init_values * torch.ones((dim)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gamma * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 num_heads,\n",
    "                 mlp_ratio=4.,\n",
    "                 qkv_bias=False,\n",
    "                 drop=0.,\n",
    "                 attn_drop=0.,\n",
    "                 drop_path=0.,\n",
    "                 act_layer=nn.GELU,\n",
    "                 norm_layer=nn.LayerNorm,\n",
    "                 **kwargs\n",
    "                ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.ls1 = LayerScale(dim)\n",
    "        self.attn = AttentionConv(dim=dim,\n",
    "                                  num_heads=num_heads,\n",
    "                                  qkv_bias=qkv_bias,\n",
    "                                  attn_drop=attn_drop,\n",
    "                                  proj_drop=drop,\n",
    "                                  **kwargs)        \n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        self.ls2 = LayerScale(dim)\n",
    "        mlp_hidden_dim = int(dim*mlp_ratio)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, mlp_hidden_dim),\n",
    "            act_layer(),\n",
    "            nn.Linear(mlp_hidden_dim, dim),\n",
    "            nn.Dropout(drop),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, h, w):\n",
    "        res = x\n",
    "        x = self.norm1(x)\n",
    "        attn = self.attn(x, h, w)\n",
    "        x = res + self.drop_path(self.ls1(attn))\n",
    "        x = x + self.drop_path(self.ls2(self.mlp(self.norm2(x))))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = torch.Tensor(np.zeros((2,3,224,224))) # B, C, H, W\n",
    "\n",
    "block = Block(dim=64,\n",
    "              num_heads=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stage 1 | img shape: torch.Size([2, 3, 224, 224]) → Conv Embed Shape: torch.Size([2, 64, 56, 56])\n"
     ]
    }
   ],
   "source": [
    "# Stage 1 \n",
    "\n",
    "## Patch Embedding\n",
    "convembed = ConvEmbed(patch_size=7, stride=4, padding=2)\n",
    "stage1_img = convembed(test_img)\n",
    "\n",
    "## Attention with Convolution\n",
    "b, c, h, w = stage1_img.shape\n",
    "stage1_img = rearrange(stage1_img, 'b c h w -> b (h w) c')\n",
    "stage1_img = block(stage1_img, h=h, w=w)\n",
    "stage1_img = rearrange(stage1_img, 'b (h w) c -> b c h w', h=h, w=w)\n",
    "\n",
    "## Check Result\n",
    "print(f'stage 1 | img shape: {test_img.shape} → Conv Embed Shape: {stage1_img.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stage 2 | img shape: torch.Size([2, 64, 56, 56]) → Conv Embed Shape: torch.Size([2, 64, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# Stage 2 \n",
    "\n",
    "## Patch Embedding\n",
    "convembed = ConvEmbed(patch_size=3, in_chans=64, stride=2, padding=1)\n",
    "stage2_img = convembed(stage1_img)\n",
    "\n",
    "## Attention with Convolution\n",
    "b, c, h, w = stage2_img.shape\n",
    "stage2_img = rearrange(stage2_img, 'b c h w -> b (h w) c')\n",
    "stage2_img = block(stage2_img, h=h, w=w)\n",
    "stage2_img = rearrange(stage2_img, 'b (h w) c -> b c h w', h=h, w=w)\n",
    "\n",
    "## Check Result\n",
    "print(f'stage 2 | img shape: {stage1_img.shape} → Conv Embed Shape: {stage2_img.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stage 3 | img shape: torch.Size([2, 64, 28, 28]) → Conv Embed Shape: torch.Size([2, 64, 14, 14])\n"
     ]
    }
   ],
   "source": [
    "# Stage 3 \n",
    "\n",
    "## Patch Embedding\n",
    "convembed = ConvEmbed(patch_size=3, in_chans=64, stride=2, padding=1)\n",
    "stage3_img = convembed(stage2_img)\n",
    "\n",
    "## Attention with Convolution\n",
    "b, c, h, w = stage3_img.shape\n",
    "stage3_img = rearrange(stage3_img, 'b c h w -> b (h w) c')\n",
    "stage3_img = block(stage3_img, h=h, w=w)\n",
    "stage3_img = rearrange(stage3_img, 'b (h w) c -> b c h w', h=h, w=w)\n",
    "\n",
    "## Check Result\n",
    "print(f'stage 3 | img shape: {stage2_img.shape} → Conv Embed Shape: {stage3_img.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 patch_size=16,\n",
    "                 patch_stride=16,\n",
    "                 patch_padding=0,\n",
    "                 in_chans=3,\n",
    "                 embed_dim=768,\n",
    "                 depth=12,\n",
    "                 num_heads=12,\n",
    "                 mlp_ratio=4.,\n",
    "                 qkv_bias=False,\n",
    "                 drop_rate=0.,\n",
    "                 attn_drop_rate=0.,\n",
    "                 drop_path_rate=0.,\n",
    "                 act_layer=nn.GELU,\n",
    "                 norm_layer=nn.LayerNorm,\n",
    "                 init='trunc_norm',\n",
    "                 **kwargs\n",
    "                 ):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_embed = ConvEmbed(\n",
    "            patch_size=patch_size,\n",
    "            in_chans=in_chans,\n",
    "            stride=patch_stride,\n",
    "            padding=patch_padding,\n",
    "            embed_dim=embed_dim,\n",
    "            norm_layer=norm_layer\n",
    "        )\n",
    "\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                drop=drop_rate,\n",
    "                attn_drop=attn_drop_rate,\n",
    "                drop_path=drop_path_rate,\n",
    "                act_layer=act_layer,\n",
    "                norm_layer=norm_layer,\n",
    "                **kwargs\n",
    "            ) for _ in range(depth)\n",
    "        ])\n",
    "\n",
    "        if init == 'xavier':\n",
    "            self.apply(self._init_weights_xavier)\n",
    "        else:\n",
    "            self.apply(self._init_weights_trunc_normal)\n",
    "\n",
    "    def _init_weights_trunc_normal(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, (nn.LayerNorm, nn.BatchNorm2d)):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def _init_weights_xavier(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, (nn.LayerNorm, nn.BatchNorm2d)):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        _, _, H, W = x.size()\n",
    "\n",
    "        x = rearrange(x, 'b c h w -> b (h w) c')\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for _, blk in enumerate(self.blocks):\n",
    "            x = blk(x, H, W)\n",
    "        x = rearrange(x, 'b (h w) c -> b c h w', h=H, w=W)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalVisionTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_chans=3,\n",
    "                 num_classes=100,\n",
    "                 act_layer=nn.GELU,\n",
    "                 norm_layer=nn.LayerNorm,\n",
    "                 init='trunc_norm',\n",
    "                 spec=None):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.num_stages = spec['NUM_STAGES']\n",
    "        self.stages = nn.ModuleList()\n",
    "        for i in range(self.num_stages):\n",
    "            kwargs = {\n",
    "                'patch_size': spec['PATCH_SIZE'][i],\n",
    "                'patch_stride': spec['PATCH_STRIDE'][i],\n",
    "                'patch_padding': spec['PATCH_PADDING'][i],\n",
    "                'embed_dim': spec['DIM_EMBED'][i],\n",
    "                'depth': spec['DEPTH'][i],\n",
    "                'num_heads': spec['NUM_HEADS'][i],\n",
    "                'mlp_ratio': spec['MLP_RATIO'][i],\n",
    "                'qkv_bias': spec['QKV_BIAS'][i],\n",
    "                'drop_rate': spec['DROP_RATE'][i],\n",
    "                'attn_drop_rate': spec['ATTN_DROP_RATE'][i],\n",
    "                'drop_path_rate': spec['DROP_PATH_RATE'][i],\n",
    "                'kernel_size': spec['KERNEL_QKV'][i],\n",
    "                'padding_q': spec['PADDING_Q'][i],\n",
    "                'padding_kv': spec['PADDING_KV'][i],\n",
    "                'stride_q': spec['STRIDE_Q'][i],\n",
    "                'stride_kv': spec['STRIDE_KV'][i],\n",
    "            }\n",
    "\n",
    "            stage = VisionTransformer(\n",
    "                in_chans=in_chans,\n",
    "                init=init,\n",
    "                act_layer=act_layer,\n",
    "                norm_layer=norm_layer,\n",
    "                **kwargs\n",
    "            )\n",
    "            \n",
    "            self.stages.append(stage)\n",
    "\n",
    "            in_chans = spec['DIM_EMBED'][i]\n",
    "\n",
    "        dim_embed = spec['DIM_EMBED'][-1]\n",
    "        self.norm = norm_layer(dim_embed)\n",
    "        self.pooler = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "        # Classifier head\n",
    "        self.head = nn.Linear(dim_embed, num_classes) if num_classes > 0 else nn.Identity()\n",
    "        trunc_normal_(self.head.weight, std=0.02)\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        for stage in self.stages:\n",
    "            x = stage(x)\n",
    "\n",
    "        x = rearrange(x, 'b c h w -> b (h w) c') # (B, L, C)\n",
    "        x = self.norm(x)                         # (B, L, C)\n",
    "        x = self.pooler(x.transpose(1,2))        # (B, C, 1)\n",
    "        x = torch.flatten(x, 1)                  # (B, C)\n",
    "        # x = torch.mean(x, dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.head(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuickGELU(nn.Module):\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return x * torch.sigmoid(1.702 * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NUM_STAGES': 3,\n",
       " 'PATCH_SIZE': [7, 3, 3],\n",
       " 'PATCH_STRIDE': [4, 2, 2],\n",
       " 'PATCH_PADDING': [2, 1, 1],\n",
       " 'DIM_EMBED': [64, 192, 384],\n",
       " 'DEPTH': [1, 2, 10],\n",
       " 'NUM_HEADS': [1, 3, 6],\n",
       " 'MLP_RATIO': [4.0, 4.0, 4.0],\n",
       " 'QKV_BIAS': [True, True, True],\n",
       " 'DROP_RATE': [0.0, 0.0, 0.0],\n",
       " 'ATTN_DROP_RATE': [0.0, 0.0, 0.0],\n",
       " 'DROP_PATH_RATE': [0.0, 0.0, 0.1],\n",
       " 'KERNEL_QKV': [3, 3, 3],\n",
       " 'PADDING_Q': [1, 1, 1],\n",
       " 'PADDING_KV': [1, 1, 1],\n",
       " 'STRIDE_Q': [1, 1, 1],\n",
       " 'STRIDE_KV': [2, 2, 2]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec = {\n",
    "    'NUM_STAGES': 3,\n",
    "    'PATCH_SIZE': [7,3,3],\n",
    "    'PATCH_STRIDE': [4,2,2],\n",
    "    'PATCH_PADDING': [2,1,1],\n",
    "    'DIM_EMBED': [64,192,384],\n",
    "    'DEPTH': [1,2,10],\n",
    "    'NUM_HEADS': [1,3,6],   # original : [1,3,6]\n",
    "    'MLP_RATIO': [4.,4.,4.],\n",
    "    'QKV_BIAS': [True, True, True],\n",
    "    'DROP_RATE': [0.,0.,0.],\n",
    "    'ATTN_DROP_RATE': [0.,0.,0.],\n",
    "    'DROP_PATH_RATE': [0.,0.,0.1],\n",
    "    'KERNEL_QKV': [3,3,3],\n",
    "    'PADDING_Q': [1,1,1],\n",
    "    'PADDING_KV': [1,1,1],\n",
    "    'STRIDE_Q': [1,1,1],\n",
    "    'STRIDE_KV': [2,2,2],\n",
    "}\n",
    "\n",
    "spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 224, 224]),\n",
       " torch.Size([2, 100]),\n",
       " tensor([[ 0.0211, -0.0558, -0.4906,  0.2948,  0.3206,  0.6026, -0.4612,  0.1550,\n",
       "           0.0135,  0.5312, -0.3943,  0.3365, -0.5384, -0.6001, -0.2411, -0.3099,\n",
       "           0.1318,  0.2968,  0.3758,  0.7561,  0.2106, -0.3650,  0.4291,  0.1401,\n",
       "           0.7766, -0.0212,  0.5200,  0.2039,  0.4841, -0.0210, -0.5029,  0.2329,\n",
       "          -0.1808, -0.3701, -0.1268,  0.2263, -0.0617, -0.4244, -0.4052, -0.4202,\n",
       "          -0.0278,  0.2804, -0.1042,  0.2972,  0.0252, -0.0428,  0.4662, -0.2253,\n",
       "           0.1695,  0.4004, -0.2699,  0.1824,  0.4349, -0.5129, -0.5117,  0.0374,\n",
       "          -0.2373, -0.0456, -0.3491, -0.3741,  0.6553,  0.1080,  0.0342, -0.0314,\n",
       "          -0.3117,  0.0951, -0.1339, -0.3594,  0.1543, -0.3884, -0.2792,  0.5480,\n",
       "           0.1498, -0.0871,  0.1263, -0.4148, -0.0138, -1.0696,  0.4535,  0.7342,\n",
       "           0.3567,  0.4345, -0.4056, -0.6031, -0.5248, -0.7621,  0.4022, -0.0378,\n",
       "          -0.6098,  0.6182, -0.5991, -0.1844, -0.1089, -0.2225,  0.4272,  0.0622,\n",
       "          -0.0020, -0.3287,  0.0339, -0.2539],\n",
       "         [ 0.0211, -0.0558, -0.4906,  0.2948,  0.3206,  0.6026, -0.4612,  0.1550,\n",
       "           0.0135,  0.5312, -0.3943,  0.3365, -0.5384, -0.6001, -0.2411, -0.3099,\n",
       "           0.1318,  0.2968,  0.3758,  0.7561,  0.2106, -0.3650,  0.4291,  0.1401,\n",
       "           0.7766, -0.0212,  0.5200,  0.2039,  0.4841, -0.0210, -0.5029,  0.2329,\n",
       "          -0.1808, -0.3701, -0.1268,  0.2263, -0.0617, -0.4244, -0.4052, -0.4202,\n",
       "          -0.0278,  0.2804, -0.1042,  0.2972,  0.0252, -0.0428,  0.4662, -0.2253,\n",
       "           0.1695,  0.4004, -0.2699,  0.1824,  0.4349, -0.5129, -0.5117,  0.0374,\n",
       "          -0.2373, -0.0456, -0.3491, -0.3741,  0.6553,  0.1080,  0.0342, -0.0314,\n",
       "          -0.3117,  0.0951, -0.1339, -0.3594,  0.1543, -0.3884, -0.2792,  0.5480,\n",
       "           0.1498, -0.0871,  0.1263, -0.4148, -0.0138, -1.0696,  0.4535,  0.7342,\n",
       "           0.3567,  0.4345, -0.4056, -0.6031, -0.5248, -0.7621,  0.4022, -0.0378,\n",
       "          -0.6098,  0.6182, -0.5991, -0.1844, -0.1089, -0.2225,  0.4272,  0.0622,\n",
       "          -0.0020, -0.3287,  0.0339, -0.2539]], grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ConvolutionalVisionTransformer(act_layer=QuickGELU, spec=spec)\n",
    "\n",
    "test_result = model(test_img)\n",
    "test_img.shape, test_result.shape, test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 56, 56]           9,472\n",
      "         LayerNorm-2             [-1, 3136, 64]             128\n",
      "         ConvEmbed-3           [-1, 64, 56, 56]               0\n",
      "           Dropout-4             [-1, 3136, 64]               0\n",
      "         LayerNorm-5             [-1, 3136, 64]             128\n",
      "            Conv2d-6           [-1, 64, 56, 56]             576\n",
      "       BatchNorm2d-7           [-1, 64, 56, 56]             128\n",
      "            Conv2d-8           [-1, 64, 56, 56]           4,160\n",
      "         Rearrange-9             [-1, 3136, 64]               0\n",
      "           Conv2d-10           [-1, 64, 28, 28]             576\n",
      "      BatchNorm2d-11           [-1, 64, 28, 28]             128\n",
      "           Conv2d-12           [-1, 64, 28, 28]           4,160\n",
      "        Rearrange-13              [-1, 784, 64]               0\n",
      "           Conv2d-14           [-1, 64, 28, 28]             576\n",
      "      BatchNorm2d-15           [-1, 64, 28, 28]             128\n",
      "           Conv2d-16           [-1, 64, 28, 28]           4,160\n",
      "        Rearrange-17              [-1, 784, 64]               0\n",
      "           Linear-18             [-1, 3136, 64]           4,160\n",
      "           Linear-19              [-1, 784, 64]           4,160\n",
      "           Linear-20              [-1, 784, 64]           4,160\n",
      "          Dropout-21         [-1, 1, 3136, 784]               0\n",
      "           Linear-22             [-1, 3136, 64]           4,160\n",
      "          Dropout-23             [-1, 3136, 64]               0\n",
      "    AttentionConv-24             [-1, 3136, 64]               0\n",
      "       LayerScale-25             [-1, 3136, 64]               0\n",
      "         Identity-26             [-1, 3136, 64]               0\n",
      "        LayerNorm-27             [-1, 3136, 64]             128\n",
      "           Linear-28            [-1, 3136, 256]          16,640\n",
      "        QuickGELU-29            [-1, 3136, 256]               0\n",
      "           Linear-30             [-1, 3136, 64]          16,448\n",
      "          Dropout-31             [-1, 3136, 64]               0\n",
      "       LayerScale-32             [-1, 3136, 64]               0\n",
      "         Identity-33             [-1, 3136, 64]               0\n",
      "            Block-34             [-1, 3136, 64]               0\n",
      "VisionTransformer-35           [-1, 64, 56, 56]               0\n",
      "           Conv2d-36          [-1, 192, 28, 28]         110,784\n",
      "        LayerNorm-37             [-1, 784, 192]             384\n",
      "        ConvEmbed-38          [-1, 192, 28, 28]               0\n",
      "          Dropout-39             [-1, 784, 192]               0\n",
      "        LayerNorm-40             [-1, 784, 192]             384\n",
      "           Conv2d-41          [-1, 192, 28, 28]           1,728\n",
      "      BatchNorm2d-42          [-1, 192, 28, 28]             384\n",
      "           Conv2d-43          [-1, 192, 28, 28]          37,056\n",
      "        Rearrange-44             [-1, 784, 192]               0\n",
      "           Conv2d-45          [-1, 192, 14, 14]           1,728\n",
      "      BatchNorm2d-46          [-1, 192, 14, 14]             384\n",
      "           Conv2d-47          [-1, 192, 14, 14]          37,056\n",
      "        Rearrange-48             [-1, 196, 192]               0\n",
      "           Conv2d-49          [-1, 192, 14, 14]           1,728\n",
      "      BatchNorm2d-50          [-1, 192, 14, 14]             384\n",
      "           Conv2d-51          [-1, 192, 14, 14]          37,056\n",
      "        Rearrange-52             [-1, 196, 192]               0\n",
      "           Linear-53             [-1, 784, 192]          37,056\n",
      "           Linear-54             [-1, 196, 192]          37,056\n",
      "           Linear-55             [-1, 196, 192]          37,056\n",
      "          Dropout-56          [-1, 3, 784, 196]               0\n",
      "           Linear-57             [-1, 784, 192]          37,056\n",
      "          Dropout-58             [-1, 784, 192]               0\n",
      "    AttentionConv-59             [-1, 784, 192]               0\n",
      "       LayerScale-60             [-1, 784, 192]               0\n",
      "         Identity-61             [-1, 784, 192]               0\n",
      "        LayerNorm-62             [-1, 784, 192]             384\n",
      "           Linear-63             [-1, 784, 768]         148,224\n",
      "        QuickGELU-64             [-1, 784, 768]               0\n",
      "           Linear-65             [-1, 784, 192]         147,648\n",
      "          Dropout-66             [-1, 784, 192]               0\n",
      "       LayerScale-67             [-1, 784, 192]               0\n",
      "         Identity-68             [-1, 784, 192]               0\n",
      "            Block-69             [-1, 784, 192]               0\n",
      "        LayerNorm-70             [-1, 784, 192]             384\n",
      "           Conv2d-71          [-1, 192, 28, 28]           1,728\n",
      "      BatchNorm2d-72          [-1, 192, 28, 28]             384\n",
      "           Conv2d-73          [-1, 192, 28, 28]          37,056\n",
      "        Rearrange-74             [-1, 784, 192]               0\n",
      "           Conv2d-75          [-1, 192, 14, 14]           1,728\n",
      "      BatchNorm2d-76          [-1, 192, 14, 14]             384\n",
      "           Conv2d-77          [-1, 192, 14, 14]          37,056\n",
      "        Rearrange-78             [-1, 196, 192]               0\n",
      "           Conv2d-79          [-1, 192, 14, 14]           1,728\n",
      "      BatchNorm2d-80          [-1, 192, 14, 14]             384\n",
      "           Conv2d-81          [-1, 192, 14, 14]          37,056\n",
      "        Rearrange-82             [-1, 196, 192]               0\n",
      "           Linear-83             [-1, 784, 192]          37,056\n",
      "           Linear-84             [-1, 196, 192]          37,056\n",
      "           Linear-85             [-1, 196, 192]          37,056\n",
      "          Dropout-86          [-1, 3, 784, 196]               0\n",
      "           Linear-87             [-1, 784, 192]          37,056\n",
      "          Dropout-88             [-1, 784, 192]               0\n",
      "    AttentionConv-89             [-1, 784, 192]               0\n",
      "       LayerScale-90             [-1, 784, 192]               0\n",
      "         Identity-91             [-1, 784, 192]               0\n",
      "        LayerNorm-92             [-1, 784, 192]             384\n",
      "           Linear-93             [-1, 784, 768]         148,224\n",
      "        QuickGELU-94             [-1, 784, 768]               0\n",
      "           Linear-95             [-1, 784, 192]         147,648\n",
      "          Dropout-96             [-1, 784, 192]               0\n",
      "       LayerScale-97             [-1, 784, 192]               0\n",
      "         Identity-98             [-1, 784, 192]               0\n",
      "            Block-99             [-1, 784, 192]               0\n",
      "VisionTransformer-100          [-1, 192, 28, 28]               0\n",
      "          Conv2d-101          [-1, 384, 14, 14]         663,936\n",
      "       LayerNorm-102             [-1, 196, 384]             768\n",
      "       ConvEmbed-103          [-1, 384, 14, 14]               0\n",
      "         Dropout-104             [-1, 196, 384]               0\n",
      "       LayerNorm-105             [-1, 196, 384]             768\n",
      "          Conv2d-106          [-1, 384, 14, 14]           3,456\n",
      "     BatchNorm2d-107          [-1, 384, 14, 14]             768\n",
      "          Conv2d-108          [-1, 384, 14, 14]         147,840\n",
      "       Rearrange-109             [-1, 196, 384]               0\n",
      "          Conv2d-110            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-111            [-1, 384, 7, 7]             768\n",
      "          Conv2d-112            [-1, 384, 7, 7]         147,840\n",
      "       Rearrange-113              [-1, 49, 384]               0\n",
      "          Conv2d-114            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-115            [-1, 384, 7, 7]             768\n",
      "          Conv2d-116            [-1, 384, 7, 7]         147,840\n",
      "       Rearrange-117              [-1, 49, 384]               0\n",
      "          Linear-118             [-1, 196, 384]         147,840\n",
      "          Linear-119              [-1, 49, 384]         147,840\n",
      "          Linear-120              [-1, 49, 384]         147,840\n",
      "         Dropout-121           [-1, 6, 196, 49]               0\n",
      "          Linear-122             [-1, 196, 384]         147,840\n",
      "         Dropout-123             [-1, 196, 384]               0\n",
      "   AttentionConv-124             [-1, 196, 384]               0\n",
      "      LayerScale-125             [-1, 196, 384]               0\n",
      "        DropPath-126             [-1, 196, 384]               0\n",
      "       LayerNorm-127             [-1, 196, 384]             768\n",
      "          Linear-128            [-1, 196, 1536]         591,360\n",
      "       QuickGELU-129            [-1, 196, 1536]               0\n",
      "          Linear-130             [-1, 196, 384]         590,208\n",
      "         Dropout-131             [-1, 196, 384]               0\n",
      "      LayerScale-132             [-1, 196, 384]               0\n",
      "        DropPath-133             [-1, 196, 384]               0\n",
      "           Block-134             [-1, 196, 384]               0\n",
      "       LayerNorm-135             [-1, 196, 384]             768\n",
      "          Conv2d-136          [-1, 384, 14, 14]           3,456\n",
      "     BatchNorm2d-137          [-1, 384, 14, 14]             768\n",
      "          Conv2d-138          [-1, 384, 14, 14]         147,840\n",
      "       Rearrange-139             [-1, 196, 384]               0\n",
      "          Conv2d-140            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-141            [-1, 384, 7, 7]             768\n",
      "          Conv2d-142            [-1, 384, 7, 7]         147,840\n",
      "       Rearrange-143              [-1, 49, 384]               0\n",
      "          Conv2d-144            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-145            [-1, 384, 7, 7]             768\n",
      "          Conv2d-146            [-1, 384, 7, 7]         147,840\n",
      "       Rearrange-147              [-1, 49, 384]               0\n",
      "          Linear-148             [-1, 196, 384]         147,840\n",
      "          Linear-149              [-1, 49, 384]         147,840\n",
      "          Linear-150              [-1, 49, 384]         147,840\n",
      "         Dropout-151           [-1, 6, 196, 49]               0\n",
      "          Linear-152             [-1, 196, 384]         147,840\n",
      "         Dropout-153             [-1, 196, 384]               0\n",
      "   AttentionConv-154             [-1, 196, 384]               0\n",
      "      LayerScale-155             [-1, 196, 384]               0\n",
      "        DropPath-156             [-1, 196, 384]               0\n",
      "       LayerNorm-157             [-1, 196, 384]             768\n",
      "          Linear-158            [-1, 196, 1536]         591,360\n",
      "       QuickGELU-159            [-1, 196, 1536]               0\n",
      "          Linear-160             [-1, 196, 384]         590,208\n",
      "         Dropout-161             [-1, 196, 384]               0\n",
      "      LayerScale-162             [-1, 196, 384]               0\n",
      "        DropPath-163             [-1, 196, 384]               0\n",
      "           Block-164             [-1, 196, 384]               0\n",
      "       LayerNorm-165             [-1, 196, 384]             768\n",
      "          Conv2d-166          [-1, 384, 14, 14]           3,456\n",
      "     BatchNorm2d-167          [-1, 384, 14, 14]             768\n",
      "          Conv2d-168          [-1, 384, 14, 14]         147,840\n",
      "       Rearrange-169             [-1, 196, 384]               0\n",
      "          Conv2d-170            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-171            [-1, 384, 7, 7]             768\n",
      "          Conv2d-172            [-1, 384, 7, 7]         147,840\n",
      "       Rearrange-173              [-1, 49, 384]               0\n",
      "          Conv2d-174            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-175            [-1, 384, 7, 7]             768\n",
      "          Conv2d-176            [-1, 384, 7, 7]         147,840\n",
      "       Rearrange-177              [-1, 49, 384]               0\n",
      "          Linear-178             [-1, 196, 384]         147,840\n",
      "          Linear-179              [-1, 49, 384]         147,840\n",
      "          Linear-180              [-1, 49, 384]         147,840\n",
      "         Dropout-181           [-1, 6, 196, 49]               0\n",
      "          Linear-182             [-1, 196, 384]         147,840\n",
      "         Dropout-183             [-1, 196, 384]               0\n",
      "   AttentionConv-184             [-1, 196, 384]               0\n",
      "      LayerScale-185             [-1, 196, 384]               0\n",
      "        DropPath-186             [-1, 196, 384]               0\n",
      "       LayerNorm-187             [-1, 196, 384]             768\n",
      "          Linear-188            [-1, 196, 1536]         591,360\n",
      "       QuickGELU-189            [-1, 196, 1536]               0\n",
      "          Linear-190             [-1, 196, 384]         590,208\n",
      "         Dropout-191             [-1, 196, 384]               0\n",
      "      LayerScale-192             [-1, 196, 384]               0\n",
      "        DropPath-193             [-1, 196, 384]               0\n",
      "           Block-194             [-1, 196, 384]               0\n",
      "       LayerNorm-195             [-1, 196, 384]             768\n",
      "          Conv2d-196          [-1, 384, 14, 14]           3,456\n",
      "     BatchNorm2d-197          [-1, 384, 14, 14]             768\n",
      "          Conv2d-198          [-1, 384, 14, 14]         147,840\n",
      "       Rearrange-199             [-1, 196, 384]               0\n",
      "          Conv2d-200            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-201            [-1, 384, 7, 7]             768\n",
      "          Conv2d-202            [-1, 384, 7, 7]         147,840\n",
      "       Rearrange-203              [-1, 49, 384]               0\n",
      "          Conv2d-204            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-205            [-1, 384, 7, 7]             768\n",
      "          Conv2d-206            [-1, 384, 7, 7]         147,840\n",
      "       Rearrange-207              [-1, 49, 384]               0\n",
      "          Linear-208             [-1, 196, 384]         147,840\n",
      "          Linear-209              [-1, 49, 384]         147,840\n",
      "          Linear-210              [-1, 49, 384]         147,840\n",
      "         Dropout-211           [-1, 6, 196, 49]               0\n",
      "          Linear-212             [-1, 196, 384]         147,840\n",
      "         Dropout-213             [-1, 196, 384]               0\n",
      "   AttentionConv-214             [-1, 196, 384]               0\n",
      "      LayerScale-215             [-1, 196, 384]               0\n",
      "        DropPath-216             [-1, 196, 384]               0\n",
      "       LayerNorm-217             [-1, 196, 384]             768\n",
      "          Linear-218            [-1, 196, 1536]         591,360\n",
      "       QuickGELU-219            [-1, 196, 1536]               0\n",
      "          Linear-220             [-1, 196, 384]         590,208\n",
      "         Dropout-221             [-1, 196, 384]               0\n",
      "      LayerScale-222             [-1, 196, 384]               0\n",
      "        DropPath-223             [-1, 196, 384]               0\n",
      "           Block-224             [-1, 196, 384]               0\n",
      "       LayerNorm-225             [-1, 196, 384]             768\n",
      "          Conv2d-226          [-1, 384, 14, 14]           3,456\n",
      "     BatchNorm2d-227          [-1, 384, 14, 14]             768\n",
      "          Conv2d-228          [-1, 384, 14, 14]         147,840\n",
      "       Rearrange-229             [-1, 196, 384]               0\n",
      "          Conv2d-230            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-231            [-1, 384, 7, 7]             768\n",
      "          Conv2d-232            [-1, 384, 7, 7]         147,840\n",
      "       Rearrange-233              [-1, 49, 384]               0\n",
      "          Conv2d-234            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-235            [-1, 384, 7, 7]             768\n",
      "          Conv2d-236            [-1, 384, 7, 7]         147,840\n",
      "       Rearrange-237              [-1, 49, 384]               0\n",
      "          Linear-238             [-1, 196, 384]         147,840\n",
      "          Linear-239              [-1, 49, 384]         147,840\n",
      "          Linear-240              [-1, 49, 384]         147,840\n",
      "         Dropout-241           [-1, 6, 196, 49]               0\n",
      "          Linear-242             [-1, 196, 384]         147,840\n",
      "         Dropout-243             [-1, 196, 384]               0\n",
      "   AttentionConv-244             [-1, 196, 384]               0\n",
      "      LayerScale-245             [-1, 196, 384]               0\n",
      "        DropPath-246             [-1, 196, 384]               0\n",
      "       LayerNorm-247             [-1, 196, 384]             768\n",
      "          Linear-248            [-1, 196, 1536]         591,360\n",
      "       QuickGELU-249            [-1, 196, 1536]               0\n",
      "          Linear-250             [-1, 196, 384]         590,208\n",
      "         Dropout-251             [-1, 196, 384]               0\n",
      "      LayerScale-252             [-1, 196, 384]               0\n",
      "        DropPath-253             [-1, 196, 384]               0\n",
      "           Block-254             [-1, 196, 384]               0\n",
      "       LayerNorm-255             [-1, 196, 384]             768\n",
      "          Conv2d-256          [-1, 384, 14, 14]           3,456\n",
      "     BatchNorm2d-257          [-1, 384, 14, 14]             768\n",
      "          Conv2d-258          [-1, 384, 14, 14]         147,840\n",
      "       Rearrange-259             [-1, 196, 384]               0\n",
      "          Conv2d-260            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-261            [-1, 384, 7, 7]             768\n",
      "          Conv2d-262            [-1, 384, 7, 7]         147,840\n",
      "       Rearrange-263              [-1, 49, 384]               0\n",
      "          Conv2d-264            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-265            [-1, 384, 7, 7]             768\n",
      "          Conv2d-266            [-1, 384, 7, 7]         147,840\n",
      "       Rearrange-267              [-1, 49, 384]               0\n",
      "          Linear-268             [-1, 196, 384]         147,840\n",
      "          Linear-269              [-1, 49, 384]         147,840\n",
      "          Linear-270              [-1, 49, 384]         147,840\n",
      "         Dropout-271           [-1, 6, 196, 49]               0\n",
      "          Linear-272             [-1, 196, 384]         147,840\n",
      "         Dropout-273             [-1, 196, 384]               0\n",
      "   AttentionConv-274             [-1, 196, 384]               0\n",
      "      LayerScale-275             [-1, 196, 384]               0\n",
      "        DropPath-276             [-1, 196, 384]               0\n",
      "       LayerNorm-277             [-1, 196, 384]             768\n",
      "          Linear-278            [-1, 196, 1536]         591,360\n",
      "       QuickGELU-279            [-1, 196, 1536]               0\n",
      "          Linear-280             [-1, 196, 384]         590,208\n",
      "         Dropout-281             [-1, 196, 384]               0\n",
      "      LayerScale-282             [-1, 196, 384]               0\n",
      "        DropPath-283             [-1, 196, 384]               0\n",
      "           Block-284             [-1, 196, 384]               0\n",
      "       LayerNorm-285             [-1, 196, 384]             768\n",
      "          Conv2d-286          [-1, 384, 14, 14]           3,456\n",
      "     BatchNorm2d-287          [-1, 384, 14, 14]             768\n",
      "          Conv2d-288          [-1, 384, 14, 14]         147,840\n",
      "       Rearrange-289             [-1, 196, 384]               0\n",
      "          Conv2d-290            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-291            [-1, 384, 7, 7]             768\n",
      "          Conv2d-292            [-1, 384, 7, 7]         147,840\n",
      "       Rearrange-293              [-1, 49, 384]               0\n",
      "          Conv2d-294            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-295            [-1, 384, 7, 7]             768\n",
      "          Conv2d-296            [-1, 384, 7, 7]         147,840\n",
      "       Rearrange-297              [-1, 49, 384]               0\n",
      "          Linear-298             [-1, 196, 384]         147,840\n",
      "          Linear-299              [-1, 49, 384]         147,840\n",
      "          Linear-300              [-1, 49, 384]         147,840\n",
      "         Dropout-301           [-1, 6, 196, 49]               0\n",
      "          Linear-302             [-1, 196, 384]         147,840\n",
      "         Dropout-303             [-1, 196, 384]               0\n",
      "   AttentionConv-304             [-1, 196, 384]               0\n",
      "      LayerScale-305             [-1, 196, 384]               0\n",
      "        DropPath-306             [-1, 196, 384]               0\n",
      "       LayerNorm-307             [-1, 196, 384]             768\n",
      "          Linear-308            [-1, 196, 1536]         591,360\n",
      "       QuickGELU-309            [-1, 196, 1536]               0\n",
      "          Linear-310             [-1, 196, 384]         590,208\n",
      "         Dropout-311             [-1, 196, 384]               0\n",
      "      LayerScale-312             [-1, 196, 384]               0\n",
      "        DropPath-313             [-1, 196, 384]               0\n",
      "           Block-314             [-1, 196, 384]               0\n",
      "       LayerNorm-315             [-1, 196, 384]             768\n",
      "          Conv2d-316          [-1, 384, 14, 14]           3,456\n",
      "     BatchNorm2d-317          [-1, 384, 14, 14]             768\n",
      "          Conv2d-318          [-1, 384, 14, 14]         147,840\n",
      "       Rearrange-319             [-1, 196, 384]               0\n",
      "          Conv2d-320            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-321            [-1, 384, 7, 7]             768\n",
      "          Conv2d-322            [-1, 384, 7, 7]         147,840\n",
      "       Rearrange-323              [-1, 49, 384]               0\n",
      "          Conv2d-324            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-325            [-1, 384, 7, 7]             768\n",
      "          Conv2d-326            [-1, 384, 7, 7]         147,840\n",
      "       Rearrange-327              [-1, 49, 384]               0\n",
      "          Linear-328             [-1, 196, 384]         147,840\n",
      "          Linear-329              [-1, 49, 384]         147,840\n",
      "          Linear-330              [-1, 49, 384]         147,840\n",
      "         Dropout-331           [-1, 6, 196, 49]               0\n",
      "          Linear-332             [-1, 196, 384]         147,840\n",
      "         Dropout-333             [-1, 196, 384]               0\n",
      "   AttentionConv-334             [-1, 196, 384]               0\n",
      "      LayerScale-335             [-1, 196, 384]               0\n",
      "        DropPath-336             [-1, 196, 384]               0\n",
      "       LayerNorm-337             [-1, 196, 384]             768\n",
      "          Linear-338            [-1, 196, 1536]         591,360\n",
      "       QuickGELU-339            [-1, 196, 1536]               0\n",
      "          Linear-340             [-1, 196, 384]         590,208\n",
      "         Dropout-341             [-1, 196, 384]               0\n",
      "      LayerScale-342             [-1, 196, 384]               0\n",
      "        DropPath-343             [-1, 196, 384]               0\n",
      "           Block-344             [-1, 196, 384]               0\n",
      "       LayerNorm-345             [-1, 196, 384]             768\n",
      "          Conv2d-346          [-1, 384, 14, 14]           3,456\n",
      "     BatchNorm2d-347          [-1, 384, 14, 14]             768\n",
      "          Conv2d-348          [-1, 384, 14, 14]         147,840\n",
      "       Rearrange-349             [-1, 196, 384]               0\n",
      "          Conv2d-350            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-351            [-1, 384, 7, 7]             768\n",
      "          Conv2d-352            [-1, 384, 7, 7]         147,840\n",
      "       Rearrange-353              [-1, 49, 384]               0\n",
      "          Conv2d-354            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-355            [-1, 384, 7, 7]             768\n",
      "          Conv2d-356            [-1, 384, 7, 7]         147,840\n",
      "       Rearrange-357              [-1, 49, 384]               0\n",
      "          Linear-358             [-1, 196, 384]         147,840\n",
      "          Linear-359              [-1, 49, 384]         147,840\n",
      "          Linear-360              [-1, 49, 384]         147,840\n",
      "         Dropout-361           [-1, 6, 196, 49]               0\n",
      "          Linear-362             [-1, 196, 384]         147,840\n",
      "         Dropout-363             [-1, 196, 384]               0\n",
      "   AttentionConv-364             [-1, 196, 384]               0\n",
      "      LayerScale-365             [-1, 196, 384]               0\n",
      "        DropPath-366             [-1, 196, 384]               0\n",
      "       LayerNorm-367             [-1, 196, 384]             768\n",
      "          Linear-368            [-1, 196, 1536]         591,360\n",
      "       QuickGELU-369            [-1, 196, 1536]               0\n",
      "          Linear-370             [-1, 196, 384]         590,208\n",
      "         Dropout-371             [-1, 196, 384]               0\n",
      "      LayerScale-372             [-1, 196, 384]               0\n",
      "        DropPath-373             [-1, 196, 384]               0\n",
      "           Block-374             [-1, 196, 384]               0\n",
      "       LayerNorm-375             [-1, 196, 384]             768\n",
      "          Conv2d-376          [-1, 384, 14, 14]           3,456\n",
      "     BatchNorm2d-377          [-1, 384, 14, 14]             768\n",
      "          Conv2d-378          [-1, 384, 14, 14]         147,840\n",
      "       Rearrange-379             [-1, 196, 384]               0\n",
      "          Conv2d-380            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-381            [-1, 384, 7, 7]             768\n",
      "          Conv2d-382            [-1, 384, 7, 7]         147,840\n",
      "       Rearrange-383              [-1, 49, 384]               0\n",
      "          Conv2d-384            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-385            [-1, 384, 7, 7]             768\n",
      "          Conv2d-386            [-1, 384, 7, 7]         147,840\n",
      "       Rearrange-387              [-1, 49, 384]               0\n",
      "          Linear-388             [-1, 196, 384]         147,840\n",
      "          Linear-389              [-1, 49, 384]         147,840\n",
      "          Linear-390              [-1, 49, 384]         147,840\n",
      "         Dropout-391           [-1, 6, 196, 49]               0\n",
      "          Linear-392             [-1, 196, 384]         147,840\n",
      "         Dropout-393             [-1, 196, 384]               0\n",
      "   AttentionConv-394             [-1, 196, 384]               0\n",
      "      LayerScale-395             [-1, 196, 384]               0\n",
      "        DropPath-396             [-1, 196, 384]               0\n",
      "       LayerNorm-397             [-1, 196, 384]             768\n",
      "          Linear-398            [-1, 196, 1536]         591,360\n",
      "       QuickGELU-399            [-1, 196, 1536]               0\n",
      "          Linear-400             [-1, 196, 384]         590,208\n",
      "         Dropout-401             [-1, 196, 384]               0\n",
      "      LayerScale-402             [-1, 196, 384]               0\n",
      "        DropPath-403             [-1, 196, 384]               0\n",
      "           Block-404             [-1, 196, 384]               0\n",
      "VisionTransformer-405          [-1, 384, 14, 14]               0\n",
      "       LayerNorm-406             [-1, 196, 384]             768\n",
      "AdaptiveAvgPool1d-407               [-1, 384, 1]               0\n",
      "          Linear-408                  [-1, 100]          38,500\n",
      "================================================================\n",
      "Total params: 24,320,612\n",
      "Trainable params: 24,320,612\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 310.22\n",
      "Params size (MB): 92.78\n",
      "Estimated Total Size (MB): 403.57\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(model.cuda(), (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms 정의하기\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8,1), interpolation=transforms.InterpolationMode.LANCZOS),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    transforms.RandomErasing(p=0.9, scale=(0.02, 0.33)),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "data_dir = '../data/sports'\n",
    "batch_size = 256\n",
    "\n",
    "train_path = data_dir+'/train'\n",
    "valid_path = data_dir+'/valid'\n",
    "test_path = data_dir+'/test'\n",
    "\n",
    "# dataset load\n",
    "train_data = ImageFolder(train_path, transform=train_transform)\n",
    "valid_data = ImageFolder(valid_path, transform=test_transform)\n",
    "test_data = ImageFolder(test_path, transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:2'\n",
    "max_norm = 1.0 # paper : 100 with G variants\n",
    "\n",
    "model.to(device)\n",
    "model_path = '../models/cvt/model.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixup_fn = Mixup(mixup_alpha=.7, \n",
    "                cutmix_alpha=1., \n",
    "                prob=.7, \n",
    "                switch_prob=0.5, \n",
    "                mode='batch',\n",
    "                label_smoothing=.1,\n",
    "                num_classes=100)\n",
    "\n",
    "epochs = 200\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-23 16:17:11.613248: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-23 16:17:11.613336: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-23 16:17:11.614242: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-23 16:17:11.619968: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-23 16:17:12.437782: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.AdamW(model.parameters())\n",
    "warmup_steps = int(len(train_loader)*(epochs)*0.1)\n",
    "train_steps = len(train_loader)*(epochs)\n",
    "scheduler = transformers.get_cosine_schedule_with_warmup(optimizer, \n",
    "                                                        num_warmup_steps=warmup_steps, \n",
    "                                                        num_training_steps=train_steps,\n",
    "                                                        num_cycles=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 53/53 [00:57<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.5529850923790125, Val Loss: 4.37026309967041, LR: 5e-05, Duration: 58.77 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 53/53 [01:02<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.40489535961511, Val Loss: 4.180859565734863, LR: 0.0001, Duration: 63.50 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 53/53 [01:07<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.309056021132559, Val Loss: 4.035790920257568, LR: 0.00015, Duration: 68.41 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 53/53 [01:03<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.2383646425211206, Val Loss: 3.9670908451080322, LR: 0.0002, Duration: 65.10 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 53/53 [01:02<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.195841015509839, Val Loss: 3.855345845222473, LR: 0.00025, Duration: 64.22 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 53/53 [01:03<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.124346062822162, Val Loss: 3.8080384731292725, LR: 0.0003, Duration: 64.93 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 53/53 [01:03<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.1016473680172325, Val Loss: 3.6553266048431396, LR: 0.00035, Duration: 64.76 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 53/53 [01:02<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.068467594542593, Val Loss: 3.61457622051239, LR: 0.0004, Duration: 64.58 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 53/53 [01:03<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.9984240711859935, Val Loss: 3.44623863697052, LR: 0.00045000000000000004, Duration: 64.93 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 53/53 [01:02<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.9184143678197323, Val Loss: 3.3487212657928467, LR: 0.0005, Duration: 64.58 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 53/53 [01:04<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.9126845620713144, Val Loss: 3.3722169399261475, LR: 0.00055, Duration: 65.60 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 53/53 [01:04<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.8240257254186667, Val Loss: 3.277010440826416, LR: 0.0006, Duration: 65.38 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 53/53 [01:04<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.839031017051553, Val Loss: 3.107643485069275, LR: 0.0006500000000000001, Duration: 65.59 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 53/53 [01:02<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.73113400531265, Val Loss: 3.0770044326782227, LR: 0.0007, Duration: 63.90 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 53/53 [01:01<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.6439331927389467, Val Loss: 2.882711410522461, LR: 0.00075, Duration: 63.16 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 53/53 [01:06<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.572757158639296, Val Loss: 2.8419742584228516, LR: 0.0008, Duration: 68.03 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 53/53 [01:11<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.635913025658086, Val Loss: 2.8333007097244263, LR: 0.00085, Duration: 73.22 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 53/53 [01:04<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.574366920399216, Val Loss: 2.6856285333633423, LR: 0.0009000000000000001, Duration: 65.65 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 53/53 [01:05<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.6331372575939826, Val Loss: 2.7278544902801514, LR: 0.00095, Duration: 66.63 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 53/53 [01:08<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.4598171216137006, Val Loss: 2.61009681224823, LR: 0.001, Duration: 69.34 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 53/53 [01:06<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.462913598654405, Val Loss: 2.544796347618103, LR: 0.0009999238475781956, Duration: 68.15 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 53/53 [01:05<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.3158136403785563, Val Loss: 2.3013495206832886, LR: 0.0009996954135095479, Duration: 66.42 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 53/53 [01:04<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.350925517532061, Val Loss: 2.1872788667678833, LR: 0.0009993147673772868, Duration: 66.62 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 53/53 [01:06<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.3003416241339916, Val Loss: 2.1287853717803955, LR: 0.0009987820251299122, Duration: 67.33 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 53/53 [01:04<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.41692335650606, Val Loss: 2.1545841693878174, LR: 0.0009980973490458728, Duration: 66.06 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 53/53 [01:11<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.195594216292759, Val Loss: 1.9752426147460938, LR: 0.0009972609476841367, Duration: 73.35 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 53/53 [01:04<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.1149910980800413, Val Loss: 1.9763428568840027, LR: 0.0009962730758206612, Duration: 65.87 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 53/53 [01:05<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.0295058826230608, Val Loss: 1.971318006515503, LR: 0.0009951340343707852, Duration: 66.35 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 53/53 [01:10<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.0776231738756286, Val Loss: 1.8993996381759644, LR: 0.0009938441702975688, Duration: 71.61 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 53/53 [01:04<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.099147751646222, Val Loss: 1.8955983519554138, LR: 0.000992403876506104, Duration: 65.41 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|██████████| 53/53 [01:05<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.057701011873641, Val Loss: 1.8200837969779968, LR: 0.000990813591723832, Duration: 67.70 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|██████████| 53/53 [01:01<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.0390095170938745, Val Loss: 1.7200132608413696, LR: 0.0009890738003669028, Duration: 62.57 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|██████████| 53/53 [01:04<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.9549607555821256, Val Loss: 1.696426808834076, LR: 0.0009871850323926177, Duration: 67.00 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|██████████| 53/53 [01:06<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.1136998005633085, Val Loss: 1.735174298286438, LR: 0.0009851478631379982, Duration: 68.18 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|██████████| 53/53 [01:03<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.9322088259571, Val Loss: 1.5765542387962341, LR: 0.0009829629131445341, Duration: 65.60 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|██████████| 53/53 [01:02<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.801322005829721, Val Loss: 1.4952527284622192, LR: 0.0009806308479691594, Duration: 64.56 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37: 100%|██████████| 53/53 [01:07<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.9638673134569853, Val Loss: 1.5360203981399536, LR: 0.0009781523779815178, Duration: 69.64 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|██████████| 53/53 [01:03<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.741287906214876, Val Loss: 1.4388568997383118, LR: 0.0009755282581475768, Duration: 65.32 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|██████████| 53/53 [01:05<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.833449809056408, Val Loss: 1.4746387600898743, LR: 0.0009727592877996585, Duration: 66.72 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40: 100%|██████████| 53/53 [01:07<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.928648035481291, Val Loss: 1.5736996531486511, LR: 0.0009698463103929542, Duration: 69.10 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41: 100%|██████████| 53/53 [01:02<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.884330542582386, Val Loss: 1.4341967701911926, LR: 0.0009667902132486009, Duration: 63.68 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42: 100%|██████████| 53/53 [01:04<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.769658482299661, Val Loss: 1.3337533473968506, LR: 0.0009635919272833937, Duration: 65.52 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43: 100%|██████████| 53/53 [01:05<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.803854175333707, Val Loss: 1.3092740774154663, LR: 0.0009602524267262203, Duration: 67.51 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44: 100%|██████████| 53/53 [01:10<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.9626178921393627, Val Loss: 1.4064651131629944, LR: 0.0009567727288213005, Duration: 71.03 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45: 100%|██████████| 53/53 [01:09<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.8487446870443955, Val Loss: 1.4845433235168457, LR: 0.0009531538935183251, Duration: 71.57 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46: 100%|██████████| 53/53 [01:07<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.801461842824828, Val Loss: 1.4286345839500427, LR: 0.0009493970231495835, Duration: 69.95 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47: 100%|██████████| 53/53 [01:07<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.7694190065815762, Val Loss: 1.3127281665802002, LR: 0.0009455032620941839, Duration: 68.03 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48: 100%|██████████| 53/53 [01:03<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.592758003270851, Val Loss: 1.3275494575500488, LR: 0.0009414737964294635, Duration: 64.68 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 53/53 [01:02<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.7786604683354215, Val Loss: 1.2643755078315735, LR: 0.0009373098535696979, Duration: 64.19 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50: 100%|██████████| 53/53 [01:06<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6537193959614016, Val Loss: 1.2713829278945923, LR: 0.0009330127018922195, Duration: 68.24 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51: 100%|██████████| 53/53 [01:09<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.386651889333185, Val Loss: 1.2800102829933167, LR: 0.0009285836503510562, Duration: 70.80 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52: 100%|██████████| 53/53 [01:07<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.334265519987862, Val Loss: 1.1405368447303772, LR: 0.0009240240480782129, Duration: 68.97 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53: 100%|██████████| 53/53 [01:03<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4822362099053725, Val Loss: 1.129635214805603, LR: 0.0009193352839727121, Duration: 64.88 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54: 100%|██████████| 53/53 [01:03<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6379525099160537, Val Loss: 1.3044040203094482, LR: 0.0009145187862775209, Duration: 65.00 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55: 100%|██████████| 53/53 [01:04<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5706374217879095, Val Loss: 1.0452020764350891, LR: 0.0009095760221444959, Duration: 65.58 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56: 100%|██████████| 53/53 [01:01<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6323713469055465, Val Loss: 1.130101501941681, LR: 0.0009045084971874737, Duration: 62.59 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57: 100%|██████████| 53/53 [01:03<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.487555596063722, Val Loss: 1.136031985282898, LR: 0.0008993177550236464, Duration: 65.58 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58: 100%|██████████| 53/53 [01:02<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.554370880126953, Val Loss: 1.035994440317154, LR: 0.0008940053768033609, Duration: 65.85 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59: 100%|██████████| 53/53 [01:05<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3156080268464, Val Loss: 1.0894077718257904, LR: 0.0008885729807284854, Duration: 66.76 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60: 100%|██████████| 53/53 [01:00<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.419907803805369, Val Loss: 1.0455090999603271, LR: 0.000883022221559489, Duration: 61.90 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61: 100%|██████████| 53/53 [01:07<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6332807473416597, Val Loss: 1.1262041926383972, LR: 0.000877354790111386, Duration: 69.45 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62: 100%|██████████| 53/53 [01:06<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.508747069340832, Val Loss: 1.0395189821720123, LR: 0.0008715724127386971, Duration: 67.68 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63: 100%|██████████| 53/53 [01:08<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4106781100327113, Val Loss: 1.020106166601181, LR: 0.0008656768508095852, Duration: 70.16 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64: 100%|██████████| 53/53 [01:04<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.428007258559173, Val Loss: 1.0254282057285309, LR: 0.0008596699001693256, Duration: 65.92 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65: 100%|██████████| 53/53 [01:05<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4333730733619547, Val Loss: 1.070679485797882, LR: 0.0008535533905932737, Duration: 66.71 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66: 100%|██████████| 53/53 [01:03<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0554985797630168, Val Loss: 0.9034236371517181, LR: 0.0008473291852294987, Duration: 65.46 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67: 100%|██████████| 53/53 [01:09<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2747459344144136, Val Loss: 0.9996425211429596, LR: 0.0008409991800312493, Duration: 71.11 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68: 100%|██████████| 53/53 [01:01<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.249709709635321, Val Loss: 0.9472216665744781, LR: 0.0008345653031794292, Duration: 62.46 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69: 100%|██████████| 53/53 [01:08<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.324824996714322, Val Loss: 0.866094559431076, LR: 0.0008280295144952537, Duration: 70.02 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70: 100%|██████████| 53/53 [01:03<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2101984698817416, Val Loss: 0.9031307399272919, LR: 0.0008213938048432696, Duration: 64.55 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71: 100%|██████████| 53/53 [01:08<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.18354679728454, Val Loss: 0.9304220676422119, LR: 0.0008146601955249188, Duration: 70.19 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72: 100%|██████████| 53/53 [01:04<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.293254499165517, Val Loss: 0.9383030235767365, LR: 0.0008078307376628291, Duration: 65.12 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 73: 100%|██████████| 53/53 [01:01<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.193612908417324, Val Loss: 0.9417426586151123, LR: 0.0008009075115760243, Duration: 62.66 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 74: 100%|██████████| 53/53 [01:02<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.111749158715302, Val Loss: 0.9456266760826111, LR: 0.0007938926261462366, Duration: 63.96 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75: 100%|██████████| 53/53 [01:02<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2993492225431047, Val Loss: 0.9329831004142761, LR: 0.0007867882181755231, Duration: 63.47 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 76: 100%|██████████| 53/53 [01:06<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.191153296884501, Val Loss: 0.9140001237392426, LR: 0.0007795964517353734, Duration: 68.11 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 77: 100%|██████████| 53/53 [01:07<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0403887528293536, Val Loss: 0.8437642157077789, LR: 0.0007723195175075137, Duration: 69.38 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 78: 100%|██████████| 53/53 [01:05<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2175640340121285, Val Loss: 0.9203191995620728, LR: 0.0007649596321166025, Duration: 66.56 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 79: 100%|██████████| 53/53 [01:09<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.198874741230371, Val Loss: 0.9437687695026398, LR: 0.0007575190374550272, Duration: 73.65 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 80: 100%|██████████| 53/53 [01:05<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1008130761812316, Val Loss: 0.8773991167545319, LR: 0.00075, Duration: 66.43 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 81: 100%|██████████| 53/53 [01:08<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3084756855694755, Val Loss: 0.9259405136108398, LR: 0.0007424048101231686, Duration: 70.54 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 82: 100%|██████████| 53/53 [01:10<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1820182125523404, Val Loss: 0.9349567294120789, LR: 0.0007347357813929454, Duration: 71.61 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 83: 100%|██████████| 53/53 [01:05<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.998693828312856, Val Loss: 0.8771927058696747, LR: 0.0007269952498697733, Duration: 66.18 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 84: 100%|██████████| 53/53 [01:05<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0645603638774945, Val Loss: 0.8505749702453613, LR: 0.0007191855733945387, Duration: 67.50 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 85: 100%|██████████| 53/53 [01:01<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.100299783472745, Val Loss: 0.9061897993087769, LR: 0.0007113091308703497, Duration: 62.22 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 86: 100%|██████████| 53/53 [01:02<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3644329399432777, Val Loss: 0.8690151274204254, LR: 0.0007033683215379002, Duration: 63.99 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 87: 100%|██████████| 53/53 [01:06<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.9869960186616429, Val Loss: 0.856202244758606, LR: 0.0006953655642446368, Duration: 68.04 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 88: 100%|██████████| 53/53 [01:03<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.028511490461961, Val Loss: 0.8536461889743805, LR: 0.0006873032967079561, Duration: 65.66 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 89: 100%|██████████| 53/53 [01:01<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.9153758849737779, Val Loss: 0.8237971067428589, LR: 0.0006791839747726501, Duration: 63.16 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 90: 100%|██████████| 53/53 [00:50<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1796236825439164, Val Loss: 0.8068076372146606, LR: 0.0006710100716628344, Duration: 51.64 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 91: 100%|██████████| 53/53 [00:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.943442553844092, Val Loss: 0.8271751701831818, LR: 0.0006627840772285784, Duration: 50.74 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 92: 100%|██████████| 53/53 [00:49<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0080434871169754, Val Loss: 0.8221621215343475, LR: 0.0006545084971874737, Duration: 50.62 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 93: 100%|██████████| 53/53 [00:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0155571699142456, Val Loss: 0.8657822012901306, LR: 0.0006461858523613684, Duration: 50.92 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 94: 100%|██████████| 53/53 [00:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0944578310228743, Val Loss: 0.8334069848060608, LR: 0.0006378186779084996, Duration: 50.78 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 95: 100%|██████████| 53/53 [00:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0113743431163282, Val Loss: 0.8341044783592224, LR: 0.0006294095225512603, Duration: 50.82 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 96: 100%|██████████| 53/53 [00:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.9924622931570377, Val Loss: 0.838939368724823, LR: 0.0006209609477998338, Duration: 50.78 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 97: 100%|██████████| 53/53 [00:49<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.782689872777687, Val Loss: 0.7741664946079254, LR: 0.0006124755271719325, Duration: 50.85 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 98: 100%|██████████| 53/53 [00:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.9254596885645165, Val Loss: 0.8200420141220093, LR: 0.0006039558454088796, Duration: 50.80 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 53/53 [00:49<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.9271867275238037, Val Loss: 0.8260376751422882, LR: 0.0005954044976882724, Duration: 50.61 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 100: 100%|██████████| 53/53 [00:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.8583161673455868, Val Loss: 0.8066474199295044, LR: 0.0005868240888334653, Duration: 50.80 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 101: 100%|██████████| 53/53 [00:49<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1396689594916576, Val Loss: 0.8136171996593475, LR: 0.0005782172325201155, Duration: 50.60 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 102: 100%|██████████| 53/53 [00:49<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.8848963987152532, Val Loss: 0.8826410472393036, LR: 0.0005695865504800327, Duration: 50.68 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 103: 100%|██████████| 53/53 [00:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.044515605242747, Val Loss: 0.8239445090293884, LR: 0.0005609346717025737, Duration: 50.90 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 104: 100%|██████████| 53/53 [00:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.01988373837381, Val Loss: 0.853959709405899, LR: 0.0005522642316338268, Duration: 50.95 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 105: 100%|██████████| 53/53 [00:49<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.8653327305361909, Val Loss: 0.9208323657512665, LR: 0.0005435778713738292, Duration: 50.68 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 106: 100%|██████████| 53/53 [00:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.058353797444757, Val Loss: 0.8493340611457825, LR: 0.0005348782368720626, Duration: 50.73 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 107: 100%|██████████| 53/53 [00:49<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.002583938949513, Val Loss: 0.812932550907135, LR: 0.000526167978121472, Duration: 50.67 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 108: 100%|██████████| 53/53 [00:49<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.7192679958523445, Val Loss: 0.7967312037944794, LR: 0.0005174497483512506, Duration: 50.67 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 109: 100%|██████████| 53/53 [00:49<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.9520259443319068, Val Loss: 0.8284212946891785, LR: 0.0005087262032186418, Duration: 50.68 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 110: 100%|██████████| 53/53 [00:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.8054435028220124, Val Loss: 0.7898607552051544, LR: 0.0005, Duration: 50.86 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 111: 100%|██████████| 53/53 [00:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.8536412018649984, Val Loss: 0.7811048626899719, LR: 0.0004912737967813582, Duration: 50.85 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 112: 100%|██████████| 53/53 [00:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.945035754509692, Val Loss: 0.9615026414394379, LR: 0.0004825502516487497, Duration: 50.78 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 113: 100%|██████████| 53/53 [00:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.7776776945815895, Val Loss: 0.8694575726985931, LR: 0.0004738320218785281, Duration: 50.79 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 114: 100%|██████████| 53/53 [00:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.9089340230204024, Val Loss: 0.8249691724777222, LR: 0.00046512176312793734, Duration: 50.73 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 115: 100%|██████████| 53/53 [00:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.90153347433738, Val Loss: 0.9259986579418182, LR: 0.00045642212862617086, Duration: 50.95 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 116: 100%|██████████| 53/53 [00:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.858366388194966, Val Loss: 0.8191998898983002, LR: 0.00044773576836617336, Duration: 50.95 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 117: 100%|██████████| 53/53 [00:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.9464496385376409, Val Loss: 0.8376502990722656, LR: 0.00043906532829742634, Duration: 50.77 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 118: 100%|██████████| 53/53 [00:50<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.8132716630989651, Val Loss: 0.8611080646514893, LR: 0.0004304134495199674, Duration: 50.98 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 119: 100%|██████████| 53/53 [00:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.8526500137347095, Val Loss: 0.7707901895046234, LR: 0.0004217827674798845, Duration: 51.07 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 120: 100%|██████████| 53/53 [00:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.9216101664417196, Val Loss: 0.7744775116443634, LR: 0.00041317591116653486, Duration: 50.86 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 121: 100%|██████████| 53/53 [00:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.7938242543418452, Val Loss: 0.7844637632369995, LR: 0.0004045955023117276, Duration: 50.92 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 122: 100%|██████████| 53/53 [00:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.8023364948776532, Val Loss: 0.8331851959228516, LR: 0.0003960441545911204, Duration: 50.90 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 123: 100%|██████████| 53/53 [00:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.6481545657481786, Val Loss: 0.8141883313655853, LR: 0.0003875244728280676, Duration: 50.90 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 124: 100%|██████████| 53/53 [00:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.6974328317732181, Val Loss: 0.7755322456359863, LR: 0.0003790390522001662, Duration: 50.93 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 125: 100%|██████████| 53/53 [00:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.7494023982084022, Val Loss: 0.8592051863670349, LR: 0.0003705904774487396, Duration: 50.91 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 126: 100%|██████████| 53/53 [00:50<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.811260738462772, Val Loss: 0.7863874435424805, LR: 0.00036218132209150044, Duration: 50.99 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 127: 100%|██████████| 53/53 [00:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.7113369500861977, Val Loss: 0.8372033834457397, LR: 0.00035381414763863166, Duration: 50.79 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 128: 100%|██████████| 53/53 [00:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.8813409479159229, Val Loss: 0.8268453478813171, LR: 0.00034549150281252633, Duration: 50.83 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 129: 100%|██████████| 53/53 [00:49<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.862740751707329, Val Loss: 0.7642996609210968, LR: 0.00033721592277142175, Duration: 50.95 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 130: 100%|██████████| 53/53 [00:49<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0038391093038164, Val Loss: 0.8930239081382751, LR: 0.0003289899283371657, Duration: 50.69 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 131: 100%|██████████| 53/53 [00:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.5871035085534149, Val Loss: 0.7913223206996918, LR: 0.00032081602522734986, Duration: 50.82 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 132: 100%|██████████| 53/53 [00:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.73360401954291, Val Loss: 0.7970458269119263, LR: 0.00031269670329204396, Duration: 50.79 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 133: 100%|██████████| 53/53 [00:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.975220630753715, Val Loss: 0.8182066380977631, LR: 0.0003046344357553632, Duration: 50.76 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 134: 100%|██████████| 53/53 [00:49<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.8040742694206957, Val Loss: 0.8114172220230103, LR: 0.0002966316784621, Duration: 50.70 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 135: 100%|██████████| 53/53 [00:49<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.694759822116708, Val Loss: 0.7909522950649261, LR: 0.0002886908691296504, Duration: 50.67 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 136: 100%|██████████| 53/53 [00:49<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.788320946243574, Val Loss: 0.7805000841617584, LR: 0.00028081442660546124, Duration: 50.62 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 137: 100%|██████████| 53/53 [00:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.8811172809240952, Val Loss: 0.8469788432121277, LR: 0.00027300475013022663, Duration: 50.74 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 138: 100%|██████████| 53/53 [00:49<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.7295889966892746, Val Loss: 0.8124634623527527, LR: 0.00026526421860705474, Duration: 50.68 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 139: 100%|██████████| 53/53 [00:49<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.8298986374207262, Val Loss: 0.7811976969242096, LR: 0.0002575951898768315, Duration: 50.67 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 140: 100%|██████████| 53/53 [00:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.6554229967999008, Val Loss: 0.7590774893760681, LR: 0.0002500000000000001, Duration: 51.07 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 141: 100%|██████████| 53/53 [00:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.615232767923823, Val Loss: 0.7876931428909302, LR: 0.00024248096254497287, Duration: 50.74 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 142: 100%|██████████| 53/53 [00:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.4995122259517886, Val Loss: 0.7677063047885895, LR: 0.0002350403678833976, Duration: 50.77 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 143: 100%|██████████| 53/53 [00:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.557263711713395, Val Loss: 0.8178325891494751, LR: 0.00022768048249248646, Duration: 50.81 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 144: 100%|██████████| 53/53 [00:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.6274127634066455, Val Loss: 0.7941284775733948, LR: 0.00022040354826462666, Duration: 50.76 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 145: 100%|██████████| 53/53 [00:49<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.8069549270395964, Val Loss: 0.8900790214538574, LR: 0.00021321178182447708, Duration: 50.66 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 146: 100%|██████████| 53/53 [00:49<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.748389871615284, Val Loss: 0.7834450006484985, LR: 0.00020610737385376348, Duration: 50.69 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 147: 100%|██████████| 53/53 [00:49<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.6704613521413982, Val Loss: 0.8307584226131439, LR: 0.00019909248842397582, Duration: 50.59 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 148: 100%|██████████| 53/53 [00:50<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.693824806303348, Val Loss: 0.7888597846031189, LR: 0.00019216926233717085, Duration: 50.97 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 149: 100%|██████████| 53/53 [00:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.5478664524150345, Val Loss: 0.8219472169876099, LR: 0.00018533980447508135, Duration: 50.89 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 150: 100%|██████████| 53/53 [00:49<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.938036433930667, Val Loss: 0.850498616695404, LR: 0.0001786061951567303, Duration: 50.68 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 151: 100%|██████████| 53/53 [00:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.7901414823981951, Val Loss: 0.8335275053977966, LR: 0.00017197048550474643, Duration: 50.82 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 152: 100%|██████████| 53/53 [00:49<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.6726892736722838, Val Loss: 0.8032783567905426, LR: 0.00016543469682057105, Duration: 50.71 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 153: 100%|██████████| 53/53 [00:49<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.7874878183850702, Val Loss: 0.7769253849983215, LR: 0.00015900081996875082, Duration: 50.70 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 154: 100%|██████████| 53/53 [00:49<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.692796620557893, Val Loss: 0.8104836940765381, LR: 0.00015267081477050133, Duration: 50.64 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 155: 100%|██████████| 53/53 [00:49<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.701416478966767, Val Loss: 0.7963142991065979, LR: 0.00014644660940672628, Duration: 50.62 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 156: 100%|██████████| 53/53 [00:49<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.5982793693272572, Val Loss: 0.8212220072746277, LR: 0.00014033009983067452, Duration: 50.73 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 157: 100%|██████████| 53/53 [00:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.6638056748318222, Val Loss: 0.8303169310092926, LR: 0.00013432314919041476, Duration: 50.92 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 158: 100%|██████████| 53/53 [00:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.50769512383443, Val Loss: 0.8217476308345795, LR: 0.00012842758726130281, Duration: 50.98 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 159: 100%|██████████| 53/53 [00:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.7142701610079352, Val Loss: 0.855547308921814, LR: 0.000122645209888614, Duration: 50.81 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 160: 100%|██████████| 53/53 [00:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.706531141164168, Val Loss: 0.8480227887630463, LR: 0.00011697777844051105, Duration: 50.72 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 161: 100%|██████████| 53/53 [00:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.8025738079592866, Val Loss: 0.7994233965873718, LR: 0.00011142701927151455, Duration: 50.86 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 162: 100%|██████████| 53/53 [00:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.7700950334656913, Val Loss: 0.8133866786956787, LR: 0.00010599462319663906, Duration: 50.93 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 163: 100%|██████████| 53/53 [00:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.7380728226787638, Val Loss: 0.8373262584209442, LR: 0.00010068224497635369, Duration: 50.83 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 164: 100%|██████████| 53/53 [00:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.761056847167465, Val Loss: 0.8512865602970123, LR: 9.549150281252633e-05, Duration: 50.78 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 165: 100%|██████████| 53/53 [00:50<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.880965733303214, Val Loss: 0.8238430619239807, LR: 9.042397785550405e-05, Duration: 50.99 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 166: 100%|██████████| 53/53 [00:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.7774884104728699, Val Loss: 0.8540236949920654, LR: 8.548121372247918e-05, Duration: 50.84 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 167: 100%|██████████| 53/53 [00:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.5747479605224897, Val Loss: 0.7977403700351715, LR: 8.066471602728804e-05, Duration: 50.90 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 168: 100%|██████████| 53/53 [00:50<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.4370173519512393, Val Loss: 0.7838273048400879, LR: 7.597595192178702e-05, Duration: 51.05 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 169: 100%|██████████| 53/53 [00:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.7094492529923062, Val Loss: 0.8053511679172516, LR: 7.14163496489439e-05, Duration: 50.88 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 170: 100%|██████████| 53/53 [00:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.7100138844184156, Val Loss: 0.9190984666347504, LR: 6.698729810778065e-05, Duration: 50.79 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 171: 100%|██████████| 53/53 [00:50<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.6914964768121827, Val Loss: 0.7874844670295715, LR: 6.269014643030213e-05, Duration: 51.21 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 172: 100%|██████████| 53/53 [00:50<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.5943153219402961, Val Loss: 0.8256789147853851, LR: 5.852620357053651e-05, Duration: 51.12 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 173: 100%|██████████| 53/53 [00:50<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.6352426477198332, Val Loss: 0.7972314059734344, LR: 5.449673790581611e-05, Duration: 51.09 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 174: 100%|██████████| 53/53 [00:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.6660690206401754, Val Loss: 0.7993861436843872, LR: 5.060297685041659e-05, Duration: 50.82 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 175: 100%|██████████| 53/53 [00:50<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.6171040636188578, Val Loss: 0.7933335900306702, LR: 4.684610648167503e-05, Duration: 51.18 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 176: 100%|██████████| 53/53 [00:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.6429559588432312, Val Loss: 0.8357365727424622, LR: 4.322727117869951e-05, Duration: 50.76 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 177: 100%|██████████| 53/53 [00:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.5567297339439392, Val Loss: 0.7928066849708557, LR: 3.974757327377981e-05, Duration: 50.90 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 178: 100%|██████████| 53/53 [00:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.5767251511789717, Val Loss: 0.819984644651413, LR: 3.6408072716606344e-05, Duration: 50.83 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 179: 100%|██████████| 53/53 [00:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.5791139209045555, Val Loss: 0.8209741711616516, LR: 3.3209786751399184e-05, Duration: 50.92 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 180: 100%|██████████| 53/53 [00:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.5406525787317529, Val Loss: 0.8095232546329498, LR: 3.0153689607045842e-05, Duration: 50.84 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 181: 100%|██████████| 53/53 [00:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.6960525726372342, Val Loss: 0.8099813461303711, LR: 2.724071220034158e-05, Duration: 50.81 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 182: 100%|██████████| 53/53 [00:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.5983049149783153, Val Loss: 0.8062703311443329, LR: 2.4471741852423235e-05, Duration: 50.81 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 183: 100%|██████████| 53/53 [00:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.6106316424765676, Val Loss: 0.7905535995960236, LR: 2.1847622018482283e-05, Duration: 50.88 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 184: 100%|██████████| 53/53 [00:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.481811619029855, Val Loss: 0.7974299788475037, LR: 1.9369152030840554e-05, Duration: 50.86 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 185: 100%|██████████| 53/53 [00:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.8859081976818588, Val Loss: 0.7922136187553406, LR: 1.70370868554659e-05, Duration: 50.87 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 186: 100%|██████████| 53/53 [00:49<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.665634090045713, Val Loss: 0.804825633764267, LR: 1.4852136862001764e-05, Duration: 50.64 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 187: 100%|██████████| 53/53 [00:49<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.6095925715734374, Val Loss: 0.8081980049610138, LR: 1.2814967607382432e-05, Duration: 50.59 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 188: 100%|██████████| 53/53 [00:49<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.734211212059237, Val Loss: 0.8108775615692139, LR: 1.0926199633097156e-05, Duration: 50.58 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 189: 100%|██████████| 53/53 [00:49<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.7038070222116866, Val Loss: 0.8126939535140991, LR: 9.186408276168012e-06, Duration: 50.58 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 190: 100%|██████████| 53/53 [00:49<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.7487599287392959, Val Loss: 0.821980893611908, LR: 7.59612349389599e-06, Duration: 50.52 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 191: 100%|██████████| 53/53 [00:49<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.7539891814285855, Val Loss: 0.8027200102806091, LR: 6.15582970243117e-06, Duration: 50.54 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 192: 100%|██████████| 53/53 [00:49<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.5562889812127598, Val Loss: 0.8198049366474152, LR: 4.865965629214819e-06, Duration: 50.51 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 193: 100%|██████████| 53/53 [00:49<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.7097318183700994, Val Loss: 0.8078886270523071, LR: 3.7269241793390084e-06, Duration: 50.53 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 194: 100%|██████████| 53/53 [00:49<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.7021825189860362, Val Loss: 0.8348654508590698, LR: 2.739052315863355e-06, Duration: 50.56 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 195: 100%|██████████| 53/53 [00:49<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.49126753604637, Val Loss: 0.8095850050449371, LR: 1.9026509541272275e-06, Duration: 50.38 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 196: 100%|██████████| 53/53 [00:49<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.6570006372793666, Val Loss: 0.8209457993507385, LR: 1.2179748700879012e-06, Duration: 50.40 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 197: 100%|██████████| 53/53 [00:49<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.6032606014665567, Val Loss: 0.7858821451663971, LR: 6.852326227130834e-07, Duration: 50.39 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 198: 100%|██████████| 53/53 [00:49<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.4993921687018197, Val Loss: 0.7885026633739471, LR: 3.0458649045211895e-07, Duration: 50.42 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 199: 100%|██████████| 53/53 [00:49<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.694274880976047, Val Loss: 0.837200254201889, LR: 7.615242180436521e-08, Duration: 50.40 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 200: 100%|██████████| 53/53 [00:49<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.6868847766012516, Val Loss: 0.8098738491535187, LR: 0.0, Duration: 50.40 sec\n",
      "Epoch 당 평균 소요시간 : 57.83초\n"
     ]
    }
   ],
   "source": [
    "training_time = 0\n",
    "losses = []\n",
    "val_losses = []\n",
    "lrs = []\n",
    "best_loss = float('inf')\n",
    "\n",
    "# GradScaler 초기화\n",
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch + 1}\")\n",
    "    \n",
    "    for _, data in pbar:\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        inputs, labels = mixup_fn(inputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # AutoCast 적용\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "        # 스케일링된 그라디언트 계산\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # 그라디언트 클리핑 전에 스케일링 제거\n",
    "        scaler.unscale_(optimizer)\n",
    "        clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "\n",
    "        # 옵티마이저 스텝 및 스케일러 업데이트\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "            \n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        lrs.append(lr)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)        \n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in valid_loader:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    val_loss /= len(valid_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # 모델 저장\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        vit_save = True\n",
    "        if vit_save:\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_duration = time.time() - start_time\n",
    "    training_time += epoch_duration\n",
    "    \n",
    "    text = f'\\tLoss: {epoch_loss}, Val Loss: {val_loss}, LR: {lr}, Duration: {epoch_duration:.2f} sec'\n",
    "    \n",
    "    if vit_save:\n",
    "        text += f' - model saved!'\n",
    "        vit_save = False\n",
    "\n",
    "    print(text)\n",
    "        \n",
    "text = f\"Epoch 당 평균 소요시간 : {training_time / epochs:.2f}초\"      \n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.870000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.884567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Recall</td>\n",
       "      <td>0.870000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F1 Score</td>\n",
       "      <td>0.863506</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Metric     Value\n",
       "0   Accuracy  0.870000\n",
       "1  Precision  0.884567\n",
       "2     Recall  0.870000\n",
       "3   F1 Score  0.863506"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예측 수행 및 레이블 저장\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# 혼동 행렬 생성\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# 예측과 실제 레이블\n",
    "y_true = all_labels  # 실제 레이블\n",
    "y_pred = all_preds  # 모델에 의해 예측된 레이블\n",
    "\n",
    "# 전체 데이터셋에 대한 정확도\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# 평균 정밀도, 리콜, F1-Score ('weighted')\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "\n",
    "# 판다스 데이터프레임으로 결과 정리\n",
    "performance_metrics = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "    'Value': [accuracy, precision, recall, f1_score]\n",
    "})\n",
    "\n",
    "# 데이터프레임 출력\n",
    "performance_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
