{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "from timm.data import Mixup\n",
    "import transformers\n",
    "\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "from timm.models.layers import DropPath, trunc_normal_, to_2tuple\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01.Convolutional Token Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvEmbed(nn.Module):\n",
    "    '''\n",
    "    img/token map to Conv Embedding\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,\n",
    "                 patch_size=7, # [7, 3, 3]\n",
    "                 in_chans=3,   # [3, dim of stage1, dim of stage2]\n",
    "                 embed_dim=64, # [64, 192, 384]\n",
    "                 stride=4,     # [4, 2, 2]\n",
    "                 padding=2,    # [2, 1, 1]\n",
    "                 norm_layer=None):\n",
    "        super().__init__()\n",
    "        self.patch_size = to_2tuple(patch_size)\n",
    "        \n",
    "        self.proj = nn.Conv2d(\n",
    "            in_channels=in_chans,\n",
    "            out_channels=embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=stride,\n",
    "            padding=padding\n",
    "        )\n",
    "        \n",
    "        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        \n",
    "        _, _, H, W = x.shape\n",
    "        x = rearrange(x, 'b c h w -> b (h w) c')\n",
    "        x = self.norm(x)\n",
    "        x = rearrange(x, 'b (h w) c -> b c h w', h=H, w=W)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionConv(nn.Module):\n",
    "    def __init__(self,\n",
    "                 dim=64,        # [64,192,384]\n",
    "                 num_heads=2,   # paper: [1,3,6], me: [2,3,6]\n",
    "                 qkv_bias=False,\n",
    "                 attn_drop=0.,\n",
    "                 proj_drop=0.,\n",
    "                 kernel_size=3,\n",
    "                 padding_q=1,\n",
    "                 padding_kv=1,\n",
    "                 stride_q=1,\n",
    "                 stride_kv=2,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.qkv_bias = qkv_bias\n",
    "        self.stride_q = stride_q\n",
    "        self.stride_kv = stride_kv\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads        \n",
    "        self.scale = dim ** -0.5\n",
    "        \n",
    "        self.conv_proj_q = self._build_projection(dim, kernel_size, padding_q, stride_q)\n",
    "        self.conv_proj_k = self._build_projection(dim, kernel_size, padding_kv, stride_kv)        \n",
    "        self.conv_proj_v = self._build_projection(dim, kernel_size, padding_kv, stride_kv)\n",
    "        \n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.linear_proj_last = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)        \n",
    "        \n",
    "    def _build_projection(self, dim, kernel_size, padding, stride):\n",
    "        \n",
    "        proj = nn.Sequential(OrderedDict([\n",
    "            ('depthwise', nn.Conv2d(\n",
    "                in_channels=dim,\n",
    "                out_channels=dim,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=padding,\n",
    "                stride=stride,\n",
    "                bias=self.qkv_bias,\n",
    "                groups=dim)),\n",
    "            ('bn', nn.BatchNorm2d(dim)),\n",
    "            ('pointwise', nn.Conv2d(\n",
    "                in_channels=dim,\n",
    "                out_channels=dim,\n",
    "                kernel_size=1,\n",
    "                bias=self.qkv_bias)),\n",
    "            ('rearrange', Rearrange('b c h w -> b (h w) c'))\n",
    "        ]))\n",
    "        \n",
    "        return proj\n",
    "    \n",
    "    def forward(self, x, h, w):\n",
    "        x = rearrange(x, 'b (h w) c -> b c h w', h=h, w=w)\n",
    "        \n",
    "        q = self.conv_proj_q(x)\n",
    "        k = self.conv_proj_k(x)\n",
    "        v = self.conv_proj_v(x)\n",
    "        \n",
    "        q = rearrange(q, 'b t (h d) -> b h t d', h=self.num_heads)\n",
    "        k = rearrange(k, 'b t (h d) -> b h t d', h=self.num_heads)\n",
    "        v = rearrange(v, 'b t (h d) -> b h t d', h=self.num_heads)\n",
    "        \n",
    "        attn_score = torch.einsum('bhlk,bhtk->bhlt', [q, k]) * self.scale\n",
    "        attn = self.attn_drop(F.softmax(attn_score, dim=-1))\n",
    "        \n",
    "        x = torch.matmul(attn, v)\n",
    "        batch_size, num_heads, seq_length, depth = x.size()\n",
    "        x = x.view(batch_size, seq_length, num_heads * depth)\n",
    "        \n",
    "        x = self.proj_drop(self.linear_proj_last(x))\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer block에 작은 스케일 인자 곱하기\n",
    "class LayerScale(nn.Module):\n",
    "    def __init__(self, dim, init_values=1e-5):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(init_values * torch.ones((dim)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gamma * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 num_heads,\n",
    "                 mlp_ratio=4.,\n",
    "                 qkv_bias=False,\n",
    "                 drop=0.,\n",
    "                 attn_drop=0.,\n",
    "                 drop_path=0.,\n",
    "                 act_layer=nn.GELU,\n",
    "                 norm_layer=nn.LayerNorm,\n",
    "                 **kwargs\n",
    "                ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.ls1 = LayerScale(dim)\n",
    "        self.attn = AttentionConv(dim=dim,\n",
    "                                  num_heads=num_heads,\n",
    "                                  qkv_bias=qkv_bias,\n",
    "                                  attn_drop=attn_drop,\n",
    "                                  proj_drop=drop,\n",
    "                                  **kwargs)        \n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        self.ls2 = LayerScale(dim)\n",
    "        mlp_hidden_dim = int(dim*mlp_ratio)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, mlp_hidden_dim),\n",
    "            act_layer(),\n",
    "            nn.Linear(mlp_hidden_dim, dim),\n",
    "            nn.Dropout(drop),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, h, w):\n",
    "        res = x\n",
    "        x = self.norm1(x)\n",
    "        attn = self.attn(x, h, w)\n",
    "        x = res + self.drop_path(self.ls1(attn))\n",
    "        x = x + self.drop_path(self.ls2(self.mlp(self.norm2(x))))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = torch.Tensor(np.zeros((2,3,224,224))) # B, C, H, W\n",
    "\n",
    "block = Block(dim=64,\n",
    "              num_heads=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stage 1 | img shape: torch.Size([2, 3, 224, 224]) → Conv Embed Shape: torch.Size([2, 64, 56, 56])\n"
     ]
    }
   ],
   "source": [
    "# Stage 1 \n",
    "\n",
    "## Patch Embedding\n",
    "convembed = ConvEmbed(patch_size=7, stride=4, padding=2)\n",
    "stage1_img = convembed(test_img)\n",
    "\n",
    "## Attention with Convolution\n",
    "b, c, h, w = stage1_img.shape\n",
    "stage1_img = rearrange(stage1_img, 'b c h w -> b (h w) c')\n",
    "stage1_img = block(stage1_img, h=h, w=w)\n",
    "stage1_img = rearrange(stage1_img, 'b (h w) c -> b c h w', h=h, w=w)\n",
    "\n",
    "## Check Result\n",
    "print(f'stage 1 | img shape: {test_img.shape} → Conv Embed Shape: {stage1_img.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stage 2 | img shape: torch.Size([2, 64, 56, 56]) → Conv Embed Shape: torch.Size([2, 64, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# Stage 2 \n",
    "\n",
    "## Patch Embedding\n",
    "convembed = ConvEmbed(patch_size=3, in_chans=64, stride=2, padding=1)\n",
    "stage2_img = convembed(stage1_img)\n",
    "\n",
    "## Attention with Convolution\n",
    "b, c, h, w = stage2_img.shape\n",
    "stage2_img = rearrange(stage2_img, 'b c h w -> b (h w) c')\n",
    "stage2_img = block(stage2_img, h=h, w=w)\n",
    "stage2_img = rearrange(stage2_img, 'b (h w) c -> b c h w', h=h, w=w)\n",
    "\n",
    "## Check Result\n",
    "print(f'stage 2 | img shape: {stage1_img.shape} → Conv Embed Shape: {stage2_img.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stage 3 | img shape: torch.Size([2, 64, 28, 28]) → Conv Embed Shape: torch.Size([2, 64, 14, 14])\n"
     ]
    }
   ],
   "source": [
    "# Stage 3 \n",
    "\n",
    "## Patch Embedding\n",
    "convembed = ConvEmbed(patch_size=3, in_chans=64, stride=2, padding=1)\n",
    "stage3_img = convembed(stage2_img)\n",
    "\n",
    "## Attention with Convolution\n",
    "b, c, h, w = stage3_img.shape\n",
    "stage3_img = rearrange(stage3_img, 'b c h w -> b (h w) c')\n",
    "stage3_img = block(stage3_img, h=h, w=w)\n",
    "stage3_img = rearrange(stage3_img, 'b (h w) c -> b c h w', h=h, w=w)\n",
    "\n",
    "## Check Result\n",
    "print(f'stage 3 | img shape: {stage2_img.shape} → Conv Embed Shape: {stage3_img.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 patch_size=16,\n",
    "                 patch_stride=16,\n",
    "                 patch_padding=0,\n",
    "                 in_chans=3,\n",
    "                 embed_dim=768,\n",
    "                 depth=12,\n",
    "                 num_heads=12,\n",
    "                 mlp_ratio=4.,\n",
    "                 qkv_bias=False,\n",
    "                 drop_rate=0.,\n",
    "                 attn_drop_rate=0.,\n",
    "                 drop_path_rate=0.,\n",
    "                 act_layer=nn.GELU,\n",
    "                 norm_layer=nn.LayerNorm,\n",
    "                 init='trunc_norm',\n",
    "                 **kwargs\n",
    "                 ):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_embed = ConvEmbed(\n",
    "            patch_size=patch_size,\n",
    "            in_chans=in_chans,\n",
    "            stride=patch_stride,\n",
    "            padding=patch_padding,\n",
    "            embed_dim=embed_dim,\n",
    "            norm_layer=norm_layer\n",
    "        )\n",
    "\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                drop=drop_rate,\n",
    "                attn_drop=attn_drop_rate,\n",
    "                drop_path=drop_path_rate,\n",
    "                act_layer=act_layer,\n",
    "                norm_layer=norm_layer,\n",
    "                **kwargs\n",
    "            ) for _ in range(depth)\n",
    "        ])\n",
    "\n",
    "        if init == 'xavier':\n",
    "            self.apply(self._init_weights_xavier)\n",
    "        else:\n",
    "            self.apply(self._init_weights_trunc_normal)\n",
    "\n",
    "    def _init_weights_trunc_normal(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, (nn.LayerNorm, nn.BatchNorm2d)):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def _init_weights_xavier(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, (nn.LayerNorm, nn.BatchNorm2d)):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        _, _, H, W = x.size()\n",
    "\n",
    "        x = rearrange(x, 'b c h w -> b (h w) c')\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for _, blk in enumerate(self.blocks):\n",
    "            x = blk(x, H, W)\n",
    "        x = rearrange(x, 'b (h w) c -> b c h w', h=H, w=W)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalVisionTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_chans=3,\n",
    "                 num_classes=100,\n",
    "                 act_layer=nn.GELU,\n",
    "                 norm_layer=nn.LayerNorm,\n",
    "                 init='trunc_norm',\n",
    "                 spec=None):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.num_stages = spec['NUM_STAGES']\n",
    "        self.stages = nn.ModuleList()\n",
    "        for i in range(self.num_stages):\n",
    "            kwargs = {\n",
    "                'patch_size': spec['PATCH_SIZE'][i],\n",
    "                'patch_stride': spec['PATCH_STRIDE'][i],\n",
    "                'patch_padding': spec['PATCH_PADDING'][i],\n",
    "                'embed_dim': spec['DIM_EMBED'][i],\n",
    "                'depth': spec['DEPTH'][i],\n",
    "                'num_heads': spec['NUM_HEADS'][i],\n",
    "                'mlp_ratio': spec['MLP_RATIO'][i],\n",
    "                'qkv_bias': spec['QKV_BIAS'][i],\n",
    "                'drop_rate': spec['DROP_RATE'][i],\n",
    "                'attn_drop_rate': spec['ATTN_DROP_RATE'][i],\n",
    "                'drop_path_rate': spec['DROP_PATH_RATE'][i],\n",
    "                'kernel_size': spec['KERNEL_QKV'][i],\n",
    "                'padding_q': spec['PADDING_Q'][i],\n",
    "                'padding_kv': spec['PADDING_KV'][i],\n",
    "                'stride_q': spec['STRIDE_Q'][i],\n",
    "                'stride_kv': spec['STRIDE_KV'][i],\n",
    "            }\n",
    "\n",
    "            stage = VisionTransformer(\n",
    "                in_chans=in_chans,\n",
    "                init=init,\n",
    "                act_layer=act_layer,\n",
    "                norm_layer=norm_layer,\n",
    "                **kwargs\n",
    "            )\n",
    "            \n",
    "            self.stages.append(stage)\n",
    "\n",
    "            in_chans = spec['DIM_EMBED'][i]\n",
    "\n",
    "        dim_embed = spec['DIM_EMBED'][-1]\n",
    "        self.norm = norm_layer(dim_embed)\n",
    "        self.pooler = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "        # Classifier head\n",
    "        self.head = nn.Linear(dim_embed, num_classes) if num_classes > 0 else nn.Identity()\n",
    "        trunc_normal_(self.head.weight, std=0.02)\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        for stage in self.stages:\n",
    "            x = stage(x)\n",
    "\n",
    "        x = rearrange(x, 'b c h w -> b (h w) c') # (B, L, C)\n",
    "        x = self.norm(x)                         # (B, L, C)\n",
    "        x = self.pooler(x.transpose(1,2))        # (B, C, 1)\n",
    "        x = torch.flatten(x, 1)                  # (B, C)\n",
    "        # x = torch.mean(x, dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.head(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuickGELU(nn.Module):\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return x * torch.sigmoid(1.702 * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NUM_STAGES': 3,\n",
       " 'PATCH_SIZE': [7, 3, 3],\n",
       " 'PATCH_STRIDE': [4, 2, 2],\n",
       " 'PATCH_PADDING': [2, 1, 1],\n",
       " 'DIM_EMBED': [64, 192, 384],\n",
       " 'DEPTH': [1, 2, 10],\n",
       " 'NUM_HEADS': [1, 3, 6],\n",
       " 'MLP_RATIO': [4.0, 4.0, 4.0],\n",
       " 'QKV_BIAS': [True, True, True],\n",
       " 'DROP_RATE': [0.0, 0.0, 0.0],\n",
       " 'ATTN_DROP_RATE': [0.0, 0.0, 0.0],\n",
       " 'DROP_PATH_RATE': [0.0, 0.0, 0.1],\n",
       " 'KERNEL_QKV': [3, 3, 3],\n",
       " 'PADDING_Q': [1, 1, 1],\n",
       " 'PADDING_KV': [1, 1, 1],\n",
       " 'STRIDE_Q': [1, 1, 1],\n",
       " 'STRIDE_KV': [2, 2, 2]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec = {\n",
    "    'NUM_STAGES': 3,\n",
    "    'PATCH_SIZE': [7,3,3],\n",
    "    'PATCH_STRIDE': [4,2,2],\n",
    "    'PATCH_PADDING': [2,1,1],\n",
    "    'DIM_EMBED': [64,192,384],\n",
    "    'DEPTH': [1,2,10],\n",
    "    'NUM_HEADS': [1,3,6],   # original : [1,3,6]\n",
    "    'MLP_RATIO': [4.,4.,4.],\n",
    "    'QKV_BIAS': [True, True, True],\n",
    "    'DROP_RATE': [0.,0.,0.],\n",
    "    'ATTN_DROP_RATE': [0.,0.,0.],\n",
    "    'DROP_PATH_RATE': [0.,0.,0.1],\n",
    "    'KERNEL_QKV': [3,3,3],\n",
    "    'PADDING_Q': [1,1,1],\n",
    "    'PADDING_KV': [1,1,1],\n",
    "    'STRIDE_Q': [1,1,1],\n",
    "    'STRIDE_KV': [2,2,2],\n",
    "}\n",
    "\n",
    "spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 224, 224]),\n",
       " torch.Size([2, 100]),\n",
       " tensor([[ 0.0560, -0.1735, -0.0373, -0.1633, -0.6409,  0.2103, -0.4621,  0.5007,\n",
       "           0.3879, -0.2455, -0.0682,  0.3398,  0.0025, -0.1950, -0.1589, -0.3436,\n",
       "          -0.1607, -0.3016,  0.3848,  0.1068, -0.7459, -0.2285,  0.0033, -0.5526,\n",
       "          -0.0825,  0.2634,  0.3724, -0.7510, -0.6758,  0.4030,  0.0073, -0.6415,\n",
       "           0.2723,  0.3547, -0.4324,  0.4879,  0.7599,  0.1292,  0.1512, -0.5787,\n",
       "          -0.1569,  0.3840,  0.1062, -0.6515,  0.2632, -0.1191,  0.8281,  0.2390,\n",
       "          -0.6204,  0.5271,  0.3912, -0.0870, -0.1362, -0.1230,  0.1171, -0.4210,\n",
       "           0.2268, -0.2331,  0.8634, -0.1080,  0.1995, -0.2178,  0.2983, -0.4052,\n",
       "           0.0024,  0.2901,  0.3668,  0.0720,  0.0404,  0.1893,  0.0742, -0.7949,\n",
       "           0.0808, -0.4411, -0.4020, -0.6345,  0.4509, -0.1903,  0.1198, -0.6331,\n",
       "          -0.3725, -0.2771, -0.2556, -0.1073, -0.0675, -0.3343,  0.2094, -0.1463,\n",
       "          -0.3211, -0.4837, -0.4158,  0.0366,  0.0526,  1.0233,  0.0936, -0.1516,\n",
       "           0.3559,  0.3089,  0.0432,  0.3521],\n",
       "         [ 0.0560, -0.1735, -0.0373, -0.1633, -0.6409,  0.2103, -0.4621,  0.5007,\n",
       "           0.3879, -0.2455, -0.0682,  0.3398,  0.0025, -0.1950, -0.1589, -0.3436,\n",
       "          -0.1607, -0.3016,  0.3848,  0.1068, -0.7459, -0.2285,  0.0033, -0.5526,\n",
       "          -0.0825,  0.2634,  0.3724, -0.7510, -0.6758,  0.4030,  0.0073, -0.6415,\n",
       "           0.2722,  0.3548, -0.4324,  0.4879,  0.7599,  0.1292,  0.1512, -0.5787,\n",
       "          -0.1569,  0.3840,  0.1062, -0.6515,  0.2632, -0.1191,  0.8281,  0.2390,\n",
       "          -0.6204,  0.5271,  0.3912, -0.0870, -0.1362, -0.1230,  0.1171, -0.4210,\n",
       "           0.2268, -0.2331,  0.8634, -0.1080,  0.1995, -0.2178,  0.2983, -0.4052,\n",
       "           0.0024,  0.2901,  0.3668,  0.0720,  0.0404,  0.1893,  0.0742, -0.7949,\n",
       "           0.0808, -0.4411, -0.4020, -0.6345,  0.4509, -0.1903,  0.1198, -0.6331,\n",
       "          -0.3725, -0.2771, -0.2556, -0.1073, -0.0675, -0.3343,  0.2094, -0.1463,\n",
       "          -0.3211, -0.4837, -0.4158,  0.0366,  0.0526,  1.0233,  0.0936, -0.1516,\n",
       "           0.3559,  0.3089,  0.0432,  0.3521]], grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ConvolutionalVisionTransformer(act_layer=QuickGELU, spec=spec)\n",
    "\n",
    "test_result = model(test_img)\n",
    "test_img.shape, test_result.shape, test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 56, 56]           9,472\n",
      "         LayerNorm-2             [-1, 3136, 64]             128\n",
      "         ConvEmbed-3           [-1, 64, 56, 56]               0\n",
      "           Dropout-4             [-1, 3136, 64]               0\n",
      "         LayerNorm-5             [-1, 3136, 64]             128\n",
      "            Conv2d-6           [-1, 64, 56, 56]             576\n",
      "       BatchNorm2d-7           [-1, 64, 56, 56]             128\n",
      "            Conv2d-8           [-1, 64, 56, 56]           4,160\n",
      "         Rearrange-9             [-1, 3136, 64]               0\n",
      "           Conv2d-10           [-1, 64, 28, 28]             576\n",
      "      BatchNorm2d-11           [-1, 64, 28, 28]             128\n",
      "           Conv2d-12           [-1, 64, 28, 28]           4,160\n",
      "        Rearrange-13              [-1, 784, 64]               0\n",
      "           Conv2d-14           [-1, 64, 28, 28]             576\n",
      "      BatchNorm2d-15           [-1, 64, 28, 28]             128\n",
      "           Conv2d-16           [-1, 64, 28, 28]           4,160\n",
      "        Rearrange-17              [-1, 784, 64]               0\n",
      "          Dropout-18         [-1, 1, 3136, 784]               0\n",
      "           Linear-19             [-1, 3136, 64]           4,160\n",
      "          Dropout-20             [-1, 3136, 64]               0\n",
      "    AttentionConv-21             [-1, 3136, 64]               0\n",
      "       LayerScale-22             [-1, 3136, 64]               0\n",
      "         Identity-23             [-1, 3136, 64]               0\n",
      "        LayerNorm-24             [-1, 3136, 64]             128\n",
      "           Linear-25            [-1, 3136, 256]          16,640\n",
      "        QuickGELU-26            [-1, 3136, 256]               0\n",
      "           Linear-27             [-1, 3136, 64]          16,448\n",
      "          Dropout-28             [-1, 3136, 64]               0\n",
      "       LayerScale-29             [-1, 3136, 64]               0\n",
      "         Identity-30             [-1, 3136, 64]               0\n",
      "            Block-31             [-1, 3136, 64]               0\n",
      "VisionTransformer-32           [-1, 64, 56, 56]               0\n",
      "           Conv2d-33          [-1, 192, 28, 28]         110,784\n",
      "        LayerNorm-34             [-1, 784, 192]             384\n",
      "        ConvEmbed-35          [-1, 192, 28, 28]               0\n",
      "          Dropout-36             [-1, 784, 192]               0\n",
      "        LayerNorm-37             [-1, 784, 192]             384\n",
      "           Conv2d-38          [-1, 192, 28, 28]           1,728\n",
      "      BatchNorm2d-39          [-1, 192, 28, 28]             384\n",
      "           Conv2d-40          [-1, 192, 28, 28]          37,056\n",
      "        Rearrange-41             [-1, 784, 192]               0\n",
      "           Conv2d-42          [-1, 192, 14, 14]           1,728\n",
      "      BatchNorm2d-43          [-1, 192, 14, 14]             384\n",
      "           Conv2d-44          [-1, 192, 14, 14]          37,056\n",
      "        Rearrange-45             [-1, 196, 192]               0\n",
      "           Conv2d-46          [-1, 192, 14, 14]           1,728\n",
      "      BatchNorm2d-47          [-1, 192, 14, 14]             384\n",
      "           Conv2d-48          [-1, 192, 14, 14]          37,056\n",
      "        Rearrange-49             [-1, 196, 192]               0\n",
      "          Dropout-50          [-1, 3, 784, 196]               0\n",
      "           Linear-51             [-1, 784, 192]          37,056\n",
      "          Dropout-52             [-1, 784, 192]               0\n",
      "    AttentionConv-53             [-1, 784, 192]               0\n",
      "       LayerScale-54             [-1, 784, 192]               0\n",
      "         Identity-55             [-1, 784, 192]               0\n",
      "        LayerNorm-56             [-1, 784, 192]             384\n",
      "           Linear-57             [-1, 784, 768]         148,224\n",
      "        QuickGELU-58             [-1, 784, 768]               0\n",
      "           Linear-59             [-1, 784, 192]         147,648\n",
      "          Dropout-60             [-1, 784, 192]               0\n",
      "       LayerScale-61             [-1, 784, 192]               0\n",
      "         Identity-62             [-1, 784, 192]               0\n",
      "            Block-63             [-1, 784, 192]               0\n",
      "        LayerNorm-64             [-1, 784, 192]             384\n",
      "           Conv2d-65          [-1, 192, 28, 28]           1,728\n",
      "      BatchNorm2d-66          [-1, 192, 28, 28]             384\n",
      "           Conv2d-67          [-1, 192, 28, 28]          37,056\n",
      "        Rearrange-68             [-1, 784, 192]               0\n",
      "           Conv2d-69          [-1, 192, 14, 14]           1,728\n",
      "      BatchNorm2d-70          [-1, 192, 14, 14]             384\n",
      "           Conv2d-71          [-1, 192, 14, 14]          37,056\n",
      "        Rearrange-72             [-1, 196, 192]               0\n",
      "           Conv2d-73          [-1, 192, 14, 14]           1,728\n",
      "      BatchNorm2d-74          [-1, 192, 14, 14]             384\n",
      "           Conv2d-75          [-1, 192, 14, 14]          37,056\n",
      "        Rearrange-76             [-1, 196, 192]               0\n",
      "          Dropout-77          [-1, 3, 784, 196]               0\n",
      "           Linear-78             [-1, 784, 192]          37,056\n",
      "          Dropout-79             [-1, 784, 192]               0\n",
      "    AttentionConv-80             [-1, 784, 192]               0\n",
      "       LayerScale-81             [-1, 784, 192]               0\n",
      "         Identity-82             [-1, 784, 192]               0\n",
      "        LayerNorm-83             [-1, 784, 192]             384\n",
      "           Linear-84             [-1, 784, 768]         148,224\n",
      "        QuickGELU-85             [-1, 784, 768]               0\n",
      "           Linear-86             [-1, 784, 192]         147,648\n",
      "          Dropout-87             [-1, 784, 192]               0\n",
      "       LayerScale-88             [-1, 784, 192]               0\n",
      "         Identity-89             [-1, 784, 192]               0\n",
      "            Block-90             [-1, 784, 192]               0\n",
      "VisionTransformer-91          [-1, 192, 28, 28]               0\n",
      "           Conv2d-92          [-1, 384, 14, 14]         663,936\n",
      "        LayerNorm-93             [-1, 196, 384]             768\n",
      "        ConvEmbed-94          [-1, 384, 14, 14]               0\n",
      "          Dropout-95             [-1, 196, 384]               0\n",
      "        LayerNorm-96             [-1, 196, 384]             768\n",
      "           Conv2d-97          [-1, 384, 14, 14]           3,456\n",
      "      BatchNorm2d-98          [-1, 384, 14, 14]             768\n",
      "           Conv2d-99          [-1, 384, 14, 14]         147,840\n",
      "       Rearrange-100             [-1, 196, 384]               0\n",
      "          Conv2d-101            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-102            [-1, 384, 7, 7]             768\n",
      "          Conv2d-103            [-1, 384, 7, 7]         147,840\n",
      "       Rearrange-104              [-1, 49, 384]               0\n",
      "          Conv2d-105            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-106            [-1, 384, 7, 7]             768\n",
      "          Conv2d-107            [-1, 384, 7, 7]         147,840\n",
      "       Rearrange-108              [-1, 49, 384]               0\n",
      "         Dropout-109           [-1, 6, 196, 49]               0\n",
      "          Linear-110             [-1, 196, 384]         147,840\n",
      "         Dropout-111             [-1, 196, 384]               0\n",
      "   AttentionConv-112             [-1, 196, 384]               0\n",
      "      LayerScale-113             [-1, 196, 384]               0\n",
      "        DropPath-114             [-1, 196, 384]               0\n",
      "       LayerNorm-115             [-1, 196, 384]             768\n",
      "          Linear-116            [-1, 196, 1536]         591,360\n",
      "       QuickGELU-117            [-1, 196, 1536]               0\n",
      "          Linear-118             [-1, 196, 384]         590,208\n",
      "         Dropout-119             [-1, 196, 384]               0\n",
      "      LayerScale-120             [-1, 196, 384]               0\n",
      "        DropPath-121             [-1, 196, 384]               0\n",
      "           Block-122             [-1, 196, 384]               0\n",
      "       LayerNorm-123             [-1, 196, 384]             768\n",
      "          Conv2d-124          [-1, 384, 14, 14]           3,456\n",
      "     BatchNorm2d-125          [-1, 384, 14, 14]             768\n",
      "          Conv2d-126          [-1, 384, 14, 14]         147,840\n",
      "       Rearrange-127             [-1, 196, 384]               0\n",
      "          Conv2d-128            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-129            [-1, 384, 7, 7]             768\n",
      "          Conv2d-130            [-1, 384, 7, 7]         147,840\n",
      "       Rearrange-131              [-1, 49, 384]               0\n",
      "          Conv2d-132            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-133            [-1, 384, 7, 7]             768\n",
      "          Conv2d-134            [-1, 384, 7, 7]         147,840\n",
      "       Rearrange-135              [-1, 49, 384]               0\n",
      "         Dropout-136           [-1, 6, 196, 49]               0\n",
      "          Linear-137             [-1, 196, 384]         147,840\n",
      "         Dropout-138             [-1, 196, 384]               0\n",
      "   AttentionConv-139             [-1, 196, 384]               0\n",
      "      LayerScale-140             [-1, 196, 384]               0\n",
      "        DropPath-141             [-1, 196, 384]               0\n",
      "       LayerNorm-142             [-1, 196, 384]             768\n",
      "          Linear-143            [-1, 196, 1536]         591,360\n",
      "       QuickGELU-144            [-1, 196, 1536]               0\n",
      "          Linear-145             [-1, 196, 384]         590,208\n",
      "         Dropout-146             [-1, 196, 384]               0\n",
      "      LayerScale-147             [-1, 196, 384]               0\n",
      "        DropPath-148             [-1, 196, 384]               0\n",
      "           Block-149             [-1, 196, 384]               0\n",
      "       LayerNorm-150             [-1, 196, 384]             768\n",
      "          Conv2d-151          [-1, 384, 14, 14]           3,456\n",
      "     BatchNorm2d-152          [-1, 384, 14, 14]             768\n",
      "          Conv2d-153          [-1, 384, 14, 14]         147,840\n",
      "       Rearrange-154             [-1, 196, 384]               0\n",
      "          Conv2d-155            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-156            [-1, 384, 7, 7]             768\n",
      "          Conv2d-157            [-1, 384, 7, 7]         147,840\n",
      "       Rearrange-158              [-1, 49, 384]               0\n",
      "          Conv2d-159            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-160            [-1, 384, 7, 7]             768\n",
      "          Conv2d-161            [-1, 384, 7, 7]         147,840\n",
      "       Rearrange-162              [-1, 49, 384]               0\n",
      "         Dropout-163           [-1, 6, 196, 49]               0\n",
      "          Linear-164             [-1, 196, 384]         147,840\n",
      "         Dropout-165             [-1, 196, 384]               0\n",
      "   AttentionConv-166             [-1, 196, 384]               0\n",
      "      LayerScale-167             [-1, 196, 384]               0\n",
      "        DropPath-168             [-1, 196, 384]               0\n",
      "       LayerNorm-169             [-1, 196, 384]             768\n",
      "          Linear-170            [-1, 196, 1536]         591,360\n",
      "       QuickGELU-171            [-1, 196, 1536]               0\n",
      "          Linear-172             [-1, 196, 384]         590,208\n",
      "         Dropout-173             [-1, 196, 384]               0\n",
      "      LayerScale-174             [-1, 196, 384]               0\n",
      "        DropPath-175             [-1, 196, 384]               0\n",
      "           Block-176             [-1, 196, 384]               0\n",
      "       LayerNorm-177             [-1, 196, 384]             768\n",
      "          Conv2d-178          [-1, 384, 14, 14]           3,456\n",
      "     BatchNorm2d-179          [-1, 384, 14, 14]             768\n",
      "          Conv2d-180          [-1, 384, 14, 14]         147,840\n",
      "       Rearrange-181             [-1, 196, 384]               0\n",
      "          Conv2d-182            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-183            [-1, 384, 7, 7]             768\n",
      "          Conv2d-184            [-1, 384, 7, 7]         147,840\n",
      "       Rearrange-185              [-1, 49, 384]               0\n",
      "          Conv2d-186            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-187            [-1, 384, 7, 7]             768\n",
      "          Conv2d-188            [-1, 384, 7, 7]         147,840\n",
      "       Rearrange-189              [-1, 49, 384]               0\n",
      "         Dropout-190           [-1, 6, 196, 49]               0\n",
      "          Linear-191             [-1, 196, 384]         147,840\n",
      "         Dropout-192             [-1, 196, 384]               0\n",
      "   AttentionConv-193             [-1, 196, 384]               0\n",
      "      LayerScale-194             [-1, 196, 384]               0\n",
      "        DropPath-195             [-1, 196, 384]               0\n",
      "       LayerNorm-196             [-1, 196, 384]             768\n",
      "          Linear-197            [-1, 196, 1536]         591,360\n",
      "       QuickGELU-198            [-1, 196, 1536]               0\n",
      "          Linear-199             [-1, 196, 384]         590,208\n",
      "         Dropout-200             [-1, 196, 384]               0\n",
      "      LayerScale-201             [-1, 196, 384]               0\n",
      "        DropPath-202             [-1, 196, 384]               0\n",
      "           Block-203             [-1, 196, 384]               0\n",
      "       LayerNorm-204             [-1, 196, 384]             768\n",
      "          Conv2d-205          [-1, 384, 14, 14]           3,456\n",
      "     BatchNorm2d-206          [-1, 384, 14, 14]             768\n",
      "          Conv2d-207          [-1, 384, 14, 14]         147,840\n",
      "       Rearrange-208             [-1, 196, 384]               0\n",
      "          Conv2d-209            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-210            [-1, 384, 7, 7]             768\n",
      "          Conv2d-211            [-1, 384, 7, 7]         147,840\n",
      "       Rearrange-212              [-1, 49, 384]               0\n",
      "          Conv2d-213            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-214            [-1, 384, 7, 7]             768\n",
      "          Conv2d-215            [-1, 384, 7, 7]         147,840\n",
      "       Rearrange-216              [-1, 49, 384]               0\n",
      "         Dropout-217           [-1, 6, 196, 49]               0\n",
      "          Linear-218             [-1, 196, 384]         147,840\n",
      "         Dropout-219             [-1, 196, 384]               0\n",
      "   AttentionConv-220             [-1, 196, 384]               0\n",
      "      LayerScale-221             [-1, 196, 384]               0\n",
      "        DropPath-222             [-1, 196, 384]               0\n",
      "       LayerNorm-223             [-1, 196, 384]             768\n",
      "          Linear-224            [-1, 196, 1536]         591,360\n",
      "       QuickGELU-225            [-1, 196, 1536]               0\n",
      "          Linear-226             [-1, 196, 384]         590,208\n",
      "         Dropout-227             [-1, 196, 384]               0\n",
      "      LayerScale-228             [-1, 196, 384]               0\n",
      "        DropPath-229             [-1, 196, 384]               0\n",
      "           Block-230             [-1, 196, 384]               0\n",
      "       LayerNorm-231             [-1, 196, 384]             768\n",
      "          Conv2d-232          [-1, 384, 14, 14]           3,456\n",
      "     BatchNorm2d-233          [-1, 384, 14, 14]             768\n",
      "          Conv2d-234          [-1, 384, 14, 14]         147,840\n",
      "       Rearrange-235             [-1, 196, 384]               0\n",
      "          Conv2d-236            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-237            [-1, 384, 7, 7]             768\n",
      "          Conv2d-238            [-1, 384, 7, 7]         147,840\n",
      "       Rearrange-239              [-1, 49, 384]               0\n",
      "          Conv2d-240            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-241            [-1, 384, 7, 7]             768\n",
      "          Conv2d-242            [-1, 384, 7, 7]         147,840\n",
      "       Rearrange-243              [-1, 49, 384]               0\n",
      "         Dropout-244           [-1, 6, 196, 49]               0\n",
      "          Linear-245             [-1, 196, 384]         147,840\n",
      "         Dropout-246             [-1, 196, 384]               0\n",
      "   AttentionConv-247             [-1, 196, 384]               0\n",
      "      LayerScale-248             [-1, 196, 384]               0\n",
      "        DropPath-249             [-1, 196, 384]               0\n",
      "       LayerNorm-250             [-1, 196, 384]             768\n",
      "          Linear-251            [-1, 196, 1536]         591,360\n",
      "       QuickGELU-252            [-1, 196, 1536]               0\n",
      "          Linear-253             [-1, 196, 384]         590,208\n",
      "         Dropout-254             [-1, 196, 384]               0\n",
      "      LayerScale-255             [-1, 196, 384]               0\n",
      "        DropPath-256             [-1, 196, 384]               0\n",
      "           Block-257             [-1, 196, 384]               0\n",
      "       LayerNorm-258             [-1, 196, 384]             768\n",
      "          Conv2d-259          [-1, 384, 14, 14]           3,456\n",
      "     BatchNorm2d-260          [-1, 384, 14, 14]             768\n",
      "          Conv2d-261          [-1, 384, 14, 14]         147,840\n",
      "       Rearrange-262             [-1, 196, 384]               0\n",
      "          Conv2d-263            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-264            [-1, 384, 7, 7]             768\n",
      "          Conv2d-265            [-1, 384, 7, 7]         147,840\n",
      "       Rearrange-266              [-1, 49, 384]               0\n",
      "          Conv2d-267            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-268            [-1, 384, 7, 7]             768\n",
      "          Conv2d-269            [-1, 384, 7, 7]         147,840\n",
      "       Rearrange-270              [-1, 49, 384]               0\n",
      "         Dropout-271           [-1, 6, 196, 49]               0\n",
      "          Linear-272             [-1, 196, 384]         147,840\n",
      "         Dropout-273             [-1, 196, 384]               0\n",
      "   AttentionConv-274             [-1, 196, 384]               0\n",
      "      LayerScale-275             [-1, 196, 384]               0\n",
      "        DropPath-276             [-1, 196, 384]               0\n",
      "       LayerNorm-277             [-1, 196, 384]             768\n",
      "          Linear-278            [-1, 196, 1536]         591,360\n",
      "       QuickGELU-279            [-1, 196, 1536]               0\n",
      "          Linear-280             [-1, 196, 384]         590,208\n",
      "         Dropout-281             [-1, 196, 384]               0\n",
      "      LayerScale-282             [-1, 196, 384]               0\n",
      "        DropPath-283             [-1, 196, 384]               0\n",
      "           Block-284             [-1, 196, 384]               0\n",
      "       LayerNorm-285             [-1, 196, 384]             768\n",
      "          Conv2d-286          [-1, 384, 14, 14]           3,456\n",
      "     BatchNorm2d-287          [-1, 384, 14, 14]             768\n",
      "          Conv2d-288          [-1, 384, 14, 14]         147,840\n",
      "       Rearrange-289             [-1, 196, 384]               0\n",
      "          Conv2d-290            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-291            [-1, 384, 7, 7]             768\n",
      "          Conv2d-292            [-1, 384, 7, 7]         147,840\n",
      "       Rearrange-293              [-1, 49, 384]               0\n",
      "          Conv2d-294            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-295            [-1, 384, 7, 7]             768\n",
      "          Conv2d-296            [-1, 384, 7, 7]         147,840\n",
      "       Rearrange-297              [-1, 49, 384]               0\n",
      "         Dropout-298           [-1, 6, 196, 49]               0\n",
      "          Linear-299             [-1, 196, 384]         147,840\n",
      "         Dropout-300             [-1, 196, 384]               0\n",
      "   AttentionConv-301             [-1, 196, 384]               0\n",
      "      LayerScale-302             [-1, 196, 384]               0\n",
      "        DropPath-303             [-1, 196, 384]               0\n",
      "       LayerNorm-304             [-1, 196, 384]             768\n",
      "          Linear-305            [-1, 196, 1536]         591,360\n",
      "       QuickGELU-306            [-1, 196, 1536]               0\n",
      "          Linear-307             [-1, 196, 384]         590,208\n",
      "         Dropout-308             [-1, 196, 384]               0\n",
      "      LayerScale-309             [-1, 196, 384]               0\n",
      "        DropPath-310             [-1, 196, 384]               0\n",
      "           Block-311             [-1, 196, 384]               0\n",
      "       LayerNorm-312             [-1, 196, 384]             768\n",
      "          Conv2d-313          [-1, 384, 14, 14]           3,456\n",
      "     BatchNorm2d-314          [-1, 384, 14, 14]             768\n",
      "          Conv2d-315          [-1, 384, 14, 14]         147,840\n",
      "       Rearrange-316             [-1, 196, 384]               0\n",
      "          Conv2d-317            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-318            [-1, 384, 7, 7]             768\n",
      "          Conv2d-319            [-1, 384, 7, 7]         147,840\n",
      "       Rearrange-320              [-1, 49, 384]               0\n",
      "          Conv2d-321            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-322            [-1, 384, 7, 7]             768\n",
      "          Conv2d-323            [-1, 384, 7, 7]         147,840\n",
      "       Rearrange-324              [-1, 49, 384]               0\n",
      "         Dropout-325           [-1, 6, 196, 49]               0\n",
      "          Linear-326             [-1, 196, 384]         147,840\n",
      "         Dropout-327             [-1, 196, 384]               0\n",
      "   AttentionConv-328             [-1, 196, 384]               0\n",
      "      LayerScale-329             [-1, 196, 384]               0\n",
      "        DropPath-330             [-1, 196, 384]               0\n",
      "       LayerNorm-331             [-1, 196, 384]             768\n",
      "          Linear-332            [-1, 196, 1536]         591,360\n",
      "       QuickGELU-333            [-1, 196, 1536]               0\n",
      "          Linear-334             [-1, 196, 384]         590,208\n",
      "         Dropout-335             [-1, 196, 384]               0\n",
      "      LayerScale-336             [-1, 196, 384]               0\n",
      "        DropPath-337             [-1, 196, 384]               0\n",
      "           Block-338             [-1, 196, 384]               0\n",
      "       LayerNorm-339             [-1, 196, 384]             768\n",
      "          Conv2d-340          [-1, 384, 14, 14]           3,456\n",
      "     BatchNorm2d-341          [-1, 384, 14, 14]             768\n",
      "          Conv2d-342          [-1, 384, 14, 14]         147,840\n",
      "       Rearrange-343             [-1, 196, 384]               0\n",
      "          Conv2d-344            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-345            [-1, 384, 7, 7]             768\n",
      "          Conv2d-346            [-1, 384, 7, 7]         147,840\n",
      "       Rearrange-347              [-1, 49, 384]               0\n",
      "          Conv2d-348            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-349            [-1, 384, 7, 7]             768\n",
      "          Conv2d-350            [-1, 384, 7, 7]         147,840\n",
      "       Rearrange-351              [-1, 49, 384]               0\n",
      "         Dropout-352           [-1, 6, 196, 49]               0\n",
      "          Linear-353             [-1, 196, 384]         147,840\n",
      "         Dropout-354             [-1, 196, 384]               0\n",
      "   AttentionConv-355             [-1, 196, 384]               0\n",
      "      LayerScale-356             [-1, 196, 384]               0\n",
      "        DropPath-357             [-1, 196, 384]               0\n",
      "       LayerNorm-358             [-1, 196, 384]             768\n",
      "          Linear-359            [-1, 196, 1536]         591,360\n",
      "       QuickGELU-360            [-1, 196, 1536]               0\n",
      "          Linear-361             [-1, 196, 384]         590,208\n",
      "         Dropout-362             [-1, 196, 384]               0\n",
      "      LayerScale-363             [-1, 196, 384]               0\n",
      "        DropPath-364             [-1, 196, 384]               0\n",
      "           Block-365             [-1, 196, 384]               0\n",
      "VisionTransformer-366          [-1, 384, 14, 14]               0\n",
      "       LayerNorm-367             [-1, 196, 384]             768\n",
      "AdaptiveAvgPool1d-368               [-1, 384, 1]               0\n",
      "          Linear-369                  [-1, 100]          38,500\n",
      "================================================================\n",
      "Total params: 19,650,596\n",
      "Trainable params: 19,650,596\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 295.86\n",
      "Params size (MB): 74.96\n",
      "Estimated Total Size (MB): 371.40\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(model.cuda(), (3, 224, 224))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
