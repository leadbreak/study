{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "from timm.data import Mixup\n",
    "import transformers\n",
    "\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "from timm.models.layers import DropPath, trunc_normal_, to_2tuple\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01.Convolutional Token Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvEmbed(nn.Module):\n",
    "    '''\n",
    "    img/token map to Conv Embedding\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,\n",
    "                 patch_size=7, # [7, 3, 3]\n",
    "                 in_chans=3,   # [3, dim of stage1, dim of stage2]\n",
    "                 embed_dim=64, # [64, 192, 384]\n",
    "                 stride=4,     # [4, 2, 2]\n",
    "                 padding=2,    # [2, 1, 1]\n",
    "                 norm_layer=None):\n",
    "        super().__init__()\n",
    "        self.patch_size = to_2tuple(patch_size)\n",
    "        \n",
    "        self.proj = nn.Conv2d(\n",
    "            in_channels=in_chans,\n",
    "            out_channels=embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=stride,\n",
    "            padding=padding\n",
    "        )\n",
    "        \n",
    "        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        \n",
    "        _, _, H, W = x.shape\n",
    "        x = rearrange(x, 'b c h w -> b (h w) c')\n",
    "        x = self.norm(x)\n",
    "        x = rearrange(x, 'b (h w) c -> b c h w', h=H, w=W)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionConv(nn.Module):\n",
    "    def __init__(self,\n",
    "                 dim=64,        # [64,192,384]\n",
    "                 num_heads=4,   # paper: [1,3,6], me: [4,8,16]\n",
    "                 qkv_bias=False,\n",
    "                 attn_drop=0.,\n",
    "                 proj_drop=0.,\n",
    "                 kernel_size=3,\n",
    "                 padding_q=1,\n",
    "                 padding_kv=1,\n",
    "                 stride_q=1,\n",
    "                 stride_kv=2,\n",
    "                 **kwargs\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.stride_q = stride_q\n",
    "        self.stride_kv = stride_kv\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads        \n",
    "        self.scale = dim ** -0.5\n",
    "        \n",
    "        self.conv_proj_q = self._build_projection(dim,\n",
    "                                                  kernel_size,\n",
    "                                                  padding_q,\n",
    "                                                  stride_q,\n",
    "                                                  )\n",
    "        self.conv_proj_k = self._build_projection(dim,\n",
    "                                                  kernel_size,\n",
    "                                                  padding_kv,\n",
    "                                                  stride_kv,\n",
    "                                                  )\n",
    "        \n",
    "        self.conv_proj_v = self._build_projection(dim,\n",
    "                                                  kernel_size,\n",
    "                                                  padding_kv,\n",
    "                                                  stride_kv,\n",
    "                                                  )\n",
    "        \n",
    "        self.linear_proj_q = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.linear_proj_k = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.linear_proj_v = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        \n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.linear_proj_last = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)        \n",
    "        \n",
    "    def _build_projection(self,\n",
    "                          dim,\n",
    "                          kernel_size,\n",
    "                          padding,\n",
    "                          stride,\n",
    "                          ):\n",
    "        \n",
    "        proj = nn.Sequential(OrderedDict([\n",
    "            ('conv', nn.Conv2d(\n",
    "                dim,\n",
    "                dim,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=padding,\n",
    "                stride=stride,\n",
    "                bias=False,\n",
    "                groups=dim)),\n",
    "            ('bn', nn.BatchNorm2d(dim)),\n",
    "            ('rearrange', Rearrange('b c h w -> b (h w) c'))\n",
    "        ]))\n",
    "        \n",
    "        return proj\n",
    "    \n",
    "    def forward(self, x, h, w):\n",
    "        x = rearrange(x, 'b (h w) c -> b c h w', h=h, w=w)\n",
    "        \n",
    "        q = self.conv_proj_q(x)\n",
    "        k = self.conv_proj_k(x)\n",
    "        v = self.conv_proj_v(x)\n",
    "        \n",
    "        q = rearrange(self.linear_proj_q(q), 'b t (h d) -> b h t d', h=self.num_heads)\n",
    "        k = rearrange(self.linear_proj_k(k), 'b t (h d) -> b h t d', h=self.num_heads)\n",
    "        v = rearrange(self.linear_proj_v(v), 'b t (h d) -> b h t d', h=self.num_heads)\n",
    "        \n",
    "        attn_score = torch.einsum('bhlk,bhtk->bhlt', [q, k]) * self.scale\n",
    "        attn = self.attn_drop(F.softmax(attn_score, dim=-1))\n",
    "        \n",
    "        x = torch.matmul(attn, v)\n",
    "        batch_size, num_heads, seq_length, depth = x.size()\n",
    "        x = x.view(batch_size, seq_length, num_heads * depth)\n",
    "        \n",
    "        x = self.proj_drop(self.linear_proj_last(x))\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer block에 작은 스케일 인자 곱하기\n",
    "class LayerScale(nn.Module):\n",
    "    def __init__(self, dim, init_values=1e-5):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(init_values * torch.ones((dim)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gamma * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 num_heads,\n",
    "                 mlp_ratio=4.,\n",
    "                 qkv_bias=False,\n",
    "                 drop=0.,\n",
    "                 attn_drop=0.,\n",
    "                 drop_path=0.,\n",
    "                 act_layer=nn.GELU,\n",
    "                 norm_layer=nn.LayerNorm,\n",
    "                 **kwargs\n",
    "                ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.ls1 = LayerScale(dim)\n",
    "        self.attn = AttentionConv(dim=dim,\n",
    "                                  num_heads=num_heads,\n",
    "                                  qkv_bias=qkv_bias,\n",
    "                                  attn_drop=attn_drop,\n",
    "                                  proj_drop=drop,\n",
    "                                  **kwargs)        \n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        self.ls2 = LayerScale(dim)\n",
    "        mlp_hidden_dim = int(dim*mlp_ratio)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, mlp_hidden_dim),\n",
    "            act_layer(),\n",
    "            nn.Linear(mlp_hidden_dim, dim),\n",
    "            nn.Dropout(drop),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, h, w):\n",
    "        res = x\n",
    "        x = self.norm1(x)\n",
    "        attn = self.attn(x, h, w)\n",
    "        x = res + self.drop_path(self.ls1(attn))\n",
    "        x = x + self.drop_path(self.ls2(self.mlp(self.norm2(x))))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = torch.Tensor(np.zeros((2,3,224,224))) # B, C, H, W\n",
    "\n",
    "block = Block(dim=64,\n",
    "              num_heads=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stage 1 | img shape: torch.Size([2, 3, 224, 224]) → Conv Embed Shape: torch.Size([2, 64, 56, 56])\n"
     ]
    }
   ],
   "source": [
    "# Stage 1 \n",
    "\n",
    "## Patch Embedding\n",
    "convembed = ConvEmbed(patch_size=7, stride=4, padding=2)\n",
    "stage1_img = convembed(test_img)\n",
    "\n",
    "## Attention with Convolution\n",
    "b, c, h, w = stage1_img.shape\n",
    "stage1_img = rearrange(stage1_img, 'b c h w -> b (h w) c')\n",
    "stage1_img = block(stage1_img, h=h, w=w)\n",
    "stage1_img = rearrange(stage1_img, 'b (h w) c -> b c h w', h=h, w=w)\n",
    "\n",
    "## Check Result\n",
    "print(f'stage 1 | img shape: {test_img.shape} → Conv Embed Shape: {stage1_img.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stage 2 | img shape: torch.Size([2, 64, 56, 56]) → Conv Embed Shape: torch.Size([2, 64, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# Stage 2 \n",
    "\n",
    "## Patch Embedding\n",
    "convembed = ConvEmbed(patch_size=3, in_chans=64, stride=2, padding=1)\n",
    "stage2_img = convembed(stage1_img)\n",
    "\n",
    "## Attention with Convolution\n",
    "b, c, h, w = stage2_img.shape\n",
    "stage2_img = rearrange(stage2_img, 'b c h w -> b (h w) c')\n",
    "stage2_img = block(stage2_img, h=h, w=w)\n",
    "stage2_img = rearrange(stage2_img, 'b (h w) c -> b c h w', h=h, w=w)\n",
    "\n",
    "## Check Result\n",
    "print(f'stage 2 | img shape: {stage1_img.shape} → Conv Embed Shape: {stage2_img.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stage 3 | img shape: torch.Size([2, 64, 28, 28]) → Conv Embed Shape: torch.Size([2, 64, 14, 14])\n"
     ]
    }
   ],
   "source": [
    "# Stage 3 \n",
    "\n",
    "## Patch Embedding\n",
    "convembed = ConvEmbed(patch_size=3, in_chans=64, stride=2, padding=1)\n",
    "stage3_img = convembed(stage2_img)\n",
    "\n",
    "## Attention with Convolution\n",
    "b, c, h, w = stage3_img.shape\n",
    "stage3_img = rearrange(stage3_img, 'b c h w -> b (h w) c')\n",
    "stage3_img = block(stage3_img, h=h, w=w)\n",
    "stage3_img = rearrange(stage3_img, 'b (h w) c -> b c h w', h=h, w=w)\n",
    "\n",
    "## Check Result\n",
    "print(f'stage 3 | img shape: {stage2_img.shape} → Conv Embed Shape: {stage3_img.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 patch_size=16,\n",
    "                 patch_stride=16,\n",
    "                 patch_padding=0,\n",
    "                 in_chans=3,\n",
    "                 embed_dim=768,\n",
    "                 depth=12,\n",
    "                 num_heads=12,\n",
    "                 mlp_ratio=4.,\n",
    "                 qkv_bias=False,\n",
    "                 drop_rate=0.,\n",
    "                 attn_drop_rate=0.,\n",
    "                 drop_path_rate=0.,\n",
    "                 act_layer=nn.GELU,\n",
    "                 norm_layer=nn.LayerNorm,\n",
    "                 init='trunc_norm',\n",
    "                 **kwargs\n",
    "                 ):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_embed = ConvEmbed(\n",
    "            patch_size=patch_size,\n",
    "            in_chans=in_chans,\n",
    "            stride=patch_stride,\n",
    "            padding=patch_padding,\n",
    "            embed_dim=embed_dim,\n",
    "            norm_layer=norm_layer\n",
    "        )\n",
    "\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                drop=drop_rate,\n",
    "                attn_drop=attn_drop_rate,\n",
    "                drop_path=drop_path_rate,\n",
    "                act_layer=act_layer,\n",
    "                norm_layer=norm_layer,\n",
    "                **kwargs\n",
    "            ) for _ in range(depth)\n",
    "        ])\n",
    "\n",
    "        if init == 'xavier':\n",
    "            self.apply(self._init_weights_xavier)\n",
    "        else:\n",
    "            self.apply(self._init_weights_trunc_normal)\n",
    "\n",
    "    def _init_weights_trunc_normal(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, (nn.LayerNorm, nn.BatchNorm2d)):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def _init_weights_xavier(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, (nn.LayerNorm, nn.BatchNorm2d)):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        _, _, H, W = x.size()\n",
    "\n",
    "        x = rearrange(x, 'b c h w -> b (h w) c')\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for _, blk in enumerate(self.blocks):\n",
    "            x = blk(x, H, W)\n",
    "        x = rearrange(x, 'b (h w) c -> b c h w', h=H, w=W)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalVisionTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_chans=3,\n",
    "                 num_classes=100,\n",
    "                 act_layer=nn.GELU,\n",
    "                 norm_layer=nn.LayerNorm,\n",
    "                 init='trunc_norm',\n",
    "                 spec=None):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.num_stages = spec['NUM_STAGES']\n",
    "        self.stages = nn.ModuleList()\n",
    "        for i in range(self.num_stages):\n",
    "            kwargs = {\n",
    "                'patch_size': spec['PATCH_SIZE'][i],\n",
    "                'patch_stride': spec['PATCH_STRIDE'][i],\n",
    "                'patch_padding': spec['PATCH_PADDING'][i],\n",
    "                'embed_dim': spec['DIM_EMBED'][i],\n",
    "                'depth': spec['DEPTH'][i],\n",
    "                'num_heads': spec['NUM_HEADS'][i],\n",
    "                'mlp_ratio': spec['MLP_RATIO'][i],\n",
    "                'qkv_bias': spec['QKV_BIAS'][i],\n",
    "                'drop_rate': spec['DROP_RATE'][i],\n",
    "                'attn_drop_rate': spec['ATTN_DROP_RATE'][i],\n",
    "                'drop_path_rate': spec['DROP_PATH_RATE'][i],\n",
    "                'kernel_size': spec['KERNEL_QKV'][i],\n",
    "                'padding_q': spec['PADDING_Q'][i],\n",
    "                'padding_kv': spec['PADDING_KV'][i],\n",
    "                'stride_q': spec['STRIDE_Q'][i],\n",
    "                'stride_kv': spec['STRIDE_KV'][i],\n",
    "            }\n",
    "\n",
    "            stage = VisionTransformer(\n",
    "                in_chans=in_chans,\n",
    "                init=init,\n",
    "                act_layer=act_layer,\n",
    "                norm_layer=norm_layer,\n",
    "                **kwargs\n",
    "            )\n",
    "            \n",
    "            self.stages.append(stage)\n",
    "\n",
    "            in_chans = spec['DIM_EMBED'][i]\n",
    "\n",
    "        dim_embed = spec['DIM_EMBED'][-1]\n",
    "        self.norm = norm_layer(dim_embed)\n",
    "        self.pooler = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "        # Classifier head\n",
    "        self.head = nn.Linear(dim_embed, num_classes) if num_classes > 0 else nn.Identity()\n",
    "        trunc_normal_(self.head.weight, std=0.02)\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        for stage in self.stages:\n",
    "            x = stage(x)\n",
    "\n",
    "        x = rearrange(x, 'b c h w -> b (h w) c') # (B, L, C)\n",
    "        x = self.norm(x)                         # (B, L, C)\n",
    "        x = self.pooler(x.transpose(1,2))        # (B, C, 1)\n",
    "        x = torch.flatten(x, 1)                  # (B, C)\n",
    "        # x = torch.mean(x, dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.head(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuickGELU(nn.Module):\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return x * torch.sigmoid(1.702 * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NUM_STAGES': 3,\n",
       " 'PATCH_SIZE': [7, 3, 3],\n",
       " 'PATCH_STRIDE': [4, 2, 2],\n",
       " 'PATCH_PADDING': [2, 1, 1],\n",
       " 'DIM_EMBED': [64, 192, 384],\n",
       " 'DEPTH': [1, 4, 16],\n",
       " 'NUM_HEADS': [4, 8, 12],\n",
       " 'MLP_RATIO': [4.0, 4.0, 4.0],\n",
       " 'QKV_BIAS': [True, True, True],\n",
       " 'DROP_RATE': [0.0, 0.0, 0.0],\n",
       " 'ATTN_DROP_RATE': [0.0, 0.0, 0.0],\n",
       " 'DROP_PATH_RATE': [0.0, 0.0, 0.1],\n",
       " 'KERNEL_QKV': [3, 3, 3],\n",
       " 'PADDING_Q': [1, 1, 1],\n",
       " 'PADDING_KV': [1, 1, 1],\n",
       " 'STRIDE_Q': [1, 1, 1],\n",
       " 'STRIDE_KV': [2, 2, 2]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec = {\n",
    "    'NUM_STAGES': 3,\n",
    "    'PATCH_SIZE': [7,3,3],\n",
    "    'PATCH_STRIDE': [4,2,2],\n",
    "    'PATCH_PADDING': [2,1,1],\n",
    "    'DIM_EMBED': [64,192,384],\n",
    "    'DEPTH': [1,4,16],\n",
    "    'NUM_HEADS': [4,8,12],   # original : [1,3,6]\n",
    "    'MLP_RATIO': [4.,4.,4.],\n",
    "    'QKV_BIAS': [True, True, True],\n",
    "    'DROP_RATE': [0.,0.,0.],\n",
    "    'ATTN_DROP_RATE': [0.,0.,0.],\n",
    "    'DROP_PATH_RATE': [0.,0.,0.1],\n",
    "    'KERNEL_QKV': [3,3,3],\n",
    "    'PADDING_Q': [1,1,1],\n",
    "    'PADDING_KV': [1,1,1],\n",
    "    'STRIDE_Q': [1,1,1],\n",
    "    'STRIDE_KV': [2,2,2],\n",
    "}\n",
    "\n",
    "spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 224, 224]),\n",
       " torch.Size([2, 100]),\n",
       " tensor([[-1.6472e-01,  4.2592e-04, -8.5670e-01, -5.5013e-01, -4.0503e-01,\n",
       "           4.0134e-01,  4.9374e-01, -9.7043e-02, -7.2924e-02, -6.6139e-02,\n",
       "           9.6945e-02,  2.6969e-02,  3.7007e-01, -2.7715e-01,  5.4893e-01,\n",
       "           4.1699e-01,  9.9212e-02, -3.8950e-01,  2.5415e-01, -5.4102e-01,\n",
       "          -7.8081e-02, -2.1688e-02, -7.2538e-01,  5.8464e-01,  2.1810e-01,\n",
       "           3.0413e-01, -4.5131e-02,  5.5817e-01,  7.5532e-01,  1.2269e+00,\n",
       "          -3.9922e-01, -4.1991e-01,  1.0874e-01,  4.6523e-02, -3.4800e-02,\n",
       "          -1.4870e-01,  5.0559e-01, -2.1376e-01, -6.6045e-02, -1.6172e-01,\n",
       "           2.8710e-01, -1.2315e-01, -3.2750e-01,  7.0478e-02, -2.4193e-01,\n",
       "           5.8066e-01,  6.7945e-02,  5.8211e-01, -4.6962e-01, -4.7659e-02,\n",
       "          -1.2485e-01, -2.3644e-01, -6.9121e-01, -2.2480e-01,  3.5816e-01,\n",
       "          -8.2109e-02,  2.5002e-01,  2.3215e-02, -3.3342e-01,  3.1011e-01,\n",
       "          -1.8269e-03,  3.9734e-01,  3.6886e-02, -4.9220e-01, -2.9647e-01,\n",
       "          -2.8076e-01,  3.5034e-02, -2.5430e-01, -3.5715e-01,  1.4653e-01,\n",
       "          -1.9725e-02,  2.0600e-01, -1.8656e-01,  2.9022e-01, -5.7549e-01,\n",
       "          -3.2654e-01,  1.6024e-01, -1.6704e-02, -3.2255e-01, -9.2744e-02,\n",
       "           1.0339e+00, -1.0766e-01, -9.2275e-03,  1.1136e-01, -4.5282e-01,\n",
       "           7.0490e-02, -1.4200e-01, -3.3094e-03,  7.0130e-01, -7.3176e-01,\n",
       "           4.7701e-01, -1.1208e-01,  1.8308e-01, -2.9563e-01, -9.5486e-01,\n",
       "           2.1727e-01, -2.4186e-02,  2.6924e-01, -1.4820e-01, -4.0784e-01],\n",
       "         [-1.6472e-01,  4.2482e-04, -8.5670e-01, -5.5013e-01, -4.0503e-01,\n",
       "           4.0134e-01,  4.9374e-01, -9.7045e-02, -7.2925e-02, -6.6136e-02,\n",
       "           9.6946e-02,  2.6971e-02,  3.7007e-01, -2.7715e-01,  5.4892e-01,\n",
       "           4.1699e-01,  9.9212e-02, -3.8950e-01,  2.5415e-01, -5.4102e-01,\n",
       "          -7.8080e-02, -2.1689e-02, -7.2538e-01,  5.8464e-01,  2.1810e-01,\n",
       "           3.0413e-01, -4.5130e-02,  5.5817e-01,  7.5532e-01,  1.2269e+00,\n",
       "          -3.9922e-01, -4.1991e-01,  1.0874e-01,  4.6524e-02, -3.4799e-02,\n",
       "          -1.4870e-01,  5.0559e-01, -2.1376e-01, -6.6045e-02, -1.6172e-01,\n",
       "           2.8710e-01, -1.2315e-01, -3.2750e-01,  7.0478e-02, -2.4193e-01,\n",
       "           5.8066e-01,  6.7947e-02,  5.8211e-01, -4.6962e-01, -4.7658e-02,\n",
       "          -1.2485e-01, -2.3643e-01, -6.9121e-01, -2.2480e-01,  3.5815e-01,\n",
       "          -8.2106e-02,  2.5002e-01,  2.3218e-02, -3.3342e-01,  3.1011e-01,\n",
       "          -1.8279e-03,  3.9734e-01,  3.6886e-02, -4.9221e-01, -2.9647e-01,\n",
       "          -2.8076e-01,  3.5033e-02, -2.5430e-01, -3.5714e-01,  1.4653e-01,\n",
       "          -1.9723e-02,  2.0600e-01, -1.8656e-01,  2.9022e-01, -5.7549e-01,\n",
       "          -3.2654e-01,  1.6024e-01, -1.6704e-02, -3.2255e-01, -9.2743e-02,\n",
       "           1.0339e+00, -1.0766e-01, -9.2301e-03,  1.1135e-01, -4.5282e-01,\n",
       "           7.0486e-02, -1.4200e-01, -3.3060e-03,  7.0130e-01, -7.3176e-01,\n",
       "           4.7701e-01, -1.1208e-01,  1.8307e-01, -2.9563e-01, -9.5486e-01,\n",
       "           2.1727e-01, -2.4184e-02,  2.6924e-01, -1.4820e-01, -4.0784e-01]],\n",
       "        grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ConvolutionalVisionTransformer(act_layer=QuickGELU, spec=spec)\n",
    "\n",
    "test_result = model(test_img)\n",
    "test_img.shape, test_result.shape, test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 56, 56]           9,472\n",
      "         LayerNorm-2             [-1, 3136, 64]             128\n",
      "         ConvEmbed-3           [-1, 64, 56, 56]               0\n",
      "           Dropout-4             [-1, 3136, 64]               0\n",
      "         LayerNorm-5             [-1, 3136, 64]             128\n",
      "            Conv2d-6           [-1, 64, 56, 56]             576\n",
      "       BatchNorm2d-7           [-1, 64, 56, 56]             128\n",
      "         Rearrange-8             [-1, 3136, 64]               0\n",
      "            Conv2d-9           [-1, 64, 28, 28]             576\n",
      "      BatchNorm2d-10           [-1, 64, 28, 28]             128\n",
      "        Rearrange-11              [-1, 784, 64]               0\n",
      "           Conv2d-12           [-1, 64, 28, 28]             576\n",
      "      BatchNorm2d-13           [-1, 64, 28, 28]             128\n",
      "        Rearrange-14              [-1, 784, 64]               0\n",
      "           Linear-15             [-1, 3136, 64]           4,160\n",
      "           Linear-16              [-1, 784, 64]           4,160\n",
      "           Linear-17              [-1, 784, 64]           4,160\n",
      "          Dropout-18         [-1, 4, 3136, 784]               0\n",
      "           Linear-19             [-1, 3136, 64]           4,160\n",
      "          Dropout-20             [-1, 3136, 64]               0\n",
      "    AttentionConv-21             [-1, 3136, 64]               0\n",
      "       LayerScale-22             [-1, 3136, 64]               0\n",
      "         Identity-23             [-1, 3136, 64]               0\n",
      "        LayerNorm-24             [-1, 3136, 64]             128\n",
      "           Linear-25            [-1, 3136, 256]          16,640\n",
      "        QuickGELU-26            [-1, 3136, 256]               0\n",
      "           Linear-27             [-1, 3136, 64]          16,448\n",
      "          Dropout-28             [-1, 3136, 64]               0\n",
      "       LayerScale-29             [-1, 3136, 64]               0\n",
      "         Identity-30             [-1, 3136, 64]               0\n",
      "            Block-31             [-1, 3136, 64]               0\n",
      "VisionTransformer-32           [-1, 64, 56, 56]               0\n",
      "           Conv2d-33          [-1, 192, 28, 28]         110,784\n",
      "        LayerNorm-34             [-1, 784, 192]             384\n",
      "        ConvEmbed-35          [-1, 192, 28, 28]               0\n",
      "          Dropout-36             [-1, 784, 192]               0\n",
      "        LayerNorm-37             [-1, 784, 192]             384\n",
      "           Conv2d-38          [-1, 192, 28, 28]           1,728\n",
      "      BatchNorm2d-39          [-1, 192, 28, 28]             384\n",
      "        Rearrange-40             [-1, 784, 192]               0\n",
      "           Conv2d-41          [-1, 192, 14, 14]           1,728\n",
      "      BatchNorm2d-42          [-1, 192, 14, 14]             384\n",
      "        Rearrange-43             [-1, 196, 192]               0\n",
      "           Conv2d-44          [-1, 192, 14, 14]           1,728\n",
      "      BatchNorm2d-45          [-1, 192, 14, 14]             384\n",
      "        Rearrange-46             [-1, 196, 192]               0\n",
      "           Linear-47             [-1, 784, 192]          37,056\n",
      "           Linear-48             [-1, 196, 192]          37,056\n",
      "           Linear-49             [-1, 196, 192]          37,056\n",
      "          Dropout-50          [-1, 8, 784, 196]               0\n",
      "           Linear-51             [-1, 784, 192]          37,056\n",
      "          Dropout-52             [-1, 784, 192]               0\n",
      "    AttentionConv-53             [-1, 784, 192]               0\n",
      "       LayerScale-54             [-1, 784, 192]               0\n",
      "         Identity-55             [-1, 784, 192]               0\n",
      "        LayerNorm-56             [-1, 784, 192]             384\n",
      "           Linear-57             [-1, 784, 768]         148,224\n",
      "        QuickGELU-58             [-1, 784, 768]               0\n",
      "           Linear-59             [-1, 784, 192]         147,648\n",
      "          Dropout-60             [-1, 784, 192]               0\n",
      "       LayerScale-61             [-1, 784, 192]               0\n",
      "         Identity-62             [-1, 784, 192]               0\n",
      "            Block-63             [-1, 784, 192]               0\n",
      "        LayerNorm-64             [-1, 784, 192]             384\n",
      "           Conv2d-65          [-1, 192, 28, 28]           1,728\n",
      "      BatchNorm2d-66          [-1, 192, 28, 28]             384\n",
      "        Rearrange-67             [-1, 784, 192]               0\n",
      "           Conv2d-68          [-1, 192, 14, 14]           1,728\n",
      "      BatchNorm2d-69          [-1, 192, 14, 14]             384\n",
      "        Rearrange-70             [-1, 196, 192]               0\n",
      "           Conv2d-71          [-1, 192, 14, 14]           1,728\n",
      "      BatchNorm2d-72          [-1, 192, 14, 14]             384\n",
      "        Rearrange-73             [-1, 196, 192]               0\n",
      "           Linear-74             [-1, 784, 192]          37,056\n",
      "           Linear-75             [-1, 196, 192]          37,056\n",
      "           Linear-76             [-1, 196, 192]          37,056\n",
      "          Dropout-77          [-1, 8, 784, 196]               0\n",
      "           Linear-78             [-1, 784, 192]          37,056\n",
      "          Dropout-79             [-1, 784, 192]               0\n",
      "    AttentionConv-80             [-1, 784, 192]               0\n",
      "       LayerScale-81             [-1, 784, 192]               0\n",
      "         Identity-82             [-1, 784, 192]               0\n",
      "        LayerNorm-83             [-1, 784, 192]             384\n",
      "           Linear-84             [-1, 784, 768]         148,224\n",
      "        QuickGELU-85             [-1, 784, 768]               0\n",
      "           Linear-86             [-1, 784, 192]         147,648\n",
      "          Dropout-87             [-1, 784, 192]               0\n",
      "       LayerScale-88             [-1, 784, 192]               0\n",
      "         Identity-89             [-1, 784, 192]               0\n",
      "            Block-90             [-1, 784, 192]               0\n",
      "        LayerNorm-91             [-1, 784, 192]             384\n",
      "           Conv2d-92          [-1, 192, 28, 28]           1,728\n",
      "      BatchNorm2d-93          [-1, 192, 28, 28]             384\n",
      "        Rearrange-94             [-1, 784, 192]               0\n",
      "           Conv2d-95          [-1, 192, 14, 14]           1,728\n",
      "      BatchNorm2d-96          [-1, 192, 14, 14]             384\n",
      "        Rearrange-97             [-1, 196, 192]               0\n",
      "           Conv2d-98          [-1, 192, 14, 14]           1,728\n",
      "      BatchNorm2d-99          [-1, 192, 14, 14]             384\n",
      "       Rearrange-100             [-1, 196, 192]               0\n",
      "          Linear-101             [-1, 784, 192]          37,056\n",
      "          Linear-102             [-1, 196, 192]          37,056\n",
      "          Linear-103             [-1, 196, 192]          37,056\n",
      "         Dropout-104          [-1, 8, 784, 196]               0\n",
      "          Linear-105             [-1, 784, 192]          37,056\n",
      "         Dropout-106             [-1, 784, 192]               0\n",
      "   AttentionConv-107             [-1, 784, 192]               0\n",
      "      LayerScale-108             [-1, 784, 192]               0\n",
      "        Identity-109             [-1, 784, 192]               0\n",
      "       LayerNorm-110             [-1, 784, 192]             384\n",
      "          Linear-111             [-1, 784, 768]         148,224\n",
      "       QuickGELU-112             [-1, 784, 768]               0\n",
      "          Linear-113             [-1, 784, 192]         147,648\n",
      "         Dropout-114             [-1, 784, 192]               0\n",
      "      LayerScale-115             [-1, 784, 192]               0\n",
      "        Identity-116             [-1, 784, 192]               0\n",
      "           Block-117             [-1, 784, 192]               0\n",
      "       LayerNorm-118             [-1, 784, 192]             384\n",
      "          Conv2d-119          [-1, 192, 28, 28]           1,728\n",
      "     BatchNorm2d-120          [-1, 192, 28, 28]             384\n",
      "       Rearrange-121             [-1, 784, 192]               0\n",
      "          Conv2d-122          [-1, 192, 14, 14]           1,728\n",
      "     BatchNorm2d-123          [-1, 192, 14, 14]             384\n",
      "       Rearrange-124             [-1, 196, 192]               0\n",
      "          Conv2d-125          [-1, 192, 14, 14]           1,728\n",
      "     BatchNorm2d-126          [-1, 192, 14, 14]             384\n",
      "       Rearrange-127             [-1, 196, 192]               0\n",
      "          Linear-128             [-1, 784, 192]          37,056\n",
      "          Linear-129             [-1, 196, 192]          37,056\n",
      "          Linear-130             [-1, 196, 192]          37,056\n",
      "         Dropout-131          [-1, 8, 784, 196]               0\n",
      "          Linear-132             [-1, 784, 192]          37,056\n",
      "         Dropout-133             [-1, 784, 192]               0\n",
      "   AttentionConv-134             [-1, 784, 192]               0\n",
      "      LayerScale-135             [-1, 784, 192]               0\n",
      "        Identity-136             [-1, 784, 192]               0\n",
      "       LayerNorm-137             [-1, 784, 192]             384\n",
      "          Linear-138             [-1, 784, 768]         148,224\n",
      "       QuickGELU-139             [-1, 784, 768]               0\n",
      "          Linear-140             [-1, 784, 192]         147,648\n",
      "         Dropout-141             [-1, 784, 192]               0\n",
      "      LayerScale-142             [-1, 784, 192]               0\n",
      "        Identity-143             [-1, 784, 192]               0\n",
      "           Block-144             [-1, 784, 192]               0\n",
      "VisionTransformer-145          [-1, 192, 28, 28]               0\n",
      "          Conv2d-146          [-1, 384, 14, 14]         663,936\n",
      "       LayerNorm-147             [-1, 196, 384]             768\n",
      "       ConvEmbed-148          [-1, 384, 14, 14]               0\n",
      "         Dropout-149             [-1, 196, 384]               0\n",
      "       LayerNorm-150             [-1, 196, 384]             768\n",
      "          Conv2d-151          [-1, 384, 14, 14]           3,456\n",
      "     BatchNorm2d-152          [-1, 384, 14, 14]             768\n",
      "       Rearrange-153             [-1, 196, 384]               0\n",
      "          Conv2d-154            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-155            [-1, 384, 7, 7]             768\n",
      "       Rearrange-156              [-1, 49, 384]               0\n",
      "          Conv2d-157            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-158            [-1, 384, 7, 7]             768\n",
      "       Rearrange-159              [-1, 49, 384]               0\n",
      "          Linear-160             [-1, 196, 384]         147,840\n",
      "          Linear-161              [-1, 49, 384]         147,840\n",
      "          Linear-162              [-1, 49, 384]         147,840\n",
      "         Dropout-163          [-1, 12, 196, 49]               0\n",
      "          Linear-164             [-1, 196, 384]         147,840\n",
      "         Dropout-165             [-1, 196, 384]               0\n",
      "   AttentionConv-166             [-1, 196, 384]               0\n",
      "      LayerScale-167             [-1, 196, 384]               0\n",
      "        DropPath-168             [-1, 196, 384]               0\n",
      "       LayerNorm-169             [-1, 196, 384]             768\n",
      "          Linear-170            [-1, 196, 1536]         591,360\n",
      "       QuickGELU-171            [-1, 196, 1536]               0\n",
      "          Linear-172             [-1, 196, 384]         590,208\n",
      "         Dropout-173             [-1, 196, 384]               0\n",
      "      LayerScale-174             [-1, 196, 384]               0\n",
      "        DropPath-175             [-1, 196, 384]               0\n",
      "           Block-176             [-1, 196, 384]               0\n",
      "       LayerNorm-177             [-1, 196, 384]             768\n",
      "          Conv2d-178          [-1, 384, 14, 14]           3,456\n",
      "     BatchNorm2d-179          [-1, 384, 14, 14]             768\n",
      "       Rearrange-180             [-1, 196, 384]               0\n",
      "          Conv2d-181            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-182            [-1, 384, 7, 7]             768\n",
      "       Rearrange-183              [-1, 49, 384]               0\n",
      "          Conv2d-184            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-185            [-1, 384, 7, 7]             768\n",
      "       Rearrange-186              [-1, 49, 384]               0\n",
      "          Linear-187             [-1, 196, 384]         147,840\n",
      "          Linear-188              [-1, 49, 384]         147,840\n",
      "          Linear-189              [-1, 49, 384]         147,840\n",
      "         Dropout-190          [-1, 12, 196, 49]               0\n",
      "          Linear-191             [-1, 196, 384]         147,840\n",
      "         Dropout-192             [-1, 196, 384]               0\n",
      "   AttentionConv-193             [-1, 196, 384]               0\n",
      "      LayerScale-194             [-1, 196, 384]               0\n",
      "        DropPath-195             [-1, 196, 384]               0\n",
      "       LayerNorm-196             [-1, 196, 384]             768\n",
      "          Linear-197            [-1, 196, 1536]         591,360\n",
      "       QuickGELU-198            [-1, 196, 1536]               0\n",
      "          Linear-199             [-1, 196, 384]         590,208\n",
      "         Dropout-200             [-1, 196, 384]               0\n",
      "      LayerScale-201             [-1, 196, 384]               0\n",
      "        DropPath-202             [-1, 196, 384]               0\n",
      "           Block-203             [-1, 196, 384]               0\n",
      "       LayerNorm-204             [-1, 196, 384]             768\n",
      "          Conv2d-205          [-1, 384, 14, 14]           3,456\n",
      "     BatchNorm2d-206          [-1, 384, 14, 14]             768\n",
      "       Rearrange-207             [-1, 196, 384]               0\n",
      "          Conv2d-208            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-209            [-1, 384, 7, 7]             768\n",
      "       Rearrange-210              [-1, 49, 384]               0\n",
      "          Conv2d-211            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-212            [-1, 384, 7, 7]             768\n",
      "       Rearrange-213              [-1, 49, 384]               0\n",
      "          Linear-214             [-1, 196, 384]         147,840\n",
      "          Linear-215              [-1, 49, 384]         147,840\n",
      "          Linear-216              [-1, 49, 384]         147,840\n",
      "         Dropout-217          [-1, 12, 196, 49]               0\n",
      "          Linear-218             [-1, 196, 384]         147,840\n",
      "         Dropout-219             [-1, 196, 384]               0\n",
      "   AttentionConv-220             [-1, 196, 384]               0\n",
      "      LayerScale-221             [-1, 196, 384]               0\n",
      "        DropPath-222             [-1, 196, 384]               0\n",
      "       LayerNorm-223             [-1, 196, 384]             768\n",
      "          Linear-224            [-1, 196, 1536]         591,360\n",
      "       QuickGELU-225            [-1, 196, 1536]               0\n",
      "          Linear-226             [-1, 196, 384]         590,208\n",
      "         Dropout-227             [-1, 196, 384]               0\n",
      "      LayerScale-228             [-1, 196, 384]               0\n",
      "        DropPath-229             [-1, 196, 384]               0\n",
      "           Block-230             [-1, 196, 384]               0\n",
      "       LayerNorm-231             [-1, 196, 384]             768\n",
      "          Conv2d-232          [-1, 384, 14, 14]           3,456\n",
      "     BatchNorm2d-233          [-1, 384, 14, 14]             768\n",
      "       Rearrange-234             [-1, 196, 384]               0\n",
      "          Conv2d-235            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-236            [-1, 384, 7, 7]             768\n",
      "       Rearrange-237              [-1, 49, 384]               0\n",
      "          Conv2d-238            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-239            [-1, 384, 7, 7]             768\n",
      "       Rearrange-240              [-1, 49, 384]               0\n",
      "          Linear-241             [-1, 196, 384]         147,840\n",
      "          Linear-242              [-1, 49, 384]         147,840\n",
      "          Linear-243              [-1, 49, 384]         147,840\n",
      "         Dropout-244          [-1, 12, 196, 49]               0\n",
      "          Linear-245             [-1, 196, 384]         147,840\n",
      "         Dropout-246             [-1, 196, 384]               0\n",
      "   AttentionConv-247             [-1, 196, 384]               0\n",
      "      LayerScale-248             [-1, 196, 384]               0\n",
      "        DropPath-249             [-1, 196, 384]               0\n",
      "       LayerNorm-250             [-1, 196, 384]             768\n",
      "          Linear-251            [-1, 196, 1536]         591,360\n",
      "       QuickGELU-252            [-1, 196, 1536]               0\n",
      "          Linear-253             [-1, 196, 384]         590,208\n",
      "         Dropout-254             [-1, 196, 384]               0\n",
      "      LayerScale-255             [-1, 196, 384]               0\n",
      "        DropPath-256             [-1, 196, 384]               0\n",
      "           Block-257             [-1, 196, 384]               0\n",
      "       LayerNorm-258             [-1, 196, 384]             768\n",
      "          Conv2d-259          [-1, 384, 14, 14]           3,456\n",
      "     BatchNorm2d-260          [-1, 384, 14, 14]             768\n",
      "       Rearrange-261             [-1, 196, 384]               0\n",
      "          Conv2d-262            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-263            [-1, 384, 7, 7]             768\n",
      "       Rearrange-264              [-1, 49, 384]               0\n",
      "          Conv2d-265            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-266            [-1, 384, 7, 7]             768\n",
      "       Rearrange-267              [-1, 49, 384]               0\n",
      "          Linear-268             [-1, 196, 384]         147,840\n",
      "          Linear-269              [-1, 49, 384]         147,840\n",
      "          Linear-270              [-1, 49, 384]         147,840\n",
      "         Dropout-271          [-1, 12, 196, 49]               0\n",
      "          Linear-272             [-1, 196, 384]         147,840\n",
      "         Dropout-273             [-1, 196, 384]               0\n",
      "   AttentionConv-274             [-1, 196, 384]               0\n",
      "      LayerScale-275             [-1, 196, 384]               0\n",
      "        DropPath-276             [-1, 196, 384]               0\n",
      "       LayerNorm-277             [-1, 196, 384]             768\n",
      "          Linear-278            [-1, 196, 1536]         591,360\n",
      "       QuickGELU-279            [-1, 196, 1536]               0\n",
      "          Linear-280             [-1, 196, 384]         590,208\n",
      "         Dropout-281             [-1, 196, 384]               0\n",
      "      LayerScale-282             [-1, 196, 384]               0\n",
      "        DropPath-283             [-1, 196, 384]               0\n",
      "           Block-284             [-1, 196, 384]               0\n",
      "       LayerNorm-285             [-1, 196, 384]             768\n",
      "          Conv2d-286          [-1, 384, 14, 14]           3,456\n",
      "     BatchNorm2d-287          [-1, 384, 14, 14]             768\n",
      "       Rearrange-288             [-1, 196, 384]               0\n",
      "          Conv2d-289            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-290            [-1, 384, 7, 7]             768\n",
      "       Rearrange-291              [-1, 49, 384]               0\n",
      "          Conv2d-292            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-293            [-1, 384, 7, 7]             768\n",
      "       Rearrange-294              [-1, 49, 384]               0\n",
      "          Linear-295             [-1, 196, 384]         147,840\n",
      "          Linear-296              [-1, 49, 384]         147,840\n",
      "          Linear-297              [-1, 49, 384]         147,840\n",
      "         Dropout-298          [-1, 12, 196, 49]               0\n",
      "          Linear-299             [-1, 196, 384]         147,840\n",
      "         Dropout-300             [-1, 196, 384]               0\n",
      "   AttentionConv-301             [-1, 196, 384]               0\n",
      "      LayerScale-302             [-1, 196, 384]               0\n",
      "        DropPath-303             [-1, 196, 384]               0\n",
      "       LayerNorm-304             [-1, 196, 384]             768\n",
      "          Linear-305            [-1, 196, 1536]         591,360\n",
      "       QuickGELU-306            [-1, 196, 1536]               0\n",
      "          Linear-307             [-1, 196, 384]         590,208\n",
      "         Dropout-308             [-1, 196, 384]               0\n",
      "      LayerScale-309             [-1, 196, 384]               0\n",
      "        DropPath-310             [-1, 196, 384]               0\n",
      "           Block-311             [-1, 196, 384]               0\n",
      "       LayerNorm-312             [-1, 196, 384]             768\n",
      "          Conv2d-313          [-1, 384, 14, 14]           3,456\n",
      "     BatchNorm2d-314          [-1, 384, 14, 14]             768\n",
      "       Rearrange-315             [-1, 196, 384]               0\n",
      "          Conv2d-316            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-317            [-1, 384, 7, 7]             768\n",
      "       Rearrange-318              [-1, 49, 384]               0\n",
      "          Conv2d-319            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-320            [-1, 384, 7, 7]             768\n",
      "       Rearrange-321              [-1, 49, 384]               0\n",
      "          Linear-322             [-1, 196, 384]         147,840\n",
      "          Linear-323              [-1, 49, 384]         147,840\n",
      "          Linear-324              [-1, 49, 384]         147,840\n",
      "         Dropout-325          [-1, 12, 196, 49]               0\n",
      "          Linear-326             [-1, 196, 384]         147,840\n",
      "         Dropout-327             [-1, 196, 384]               0\n",
      "   AttentionConv-328             [-1, 196, 384]               0\n",
      "      LayerScale-329             [-1, 196, 384]               0\n",
      "        DropPath-330             [-1, 196, 384]               0\n",
      "       LayerNorm-331             [-1, 196, 384]             768\n",
      "          Linear-332            [-1, 196, 1536]         591,360\n",
      "       QuickGELU-333            [-1, 196, 1536]               0\n",
      "          Linear-334             [-1, 196, 384]         590,208\n",
      "         Dropout-335             [-1, 196, 384]               0\n",
      "      LayerScale-336             [-1, 196, 384]               0\n",
      "        DropPath-337             [-1, 196, 384]               0\n",
      "           Block-338             [-1, 196, 384]               0\n",
      "       LayerNorm-339             [-1, 196, 384]             768\n",
      "          Conv2d-340          [-1, 384, 14, 14]           3,456\n",
      "     BatchNorm2d-341          [-1, 384, 14, 14]             768\n",
      "       Rearrange-342             [-1, 196, 384]               0\n",
      "          Conv2d-343            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-344            [-1, 384, 7, 7]             768\n",
      "       Rearrange-345              [-1, 49, 384]               0\n",
      "          Conv2d-346            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-347            [-1, 384, 7, 7]             768\n",
      "       Rearrange-348              [-1, 49, 384]               0\n",
      "          Linear-349             [-1, 196, 384]         147,840\n",
      "          Linear-350              [-1, 49, 384]         147,840\n",
      "          Linear-351              [-1, 49, 384]         147,840\n",
      "         Dropout-352          [-1, 12, 196, 49]               0\n",
      "          Linear-353             [-1, 196, 384]         147,840\n",
      "         Dropout-354             [-1, 196, 384]               0\n",
      "   AttentionConv-355             [-1, 196, 384]               0\n",
      "      LayerScale-356             [-1, 196, 384]               0\n",
      "        DropPath-357             [-1, 196, 384]               0\n",
      "       LayerNorm-358             [-1, 196, 384]             768\n",
      "          Linear-359            [-1, 196, 1536]         591,360\n",
      "       QuickGELU-360            [-1, 196, 1536]               0\n",
      "          Linear-361             [-1, 196, 384]         590,208\n",
      "         Dropout-362             [-1, 196, 384]               0\n",
      "      LayerScale-363             [-1, 196, 384]               0\n",
      "        DropPath-364             [-1, 196, 384]               0\n",
      "           Block-365             [-1, 196, 384]               0\n",
      "       LayerNorm-366             [-1, 196, 384]             768\n",
      "          Conv2d-367          [-1, 384, 14, 14]           3,456\n",
      "     BatchNorm2d-368          [-1, 384, 14, 14]             768\n",
      "       Rearrange-369             [-1, 196, 384]               0\n",
      "          Conv2d-370            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-371            [-1, 384, 7, 7]             768\n",
      "       Rearrange-372              [-1, 49, 384]               0\n",
      "          Conv2d-373            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-374            [-1, 384, 7, 7]             768\n",
      "       Rearrange-375              [-1, 49, 384]               0\n",
      "          Linear-376             [-1, 196, 384]         147,840\n",
      "          Linear-377              [-1, 49, 384]         147,840\n",
      "          Linear-378              [-1, 49, 384]         147,840\n",
      "         Dropout-379          [-1, 12, 196, 49]               0\n",
      "          Linear-380             [-1, 196, 384]         147,840\n",
      "         Dropout-381             [-1, 196, 384]               0\n",
      "   AttentionConv-382             [-1, 196, 384]               0\n",
      "      LayerScale-383             [-1, 196, 384]               0\n",
      "        DropPath-384             [-1, 196, 384]               0\n",
      "       LayerNorm-385             [-1, 196, 384]             768\n",
      "          Linear-386            [-1, 196, 1536]         591,360\n",
      "       QuickGELU-387            [-1, 196, 1536]               0\n",
      "          Linear-388             [-1, 196, 384]         590,208\n",
      "         Dropout-389             [-1, 196, 384]               0\n",
      "      LayerScale-390             [-1, 196, 384]               0\n",
      "        DropPath-391             [-1, 196, 384]               0\n",
      "           Block-392             [-1, 196, 384]               0\n",
      "       LayerNorm-393             [-1, 196, 384]             768\n",
      "          Conv2d-394          [-1, 384, 14, 14]           3,456\n",
      "     BatchNorm2d-395          [-1, 384, 14, 14]             768\n",
      "       Rearrange-396             [-1, 196, 384]               0\n",
      "          Conv2d-397            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-398            [-1, 384, 7, 7]             768\n",
      "       Rearrange-399              [-1, 49, 384]               0\n",
      "          Conv2d-400            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-401            [-1, 384, 7, 7]             768\n",
      "       Rearrange-402              [-1, 49, 384]               0\n",
      "          Linear-403             [-1, 196, 384]         147,840\n",
      "          Linear-404              [-1, 49, 384]         147,840\n",
      "          Linear-405              [-1, 49, 384]         147,840\n",
      "         Dropout-406          [-1, 12, 196, 49]               0\n",
      "          Linear-407             [-1, 196, 384]         147,840\n",
      "         Dropout-408             [-1, 196, 384]               0\n",
      "   AttentionConv-409             [-1, 196, 384]               0\n",
      "      LayerScale-410             [-1, 196, 384]               0\n",
      "        DropPath-411             [-1, 196, 384]               0\n",
      "       LayerNorm-412             [-1, 196, 384]             768\n",
      "          Linear-413            [-1, 196, 1536]         591,360\n",
      "       QuickGELU-414            [-1, 196, 1536]               0\n",
      "          Linear-415             [-1, 196, 384]         590,208\n",
      "         Dropout-416             [-1, 196, 384]               0\n",
      "      LayerScale-417             [-1, 196, 384]               0\n",
      "        DropPath-418             [-1, 196, 384]               0\n",
      "           Block-419             [-1, 196, 384]               0\n",
      "       LayerNorm-420             [-1, 196, 384]             768\n",
      "          Conv2d-421          [-1, 384, 14, 14]           3,456\n",
      "     BatchNorm2d-422          [-1, 384, 14, 14]             768\n",
      "       Rearrange-423             [-1, 196, 384]               0\n",
      "          Conv2d-424            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-425            [-1, 384, 7, 7]             768\n",
      "       Rearrange-426              [-1, 49, 384]               0\n",
      "          Conv2d-427            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-428            [-1, 384, 7, 7]             768\n",
      "       Rearrange-429              [-1, 49, 384]               0\n",
      "          Linear-430             [-1, 196, 384]         147,840\n",
      "          Linear-431              [-1, 49, 384]         147,840\n",
      "          Linear-432              [-1, 49, 384]         147,840\n",
      "         Dropout-433          [-1, 12, 196, 49]               0\n",
      "          Linear-434             [-1, 196, 384]         147,840\n",
      "         Dropout-435             [-1, 196, 384]               0\n",
      "   AttentionConv-436             [-1, 196, 384]               0\n",
      "      LayerScale-437             [-1, 196, 384]               0\n",
      "        DropPath-438             [-1, 196, 384]               0\n",
      "       LayerNorm-439             [-1, 196, 384]             768\n",
      "          Linear-440            [-1, 196, 1536]         591,360\n",
      "       QuickGELU-441            [-1, 196, 1536]               0\n",
      "          Linear-442             [-1, 196, 384]         590,208\n",
      "         Dropout-443             [-1, 196, 384]               0\n",
      "      LayerScale-444             [-1, 196, 384]               0\n",
      "        DropPath-445             [-1, 196, 384]               0\n",
      "           Block-446             [-1, 196, 384]               0\n",
      "       LayerNorm-447             [-1, 196, 384]             768\n",
      "          Conv2d-448          [-1, 384, 14, 14]           3,456\n",
      "     BatchNorm2d-449          [-1, 384, 14, 14]             768\n",
      "       Rearrange-450             [-1, 196, 384]               0\n",
      "          Conv2d-451            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-452            [-1, 384, 7, 7]             768\n",
      "       Rearrange-453              [-1, 49, 384]               0\n",
      "          Conv2d-454            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-455            [-1, 384, 7, 7]             768\n",
      "       Rearrange-456              [-1, 49, 384]               0\n",
      "          Linear-457             [-1, 196, 384]         147,840\n",
      "          Linear-458              [-1, 49, 384]         147,840\n",
      "          Linear-459              [-1, 49, 384]         147,840\n",
      "         Dropout-460          [-1, 12, 196, 49]               0\n",
      "          Linear-461             [-1, 196, 384]         147,840\n",
      "         Dropout-462             [-1, 196, 384]               0\n",
      "   AttentionConv-463             [-1, 196, 384]               0\n",
      "      LayerScale-464             [-1, 196, 384]               0\n",
      "        DropPath-465             [-1, 196, 384]               0\n",
      "       LayerNorm-466             [-1, 196, 384]             768\n",
      "          Linear-467            [-1, 196, 1536]         591,360\n",
      "       QuickGELU-468            [-1, 196, 1536]               0\n",
      "          Linear-469             [-1, 196, 384]         590,208\n",
      "         Dropout-470             [-1, 196, 384]               0\n",
      "      LayerScale-471             [-1, 196, 384]               0\n",
      "        DropPath-472             [-1, 196, 384]               0\n",
      "           Block-473             [-1, 196, 384]               0\n",
      "       LayerNorm-474             [-1, 196, 384]             768\n",
      "          Conv2d-475          [-1, 384, 14, 14]           3,456\n",
      "     BatchNorm2d-476          [-1, 384, 14, 14]             768\n",
      "       Rearrange-477             [-1, 196, 384]               0\n",
      "          Conv2d-478            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-479            [-1, 384, 7, 7]             768\n",
      "       Rearrange-480              [-1, 49, 384]               0\n",
      "          Conv2d-481            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-482            [-1, 384, 7, 7]             768\n",
      "       Rearrange-483              [-1, 49, 384]               0\n",
      "          Linear-484             [-1, 196, 384]         147,840\n",
      "          Linear-485              [-1, 49, 384]         147,840\n",
      "          Linear-486              [-1, 49, 384]         147,840\n",
      "         Dropout-487          [-1, 12, 196, 49]               0\n",
      "          Linear-488             [-1, 196, 384]         147,840\n",
      "         Dropout-489             [-1, 196, 384]               0\n",
      "   AttentionConv-490             [-1, 196, 384]               0\n",
      "      LayerScale-491             [-1, 196, 384]               0\n",
      "        DropPath-492             [-1, 196, 384]               0\n",
      "       LayerNorm-493             [-1, 196, 384]             768\n",
      "          Linear-494            [-1, 196, 1536]         591,360\n",
      "       QuickGELU-495            [-1, 196, 1536]               0\n",
      "          Linear-496             [-1, 196, 384]         590,208\n",
      "         Dropout-497             [-1, 196, 384]               0\n",
      "      LayerScale-498             [-1, 196, 384]               0\n",
      "        DropPath-499             [-1, 196, 384]               0\n",
      "           Block-500             [-1, 196, 384]               0\n",
      "       LayerNorm-501             [-1, 196, 384]             768\n",
      "          Conv2d-502          [-1, 384, 14, 14]           3,456\n",
      "     BatchNorm2d-503          [-1, 384, 14, 14]             768\n",
      "       Rearrange-504             [-1, 196, 384]               0\n",
      "          Conv2d-505            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-506            [-1, 384, 7, 7]             768\n",
      "       Rearrange-507              [-1, 49, 384]               0\n",
      "          Conv2d-508            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-509            [-1, 384, 7, 7]             768\n",
      "       Rearrange-510              [-1, 49, 384]               0\n",
      "          Linear-511             [-1, 196, 384]         147,840\n",
      "          Linear-512              [-1, 49, 384]         147,840\n",
      "          Linear-513              [-1, 49, 384]         147,840\n",
      "         Dropout-514          [-1, 12, 196, 49]               0\n",
      "          Linear-515             [-1, 196, 384]         147,840\n",
      "         Dropout-516             [-1, 196, 384]               0\n",
      "   AttentionConv-517             [-1, 196, 384]               0\n",
      "      LayerScale-518             [-1, 196, 384]               0\n",
      "        DropPath-519             [-1, 196, 384]               0\n",
      "       LayerNorm-520             [-1, 196, 384]             768\n",
      "          Linear-521            [-1, 196, 1536]         591,360\n",
      "       QuickGELU-522            [-1, 196, 1536]               0\n",
      "          Linear-523             [-1, 196, 384]         590,208\n",
      "         Dropout-524             [-1, 196, 384]               0\n",
      "      LayerScale-525             [-1, 196, 384]               0\n",
      "        DropPath-526             [-1, 196, 384]               0\n",
      "           Block-527             [-1, 196, 384]               0\n",
      "       LayerNorm-528             [-1, 196, 384]             768\n",
      "          Conv2d-529          [-1, 384, 14, 14]           3,456\n",
      "     BatchNorm2d-530          [-1, 384, 14, 14]             768\n",
      "       Rearrange-531             [-1, 196, 384]               0\n",
      "          Conv2d-532            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-533            [-1, 384, 7, 7]             768\n",
      "       Rearrange-534              [-1, 49, 384]               0\n",
      "          Conv2d-535            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-536            [-1, 384, 7, 7]             768\n",
      "       Rearrange-537              [-1, 49, 384]               0\n",
      "          Linear-538             [-1, 196, 384]         147,840\n",
      "          Linear-539              [-1, 49, 384]         147,840\n",
      "          Linear-540              [-1, 49, 384]         147,840\n",
      "         Dropout-541          [-1, 12, 196, 49]               0\n",
      "          Linear-542             [-1, 196, 384]         147,840\n",
      "         Dropout-543             [-1, 196, 384]               0\n",
      "   AttentionConv-544             [-1, 196, 384]               0\n",
      "      LayerScale-545             [-1, 196, 384]               0\n",
      "        DropPath-546             [-1, 196, 384]               0\n",
      "       LayerNorm-547             [-1, 196, 384]             768\n",
      "          Linear-548            [-1, 196, 1536]         591,360\n",
      "       QuickGELU-549            [-1, 196, 1536]               0\n",
      "          Linear-550             [-1, 196, 384]         590,208\n",
      "         Dropout-551             [-1, 196, 384]               0\n",
      "      LayerScale-552             [-1, 196, 384]               0\n",
      "        DropPath-553             [-1, 196, 384]               0\n",
      "           Block-554             [-1, 196, 384]               0\n",
      "       LayerNorm-555             [-1, 196, 384]             768\n",
      "          Conv2d-556          [-1, 384, 14, 14]           3,456\n",
      "     BatchNorm2d-557          [-1, 384, 14, 14]             768\n",
      "       Rearrange-558             [-1, 196, 384]               0\n",
      "          Conv2d-559            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-560            [-1, 384, 7, 7]             768\n",
      "       Rearrange-561              [-1, 49, 384]               0\n",
      "          Conv2d-562            [-1, 384, 7, 7]           3,456\n",
      "     BatchNorm2d-563            [-1, 384, 7, 7]             768\n",
      "       Rearrange-564              [-1, 49, 384]               0\n",
      "          Linear-565             [-1, 196, 384]         147,840\n",
      "          Linear-566              [-1, 49, 384]         147,840\n",
      "          Linear-567              [-1, 49, 384]         147,840\n",
      "         Dropout-568          [-1, 12, 196, 49]               0\n",
      "          Linear-569             [-1, 196, 384]         147,840\n",
      "         Dropout-570             [-1, 196, 384]               0\n",
      "   AttentionConv-571             [-1, 196, 384]               0\n",
      "      LayerScale-572             [-1, 196, 384]               0\n",
      "        DropPath-573             [-1, 196, 384]               0\n",
      "       LayerNorm-574             [-1, 196, 384]             768\n",
      "          Linear-575            [-1, 196, 1536]         591,360\n",
      "       QuickGELU-576            [-1, 196, 1536]               0\n",
      "          Linear-577             [-1, 196, 384]         590,208\n",
      "         Dropout-578             [-1, 196, 384]               0\n",
      "      LayerScale-579             [-1, 196, 384]               0\n",
      "        DropPath-580             [-1, 196, 384]               0\n",
      "           Block-581             [-1, 196, 384]               0\n",
      "VisionTransformer-582          [-1, 384, 14, 14]               0\n",
      "       LayerNorm-583             [-1, 196, 384]             768\n",
      "AdaptiveAvgPool1d-584               [-1, 384, 1]               0\n",
      "          Linear-585                  [-1, 100]          38,500\n",
      "================================================================\n",
      "Total params: 31,275,812\n",
      "Trainable params: 31,275,812\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 541.59\n",
      "Params size (MB): 119.31\n",
      "Estimated Total Size (MB): 661.47\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(model.cuda(), (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms 정의하기\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8,1), interpolation=transforms.InterpolationMode.LANCZOS),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    transforms.RandomErasing(p=0.9, scale=(0.02, 0.33)),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "data_dir = '../data/sports'\n",
    "batch_size = 256\n",
    "\n",
    "train_path = data_dir+'/train'\n",
    "valid_path = data_dir+'/valid'\n",
    "test_path = data_dir+'/test'\n",
    "\n",
    "# dataset load\n",
    "train_data = ImageFolder(train_path, transform=train_transform)\n",
    "valid_data = ImageFolder(valid_path, transform=test_transform)\n",
    "test_data = ImageFolder(test_path, transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:3'\n",
    "max_norm = 1.0 # paper : 100 with G variants\n",
    "\n",
    "model.to(device)\n",
    "model_path = '../models/cvt/model.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixup_fn = Mixup(mixup_alpha=.7, \n",
    "                cutmix_alpha=1., \n",
    "                prob=.7, \n",
    "                switch_prob=0.5, \n",
    "                mode='batch',\n",
    "                label_smoothing=.1,\n",
    "                num_classes=100)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-22 16:22:44.046636: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-22 16:22:44.046715: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-22 16:22:44.047510: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-22 16:22:44.052795: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-22 16:22:44.897051: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.AdamW(model.parameters())\n",
    "warmup_steps = int(len(train_loader)*(epochs)*0.1)\n",
    "train_steps = len(train_loader)*(epochs)\n",
    "scheduler = transformers.get_cosine_schedule_with_warmup(optimizer, \n",
    "                                                        num_warmup_steps=warmup_steps, \n",
    "                                                        num_training_steps=train_steps,\n",
    "                                                        num_cycles=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 53/53 [01:00<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.5176772711411965, Val Loss: 4.275862455368042, LR: 0.0001, Duration: 62.52 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.340850038348504, Val Loss: 4.051362872123718, LR: 0.0002, Duration: 61.46 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.248786791315618, Val Loss: 3.9518173933029175, LR: 0.0003, Duration: 61.28 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.18199447865756, Val Loss: 3.860118865966797, LR: 0.0004, Duration: 61.36 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.134634773686247, Val Loss: 3.6629726886749268, LR: 0.0005, Duration: 61.46 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.022111523826167, Val Loss: 3.5491201877593994, LR: 0.0006, Duration: 61.27 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.923790747264646, Val Loss: 3.348009467124939, LR: 0.0007, Duration: 61.36 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.84987625535929, Val Loss: 3.234805464744568, LR: 0.0008, Duration: 61.52 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.8129933600155814, Val Loss: 3.1426056623458862, LR: 0.0009000000000000001, Duration: 61.47 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.723661679141926, Val Loss: 3.0236456394195557, LR: 0.001, Duration: 61.46 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.7302471151891745, Val Loss: 2.92398738861084, LR: 0.0009996954135095479, Duration: 61.38 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.59363713804281, Val Loss: 2.691341280937195, LR: 0.0009987820251299122, Duration: 61.20 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.5361022859249474, Val Loss: 2.5886975526809692, LR: 0.0009972609476841367, Duration: 61.40 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.45705398973429, Val Loss: 2.459357500076294, LR: 0.0009951340343707852, Duration: 61.50 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.4691219734695724, Val Loss: 2.3894344568252563, LR: 0.000992403876506104, Duration: 61.15 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.3697052541768775, Val Loss: 2.3991984128952026, LR: 0.0009890738003669028, Duration: 61.09 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.4217684898736342, Val Loss: 2.29913592338562, LR: 0.0009851478631379982, Duration: 61.19 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 53/53 [00:59<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.308866563832985, Val Loss: 2.187365770339966, LR: 0.0009806308479691594, Duration: 61.03 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.275544818842186, Val Loss: 2.1798723936080933, LR: 0.0009755282581475768, Duration: 61.29 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.118829943099112, Val Loss: 2.0236780643463135, LR: 0.0009698463103929542, Duration: 61.28 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.0995430226595895, Val Loss: 1.8579109907150269, LR: 0.0009635919272833937, Duration: 61.15 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.2146958450101457, Val Loss: 1.8958840370178223, LR: 0.0009567727288213005, Duration: 60.95 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.1975224557912574, Val Loss: 1.892210066318512, LR: 0.0009493970231495835, Duration: 60.90 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.8209283711775295, Val Loss: 1.81296306848526, LR: 0.0009414737964294635, Duration: 61.14 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.0769637035873703, Val Loss: 1.7376048564910889, LR: 0.0009330127018922195, Duration: 61.21 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.9162445563190387, Val Loss: 1.699297547340393, LR: 0.0009240240480782129, Duration: 61.25 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.848009653811185, Val Loss: 1.6142675280570984, LR: 0.0009145187862775209, Duration: 61.44 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.867131048778318, Val Loss: 1.6067493557929993, LR: 0.0009045084971874737, Duration: 61.27 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.9047826101195136, Val Loss: 1.5431160926818848, LR: 0.0008940053768033609, Duration: 61.33 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.9270669154401094, Val Loss: 1.4578586220741272, LR: 0.000883022221559489, Duration: 61.27 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.9469603277602285, Val Loss: 1.4203605651855469, LR: 0.0008715724127386971, Duration: 61.30 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.8542287057300784, Val Loss: 1.5445334911346436, LR: 0.0008596699001693256, Duration: 61.05 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.664223032177619, Val Loss: 1.4367051124572754, LR: 0.0008473291852294987, Duration: 60.92 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.7087780619567297, Val Loss: 1.3905593156814575, LR: 0.0008345653031794292, Duration: 61.38 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.7315635073859736, Val Loss: 1.5502561926841736, LR: 0.0008213938048432696, Duration: 60.95 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.669286556963651, Val Loss: 1.3563570380210876, LR: 0.0008078307376628291, Duration: 61.28 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.702760415257148, Val Loss: 1.3659308552742004, LR: 0.0007938926261462366, Duration: 61.06 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.485235200738007, Val Loss: 1.2890332341194153, LR: 0.0007795964517353734, Duration: 61.28 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.517184858052236, Val Loss: 1.2185967564582825, LR: 0.0007649596321166025, Duration: 61.37 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.47196885324874, Val Loss: 1.1850852370262146, LR: 0.00075, Duration: 61.29 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41: 100%|██████████| 53/53 [00:59<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6478315411873585, Val Loss: 1.2451592683792114, LR: 0.0007347357813929454, Duration: 60.89 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5724040247359365, Val Loss: 1.200536847114563, LR: 0.0007191855733945387, Duration: 61.02 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3756643331275797, Val Loss: 1.1365996599197388, LR: 0.0007033683215379002, Duration: 61.30 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.569962542012053, Val Loss: 1.1205300092697144, LR: 0.0006873032967079561, Duration: 61.27 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.498000034746134, Val Loss: 1.1569066047668457, LR: 0.0006710100716628344, Duration: 61.01 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.392655273653426, Val Loss: 1.0709334015846252, LR: 0.0006545084971874737, Duration: 61.24 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4386870906038105, Val Loss: 1.2000868320465088, LR: 0.0006378186779084996, Duration: 61.23 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48: 100%|██████████| 53/53 [01:00<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5575673850077503, Val Loss: 1.0748572945594788, LR: 0.0006209609477998338, Duration: 61.35 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4010191571037725, Val Loss: 1.063640534877777, LR: 0.0006039558454088796, Duration: 61.27 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2047302835392504, Val Loss: 1.0811036825180054, LR: 0.0005868240888334653, Duration: 60.91 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2882631162427507, Val Loss: 1.0250433385372162, LR: 0.0005695865504800327, Duration: 61.22 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.449763106849958, Val Loss: 1.0044510066509247, LR: 0.0005522642316338268, Duration: 61.26 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2428915050794496, Val Loss: 1.0448208451271057, LR: 0.0005348782368720626, Duration: 60.91 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54: 100%|██████████| 53/53 [00:59<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.272478573727158, Val Loss: 1.0438571274280548, LR: 0.0005174497483512506, Duration: 60.86 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.39763332537885, Val Loss: 0.9449300765991211, LR: 0.0005, Duration: 61.18 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4570005142463827, Val Loss: 0.9400605261325836, LR: 0.0004825502516487497, Duration: 61.21 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2823076990415467, Val Loss: 0.9713702499866486, LR: 0.00046512176312793734, Duration: 61.05 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.340283231915168, Val Loss: 0.9588016271591187, LR: 0.00044773576836617336, Duration: 60.97 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59: 100%|██████████| 53/53 [00:59<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.150503835588131, Val Loss: 0.9099699854850769, LR: 0.0004304134495199674, Duration: 61.05 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2938443782194606, Val Loss: 0.931870311498642, LR: 0.00041317591116653486, Duration: 60.93 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61: 100%|██████████| 53/53 [00:59<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.9199455616609105, Val Loss: 0.8982313573360443, LR: 0.0003960441545911204, Duration: 61.07 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62: 100%|██████████| 53/53 [00:59<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.081779664417483, Val Loss: 0.9276313185691833, LR: 0.0003790390522001662, Duration: 60.85 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.082164060394719, Val Loss: 0.9122936725616455, LR: 0.00036218132209150044, Duration: 60.92 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64: 100%|██████████| 53/53 [00:59<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1685283184051514, Val Loss: 0.8643865585327148, LR: 0.00034549150281252633, Duration: 61.07 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65: 100%|██████████| 53/53 [00:59<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0720780570552035, Val Loss: 0.8900442123413086, LR: 0.0003289899283371657, Duration: 60.85 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.126668043856351, Val Loss: 0.8892796337604523, LR: 0.00031269670329204396, Duration: 60.92 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0816283158536226, Val Loss: 0.880836009979248, LR: 0.0002966316784621, Duration: 60.90 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68: 100%|██████████| 53/53 [00:59<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2321373629120163, Val Loss: 0.9072782695293427, LR: 0.00028081442660546124, Duration: 60.83 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69: 100%|██████████| 53/53 [00:59<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.9541389604784407, Val Loss: 0.8659847676753998, LR: 0.00026526421860705474, Duration: 60.80 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.9777079190848008, Val Loss: 0.8382957577705383, LR: 0.0002500000000000001, Duration: 61.27 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71: 100%|██████████| 53/53 [00:59<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1762764341426344, Val Loss: 0.8777690827846527, LR: 0.0002350403678833976, Duration: 60.79 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.9129224588286202, Val Loss: 0.9156336188316345, LR: 0.00022040354826462666, Duration: 61.05 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 73: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.8911963476324982, Val Loss: 0.8497775495052338, LR: 0.00020610737385376348, Duration: 61.01 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 74: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2796702857287423, Val Loss: 0.8884912729263306, LR: 0.00019216926233717085, Duration: 60.90 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.9414748965569262, Val Loss: 0.8520947098731995, LR: 0.0001786061951567303, Duration: 61.06 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 76: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.9591983984101493, Val Loss: 0.8304752707481384, LR: 0.00016543469682057105, Duration: 61.36 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 77: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.8654400042767794, Val Loss: 0.878604382276535, LR: 0.00015267081477050133, Duration: 61.11 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 78: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.9867878162635948, Val Loss: 0.850865513086319, LR: 0.00014033009983067452, Duration: 60.97 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 79: 100%|██████████| 53/53 [00:59<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.8833969786482037, Val Loss: 0.8608584702014923, LR: 0.00012842758726130281, Duration: 60.88 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 80: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.8628741367807928, Val Loss: 0.8487681448459625, LR: 0.00011697777844051105, Duration: 60.90 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 81: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.04902432437213, Val Loss: 0.8438940048217773, LR: 0.00010599462319663906, Duration: 61.08 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 82: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.8145817731911282, Val Loss: 0.8441591262817383, LR: 9.549150281252633e-05, Duration: 61.05 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 83: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0121662684206694, Val Loss: 0.8758158981800079, LR: 8.548121372247918e-05, Duration: 60.98 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 84: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.867400014175559, Val Loss: 0.8321211636066437, LR: 7.597595192178702e-05, Duration: 60.96 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 85: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0190575010371656, Val Loss: 0.8450649976730347, LR: 6.698729810778065e-05, Duration: 61.05 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 86: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0692255272055573, Val Loss: 0.865563839673996, LR: 5.852620357053651e-05, Duration: 61.02 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 87: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.9420691773576557, Val Loss: 0.8625101745128632, LR: 5.060297685041659e-05, Duration: 60.94 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 88: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0279633515285997, Val Loss: 0.8798494040966034, LR: 4.322727117869951e-05, Duration: 61.05 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 89: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.8175121714484017, Val Loss: 0.816498339176178, LR: 3.6408072716606344e-05, Duration: 61.22 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 90: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.9998960191348814, Val Loss: 0.8789870142936707, LR: 3.0153689607045842e-05, Duration: 61.05 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 91: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0045287856515848, Val Loss: 0.8453486263751984, LR: 2.4471741852423235e-05, Duration: 61.07 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 92: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0064103131024345, Val Loss: 0.8338011503219604, LR: 1.9369152030840554e-05, Duration: 61.10 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 93: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.116128187134581, Val Loss: 0.8252589106559753, LR: 1.4852136862001764e-05, Duration: 61.14 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 94: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.9884016896193881, Val Loss: 0.8495928943157196, LR: 1.0926199633097156e-05, Duration: 61.07 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 95: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0644492034642203, Val Loss: 0.8432143926620483, LR: 7.59612349389599e-06, Duration: 61.21 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 96: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.9598800236324094, Val Loss: 0.851740300655365, LR: 4.865965629214819e-06, Duration: 61.17 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 97: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.9020088686133332, Val Loss: 0.8598828911781311, LR: 2.739052315863355e-06, Duration: 61.03 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 98: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.9122466433723018, Val Loss: 0.8351297676563263, LR: 1.2179748700879012e-06, Duration: 61.20 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.991920972770115, Val Loss: 0.8554719388484955, LR: 3.0458649045211895e-07, Duration: 61.21 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 100: 100%|██████████| 53/53 [01:00<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0103978514671326, Val Loss: 0.8361976146697998, LR: 0.0, Duration: 61.29 sec\n",
      "Epoch 당 평균 소요시간 : 61.15초\n"
     ]
    }
   ],
   "source": [
    "training_time = 0\n",
    "losses = []\n",
    "val_losses = []\n",
    "lrs = []\n",
    "best_loss = float('inf')\n",
    "\n",
    "# GradScaler 초기화\n",
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch + 1}\")\n",
    "    \n",
    "    for _, data in pbar:\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        inputs, labels = mixup_fn(inputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # AutoCast 적용\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "        # 스케일링된 그라디언트 계산\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # 그라디언트 클리핑 전에 스케일링 제거\n",
    "        scaler.unscale_(optimizer)\n",
    "        clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "\n",
    "        # 옵티마이저 스텝 및 스케일러 업데이트\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "            \n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        lrs.append(lr)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)        \n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in valid_loader:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    val_loss /= len(valid_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # 모델 저장\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        vit_save = True\n",
    "        if vit_save:\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_duration = time.time() - start_time\n",
    "    training_time += epoch_duration\n",
    "    \n",
    "    text = f'\\tLoss: {epoch_loss}, Val Loss: {val_loss}, LR: {lr}, Duration: {epoch_duration:.2f} sec'\n",
    "    \n",
    "    if vit_save:\n",
    "        text += f' - model saved!'\n",
    "        vit_save = False\n",
    "\n",
    "    print(text)\n",
    "        \n",
    "text = f\"Epoch 당 평균 소요시간 : {training_time / epochs:.2f}초\"      \n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.848000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.865429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Recall</td>\n",
       "      <td>0.848000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F1 Score</td>\n",
       "      <td>0.844393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Metric     Value\n",
       "0   Accuracy  0.848000\n",
       "1  Precision  0.865429\n",
       "2     Recall  0.848000\n",
       "3   F1 Score  0.844393"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예측 수행 및 레이블 저장\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# 혼동 행렬 생성\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# 예측과 실제 레이블\n",
    "y_true = all_labels  # 실제 레이블\n",
    "y_pred = all_preds  # 모델에 의해 예측된 레이블\n",
    "\n",
    "# 전체 데이터셋에 대한 정확도\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# 평균 정밀도, 리콜, F1-Score ('weighted')\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "\n",
    "# 판다스 데이터프레임으로 결과 정리\n",
    "performance_metrics = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "    'Value': [accuracy, precision, recall, f1_score]\n",
    "})\n",
    "\n",
    "# 데이터프레임 출력\n",
    "performance_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
