{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "598ba665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 100, 100]) torch.Size([64, 100])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class SimpleSCMPrior:\n",
    "    \"\"\"\n",
    "    단순한 구조적 인과 모델(SCM)을 사용하여 합성 데이터를 생성하는 클래스.\n",
    "    생성 시 n_features와 n_classes를 설정받습니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features=100, n_classes=2):\n",
    "        \"\"\"\n",
    "        클래스 생성자 (Initializer)\n",
    "        생성 시 전달받은 특성 및 클래스 개수를 인스턴스 변수로 저장합니다.\n",
    "        \"\"\"\n",
    "        self.n_features = n_features\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "    def sample(self, batch_size=64, n_samples=100):\n",
    "        \"\"\"\n",
    "        SCM으로부터 하나의 배치 데이터를 샘플링합니다.\n",
    "        \"\"\"\n",
    "        # 1. 특성(X) 및 잠재 변수(logits) 생성\n",
    "        # __init__에서 저장한 self.n_features를 사용합니다.\n",
    "        X = torch.randn(batch_size, n_samples, self.n_features)\n",
    "        causal_weights = torch.randn(batch_size, self.n_features, 1)\n",
    "        logits = X @ causal_weights\n",
    "\n",
    "        # 2. 안정적인 방식으로 임계값(Thresholds) 계산\n",
    "        min_vals, _ = torch.min(logits, dim=1, keepdim=True)\n",
    "        max_vals, _ = torch.max(logits, dim=1, keepdim=True)\n",
    "        \n",
    "        # __init__에서 저장한 self.n_classes를 사용합니다.\n",
    "        ratios = torch.linspace(0, 1, self.n_classes + 1, device=logits.device)[1:-1]\n",
    "        ratios = ratios.view(1, -1, 1)\n",
    "        thresholds = min_vals + (max_vals - min_vals) * ratios\n",
    "        \n",
    "        # 3. 로짓과 임계값 비교를 통한 레이블(y) 생성\n",
    "        y = torch.sum(logits > thresholds.transpose(1, 2), dim=2)\n",
    "\n",
    "        return X.float(), y.long()\n",
    "\n",
    "# 사용 예시\n",
    "prior = SimpleSCMPrior()\n",
    "X_batch, y_batch = prior.sample()\n",
    "print(X_batch.shape, y_batch.shape) # torch.Size() torch.Size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab887144",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, src_mask=None):\n",
    "        # Multi-head Self-attention\n",
    "        src2, _ = self.self_attn(src, src, src, attn_mask=src_mask)\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "        \n",
    "        # Feed-forward Network\n",
    "        src2 = self.linear2(self.dropout(F.relu(self.linear1(src))))\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, encoder_layer, num_layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([encoder_layer for _ in range(num_layers)])\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def forward(self, src, mask=None):\n",
    "        output = src\n",
    "        for mod in self.layers:\n",
    "            output = mod(output, src_mask=mask)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "216c609f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PFN(nn.Module):\n",
    "    def __init__(self, d_model=256, nhead=4, num_encoder_layers=6, n_features=10, n_classes=2):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # 입력 임베딩 레이어\n",
    "        self.feature_embedding = nn.Linear(n_features, d_model)\n",
    "        self.label_embedding = nn.Embedding(n_classes, d_model)\n",
    "        \n",
    "        # 트랜스포머 인코더\n",
    "        encoder_layer = TransformerEncoderLayer(d_model, nhead)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layer, num_encoder_layers)\n",
    "        \n",
    "        # 출력(예측) 헤드\n",
    "        self.output_head = nn.Linear(d_model, n_classes)\n",
    "\n",
    "    def _generate_attention_mask(self, n_train, n_test, device):\n",
    "        \"\"\"테스트 포인트가 서로 attend하지 못하도록 마스크 생성\"\"\"\n",
    "        total_len = n_train + n_test\n",
    "        mask = torch.zeros(total_len, total_len, device=device).bool()\n",
    "        # 테스트 -> 테스트 영역을 True로 설정하여 어텐션 차단\n",
    "        mask[n_train:, n_train:] = True\n",
    "        # 대각선은 False로 유지 (자기 자신은 attend 가능)\n",
    "        mask.fill_diagonal_(False)\n",
    "        return mask\n",
    "\n",
    "    def forward(self, train_x, train_y, test_x):\n",
    "        # train_x: (batch, n_train, n_features)\n",
    "        # train_y: (batch, n_train)\n",
    "        # test_x: (batch, n_test, n_features)\n",
    "        \n",
    "        batch_size, n_train, _ = train_x.shape\n",
    "        _, n_test, _ = test_x.shape\n",
    "\n",
    "        # 1. 토큰화 (Embedding)\n",
    "        # 훈련 데이터: 특성과 레이블 임베딩을 합침\n",
    "        train_x_emb = self.feature_embedding(train_x)\n",
    "        train_y_emb = self.label_embedding(train_y)\n",
    "        train_tokens = train_x_emb + train_y_emb\n",
    "\n",
    "        # 테스트 데이터: 특성만 임베딩 (레이블은 예측 대상)\n",
    "        test_tokens = self.feature_embedding(test_x)\n",
    "\n",
    "        # 2. 시퀀스 결합\n",
    "        # (batch, n_train + n_test, d_model)\n",
    "        full_sequence = torch.cat([train_tokens, test_tokens], dim=1)\n",
    "\n",
    "        # 3. 어텐션 마스크 생성\n",
    "        attn_mask = self._generate_attention_mask(n_train, n_test, train_x.device)\n",
    "\n",
    "        # 4. 트랜스포머 순전파\n",
    "        transformer_output = self.transformer_encoder(full_sequence, mask=attn_mask)\n",
    "\n",
    "        # 5. 예측 헤드\n",
    "        # 테스트 데이터에 해당하는 출력만 사용\n",
    "        test_output_embeddings = transformer_output[:, n_train:, :]\n",
    "        logits = self.output_head(test_output_embeddings) # (batch, n_test, n_classes)\n",
    "        \n",
    "        return F.log_softmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b93e4ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.8416905403137207\n",
      "Epoch 100, Loss: 0.6191109418869019\n",
      "Epoch 200, Loss: 0.20562173426151276\n",
      "Epoch 300, Loss: 0.15846505761146545\n",
      "Epoch 400, Loss: 0.15683847665786743\n",
      "Epoch 500, Loss: 0.1532370001077652\n",
      "Epoch 600, Loss: 0.14319001138210297\n",
      "Epoch 700, Loss: 0.1472017467021942\n",
      "Epoch 800, Loss: 0.14867673814296722\n",
      "Epoch 900, Loss: 0.13636593520641327\n",
      "Epoch 1000, Loss: 0.13314341008663177\n",
      "Epoch 1100, Loss: 0.13291247189044952\n",
      "Epoch 1200, Loss: 0.13408222794532776\n",
      "Epoch 1300, Loss: 0.12257137894630432\n",
      "Epoch 1400, Loss: 0.12016956508159637\n",
      "Epoch 1500, Loss: 0.11561991274356842\n",
      "Epoch 1600, Loss: 0.11991330981254578\n",
      "Epoch 1700, Loss: 0.11594480276107788\n",
      "Epoch 1800, Loss: 0.11133155971765518\n",
      "Epoch 1900, Loss: 0.10503566265106201\n",
      "Epoch 2000, Loss: 0.10347411036491394\n",
      "Epoch 2100, Loss: 0.09855938702821732\n",
      "Epoch 2200, Loss: 0.10291358083486557\n",
      "Epoch 2300, Loss: 0.10001041740179062\n",
      "Epoch 2400, Loss: 0.10080769658088684\n",
      "Epoch 2500, Loss: 0.09456207603216171\n",
      "Epoch 2600, Loss: 0.09484438598155975\n",
      "Epoch 2700, Loss: 0.09135428816080093\n",
      "Epoch 2800, Loss: 0.09551502764225006\n",
      "Epoch 2900, Loss: 0.0900910496711731\n",
      "Epoch 3000, Loss: 0.08785755932331085\n",
      "Epoch 3100, Loss: 0.08952610194683075\n",
      "Epoch 3200, Loss: 0.08426228910684586\n",
      "Epoch 3300, Loss: 0.08864641189575195\n",
      "Epoch 3400, Loss: 0.0783095732331276\n",
      "Epoch 3500, Loss: 0.08263243734836578\n",
      "Epoch 3600, Loss: 0.0789458155632019\n",
      "Epoch 3700, Loss: 0.0829756110906601\n",
      "Epoch 3800, Loss: 0.07602792978286743\n",
      "Epoch 3900, Loss: 0.07933152467012405\n",
      "Epoch 4000, Loss: 0.07353377342224121\n",
      "Epoch 4100, Loss: 0.0742998868227005\n",
      "Epoch 4200, Loss: 0.07170552015304565\n",
      "Epoch 4300, Loss: 0.07322229444980621\n",
      "Epoch 4400, Loss: 0.06981519609689713\n",
      "Epoch 4500, Loss: 0.08305668830871582\n",
      "Epoch 4600, Loss: 0.07288801670074463\n",
      "Epoch 4700, Loss: 0.07447761297225952\n",
      "Epoch 4800, Loss: 0.06991308182477951\n",
      "Epoch 4900, Loss: 0.06567391008138657\n",
      "Epoch 5000, Loss: 0.07042965292930603\n",
      "Epoch 5100, Loss: 0.06916626542806625\n",
      "Epoch 5200, Loss: 0.06507644802331924\n",
      "Epoch 5300, Loss: 0.07457283139228821\n",
      "Epoch 5400, Loss: 0.0617215521633625\n",
      "Epoch 5500, Loss: 0.0713183656334877\n",
      "Epoch 5600, Loss: 0.0702110081911087\n",
      "Epoch 5700, Loss: 0.06429748237133026\n",
      "Epoch 5800, Loss: 0.058880336582660675\n",
      "Epoch 5900, Loss: 0.06580966711044312\n",
      "Epoch 6000, Loss: 0.06350880861282349\n",
      "Epoch 6100, Loss: 0.06289951503276825\n",
      "Epoch 6200, Loss: 0.06313873827457428\n",
      "Epoch 6300, Loss: 0.06081128865480423\n",
      "Epoch 6400, Loss: 0.061138272285461426\n",
      "Epoch 6500, Loss: 0.062200937420129776\n",
      "Epoch 6600, Loss: 0.05785897374153137\n",
      "Epoch 6700, Loss: 0.0614626407623291\n",
      "Epoch 6800, Loss: 0.057381391525268555\n",
      "Epoch 6900, Loss: 0.06134582310914993\n",
      "Epoch 7000, Loss: 0.05909295007586479\n",
      "Epoch 7100, Loss: 0.06370006501674652\n",
      "Epoch 7200, Loss: 0.06158971041440964\n",
      "Epoch 7300, Loss: 0.06456340849399567\n",
      "Epoch 7400, Loss: 0.05991387367248535\n",
      "Epoch 7500, Loss: 0.05803079158067703\n",
      "Epoch 7600, Loss: 0.059100065380334854\n",
      "Epoch 7700, Loss: 0.05935846269130707\n",
      "Epoch 7800, Loss: 0.056513406336307526\n",
      "Epoch 7900, Loss: 0.0652034655213356\n",
      "Epoch 8000, Loss: 0.05732479318976402\n",
      "Epoch 8100, Loss: 0.058461859822273254\n",
      "Epoch 8200, Loss: 0.05908484756946564\n",
      "Epoch 8300, Loss: 0.05233469605445862\n",
      "Epoch 8400, Loss: 0.05770733579993248\n",
      "Epoch 8500, Loss: 0.059141844511032104\n",
      "Epoch 8600, Loss: 0.05768224969506264\n",
      "Epoch 8700, Loss: 0.05605876445770264\n",
      "Epoch 8800, Loss: 0.059922851622104645\n",
      "Epoch 8900, Loss: 0.05786149576306343\n",
      "Epoch 9000, Loss: 0.05124984681606293\n",
      "Epoch 9100, Loss: 0.05509364604949951\n",
      "Epoch 9200, Loss: 0.05592897906899452\n",
      "Epoch 9300, Loss: 0.05844517797231674\n",
      "Epoch 9400, Loss: 0.05400639772415161\n",
      "Epoch 9500, Loss: 0.05876244977116585\n",
      "Epoch 9600, Loss: 0.05896482244133949\n",
      "Epoch 9700, Loss: 0.05106697604060173\n",
      "Epoch 9800, Loss: 0.056510575115680695\n",
      "Epoch 9900, Loss: 0.05471251904964447\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# 하이퍼파라미터\n",
    "N_FEATURES = 10\n",
    "N_CLASSES = 2\n",
    "D_MODEL = 128\n",
    "N_HEAD = 4\n",
    "NUM_LAYERS = 4\n",
    "BATCH_SIZE = 32\n",
    "N_SAMPLES = 1024 # 사전 분포에서 샘플링할 데이터셋의 크기\n",
    "TRAIN_RATIO = 0.5\n",
    "LEARNING_RATE = 1e-4\n",
    "EPOCHS = 10000\n",
    "\n",
    "# 모델, 사전 분포, 옵티마이저 초기화\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "prior = SimpleSCMPrior(n_features=N_FEATURES, n_classes=N_CLASSES)\n",
    "model = PFN(d_model=D_MODEL, nhead=N_HEAD, num_encoder_layers=NUM_LAYERS, \n",
    "            n_features=N_FEATURES, n_classes=N_CLASSES).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = nn.NLLLoss()\n",
    "\n",
    "# 훈련 루프\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    \n",
    "    # 1. 사전 분포에서 데이터셋 배치 샘플링\n",
    "    X, y = prior.sample(batch_size=BATCH_SIZE, n_samples=N_SAMPLES)\n",
    "    X, y = X.to(device), y.to(device)\n",
    "    \n",
    "    # 2. 훈련/테스트 분할\n",
    "    n_train = int(N_SAMPLES * TRAIN_RATIO)\n",
    "    train_x, train_y = X[:, :n_train], y[:, :n_train]\n",
    "    test_x, test_y = X[:, n_train:], y[:, n_train:]\n",
    "    \n",
    "    # 3. 순전파 및 손실 계산\n",
    "    optimizer.zero_grad()\n",
    "    predicted_log_probs = model(train_x, train_y, test_x) # (batch, n_test, n_classes)\n",
    "    \n",
    "    # 손실 계산을 위해 차원 재정렬\n",
    "    # (batch * n_test, n_classes)와 (batch * n_test)\n",
    "    loss = loss_fn(predicted_log_probs.reshape(-1, N_CLASSES), test_y.reshape(-1))\n",
    "    \n",
    "    # 4. 역전파 및 파라미터 업데이트\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "# 모델 저장\n",
    "torch.save(model.state_dict(), \"simple_pfn_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3d50a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
