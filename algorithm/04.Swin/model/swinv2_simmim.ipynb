{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from box import Box\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import simmim\n",
    "from swin_v2 import SwinTransformerV2\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "from timm.data import Mixup\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MODEL': {'TYPE': 'swinv2',\n",
       "  'NAME': 'simmim_pretrain',\n",
       "  'DROP_PATH_RATE': 0.0,\n",
       "  'SWIN': {'EMBED_DIM': 96,\n",
       "   'DEPTHS': [2, 2, 6, 2],\n",
       "   'NUM_HEADS': [3, 6, 12, 24],\n",
       "   'WINDOW_SIZE': 6,\n",
       "   'PATCH_SIZE': 4}},\n",
       " 'DATA': {'IMG_SIZE': 192,\n",
       "  'MASK_PATCH_SIZE': 32,\n",
       "  'MASK_RATIO': 0.6,\n",
       "  'BATCH_SIZE': 1024,\n",
       "  'NUM_WORKERS': 24,\n",
       "  'DATA_PATH': '../../data/sports'},\n",
       " 'TRAIN': {'EPOCHS': 100,\n",
       "  'WARMUP_EPOCHS': 10,\n",
       "  'BASE_LR': 0.0014,\n",
       "  'WEIGHT_DECAY': 0.05,\n",
       "  'CLIP_GRAD': 5}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simmim_config = yaml.load(open('config/pretrain.yaml'), Loader=yaml.FullLoader)\n",
    "simmim_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_config = {'img_size':simmim_config['DATA']['IMG_SIZE'], \n",
    "                'patch_size':simmim_config['MODEL']['SWIN']['PATCH_SIZE'], \n",
    "                'in_chans':3, \n",
    "                'num_classes':100,\n",
    "                'embed_dim':simmim_config['MODEL']['SWIN']['EMBED_DIM'], \n",
    "                'depths':simmim_config['MODEL']['SWIN']['DEPTHS'], \n",
    "                'num_heads':simmim_config['MODEL']['SWIN']['NUM_HEADS'],           \n",
    "                'window_size':simmim_config['MODEL']['SWIN']['WINDOW_SIZE'], \n",
    "                'mlp_ratio':4., \n",
    "                'qkv_bias':True, \n",
    "                'qk_scale':None,\n",
    "                'drop_rate':0., \n",
    "                'attn_drop_rate':0., \n",
    "                'drop_path_rate':simmim_config['MODEL']['DROP_PATH_RATE'],\n",
    "                'norm_layer':nn.LayerNorm, \n",
    "                'patch_norm':True, \n",
    "                'pretrained_window_sizes':[0,0,0,0],\n",
    "                'ape':True}\n",
    "\n",
    "encoder_stride = 32\n",
    "in_chans = encoder_config['in_chans']\n",
    "patch_size = encoder_config['patch_size']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Load SimMIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "encoder = simmim.SwinTransformerV2ForSimMIM(**encoder_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = simmim.SimMIM( encoder=encoder, \n",
    "                       encoder_stride=encoder_stride, \n",
    "                       in_chans=in_chans, \n",
    "                       patch_size=patch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask Generator Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [1, 1, 0, 1, 1, 0, 1, 0],\n",
       "       [1, 1, 1, 0, 0, 0, 1, 1],\n",
       "       [1, 1, 1, 0, 1, 1, 1, 0],\n",
       "       [1, 1, 0, 1, 1, 1, 1, 1],\n",
       "       [1, 0, 0, 1, 1, 0, 0, 0],\n",
       "       [0, 1, 1, 1, 0, 0, 1, 0],\n",
       "       [0, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_generator = simmim.MaskGenerator(input_size=224,\n",
    "                                      mask_patch_size=28,\n",
    "                                      model_patch_size=28,\n",
    "                                      mask_ratio=.6)\n",
    "mask = mask_generator()\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생성된 mask의 비율은 60.9375%\n"
     ]
    }
   ],
   "source": [
    "print(f\"생성된 mask의 비율은 {mask.sum() / (mask.shape[0]*mask.shape[1])*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SimMIM DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simmim_config = Box(simmim_config)\n",
    "dataloader = simmim.build_loader_simmim(simmim_config)\n",
    "\n",
    "samples = next(iter(dataloader))\n",
    "len(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1024, 3, 192, 192]),\n",
       " torch.Size([1024, 48, 48]),\n",
       " torch.Size([1024]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples[0].shape, samples[1].shape, samples[2].shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Parameters and etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_lr = float(simmim_config.TRAIN.BASE_LR)\n",
    "weight_decay = simmim_config.TRAIN.WEIGHT_DECAY\n",
    "optimizer = optim.AdamW(model.parameters(), lr=base_lr, weight_decay=weight_decay)\n",
    "warmup_epochs = simmim_config.TRAIN.WARMUP_EPOCHS\n",
    "train_epochs = simmim_config.TRAIN.EPOCHS\n",
    "\n",
    "scheduler = transformers.get_cosine_schedule_with_warmup(optimizer, \n",
    "                                                        num_warmup_steps=warmup_epochs*len(dataloader), \n",
    "                                                        num_training_steps=train_epochs*len(dataloader),\n",
    "                                                        num_cycles=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:3'\n",
    "model.to(device)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "model_save = True\n",
    "simmim_path = '../../models/swin2/simmim.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.Train SimMIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 14/14 [00:19<00:00,  1.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.1431, LR: 0.00014000000000000001, Duration: 20.80 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 14/14 [00:16<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.0948, LR: 0.00028000000000000003, Duration: 18.31 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 14/14 [00:17<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.0480, LR: 0.00041999999999999996, Duration: 18.71 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 14/14 [00:16<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.9340, LR: 0.0005600000000000001, Duration: 18.18 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 14/14 [00:17<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.8531, LR: 0.0007, Duration: 19.26 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 14/14 [00:16<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.8270, LR: 0.0008399999999999999, Duration: 18.23 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 14/14 [00:16<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.7686, LR: 0.00098, Duration: 18.27 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 14/14 [00:17<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.7524, LR: 0.0011200000000000001, Duration: 19.08 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 14/14 [00:16<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.7315, LR: 0.00126, Duration: 18.45 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 14/14 [00:17<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.7071, LR: 0.0014, Duration: 19.07 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 14/14 [00:18<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.6956, LR: 0.001399573578913367, Duration: 19.40 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 14/14 [00:17<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.6847, LR: 0.001398294835181877, Duration: 19.29 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 14/14 [00:17<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.6833, LR: 0.0013961653267577914, Duration: 18.49 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 14/14 [00:16<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.6623, LR: 0.0013931876481190993, Duration: 17.99 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 14/14 [00:17<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.6521, LR: 0.0013893654271085456, Duration: 19.18 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 14/14 [00:16<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.6384, LR: 0.001384703320513664, Duration: 18.03 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 14/14 [00:18<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.6315, LR: 0.0013792070083931975, Duration: 19.42 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 14/14 [00:18<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.6190, LR: 0.0013728831871568231, Duration: 19.62 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 14/14 [00:17<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.6035, LR: 0.0013657395614066075, Duration: 19.10 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 14/14 [00:16<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.6075, LR: 0.001357784834550136, Duration: 17.82 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 14/14 [00:17<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5977, LR: 0.0013490286981967512, Duration: 19.43 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 14/14 [00:18<00:00,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5994, LR: 0.0013394818203498204, Duration: 19.08 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23: 100%|██████████| 14/14 [00:16<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5893, LR: 0.0013291558324094168, Duration: 17.90 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 14/14 [00:17<00:00,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5946, LR: 0.0013180633150012488, Duration: 18.77 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 25: 100%|██████████| 14/14 [00:17<00:00,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5932, LR: 0.0013062177826491071, Duration: 18.91 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26: 100%|██████████| 14/14 [00:16<00:00,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5922, LR: 0.001293633667309498, Duration: 17.75 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27: 100%|██████████| 14/14 [00:18<00:00,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5905, LR: 0.001280326300788529, Duration: 19.24 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28: 100%|██████████| 14/14 [00:17<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5792, LR: 0.0012663118960624632, Duration: 18.83 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 14/14 [00:17<00:00,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5897, LR: 0.0012516075275247052, Duration: 18.90 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 30: 100%|██████████| 14/14 [00:16<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5771, LR: 0.0012362311101832846, Duration: 18.40 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|██████████| 14/14 [00:17<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5723, LR: 0.001220201377834176, Duration: 19.26 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|██████████| 14/14 [00:17<00:00,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5776, LR: 0.0012035378602370558, Duration: 18.77 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 33: 100%|██████████| 14/14 [00:18<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5718, LR: 0.0011862608593212981, Duration: 19.73 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|██████████| 14/14 [00:17<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5701, LR: 0.0011683914244512007, Duration: 18.63 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|██████████| 14/14 [00:17<00:00,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5737, LR: 0.0011499513267805774, Duration: 18.99 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 36: 100%|██████████| 14/14 [00:17<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5694, LR: 0.0011309630327279608, Duration: 18.95 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37: 100%|██████████| 14/14 [00:17<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5686, LR: 0.0011114496766047313, Duration: 18.53 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|██████████| 14/14 [00:16<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5640, LR: 0.0010914350324295228, Duration: 18.14 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|██████████| 14/14 [00:18<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5606, LR: 0.0010709434849632434, Duration: 19.64 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40: 100%|██████████| 14/14 [00:16<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5555, LR: 0.00105, Duration: 18.23 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41: 100%|██████████| 14/14 [00:17<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5513, LR: 0.0010286300939501235, Duration: 19.22 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42: 100%|██████████| 14/14 [00:17<00:00,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5524, LR: 0.001006859802752354, Duration: 19.17 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 43: 100%|██████████| 14/14 [00:17<00:00,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5520, LR: 0.0009847156501530602, Duration: 18.76 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 44: 100%|██████████| 14/14 [00:17<00:00,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5520, LR: 0.0009622246153911386, Duration: 19.18 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 45: 100%|██████████| 14/14 [00:17<00:00,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5608, LR: 0.0009394141003279682, Duration: 19.07 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 46: 100%|██████████| 14/14 [00:16<00:00,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5657, LR: 0.0009163118960624632, Duration: 18.10 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 47: 100%|██████████| 14/14 [00:17<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5494, LR: 0.0008929461490718994, Duration: 18.90 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48: 100%|██████████| 14/14 [00:17<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5461, LR: 0.0008693453269197673, Duration: 18.92 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 14/14 [00:17<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5436, LR: 0.0008455381835724314, Duration: 19.28 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50: 100%|██████████| 14/14 [00:18<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5433, LR: 0.0008215537243668514, Duration: 19.60 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51: 100%|██████████| 14/14 [00:16<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5417, LR: 0.0007974211706720458, Duration: 18.22 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52: 100%|██████████| 14/14 [00:16<00:00,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5434, LR: 0.0007731699242873575, Duration: 17.62 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 53: 100%|██████████| 14/14 [00:18<00:00,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5431, LR: 0.0007488295316208876, Duration: 19.35 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 54: 100%|██████████| 14/14 [00:17<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5378, LR: 0.0007244296476917508, Duration: 18.86 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55: 100%|██████████| 14/14 [00:16<00:00,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5424, LR: 0.0007, Duration: 18.14 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 56: 100%|██████████| 14/14 [00:17<00:00,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5508, LR: 0.0006755703523082495, Duration: 18.60 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 57: 100%|██████████| 14/14 [00:18<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5443, LR: 0.0006511704683791123, Duration: 19.53 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58: 100%|██████████| 14/14 [00:18<00:00,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5384, LR: 0.0006268300757126426, Duration: 19.36 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 59: 100%|██████████| 14/14 [00:18<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5340, LR: 0.0006025788293279544, Duration: 19.71 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60: 100%|██████████| 14/14 [00:17<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5321, LR: 0.0005784462756331488, Duration: 18.53 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61: 100%|██████████| 14/14 [00:17<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5317, LR: 0.0005544618164275686, Duration: 18.94 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62: 100%|██████████| 14/14 [00:16<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5306, LR: 0.0005306546730802327, Duration: 18.01 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63: 100%|██████████| 14/14 [00:18<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5293, LR: 0.0005070538509281006, Duration: 19.52 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64: 100%|██████████| 14/14 [00:16<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5289, LR: 0.0004836881039375369, Duration: 18.33 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65: 100%|██████████| 14/14 [00:16<00:00,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5300, LR: 0.0004605858996720319, Duration: 17.92 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 66: 100%|██████████| 14/14 [00:16<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5268, LR: 0.0004377753846088615, Duration: 18.08 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67: 100%|██████████| 14/14 [00:16<00:00,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5269, LR: 0.00041528434984693997, Duration: 17.91 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 68: 100%|██████████| 14/14 [00:17<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5264, LR: 0.00039314019724764573, Duration: 18.55 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69: 100%|██████████| 14/14 [00:17<00:00,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5269, LR: 0.00037136990604987665, Duration: 18.14 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 70: 100%|██████████| 14/14 [00:18<00:00,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5282, LR: 0.00035000000000000016, Duration: 19.32 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 71: 100%|██████████| 14/14 [00:17<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5256, LR: 0.00032905651503675667, Duration: 19.32 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72: 100%|██████████| 14/14 [00:17<00:00,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5257, LR: 0.0003085649675704773, Duration: 18.38 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 73: 100%|██████████| 14/14 [00:18<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5244, LR: 0.0002885503233952689, Duration: 19.74 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 74: 100%|██████████| 14/14 [00:17<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5243, LR: 0.0002690369672720392, Duration: 19.29 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75: 100%|██████████| 14/14 [00:17<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5218, LR: 0.00025004867321942243, Duration: 18.66 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 76: 100%|██████████| 14/14 [00:17<00:00,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5222, LR: 0.00023160857554879947, Duration: 18.90 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 77: 100%|██████████| 14/14 [00:17<00:00,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5220, LR: 0.00021373914067870185, Duration: 18.83 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 78: 100%|██████████| 14/14 [00:16<00:00,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5219, LR: 0.00019646213976294433, Duration: 17.80 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 79: 100%|██████████| 14/14 [00:18<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5215, LR: 0.00017979862216582396, Duration: 19.36 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 80: 100%|██████████| 14/14 [00:16<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5205, LR: 0.00016376888981671546, Duration: 18.23 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 81: 100%|██████████| 14/14 [00:18<00:00,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5209, LR: 0.00014839247247529466, Duration: 19.33 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 82: 100%|██████████| 14/14 [00:18<00:00,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5195, LR: 0.00013368810393753685, Duration: 20.06 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 83: 100%|██████████| 14/14 [00:18<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5193, LR: 0.00011967369921147086, Duration: 19.76 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 84: 100%|██████████| 14/14 [00:18<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5185, LR: 0.00010636633269050183, Duration: 19.81 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 85: 100%|██████████| 14/14 [00:17<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5190, LR: 9.37822173508929e-05, Duration: 18.45 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 86: 100%|██████████| 14/14 [00:16<00:00,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5186, LR: 8.19366849987511e-05, Duration: 17.74 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 87: 100%|██████████| 14/14 [00:16<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5181, LR: 7.084416759058323e-05, Duration: 18.28 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 88: 100%|██████████| 14/14 [00:18<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5180, LR: 6.0518179650179314e-05, Duration: 19.41 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 89: 100%|██████████| 14/14 [00:18<00:00,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5187, LR: 5.097130180324888e-05, Duration: 19.24 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 90: 100%|██████████| 14/14 [00:17<00:00,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5184, LR: 4.221516544986418e-05, Duration: 18.78 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 91: 100%|██████████| 14/14 [00:18<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5172, LR: 3.426043859339253e-05, Duration: 19.71 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 92: 100%|██████████| 14/14 [00:16<00:00,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5179, LR: 2.7116812843176773e-05, Duration: 17.86 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 93: 100%|██████████| 14/14 [00:17<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5165, LR: 2.0792991606802468e-05, Duration: 19.20 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 94: 100%|██████████| 14/14 [00:17<00:00,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5176, LR: 1.5296679486336016e-05, Duration: 18.48 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 95: 100%|██████████| 14/14 [00:17<00:00,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5168, LR: 1.0634572891454386e-05, Duration: 18.23 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 96: 100%|██████████| 14/14 [00:17<00:00,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5180, LR: 6.812351880900747e-06, Duration: 18.67 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 97: 100%|██████████| 14/14 [00:18<00:00,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5176, LR: 3.834673242208697e-06, Duration: 19.42 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 98: 100%|██████████| 14/14 [00:16<00:00,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5173, LR: 1.7051648181230617e-06, Duration: 17.74 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 99: 100%|██████████| 14/14 [00:17<00:00,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5166, LR: 4.264210866329665e-07, Duration: 18.66 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 100: 100%|██████████| 14/14 [00:17<00:00,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5177, LR: 0.0, Duration: 18.38 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "training_time = 0\n",
    "losses = []\n",
    "val_losses = []\n",
    "lrs = []\n",
    "best_loss = float('inf')\n",
    "\n",
    "# GradScaler 초기화\n",
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in range(train_epochs):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f\"Epoch {epoch + 1}\")\n",
    "    \n",
    "    for _, data in pbar:\n",
    "        image, mask = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # AutoCast 적용\n",
    "        with autocast():\n",
    "            loss = model(image, mask)\n",
    "            \n",
    "        # 스케일링된 그라디언트 계산\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # 그라디언트 클리핑 전에 스케일링 제거\n",
    "        scaler.unscale_(optimizer)\n",
    "        if simmim_config.TRAIN.CLIP_GRAD:\n",
    "            clip_grad_norm_(model.parameters(), max_norm=simmim_config.TRAIN.CLIP_GRAD)\n",
    "        else:\n",
    "            clip_grad_norm_(model.parameters())\n",
    "\n",
    "        # 옵티마이저 스텝 및 스케일러 업데이트\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "            \n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        lrs.append(lr)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    losses.append(epoch_loss)\n",
    "\n",
    "    # 모델 저장\n",
    "    if epoch_loss < best_loss:\n",
    "        \n",
    "        best_loss = epoch_loss\n",
    "        model_save = model_save\n",
    "        if model_save:\n",
    "            torch.save(model.state_dict(), simmim_path)\n",
    "        \n",
    "    epoch_duration = time.time() - start_time\n",
    "    training_time += epoch_duration\n",
    "    \n",
    "    text = f'\\tLoss: {epoch_loss:.4f}, LR: {lr}, Duration: {epoch_duration:.2f} sec'\n",
    "    \n",
    "    if model_save:\n",
    "        text += f' - model saved!'\n",
    "        model_save = False    \n",
    "        \n",
    "    print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Del SimMIM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cpu()\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.Load Swin V2 for stage-2 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025831440/work/aten/src/ATen/native/TensorShape.cpp:3549.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "odict_keys(['absolute_pos_embed', 'embeddings.patch_embeddings.weight', 'embeddings.patch_embeddings.bias', 'embeddings.norm.weight', 'embeddings.norm.bias', 'stages.0.blocks.0.attn_mask', 'stages.0.blocks.0.attn.t_scale', 'stages.0.blocks.0.attn.relative_coords_table', 'stages.0.blocks.0.attn.relative_position_index', 'stages.0.blocks.0.attn.crpb_mlp.0.weight', 'stages.0.blocks.0.attn.crpb_mlp.0.bias', 'stages.0.blocks.0.attn.crpb_mlp.3.weight', 'stages.0.blocks.0.attn.qkv.weight', 'stages.0.blocks.0.attn.qkv.bias', 'stages.0.blocks.0.attn.proj.weight', 'stages.0.blocks.0.attn.proj.bias', 'stages.0.blocks.0.norm1.weight', 'stages.0.blocks.0.norm1.bias', 'stages.0.blocks.0.ls1.gamma', 'stages.0.blocks.0.mlp.fc1.weight', 'stages.0.blocks.0.mlp.fc1.bias', 'stages.0.blocks.0.mlp.fc2.weight', 'stages.0.blocks.0.mlp.fc2.bias', 'stages.0.blocks.0.norm2.weight', 'stages.0.blocks.0.norm2.bias', 'stages.0.blocks.0.ls2.gamma', 'stages.0.blocks.1.attn_mask', 'stages.0.blocks.1.attn.t_scale', 'stages.0.blocks.1.attn.relative_coords_table', 'stages.0.blocks.1.attn.relative_position_index', 'stages.0.blocks.1.attn.crpb_mlp.0.weight', 'stages.0.blocks.1.attn.crpb_mlp.0.bias', 'stages.0.blocks.1.attn.crpb_mlp.3.weight', 'stages.0.blocks.1.attn.qkv.weight', 'stages.0.blocks.1.attn.qkv.bias', 'stages.0.blocks.1.attn.proj.weight', 'stages.0.blocks.1.attn.proj.bias', 'stages.0.blocks.1.norm1.weight', 'stages.0.blocks.1.norm1.bias', 'stages.0.blocks.1.ls1.gamma', 'stages.0.blocks.1.mlp.fc1.weight', 'stages.0.blocks.1.mlp.fc1.bias', 'stages.0.blocks.1.mlp.fc2.weight', 'stages.0.blocks.1.mlp.fc2.bias', 'stages.0.blocks.1.norm2.weight', 'stages.0.blocks.1.norm2.bias', 'stages.0.blocks.1.ls2.gamma', 'stages.0.downsample.reduction.weight', 'stages.0.downsample.norm.weight', 'stages.0.downsample.norm.bias', 'stages.1.blocks.0.attn_mask', 'stages.1.blocks.0.attn.t_scale', 'stages.1.blocks.0.attn.relative_coords_table', 'stages.1.blocks.0.attn.relative_position_index', 'stages.1.blocks.0.attn.crpb_mlp.0.weight', 'stages.1.blocks.0.attn.crpb_mlp.0.bias', 'stages.1.blocks.0.attn.crpb_mlp.3.weight', 'stages.1.blocks.0.attn.qkv.weight', 'stages.1.blocks.0.attn.qkv.bias', 'stages.1.blocks.0.attn.proj.weight', 'stages.1.blocks.0.attn.proj.bias', 'stages.1.blocks.0.norm1.weight', 'stages.1.blocks.0.norm1.bias', 'stages.1.blocks.0.ls1.gamma', 'stages.1.blocks.0.mlp.fc1.weight', 'stages.1.blocks.0.mlp.fc1.bias', 'stages.1.blocks.0.mlp.fc2.weight', 'stages.1.blocks.0.mlp.fc2.bias', 'stages.1.blocks.0.norm2.weight', 'stages.1.blocks.0.norm2.bias', 'stages.1.blocks.0.ls2.gamma', 'stages.1.blocks.1.attn_mask', 'stages.1.blocks.1.attn.t_scale', 'stages.1.blocks.1.attn.relative_coords_table', 'stages.1.blocks.1.attn.relative_position_index', 'stages.1.blocks.1.attn.crpb_mlp.0.weight', 'stages.1.blocks.1.attn.crpb_mlp.0.bias', 'stages.1.blocks.1.attn.crpb_mlp.3.weight', 'stages.1.blocks.1.attn.qkv.weight', 'stages.1.blocks.1.attn.qkv.bias', 'stages.1.blocks.1.attn.proj.weight', 'stages.1.blocks.1.attn.proj.bias', 'stages.1.blocks.1.norm1.weight', 'stages.1.blocks.1.norm1.bias', 'stages.1.blocks.1.ls1.gamma', 'stages.1.blocks.1.mlp.fc1.weight', 'stages.1.blocks.1.mlp.fc1.bias', 'stages.1.blocks.1.mlp.fc2.weight', 'stages.1.blocks.1.mlp.fc2.bias', 'stages.1.blocks.1.norm2.weight', 'stages.1.blocks.1.norm2.bias', 'stages.1.blocks.1.ls2.gamma', 'stages.1.downsample.reduction.weight', 'stages.1.downsample.norm.weight', 'stages.1.downsample.norm.bias', 'stages.2.blocks.0.attn_mask', 'stages.2.blocks.0.attn.t_scale', 'stages.2.blocks.0.attn.relative_coords_table', 'stages.2.blocks.0.attn.relative_position_index', 'stages.2.blocks.0.attn.crpb_mlp.0.weight', 'stages.2.blocks.0.attn.crpb_mlp.0.bias', 'stages.2.blocks.0.attn.crpb_mlp.3.weight', 'stages.2.blocks.0.attn.qkv.weight', 'stages.2.blocks.0.attn.qkv.bias', 'stages.2.blocks.0.attn.proj.weight', 'stages.2.blocks.0.attn.proj.bias', 'stages.2.blocks.0.norm1.weight', 'stages.2.blocks.0.norm1.bias', 'stages.2.blocks.0.ls1.gamma', 'stages.2.blocks.0.mlp.fc1.weight', 'stages.2.blocks.0.mlp.fc1.bias', 'stages.2.blocks.0.mlp.fc2.weight', 'stages.2.blocks.0.mlp.fc2.bias', 'stages.2.blocks.0.norm2.weight', 'stages.2.blocks.0.norm2.bias', 'stages.2.blocks.0.ls2.gamma', 'stages.2.blocks.1.attn_mask', 'stages.2.blocks.1.attn.t_scale', 'stages.2.blocks.1.attn.relative_coords_table', 'stages.2.blocks.1.attn.relative_position_index', 'stages.2.blocks.1.attn.crpb_mlp.0.weight', 'stages.2.blocks.1.attn.crpb_mlp.0.bias', 'stages.2.blocks.1.attn.crpb_mlp.3.weight', 'stages.2.blocks.1.attn.qkv.weight', 'stages.2.blocks.1.attn.qkv.bias', 'stages.2.blocks.1.attn.proj.weight', 'stages.2.blocks.1.attn.proj.bias', 'stages.2.blocks.1.norm1.weight', 'stages.2.blocks.1.norm1.bias', 'stages.2.blocks.1.ls1.gamma', 'stages.2.blocks.1.mlp.fc1.weight', 'stages.2.blocks.1.mlp.fc1.bias', 'stages.2.blocks.1.mlp.fc2.weight', 'stages.2.blocks.1.mlp.fc2.bias', 'stages.2.blocks.1.norm2.weight', 'stages.2.blocks.1.norm2.bias', 'stages.2.blocks.1.ls2.gamma', 'stages.2.blocks.2.attn_mask', 'stages.2.blocks.2.attn.t_scale', 'stages.2.blocks.2.attn.relative_coords_table', 'stages.2.blocks.2.attn.relative_position_index', 'stages.2.blocks.2.attn.crpb_mlp.0.weight', 'stages.2.blocks.2.attn.crpb_mlp.0.bias', 'stages.2.blocks.2.attn.crpb_mlp.3.weight', 'stages.2.blocks.2.attn.qkv.weight', 'stages.2.blocks.2.attn.qkv.bias', 'stages.2.blocks.2.attn.proj.weight', 'stages.2.blocks.2.attn.proj.bias', 'stages.2.blocks.2.norm1.weight', 'stages.2.blocks.2.norm1.bias', 'stages.2.blocks.2.ls1.gamma', 'stages.2.blocks.2.mlp.fc1.weight', 'stages.2.blocks.2.mlp.fc1.bias', 'stages.2.blocks.2.mlp.fc2.weight', 'stages.2.blocks.2.mlp.fc2.bias', 'stages.2.blocks.2.norm2.weight', 'stages.2.blocks.2.norm2.bias', 'stages.2.blocks.2.ls2.gamma', 'stages.2.blocks.3.attn_mask', 'stages.2.blocks.3.attn.t_scale', 'stages.2.blocks.3.attn.relative_coords_table', 'stages.2.blocks.3.attn.relative_position_index', 'stages.2.blocks.3.attn.crpb_mlp.0.weight', 'stages.2.blocks.3.attn.crpb_mlp.0.bias', 'stages.2.blocks.3.attn.crpb_mlp.3.weight', 'stages.2.blocks.3.attn.qkv.weight', 'stages.2.blocks.3.attn.qkv.bias', 'stages.2.blocks.3.attn.proj.weight', 'stages.2.blocks.3.attn.proj.bias', 'stages.2.blocks.3.norm1.weight', 'stages.2.blocks.3.norm1.bias', 'stages.2.blocks.3.ls1.gamma', 'stages.2.blocks.3.mlp.fc1.weight', 'stages.2.blocks.3.mlp.fc1.bias', 'stages.2.blocks.3.mlp.fc2.weight', 'stages.2.blocks.3.mlp.fc2.bias', 'stages.2.blocks.3.norm2.weight', 'stages.2.blocks.3.norm2.bias', 'stages.2.blocks.3.ls2.gamma', 'stages.2.blocks.4.attn_mask', 'stages.2.blocks.4.attn.t_scale', 'stages.2.blocks.4.attn.relative_coords_table', 'stages.2.blocks.4.attn.relative_position_index', 'stages.2.blocks.4.attn.crpb_mlp.0.weight', 'stages.2.blocks.4.attn.crpb_mlp.0.bias', 'stages.2.blocks.4.attn.crpb_mlp.3.weight', 'stages.2.blocks.4.attn.qkv.weight', 'stages.2.blocks.4.attn.qkv.bias', 'stages.2.blocks.4.attn.proj.weight', 'stages.2.blocks.4.attn.proj.bias', 'stages.2.blocks.4.norm1.weight', 'stages.2.blocks.4.norm1.bias', 'stages.2.blocks.4.ls1.gamma', 'stages.2.blocks.4.mlp.fc1.weight', 'stages.2.blocks.4.mlp.fc1.bias', 'stages.2.blocks.4.mlp.fc2.weight', 'stages.2.blocks.4.mlp.fc2.bias', 'stages.2.blocks.4.norm2.weight', 'stages.2.blocks.4.norm2.bias', 'stages.2.blocks.4.ls2.gamma', 'stages.2.blocks.5.attn_mask', 'stages.2.blocks.5.attn.t_scale', 'stages.2.blocks.5.attn.relative_coords_table', 'stages.2.blocks.5.attn.relative_position_index', 'stages.2.blocks.5.attn.crpb_mlp.0.weight', 'stages.2.blocks.5.attn.crpb_mlp.0.bias', 'stages.2.blocks.5.attn.crpb_mlp.3.weight', 'stages.2.blocks.5.attn.qkv.weight', 'stages.2.blocks.5.attn.qkv.bias', 'stages.2.blocks.5.attn.proj.weight', 'stages.2.blocks.5.attn.proj.bias', 'stages.2.blocks.5.norm1.weight', 'stages.2.blocks.5.norm1.bias', 'stages.2.blocks.5.ls1.gamma', 'stages.2.blocks.5.mlp.fc1.weight', 'stages.2.blocks.5.mlp.fc1.bias', 'stages.2.blocks.5.mlp.fc2.weight', 'stages.2.blocks.5.mlp.fc2.bias', 'stages.2.blocks.5.norm2.weight', 'stages.2.blocks.5.norm2.bias', 'stages.2.blocks.5.ls2.gamma', 'stages.2.downsample.reduction.weight', 'stages.2.downsample.norm.weight', 'stages.2.downsample.norm.bias', 'stages.3.blocks.0.attn_mask', 'stages.3.blocks.0.attn.t_scale', 'stages.3.blocks.0.attn.relative_coords_table', 'stages.3.blocks.0.attn.relative_position_index', 'stages.3.blocks.0.attn.crpb_mlp.0.weight', 'stages.3.blocks.0.attn.crpb_mlp.0.bias', 'stages.3.blocks.0.attn.crpb_mlp.3.weight', 'stages.3.blocks.0.attn.qkv.weight', 'stages.3.blocks.0.attn.qkv.bias', 'stages.3.blocks.0.attn.proj.weight', 'stages.3.blocks.0.attn.proj.bias', 'stages.3.blocks.0.norm1.weight', 'stages.3.blocks.0.norm1.bias', 'stages.3.blocks.0.ls1.gamma', 'stages.3.blocks.0.mlp.fc1.weight', 'stages.3.blocks.0.mlp.fc1.bias', 'stages.3.blocks.0.mlp.fc2.weight', 'stages.3.blocks.0.mlp.fc2.bias', 'stages.3.blocks.0.norm2.weight', 'stages.3.blocks.0.norm2.bias', 'stages.3.blocks.0.ls2.gamma', 'stages.3.blocks.1.attn_mask', 'stages.3.blocks.1.attn.t_scale', 'stages.3.blocks.1.attn.relative_coords_table', 'stages.3.blocks.1.attn.relative_position_index', 'stages.3.blocks.1.attn.crpb_mlp.0.weight', 'stages.3.blocks.1.attn.crpb_mlp.0.bias', 'stages.3.blocks.1.attn.crpb_mlp.3.weight', 'stages.3.blocks.1.attn.qkv.weight', 'stages.3.blocks.1.attn.qkv.bias', 'stages.3.blocks.1.attn.proj.weight', 'stages.3.blocks.1.attn.proj.bias', 'stages.3.blocks.1.norm1.weight', 'stages.3.blocks.1.norm1.bias', 'stages.3.blocks.1.ls1.gamma', 'stages.3.blocks.1.mlp.fc1.weight', 'stages.3.blocks.1.mlp.fc1.bias', 'stages.3.blocks.1.mlp.fc2.weight', 'stages.3.blocks.1.mlp.fc2.bias', 'stages.3.blocks.1.norm2.weight', 'stages.3.blocks.1.norm2.bias', 'stages.3.blocks.1.ls2.gamma', 'layernorm.weight', 'layernorm.bias', 'classifier.weight', 'classifier.bias'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SwinTransformerV2(pretrained_window_sizes=[6,6,6,6], ape=True, drop_path_rate=0.3)\n",
    "model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 96, 56, 56]           4,704\n",
      "         LayerNorm-2             [-1, 3136, 96]             192\n",
      "        embeddings-3             [-1, 3136, 96]               0\n",
      "           Dropout-4             [-1, 3136, 96]               0\n",
      "            Linear-5              [-1, 49, 288]          27,936\n",
      "            Linear-6          [-1, 13, 13, 384]           1,152\n",
      "              ReLU-7          [-1, 13, 13, 384]               0\n",
      "           Dropout-8          [-1, 13, 13, 384]               0\n",
      "            Linear-9            [-1, 13, 13, 3]           1,152\n",
      "          Softmax-10            [-1, 3, 49, 49]               0\n",
      "          Dropout-11            [-1, 3, 49, 49]               0\n",
      "           Linear-12               [-1, 49, 96]           9,312\n",
      "          Dropout-13               [-1, 49, 96]               0\n",
      "  WindowAttention-14               [-1, 49, 96]               0\n",
      "       LayerScale-15             [-1, 3136, 96]               0\n",
      "         DropPath-16             [-1, 3136, 96]               0\n",
      "        LayerNorm-17             [-1, 3136, 96]             192\n",
      "           Linear-18            [-1, 3136, 384]          37,248\n",
      "             GELU-19            [-1, 3136, 384]               0\n",
      "           Linear-20             [-1, 3136, 96]          36,960\n",
      "          Dropout-21             [-1, 3136, 96]               0\n",
      "              Mlp-22             [-1, 3136, 96]               0\n",
      "       LayerScale-23             [-1, 3136, 96]               0\n",
      "         DropPath-24             [-1, 3136, 96]               0\n",
      "        LayerNorm-25             [-1, 3136, 96]             192\n",
      "SwinTransformerBlock-26             [-1, 3136, 96]               0\n",
      "           Linear-27              [-1, 49, 288]          27,936\n",
      "           Linear-28          [-1, 13, 13, 384]           1,152\n",
      "             ReLU-29          [-1, 13, 13, 384]               0\n",
      "          Dropout-30          [-1, 13, 13, 384]               0\n",
      "           Linear-31            [-1, 13, 13, 3]           1,152\n",
      "          Softmax-32            [-1, 3, 49, 49]               0\n",
      "          Dropout-33            [-1, 3, 49, 49]               0\n",
      "           Linear-34               [-1, 49, 96]           9,312\n",
      "          Dropout-35               [-1, 49, 96]               0\n",
      "  WindowAttention-36               [-1, 49, 96]               0\n",
      "       LayerScale-37             [-1, 3136, 96]               0\n",
      "         DropPath-38             [-1, 3136, 96]               0\n",
      "        LayerNorm-39             [-1, 3136, 96]             192\n",
      "           Linear-40            [-1, 3136, 384]          37,248\n",
      "             GELU-41            [-1, 3136, 384]               0\n",
      "           Linear-42             [-1, 3136, 96]          36,960\n",
      "          Dropout-43             [-1, 3136, 96]               0\n",
      "              Mlp-44             [-1, 3136, 96]               0\n",
      "       LayerScale-45             [-1, 3136, 96]               0\n",
      "         DropPath-46             [-1, 3136, 96]               0\n",
      "        LayerNorm-47             [-1, 3136, 96]             192\n",
      "SwinTransformerBlock-48             [-1, 3136, 96]               0\n",
      "        LayerNorm-49             [-1, 784, 384]             768\n",
      "           Linear-50             [-1, 784, 192]          73,728\n",
      "     PatchMerging-51             [-1, 784, 192]               0\n",
      "       StageLayer-52             [-1, 784, 192]               0\n",
      "           Linear-53              [-1, 49, 576]         111,168\n",
      "           Linear-54          [-1, 13, 13, 384]           1,152\n",
      "             ReLU-55          [-1, 13, 13, 384]               0\n",
      "          Dropout-56          [-1, 13, 13, 384]               0\n",
      "           Linear-57            [-1, 13, 13, 6]           2,304\n",
      "          Softmax-58            [-1, 6, 49, 49]               0\n",
      "          Dropout-59            [-1, 6, 49, 49]               0\n",
      "           Linear-60              [-1, 49, 192]          37,056\n",
      "          Dropout-61              [-1, 49, 192]               0\n",
      "  WindowAttention-62              [-1, 49, 192]               0\n",
      "       LayerScale-63             [-1, 784, 192]               0\n",
      "         DropPath-64             [-1, 784, 192]               0\n",
      "        LayerNorm-65             [-1, 784, 192]             384\n",
      "           Linear-66             [-1, 784, 768]         148,224\n",
      "             GELU-67             [-1, 784, 768]               0\n",
      "           Linear-68             [-1, 784, 192]         147,648\n",
      "          Dropout-69             [-1, 784, 192]               0\n",
      "              Mlp-70             [-1, 784, 192]               0\n",
      "       LayerScale-71             [-1, 784, 192]               0\n",
      "         DropPath-72             [-1, 784, 192]               0\n",
      "        LayerNorm-73             [-1, 784, 192]             384\n",
      "SwinTransformerBlock-74             [-1, 784, 192]               0\n",
      "           Linear-75              [-1, 49, 576]         111,168\n",
      "           Linear-76          [-1, 13, 13, 384]           1,152\n",
      "             ReLU-77          [-1, 13, 13, 384]               0\n",
      "          Dropout-78          [-1, 13, 13, 384]               0\n",
      "           Linear-79            [-1, 13, 13, 6]           2,304\n",
      "          Softmax-80            [-1, 6, 49, 49]               0\n",
      "          Dropout-81            [-1, 6, 49, 49]               0\n",
      "           Linear-82              [-1, 49, 192]          37,056\n",
      "          Dropout-83              [-1, 49, 192]               0\n",
      "  WindowAttention-84              [-1, 49, 192]               0\n",
      "       LayerScale-85             [-1, 784, 192]               0\n",
      "         DropPath-86             [-1, 784, 192]               0\n",
      "        LayerNorm-87             [-1, 784, 192]             384\n",
      "           Linear-88             [-1, 784, 768]         148,224\n",
      "             GELU-89             [-1, 784, 768]               0\n",
      "           Linear-90             [-1, 784, 192]         147,648\n",
      "          Dropout-91             [-1, 784, 192]               0\n",
      "              Mlp-92             [-1, 784, 192]               0\n",
      "       LayerScale-93             [-1, 784, 192]               0\n",
      "         DropPath-94             [-1, 784, 192]               0\n",
      "        LayerNorm-95             [-1, 784, 192]             384\n",
      "SwinTransformerBlock-96             [-1, 784, 192]               0\n",
      "        LayerNorm-97             [-1, 196, 768]           1,536\n",
      "           Linear-98             [-1, 196, 384]         294,912\n",
      "     PatchMerging-99             [-1, 196, 384]               0\n",
      "      StageLayer-100             [-1, 196, 384]               0\n",
      "          Linear-101             [-1, 49, 1152]         443,520\n",
      "          Linear-102          [-1, 13, 13, 384]           1,152\n",
      "            ReLU-103          [-1, 13, 13, 384]               0\n",
      "         Dropout-104          [-1, 13, 13, 384]               0\n",
      "          Linear-105           [-1, 13, 13, 12]           4,608\n",
      "         Softmax-106           [-1, 12, 49, 49]               0\n",
      "         Dropout-107           [-1, 12, 49, 49]               0\n",
      "          Linear-108              [-1, 49, 384]         147,840\n",
      "         Dropout-109              [-1, 49, 384]               0\n",
      " WindowAttention-110              [-1, 49, 384]               0\n",
      "      LayerScale-111             [-1, 196, 384]               0\n",
      "        DropPath-112             [-1, 196, 384]               0\n",
      "       LayerNorm-113             [-1, 196, 384]             768\n",
      "          Linear-114            [-1, 196, 1536]         591,360\n",
      "            GELU-115            [-1, 196, 1536]               0\n",
      "          Linear-116             [-1, 196, 384]         590,208\n",
      "         Dropout-117             [-1, 196, 384]               0\n",
      "             Mlp-118             [-1, 196, 384]               0\n",
      "      LayerScale-119             [-1, 196, 384]               0\n",
      "        DropPath-120             [-1, 196, 384]               0\n",
      "       LayerNorm-121             [-1, 196, 384]             768\n",
      "SwinTransformerBlock-122             [-1, 196, 384]               0\n",
      "          Linear-123             [-1, 49, 1152]         443,520\n",
      "          Linear-124          [-1, 13, 13, 384]           1,152\n",
      "            ReLU-125          [-1, 13, 13, 384]               0\n",
      "         Dropout-126          [-1, 13, 13, 384]               0\n",
      "          Linear-127           [-1, 13, 13, 12]           4,608\n",
      "         Softmax-128           [-1, 12, 49, 49]               0\n",
      "         Dropout-129           [-1, 12, 49, 49]               0\n",
      "          Linear-130              [-1, 49, 384]         147,840\n",
      "         Dropout-131              [-1, 49, 384]               0\n",
      " WindowAttention-132              [-1, 49, 384]               0\n",
      "      LayerScale-133             [-1, 196, 384]               0\n",
      "        DropPath-134             [-1, 196, 384]               0\n",
      "       LayerNorm-135             [-1, 196, 384]             768\n",
      "          Linear-136            [-1, 196, 1536]         591,360\n",
      "            GELU-137            [-1, 196, 1536]               0\n",
      "          Linear-138             [-1, 196, 384]         590,208\n",
      "         Dropout-139             [-1, 196, 384]               0\n",
      "             Mlp-140             [-1, 196, 384]               0\n",
      "      LayerScale-141             [-1, 196, 384]               0\n",
      "        DropPath-142             [-1, 196, 384]               0\n",
      "       LayerNorm-143             [-1, 196, 384]             768\n",
      "SwinTransformerBlock-144             [-1, 196, 384]               0\n",
      "          Linear-145             [-1, 49, 1152]         443,520\n",
      "          Linear-146          [-1, 13, 13, 384]           1,152\n",
      "            ReLU-147          [-1, 13, 13, 384]               0\n",
      "         Dropout-148          [-1, 13, 13, 384]               0\n",
      "          Linear-149           [-1, 13, 13, 12]           4,608\n",
      "         Softmax-150           [-1, 12, 49, 49]               0\n",
      "         Dropout-151           [-1, 12, 49, 49]               0\n",
      "          Linear-152              [-1, 49, 384]         147,840\n",
      "         Dropout-153              [-1, 49, 384]               0\n",
      " WindowAttention-154              [-1, 49, 384]               0\n",
      "      LayerScale-155             [-1, 196, 384]               0\n",
      "        DropPath-156             [-1, 196, 384]               0\n",
      "       LayerNorm-157             [-1, 196, 384]             768\n",
      "          Linear-158            [-1, 196, 1536]         591,360\n",
      "            GELU-159            [-1, 196, 1536]               0\n",
      "          Linear-160             [-1, 196, 384]         590,208\n",
      "         Dropout-161             [-1, 196, 384]               0\n",
      "             Mlp-162             [-1, 196, 384]               0\n",
      "      LayerScale-163             [-1, 196, 384]               0\n",
      "        DropPath-164             [-1, 196, 384]               0\n",
      "       LayerNorm-165             [-1, 196, 384]             768\n",
      "SwinTransformerBlock-166             [-1, 196, 384]               0\n",
      "          Linear-167             [-1, 49, 1152]         443,520\n",
      "          Linear-168          [-1, 13, 13, 384]           1,152\n",
      "            ReLU-169          [-1, 13, 13, 384]               0\n",
      "         Dropout-170          [-1, 13, 13, 384]               0\n",
      "          Linear-171           [-1, 13, 13, 12]           4,608\n",
      "         Softmax-172           [-1, 12, 49, 49]               0\n",
      "         Dropout-173           [-1, 12, 49, 49]               0\n",
      "          Linear-174              [-1, 49, 384]         147,840\n",
      "         Dropout-175              [-1, 49, 384]               0\n",
      " WindowAttention-176              [-1, 49, 384]               0\n",
      "      LayerScale-177             [-1, 196, 384]               0\n",
      "        DropPath-178             [-1, 196, 384]               0\n",
      "       LayerNorm-179             [-1, 196, 384]             768\n",
      "          Linear-180            [-1, 196, 1536]         591,360\n",
      "            GELU-181            [-1, 196, 1536]               0\n",
      "          Linear-182             [-1, 196, 384]         590,208\n",
      "         Dropout-183             [-1, 196, 384]               0\n",
      "             Mlp-184             [-1, 196, 384]               0\n",
      "      LayerScale-185             [-1, 196, 384]               0\n",
      "        DropPath-186             [-1, 196, 384]               0\n",
      "       LayerNorm-187             [-1, 196, 384]             768\n",
      "SwinTransformerBlock-188             [-1, 196, 384]               0\n",
      "          Linear-189             [-1, 49, 1152]         443,520\n",
      "          Linear-190          [-1, 13, 13, 384]           1,152\n",
      "            ReLU-191          [-1, 13, 13, 384]               0\n",
      "         Dropout-192          [-1, 13, 13, 384]               0\n",
      "          Linear-193           [-1, 13, 13, 12]           4,608\n",
      "         Softmax-194           [-1, 12, 49, 49]               0\n",
      "         Dropout-195           [-1, 12, 49, 49]               0\n",
      "          Linear-196              [-1, 49, 384]         147,840\n",
      "         Dropout-197              [-1, 49, 384]               0\n",
      " WindowAttention-198              [-1, 49, 384]               0\n",
      "      LayerScale-199             [-1, 196, 384]               0\n",
      "        DropPath-200             [-1, 196, 384]               0\n",
      "       LayerNorm-201             [-1, 196, 384]             768\n",
      "          Linear-202            [-1, 196, 1536]         591,360\n",
      "            GELU-203            [-1, 196, 1536]               0\n",
      "          Linear-204             [-1, 196, 384]         590,208\n",
      "         Dropout-205             [-1, 196, 384]               0\n",
      "             Mlp-206             [-1, 196, 384]               0\n",
      "      LayerScale-207             [-1, 196, 384]               0\n",
      "        DropPath-208             [-1, 196, 384]               0\n",
      "       LayerNorm-209             [-1, 196, 384]             768\n",
      "SwinTransformerBlock-210             [-1, 196, 384]               0\n",
      "          Linear-211             [-1, 49, 1152]         443,520\n",
      "          Linear-212          [-1, 13, 13, 384]           1,152\n",
      "            ReLU-213          [-1, 13, 13, 384]               0\n",
      "         Dropout-214          [-1, 13, 13, 384]               0\n",
      "          Linear-215           [-1, 13, 13, 12]           4,608\n",
      "         Softmax-216           [-1, 12, 49, 49]               0\n",
      "         Dropout-217           [-1, 12, 49, 49]               0\n",
      "          Linear-218              [-1, 49, 384]         147,840\n",
      "         Dropout-219              [-1, 49, 384]               0\n",
      " WindowAttention-220              [-1, 49, 384]               0\n",
      "      LayerScale-221             [-1, 196, 384]               0\n",
      "        DropPath-222             [-1, 196, 384]               0\n",
      "       LayerNorm-223             [-1, 196, 384]             768\n",
      "          Linear-224            [-1, 196, 1536]         591,360\n",
      "            GELU-225            [-1, 196, 1536]               0\n",
      "          Linear-226             [-1, 196, 384]         590,208\n",
      "         Dropout-227             [-1, 196, 384]               0\n",
      "             Mlp-228             [-1, 196, 384]               0\n",
      "      LayerScale-229             [-1, 196, 384]               0\n",
      "        DropPath-230             [-1, 196, 384]               0\n",
      "       LayerNorm-231             [-1, 196, 384]             768\n",
      "SwinTransformerBlock-232             [-1, 196, 384]               0\n",
      "       LayerNorm-233             [-1, 49, 1536]           3,072\n",
      "          Linear-234              [-1, 49, 768]       1,179,648\n",
      "    PatchMerging-235              [-1, 49, 768]               0\n",
      "      StageLayer-236              [-1, 49, 768]               0\n",
      "          Linear-237             [-1, 49, 2304]       1,771,776\n",
      "          Linear-238          [-1, 13, 13, 384]           1,152\n",
      "            ReLU-239          [-1, 13, 13, 384]               0\n",
      "         Dropout-240          [-1, 13, 13, 384]               0\n",
      "          Linear-241           [-1, 13, 13, 24]           9,216\n",
      "         Softmax-242           [-1, 24, 49, 49]               0\n",
      "         Dropout-243           [-1, 24, 49, 49]               0\n",
      "          Linear-244              [-1, 49, 768]         590,592\n",
      "         Dropout-245              [-1, 49, 768]               0\n",
      " WindowAttention-246              [-1, 49, 768]               0\n",
      "      LayerScale-247              [-1, 49, 768]               0\n",
      "        DropPath-248              [-1, 49, 768]               0\n",
      "       LayerNorm-249              [-1, 49, 768]           1,536\n",
      "          Linear-250             [-1, 49, 3072]       2,362,368\n",
      "            GELU-251             [-1, 49, 3072]               0\n",
      "          Linear-252              [-1, 49, 768]       2,360,064\n",
      "         Dropout-253              [-1, 49, 768]               0\n",
      "             Mlp-254              [-1, 49, 768]               0\n",
      "      LayerScale-255              [-1, 49, 768]               0\n",
      "        DropPath-256              [-1, 49, 768]               0\n",
      "       LayerNorm-257              [-1, 49, 768]           1,536\n",
      "SwinTransformerBlock-258              [-1, 49, 768]               0\n",
      "          Linear-259             [-1, 49, 2304]       1,771,776\n",
      "          Linear-260          [-1, 13, 13, 384]           1,152\n",
      "            ReLU-261          [-1, 13, 13, 384]               0\n",
      "         Dropout-262          [-1, 13, 13, 384]               0\n",
      "          Linear-263           [-1, 13, 13, 24]           9,216\n",
      "         Softmax-264           [-1, 24, 49, 49]               0\n",
      "         Dropout-265           [-1, 24, 49, 49]               0\n",
      "          Linear-266              [-1, 49, 768]         590,592\n",
      "         Dropout-267              [-1, 49, 768]               0\n",
      " WindowAttention-268              [-1, 49, 768]               0\n",
      "      LayerScale-269              [-1, 49, 768]               0\n",
      "        DropPath-270              [-1, 49, 768]               0\n",
      "       LayerNorm-271              [-1, 49, 768]           1,536\n",
      "          Linear-272             [-1, 49, 3072]       2,362,368\n",
      "            GELU-273             [-1, 49, 3072]               0\n",
      "          Linear-274              [-1, 49, 768]       2,360,064\n",
      "         Dropout-275              [-1, 49, 768]               0\n",
      "             Mlp-276              [-1, 49, 768]               0\n",
      "      LayerScale-277              [-1, 49, 768]               0\n",
      "        DropPath-278              [-1, 49, 768]               0\n",
      "       LayerNorm-279              [-1, 49, 768]           1,536\n",
      "SwinTransformerBlock-280              [-1, 49, 768]               0\n",
      "        Identity-281              [-1, 49, 768]               0\n",
      "      StageLayer-282              [-1, 49, 768]               0\n",
      "       LayerNorm-283              [-1, 49, 768]           1,536\n",
      "AdaptiveAvgPool1d-284               [-1, 768, 1]               0\n",
      "          Linear-285                  [-1, 100]          76,900\n",
      "================================================================\n",
      "Total params: 27,639,748\n",
      "Trainable params: 27,639,748\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 249.45\n",
      "Params size (MB): 105.44\n",
      "Estimated Total Size (MB): 355.46\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(model.to('cuda'), (3,224,224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default Parameter(weight) Check\n",
    "- 추후 SimMIM 가중치가 제대로 불러와졌는지 확인용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0982, -0.0303,  0.0880,  0.0434],\n",
       "         [-0.0496,  0.1174,  0.0592, -0.0250],\n",
       "         [ 0.0544,  0.0834,  0.0161, -0.0264],\n",
       "         [ 0.0629,  0.1349, -0.0920,  0.0250]],\n",
       "\n",
       "        [[-0.0895,  0.0073, -0.0723,  0.0701],\n",
       "         [ 0.0850, -0.0042,  0.0474,  0.1190],\n",
       "         [ 0.1244,  0.0692,  0.0016, -0.0569],\n",
       "         [-0.1289,  0.0428,  0.0280,  0.0450]],\n",
       "\n",
       "        [[-0.0107, -0.0314,  0.0549,  0.0370],\n",
       "         [ 0.1390, -0.0297,  0.0837,  0.1274],\n",
       "         [-0.1315, -0.0075,  0.0578, -0.1025],\n",
       "         [ 0.0094,  0.0660, -0.0369,  0.0334]]], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['embeddings.patch_embeddings.weight'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0009,  0.0250,  0.0339, -0.0226,  0.0136, -0.0180,  0.0068,  0.0204,\n",
       "         0.0155,  0.0087, -0.0077,  0.0046,  0.0079, -0.0220,  0.0043,  0.0077,\n",
       "         0.0059, -0.0091,  0.0206, -0.0257, -0.0090,  0.0238,  0.0213, -0.0427,\n",
       "         0.0148,  0.0222, -0.0188,  0.0085, -0.0206,  0.0116, -0.0026, -0.0230,\n",
       "        -0.0159, -0.0107,  0.0292,  0.0220, -0.0277, -0.0250, -0.0065, -0.0005,\n",
       "         0.0024, -0.0199, -0.0143, -0.0021, -0.0192,  0.0025, -0.0045,  0.0061,\n",
       "        -0.0229, -0.0590, -0.0228, -0.0264,  0.0308,  0.0089, -0.0155,  0.0079,\n",
       "         0.0059,  0.0226,  0.0012,  0.0221,  0.0146, -0.0172,  0.0231,  0.0056,\n",
       "         0.0128,  0.0022, -0.0232, -0.0272,  0.0022,  0.0171,  0.0316, -0.0098,\n",
       "        -0.0179, -0.0017,  0.0075, -0.0058,  0.0010, -0.0021, -0.0014,  0.0274,\n",
       "         0.0195,  0.0198,  0.0244, -0.0016,  0.0289, -0.0054,  0.0121, -0.0034,\n",
       "         0.0100,  0.0229,  0.0098, -0.0279,  0.0450,  0.0055,  0.0160, -0.0413,\n",
       "         0.0014, -0.0169,  0.0116,  0.0409,  0.0094,  0.0110, -0.0188,  0.0211,\n",
       "         0.0120,  0.0194, -0.0014, -0.0368, -0.0126, -0.0145,  0.0240,  0.0103,\n",
       "         0.0171, -0.0098,  0.0019, -0.0009, -0.0036, -0.0135,  0.0129,  0.0257,\n",
       "        -0.0221,  0.0523, -0.0222, -0.0039,  0.0279,  0.0189, -0.0052, -0.0038,\n",
       "        -0.0271, -0.0061,  0.0140, -0.0027,  0.0208,  0.0110,  0.0327,  0.0118,\n",
       "         0.0237, -0.0032,  0.0339,  0.0115, -0.0254, -0.0120, -0.0130,  0.0085,\n",
       "         0.0013, -0.0136, -0.0094,  0.0267, -0.0281, -0.0222,  0.0097, -0.0114,\n",
       "        -0.0196, -0.0227,  0.0159, -0.0167,  0.0429,  0.0082, -0.0078, -0.0062,\n",
       "        -0.0428, -0.0100,  0.0129, -0.0294,  0.0298, -0.0110, -0.0128, -0.0041,\n",
       "        -0.0125,  0.0067,  0.0253,  0.0151, -0.0034, -0.0110,  0.0005,  0.0049,\n",
       "        -0.0196, -0.0173, -0.0195,  0.0559, -0.0023, -0.0096, -0.0151,  0.0054,\n",
       "         0.0032, -0.0051, -0.0194, -0.0019,  0.0162, -0.0164, -0.0402,  0.0115,\n",
       "        -0.0190,  0.0160,  0.0076,  0.0183, -0.0136, -0.0153,  0.0583, -0.0032,\n",
       "         0.0009, -0.0089,  0.0014,  0.0033, -0.0221, -0.0228, -0.0073,  0.0169,\n",
       "         0.0034, -0.0191,  0.0361, -0.0141,  0.0252,  0.0431,  0.0142, -0.0406,\n",
       "         0.0250,  0.0045, -0.0003,  0.0228,  0.0189,  0.0084,  0.0024, -0.0142,\n",
       "         0.0053,  0.0100, -0.0231,  0.0068, -0.0398, -0.0021, -0.0154, -0.0117,\n",
       "        -0.0029, -0.0049, -0.0186,  0.0139,  0.0022, -0.0043,  0.0005,  0.0254,\n",
       "         0.0144, -0.0075,  0.0107, -0.0217, -0.0250, -0.0058,  0.0021, -0.0109,\n",
       "         0.0007, -0.0340,  0.0013, -0.0012, -0.0326,  0.0098, -0.0064,  0.0008,\n",
       "         0.0033,  0.0282,  0.0116,  0.0035, -0.0122, -0.0072,  0.0338,  0.0023,\n",
       "        -0.0021, -0.0180,  0.0125, -0.0064,  0.0012,  0.0066, -0.0228,  0.0052,\n",
       "         0.0168,  0.0128, -0.0011, -0.0097, -0.0025, -0.0270, -0.0079, -0.0188,\n",
       "        -0.0129,  0.0003, -0.0537,  0.0123, -0.0076,  0.0014,  0.0164,  0.0040,\n",
       "        -0.0001, -0.0078,  0.0157,  0.0120, -0.0465,  0.0132,  0.0060, -0.0223,\n",
       "         0.0394, -0.0398, -0.0098,  0.0024,  0.0302,  0.0134,  0.0071, -0.0177,\n",
       "         0.0121, -0.0103, -0.0041, -0.0501,  0.0056,  0.0461, -0.0277, -0.0083,\n",
       "        -0.0312,  0.0105, -0.0215,  0.0126,  0.0211, -0.0377,  0.0283, -0.0045,\n",
       "        -0.0341, -0.0328, -0.0047,  0.0033,  0.0030,  0.0153, -0.0280,  0.0114,\n",
       "         0.0261,  0.0153,  0.0231, -0.0174, -0.0141,  0.0249,  0.0106,  0.0466,\n",
       "         0.0034,  0.0060,  0.0315, -0.0159, -0.0183,  0.0052, -0.0016, -0.0242,\n",
       "         0.0215,  0.0228, -0.0012, -0.0132, -0.0412, -0.0011, -0.0603, -0.0213,\n",
       "        -0.0397, -0.0103,  0.0325,  0.0162, -0.0051,  0.0254,  0.0245, -0.0537,\n",
       "         0.0346,  0.0211, -0.0187,  0.0127,  0.0165,  0.0016,  0.0122,  0.0153,\n",
       "        -0.0246, -0.0188, -0.0320,  0.0247, -0.0130,  0.0036, -0.0054,  0.0022,\n",
       "         0.0069,  0.0084, -0.0069,  0.0222, -0.0289, -0.0047, -0.0051, -0.0215],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['stages.3.blocks.1.attn.crpb_mlp.3.weight'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Swin v2 config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MODEL': {'TYPE': 'swinv2',\n",
       "  'NAME': 'simmim_train',\n",
       "  'PRETRAINED': '../../models/swin2/simmim.pth',\n",
       "  'DROP_PATH_RATE': 0.2,\n",
       "  'SWIN': {'EMBED_DIM': 96,\n",
       "   'DEPTHS': [2, 2, 6, 2],\n",
       "   'NUM_HEADS': [3, 6, 12, 24],\n",
       "   'WINDOW_SIZE': 7,\n",
       "   'PATCH_SIZE': 4}},\n",
       " 'DATA': {'IMG_SIZE': 224,\n",
       "  'MASK_PATCH_SIZE': 32,\n",
       "  'MASK_RATIO': 0.6,\n",
       "  'BATCH_SIZE': 960,\n",
       "  'NUM_WORKERS': 24,\n",
       "  'DATA_PATH': '../../data/sports'},\n",
       " 'TRAIN': {'EPOCHS': 20,\n",
       "  'WARMUP_EPOCHS': 10,\n",
       "  'BASE_LR': '1e-4',\n",
       "  'WEIGHT_DECAY': 0.05,\n",
       "  'CLIP_GRAD': 5}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swin_config = yaml.load(open('config/train.yaml'), Loader=yaml.FullLoader)\n",
    "swin_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load weight from SimMIM Model\n",
    "- Different Image/Window Size\n",
    "- Image와 Window 사이즈의 비율은 맞춰야함\n",
    "  ex) 192÷6 = 224÷7 = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained(config, model):\n",
    "    print(f\"==============> Loading weight {config.MODEL.PRETRAINED} for fine-tuning......\")\n",
    "    state_dict = torch.load(config.MODEL.PRETRAINED, map_location='cpu')\n",
    "\n",
    "    # remain encoder only\n",
    "    not_encoder_keys = [k for k in state_dict.keys() if 'encoder' not in k]\n",
    "    for k in not_encoder_keys:\n",
    "        del state_dict[k]\n",
    "        \n",
    "    # remove prefix encoder.\n",
    "    state_dict = {k.replace('encoder.', ''):v for k, v in state_dict.items()}\n",
    "\n",
    "    # delete relative_position_index since we always re-init it\n",
    "    relative_position_index_keys = [k for k in state_dict.keys() if \"relative_position_index\" in k]\n",
    "    for k in relative_position_index_keys:\n",
    "        del state_dict[k]\n",
    "\n",
    "    # delete relative_coords_table since we always re-init it\n",
    "    relative_position_index_keys = [k for k in state_dict.keys() if \"relative_coords_table\" in k]\n",
    "    for k in relative_position_index_keys:\n",
    "        del state_dict[k]\n",
    "\n",
    "    # delete attn_mask since we always re-init it\n",
    "    attn_mask_keys = [k for k in state_dict.keys() if \"attn_mask\" in k]\n",
    "    for k in attn_mask_keys:\n",
    "        del state_dict[k]\n",
    "\n",
    "    # bicubic interpolate relative_position_bias_table if not match\n",
    "    relative_position_bias_table_keys = [k for k in state_dict.keys() if \"relative_position_bias_table\" in k]\n",
    "    for k in relative_position_bias_table_keys:\n",
    "        relative_position_bias_table_pretrained = state_dict[k]\n",
    "        relative_position_bias_table_current = model.state_dict()[k]\n",
    "        L1, nH1 = relative_position_bias_table_pretrained.size()\n",
    "        L2, nH2 = relative_position_bias_table_current.size()\n",
    "        if nH1 != nH2:\n",
    "            print(f\"Error in loading {k}, passing......\")\n",
    "        else:\n",
    "            if L1 != L2:\n",
    "                # bicubic interpolate relative_position_bias_table if not match\n",
    "                S1 = int(L1 ** 0.5)\n",
    "                S2 = int(L2 ** 0.5)\n",
    "                relative_position_bias_table_pretrained_resized = torch.nn.functional.interpolate(\n",
    "                    relative_position_bias_table_pretrained.permute(1, 0).view(1, nH1, S1, S1), size=(S2, S2),\n",
    "                    mode='bicubic')\n",
    "                state_dict[k] = relative_position_bias_table_pretrained_resized.view(nH2, L2).permute(1, 0)\n",
    "\n",
    "    # bicubic interpolate absolute_pos_embed if not match\n",
    "    absolute_pos_embed_keys = [k for k in state_dict.keys() if \"absolute_pos_embed\" in k]\n",
    "    for k in absolute_pos_embed_keys:\n",
    "        # dpe\n",
    "        absolute_pos_embed_pretrained = state_dict[k]\n",
    "        absolute_pos_embed_current = model.state_dict()[k.replace('encoder.','')]\n",
    "        _, L1, C1 = absolute_pos_embed_pretrained.size()\n",
    "        _, L2, C2 = absolute_pos_embed_current.size()\n",
    "        if C1 != C1:\n",
    "            print(f\"Error in loading {k}, passing......\")\n",
    "        else:\n",
    "            if L1 != L2:\n",
    "                S1 = int(L1 ** 0.5)\n",
    "                S2 = int(L2 ** 0.5)\n",
    "                absolute_pos_embed_pretrained = absolute_pos_embed_pretrained.reshape(-1, S1, S1, C1)\n",
    "                absolute_pos_embed_pretrained = absolute_pos_embed_pretrained.permute(0, 3, 1, 2)\n",
    "                absolute_pos_embed_pretrained_resized = torch.nn.functional.interpolate(\n",
    "                    absolute_pos_embed_pretrained, size=(S2, S2), mode='bicubic')\n",
    "                absolute_pos_embed_pretrained_resized = absolute_pos_embed_pretrained_resized.permute(0, 2, 3, 1)\n",
    "                absolute_pos_embed_pretrained_resized = absolute_pos_embed_pretrained_resized.flatten(1, 2)\n",
    "                state_dict[k] = absolute_pos_embed_pretrained_resized\n",
    "\n",
    "    # check classifier, if not match, then re-init classifier to zero\n",
    "    head_bias_pretrained = state_dict['classifier.bias']\n",
    "    Nc1 = head_bias_pretrained.shape[0]\n",
    "    Nc2 = model.classifier.bias.shape[0]\n",
    "    if (Nc1 != Nc2):\n",
    "        torch.nn.init.constant_(model.classifier.bias, 0.)\n",
    "        torch.nn.init.constant_(model.classifier.weight, 0.)\n",
    "        del state_dict['classifier.weight']\n",
    "        del state_dict['classifier.bias']\n",
    "        print(f\"Error in loading classifier head, re-init classifier head to 0\")\n",
    "\n",
    "    msg = model.load_state_dict(state_dict, strict=False)\n",
    "    print(msg)\n",
    "\n",
    "    print(f\"=> loaded successfully '{config.MODEL.PRETRAINED}'\")\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swin_config = Box(swin_config)\n",
    "load_pretrained(swin_config, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Loading Weight Results\n",
    "- 정상적으로 불러와졌는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0982, -0.0303,  0.0880,  0.0434],\n",
       "         [-0.0496,  0.1174,  0.0592, -0.0250],\n",
       "         [ 0.0544,  0.0834,  0.0161, -0.0264],\n",
       "         [ 0.0629,  0.1349, -0.0920,  0.0250]],\n",
       "\n",
       "        [[-0.0895,  0.0073, -0.0723,  0.0701],\n",
       "         [ 0.0850, -0.0042,  0.0474,  0.1190],\n",
       "         [ 0.1244,  0.0692,  0.0016, -0.0569],\n",
       "         [-0.1289,  0.0428,  0.0280,  0.0450]],\n",
       "\n",
       "        [[-0.0107, -0.0314,  0.0549,  0.0370],\n",
       "         [ 0.1390, -0.0297,  0.0837,  0.1274],\n",
       "         [-0.1315, -0.0075,  0.0578, -0.1025],\n",
       "         [ 0.0094,  0.0660, -0.0369,  0.0334]]], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['embeddings.patch_embeddings.weight'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0378, -0.0356, -0.0572,  0.0410,  0.1083,  0.0806, -0.0449,  0.1297,\n",
       "         0.1483, -0.0354, -0.0157,  0.0207,  0.0078, -0.0074, -0.0733,  0.1325,\n",
       "         0.0424, -0.0151,  0.0299,  0.0710, -0.0764,  0.0846,  0.0357,  0.0234,\n",
       "         0.0228, -0.0675,  0.0554,  0.0503,  0.1562,  0.0724, -0.0679, -0.0528,\n",
       "        -0.0609, -0.0841,  0.0542,  0.0328, -0.0305, -0.0002, -0.0288,  0.1304,\n",
       "         0.0463,  0.0064, -0.0864, -0.0470, -0.0721, -0.0730, -0.0987, -0.0274,\n",
       "        -0.0715,  0.0071,  0.0931,  0.0537,  0.1133, -0.0511, -0.0168,  0.0127,\n",
       "         0.1350,  0.0222, -0.0723,  0.0142, -0.0255, -0.0166, -0.0845,  0.0744,\n",
       "         0.0075, -0.0324, -0.0903, -0.1243,  0.0328,  0.0239,  0.0102,  0.0438,\n",
       "        -0.1146, -0.0742, -0.0043,  0.0933,  0.0712, -0.0298, -0.0953,  0.1248,\n",
       "         0.0784, -0.0411, -0.0242,  0.1539,  0.0641,  0.0427, -0.0101, -0.0378,\n",
       "        -0.0225,  0.1030,  0.0277, -0.0708, -0.0067,  0.0402,  0.0110,  0.0257,\n",
       "        -0.0235,  0.0217,  0.0561,  0.0175, -0.0799,  0.0385,  0.0279, -0.0593,\n",
       "        -0.0823, -0.0612,  0.1048,  0.1771, -0.0283, -0.0709, -0.0557, -0.0512,\n",
       "        -0.0093, -0.0234,  0.1206, -0.0727,  0.0181,  0.0519, -0.0286, -0.0311,\n",
       "        -0.0764,  0.1362, -0.0010, -0.0664,  0.0623, -0.0651,  0.0161,  0.0735,\n",
       "        -0.0353, -0.0276,  0.0331,  0.0658,  0.0336, -0.0181,  0.1246, -0.0697,\n",
       "        -0.0045,  0.1517,  0.0102,  0.0103,  0.0486,  0.0412,  0.0255, -0.0257,\n",
       "         0.1179, -0.0610, -0.0237, -0.1096,  0.0281,  0.1092,  0.0103, -0.0517,\n",
       "        -0.0444, -0.0179, -0.0883,  0.0421, -0.0037, -0.0326, -0.0678, -0.0488,\n",
       "         0.0179, -0.1017, -0.0187, -0.0235,  0.0144,  0.0391,  0.1065,  0.1296,\n",
       "        -0.0096, -0.0594, -0.0571,  0.1469, -0.0865, -0.0257,  0.0275,  0.0243,\n",
       "         0.0451, -0.0243, -0.0307, -0.0088,  0.0274, -0.0630,  0.1179,  0.1540,\n",
       "         0.0448, -0.0672, -0.0574,  0.0691, -0.0698,  0.1260, -0.0718, -0.0169,\n",
       "        -0.0528, -0.0075, -0.0376,  0.0459,  0.0210,  0.0191, -0.0411, -0.0405,\n",
       "        -0.1075,  0.0615, -0.0672,  0.0413,  0.1195, -0.0500,  0.0114, -0.0009,\n",
       "         0.0155,  0.0063,  0.0998, -0.0667,  0.0482,  0.1320,  0.0108,  0.1832,\n",
       "        -0.0511, -0.0246, -0.0348,  0.0849, -0.0863, -0.0713, -0.0017,  0.0016,\n",
       "        -0.0124, -0.0952, -0.0937,  0.0217, -0.0226,  0.0385, -0.0299,  0.0478,\n",
       "        -0.0479, -0.0303,  0.0162, -0.0451, -0.0166, -0.0645,  0.1397, -0.0446,\n",
       "         0.0854, -0.0883, -0.0133,  0.0546, -0.0572, -0.0423, -0.0676, -0.0328,\n",
       "         0.0100,  0.0610, -0.0208, -0.0202, -0.0145,  0.0822,  0.0075, -0.0262,\n",
       "        -0.0625,  0.0914, -0.0392, -0.0577, -0.0717, -0.0347,  0.1291,  0.1139,\n",
       "        -0.0600, -0.0343,  0.1074, -0.0394,  0.0143,  0.0860, -0.0744,  0.0007,\n",
       "         0.0431, -0.0746, -0.0982, -0.0219, -0.0358, -0.0206, -0.0588, -0.0478,\n",
       "        -0.1144, -0.0063,  0.0167, -0.0764, -0.0366, -0.0091, -0.0735, -0.0699,\n",
       "         0.0434, -0.0854,  0.0628, -0.0429, -0.0181, -0.0742,  0.0164, -0.0785,\n",
       "        -0.0047, -0.0028, -0.0819,  0.0566, -0.0145,  0.0385,  0.1773, -0.0945,\n",
       "        -0.0023, -0.0370,  0.0642, -0.0929, -0.0671, -0.0649, -0.0340,  0.0219,\n",
       "         0.1202,  0.0715,  0.0036,  0.1203,  0.0212, -0.0362,  0.0531, -0.0638,\n",
       "        -0.0581,  0.0979,  0.0215,  0.0360,  0.0533, -0.0231, -0.0049, -0.0495,\n",
       "         0.0284, -0.0312,  0.0013,  0.0799, -0.0394,  0.1256,  0.0256,  0.1229,\n",
       "        -0.0768, -0.0849, -0.0211, -0.0265, -0.0249, -0.0177, -0.0762, -0.0300,\n",
       "         0.1326, -0.0536,  0.0525,  0.0655, -0.0528, -0.0194,  0.0544, -0.0878,\n",
       "        -0.0890, -0.0577,  0.0176, -0.0436, -0.0143, -0.0927, -0.0843, -0.0439,\n",
       "        -0.0933, -0.0650, -0.0650, -0.0419, -0.1098,  0.0247, -0.0105,  0.1306,\n",
       "         0.0270, -0.0373, -0.0626, -0.0516,  0.0957, -0.1038, -0.1022,  0.0454,\n",
       "        -0.1066, -0.0454,  0.0731,  0.0541, -0.0316,  0.0067,  0.0411, -0.0279])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['stages.3.blocks.1.attn.crpb_mlp.3.weight'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.Stage-2 Traing\n",
    "- Supervised pre-training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Transform, Loss, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms 정의하기\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8,1), interpolation=transforms.InterpolationMode.LANCZOS),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    transforms.RandomErasing(p=0.9, scale=(0.02, 0.33)),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "data_dir = '../../data/sports'\n",
    "batch_size = 960\n",
    "\n",
    "train_path = data_dir+'/train'\n",
    "valid_path = data_dir+'/valid'\n",
    "test_path = data_dir+'/test'\n",
    "\n",
    "# dataset load\n",
    "train_data = ImageFolder(train_path, transform=train_transform)\n",
    "valid_data = ImageFolder(valid_path, transform=test_transform)\n",
    "test_data = ImageFolder(test_path, transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_norm = 1.0 # paper : 100 with G variants\n",
    "\n",
    "model.to(device)\n",
    "model_path = '../../models/swin2/model_w_simmim.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixup_fn = Mixup(mixup_alpha=.7, \n",
    "                cutmix_alpha=.7, \n",
    "                prob=.7, \n",
    "                switch_prob=0.5, \n",
    "                mode='batch',\n",
    "                label_smoothing=.1,\n",
    "                num_classes=100)\n",
    "\n",
    "epochs = 150\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer-Wise Learning Rate Decay ★"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: absolute_pos_embed\n",
      "1: embeddings.patch_embeddings.weight\n",
      "2: embeddings.patch_embeddings.bias\n",
      "3: embeddings.norm.weight\n",
      "4: embeddings.norm.bias\n",
      "5: stages.0.blocks.0.attn.t_scale\n",
      "6: stages.0.blocks.0.attn.crpb_mlp.0.weight\n",
      "7: stages.0.blocks.0.attn.crpb_mlp.0.bias\n",
      "8: stages.0.blocks.0.attn.crpb_mlp.3.weight\n",
      "9: stages.0.blocks.0.attn.qkv.weight\n",
      "10: stages.0.blocks.0.attn.qkv.bias\n",
      "11: stages.0.blocks.0.attn.proj.weight\n",
      "12: stages.0.blocks.0.attn.proj.bias\n",
      "13: stages.0.blocks.0.norm1.weight\n",
      "14: stages.0.blocks.0.norm1.bias\n",
      "15: stages.0.blocks.0.ls1.gamma\n",
      "16: stages.0.blocks.0.mlp.fc1.weight\n",
      "17: stages.0.blocks.0.mlp.fc1.bias\n",
      "18: stages.0.blocks.0.mlp.fc2.weight\n",
      "19: stages.0.blocks.0.mlp.fc2.bias\n",
      "20: stages.0.blocks.0.norm2.weight\n",
      "21: stages.0.blocks.0.norm2.bias\n",
      "22: stages.0.blocks.0.ls2.gamma\n",
      "23: stages.0.blocks.1.attn.t_scale\n",
      "24: stages.0.blocks.1.attn.crpb_mlp.0.weight\n",
      "25: stages.0.blocks.1.attn.crpb_mlp.0.bias\n",
      "26: stages.0.blocks.1.attn.crpb_mlp.3.weight\n",
      "27: stages.0.blocks.1.attn.qkv.weight\n",
      "28: stages.0.blocks.1.attn.qkv.bias\n",
      "29: stages.0.blocks.1.attn.proj.weight\n",
      "30: stages.0.blocks.1.attn.proj.bias\n",
      "31: stages.0.blocks.1.norm1.weight\n",
      "32: stages.0.blocks.1.norm1.bias\n",
      "33: stages.0.blocks.1.ls1.gamma\n",
      "34: stages.0.blocks.1.mlp.fc1.weight\n",
      "35: stages.0.blocks.1.mlp.fc1.bias\n",
      "36: stages.0.blocks.1.mlp.fc2.weight\n",
      "37: stages.0.blocks.1.mlp.fc2.bias\n",
      "38: stages.0.blocks.1.norm2.weight\n",
      "39: stages.0.blocks.1.norm2.bias\n",
      "40: stages.0.blocks.1.ls2.gamma\n",
      "41: stages.0.downsample.reduction.weight\n",
      "42: stages.0.downsample.norm.weight\n",
      "43: stages.0.downsample.norm.bias\n",
      "44: stages.1.blocks.0.attn.t_scale\n",
      "45: stages.1.blocks.0.attn.crpb_mlp.0.weight\n",
      "46: stages.1.blocks.0.attn.crpb_mlp.0.bias\n",
      "47: stages.1.blocks.0.attn.crpb_mlp.3.weight\n",
      "48: stages.1.blocks.0.attn.qkv.weight\n",
      "49: stages.1.blocks.0.attn.qkv.bias\n",
      "50: stages.1.blocks.0.attn.proj.weight\n",
      "51: stages.1.blocks.0.attn.proj.bias\n",
      "52: stages.1.blocks.0.norm1.weight\n",
      "53: stages.1.blocks.0.norm1.bias\n",
      "54: stages.1.blocks.0.ls1.gamma\n",
      "55: stages.1.blocks.0.mlp.fc1.weight\n",
      "56: stages.1.blocks.0.mlp.fc1.bias\n",
      "57: stages.1.blocks.0.mlp.fc2.weight\n",
      "58: stages.1.blocks.0.mlp.fc2.bias\n",
      "59: stages.1.blocks.0.norm2.weight\n",
      "60: stages.1.blocks.0.norm2.bias\n",
      "61: stages.1.blocks.0.ls2.gamma\n",
      "62: stages.1.blocks.1.attn.t_scale\n",
      "63: stages.1.blocks.1.attn.crpb_mlp.0.weight\n",
      "64: stages.1.blocks.1.attn.crpb_mlp.0.bias\n",
      "65: stages.1.blocks.1.attn.crpb_mlp.3.weight\n",
      "66: stages.1.blocks.1.attn.qkv.weight\n",
      "67: stages.1.blocks.1.attn.qkv.bias\n",
      "68: stages.1.blocks.1.attn.proj.weight\n",
      "69: stages.1.blocks.1.attn.proj.bias\n",
      "70: stages.1.blocks.1.norm1.weight\n",
      "71: stages.1.blocks.1.norm1.bias\n",
      "72: stages.1.blocks.1.ls1.gamma\n",
      "73: stages.1.blocks.1.mlp.fc1.weight\n",
      "74: stages.1.blocks.1.mlp.fc1.bias\n",
      "75: stages.1.blocks.1.mlp.fc2.weight\n",
      "76: stages.1.blocks.1.mlp.fc2.bias\n",
      "77: stages.1.blocks.1.norm2.weight\n",
      "78: stages.1.blocks.1.norm2.bias\n",
      "79: stages.1.blocks.1.ls2.gamma\n",
      "80: stages.1.downsample.reduction.weight\n",
      "81: stages.1.downsample.norm.weight\n",
      "82: stages.1.downsample.norm.bias\n",
      "83: stages.2.blocks.0.attn.t_scale\n",
      "84: stages.2.blocks.0.attn.crpb_mlp.0.weight\n",
      "85: stages.2.blocks.0.attn.crpb_mlp.0.bias\n",
      "86: stages.2.blocks.0.attn.crpb_mlp.3.weight\n",
      "87: stages.2.blocks.0.attn.qkv.weight\n",
      "88: stages.2.blocks.0.attn.qkv.bias\n",
      "89: stages.2.blocks.0.attn.proj.weight\n",
      "90: stages.2.blocks.0.attn.proj.bias\n",
      "91: stages.2.blocks.0.norm1.weight\n",
      "92: stages.2.blocks.0.norm1.bias\n",
      "93: stages.2.blocks.0.ls1.gamma\n",
      "94: stages.2.blocks.0.mlp.fc1.weight\n",
      "95: stages.2.blocks.0.mlp.fc1.bias\n",
      "96: stages.2.blocks.0.mlp.fc2.weight\n",
      "97: stages.2.blocks.0.mlp.fc2.bias\n",
      "98: stages.2.blocks.0.norm2.weight\n",
      "99: stages.2.blocks.0.norm2.bias\n",
      "100: stages.2.blocks.0.ls2.gamma\n",
      "101: stages.2.blocks.1.attn.t_scale\n",
      "102: stages.2.blocks.1.attn.crpb_mlp.0.weight\n",
      "103: stages.2.blocks.1.attn.crpb_mlp.0.bias\n",
      "104: stages.2.blocks.1.attn.crpb_mlp.3.weight\n",
      "105: stages.2.blocks.1.attn.qkv.weight\n",
      "106: stages.2.blocks.1.attn.qkv.bias\n",
      "107: stages.2.blocks.1.attn.proj.weight\n",
      "108: stages.2.blocks.1.attn.proj.bias\n",
      "109: stages.2.blocks.1.norm1.weight\n",
      "110: stages.2.blocks.1.norm1.bias\n",
      "111: stages.2.blocks.1.ls1.gamma\n",
      "112: stages.2.blocks.1.mlp.fc1.weight\n",
      "113: stages.2.blocks.1.mlp.fc1.bias\n",
      "114: stages.2.blocks.1.mlp.fc2.weight\n",
      "115: stages.2.blocks.1.mlp.fc2.bias\n",
      "116: stages.2.blocks.1.norm2.weight\n",
      "117: stages.2.blocks.1.norm2.bias\n",
      "118: stages.2.blocks.1.ls2.gamma\n",
      "119: stages.2.blocks.2.attn.t_scale\n",
      "120: stages.2.blocks.2.attn.crpb_mlp.0.weight\n",
      "121: stages.2.blocks.2.attn.crpb_mlp.0.bias\n",
      "122: stages.2.blocks.2.attn.crpb_mlp.3.weight\n",
      "123: stages.2.blocks.2.attn.qkv.weight\n",
      "124: stages.2.blocks.2.attn.qkv.bias\n",
      "125: stages.2.blocks.2.attn.proj.weight\n",
      "126: stages.2.blocks.2.attn.proj.bias\n",
      "127: stages.2.blocks.2.norm1.weight\n",
      "128: stages.2.blocks.2.norm1.bias\n",
      "129: stages.2.blocks.2.ls1.gamma\n",
      "130: stages.2.blocks.2.mlp.fc1.weight\n",
      "131: stages.2.blocks.2.mlp.fc1.bias\n",
      "132: stages.2.blocks.2.mlp.fc2.weight\n",
      "133: stages.2.blocks.2.mlp.fc2.bias\n",
      "134: stages.2.blocks.2.norm2.weight\n",
      "135: stages.2.blocks.2.norm2.bias\n",
      "136: stages.2.blocks.2.ls2.gamma\n",
      "137: stages.2.blocks.3.attn.t_scale\n",
      "138: stages.2.blocks.3.attn.crpb_mlp.0.weight\n",
      "139: stages.2.blocks.3.attn.crpb_mlp.0.bias\n",
      "140: stages.2.blocks.3.attn.crpb_mlp.3.weight\n",
      "141: stages.2.blocks.3.attn.qkv.weight\n",
      "142: stages.2.blocks.3.attn.qkv.bias\n",
      "143: stages.2.blocks.3.attn.proj.weight\n",
      "144: stages.2.blocks.3.attn.proj.bias\n",
      "145: stages.2.blocks.3.norm1.weight\n",
      "146: stages.2.blocks.3.norm1.bias\n",
      "147: stages.2.blocks.3.ls1.gamma\n",
      "148: stages.2.blocks.3.mlp.fc1.weight\n",
      "149: stages.2.blocks.3.mlp.fc1.bias\n",
      "150: stages.2.blocks.3.mlp.fc2.weight\n",
      "151: stages.2.blocks.3.mlp.fc2.bias\n",
      "152: stages.2.blocks.3.norm2.weight\n",
      "153: stages.2.blocks.3.norm2.bias\n",
      "154: stages.2.blocks.3.ls2.gamma\n",
      "155: stages.2.blocks.4.attn.t_scale\n",
      "156: stages.2.blocks.4.attn.crpb_mlp.0.weight\n",
      "157: stages.2.blocks.4.attn.crpb_mlp.0.bias\n",
      "158: stages.2.blocks.4.attn.crpb_mlp.3.weight\n",
      "159: stages.2.blocks.4.attn.qkv.weight\n",
      "160: stages.2.blocks.4.attn.qkv.bias\n",
      "161: stages.2.blocks.4.attn.proj.weight\n",
      "162: stages.2.blocks.4.attn.proj.bias\n",
      "163: stages.2.blocks.4.norm1.weight\n",
      "164: stages.2.blocks.4.norm1.bias\n",
      "165: stages.2.blocks.4.ls1.gamma\n",
      "166: stages.2.blocks.4.mlp.fc1.weight\n",
      "167: stages.2.blocks.4.mlp.fc1.bias\n",
      "168: stages.2.blocks.4.mlp.fc2.weight\n",
      "169: stages.2.blocks.4.mlp.fc2.bias\n",
      "170: stages.2.blocks.4.norm2.weight\n",
      "171: stages.2.blocks.4.norm2.bias\n",
      "172: stages.2.blocks.4.ls2.gamma\n",
      "173: stages.2.blocks.5.attn.t_scale\n",
      "174: stages.2.blocks.5.attn.crpb_mlp.0.weight\n",
      "175: stages.2.blocks.5.attn.crpb_mlp.0.bias\n",
      "176: stages.2.blocks.5.attn.crpb_mlp.3.weight\n",
      "177: stages.2.blocks.5.attn.qkv.weight\n",
      "178: stages.2.blocks.5.attn.qkv.bias\n",
      "179: stages.2.blocks.5.attn.proj.weight\n",
      "180: stages.2.blocks.5.attn.proj.bias\n",
      "181: stages.2.blocks.5.norm1.weight\n",
      "182: stages.2.blocks.5.norm1.bias\n",
      "183: stages.2.blocks.5.ls1.gamma\n",
      "184: stages.2.blocks.5.mlp.fc1.weight\n",
      "185: stages.2.blocks.5.mlp.fc1.bias\n",
      "186: stages.2.blocks.5.mlp.fc2.weight\n",
      "187: stages.2.blocks.5.mlp.fc2.bias\n",
      "188: stages.2.blocks.5.norm2.weight\n",
      "189: stages.2.blocks.5.norm2.bias\n",
      "190: stages.2.blocks.5.ls2.gamma\n",
      "191: stages.2.downsample.reduction.weight\n",
      "192: stages.2.downsample.norm.weight\n",
      "193: stages.2.downsample.norm.bias\n",
      "194: stages.3.blocks.0.attn.t_scale\n",
      "195: stages.3.blocks.0.attn.crpb_mlp.0.weight\n",
      "196: stages.3.blocks.0.attn.crpb_mlp.0.bias\n",
      "197: stages.3.blocks.0.attn.crpb_mlp.3.weight\n",
      "198: stages.3.blocks.0.attn.qkv.weight\n",
      "199: stages.3.blocks.0.attn.qkv.bias\n",
      "200: stages.3.blocks.0.attn.proj.weight\n",
      "201: stages.3.blocks.0.attn.proj.bias\n",
      "202: stages.3.blocks.0.norm1.weight\n",
      "203: stages.3.blocks.0.norm1.bias\n",
      "204: stages.3.blocks.0.ls1.gamma\n",
      "205: stages.3.blocks.0.mlp.fc1.weight\n",
      "206: stages.3.blocks.0.mlp.fc1.bias\n",
      "207: stages.3.blocks.0.mlp.fc2.weight\n",
      "208: stages.3.blocks.0.mlp.fc2.bias\n",
      "209: stages.3.blocks.0.norm2.weight\n",
      "210: stages.3.blocks.0.norm2.bias\n",
      "211: stages.3.blocks.0.ls2.gamma\n",
      "212: stages.3.blocks.1.attn.t_scale\n",
      "213: stages.3.blocks.1.attn.crpb_mlp.0.weight\n",
      "214: stages.3.blocks.1.attn.crpb_mlp.0.bias\n",
      "215: stages.3.blocks.1.attn.crpb_mlp.3.weight\n",
      "216: stages.3.blocks.1.attn.qkv.weight\n",
      "217: stages.3.blocks.1.attn.qkv.bias\n",
      "218: stages.3.blocks.1.attn.proj.weight\n",
      "219: stages.3.blocks.1.attn.proj.bias\n",
      "220: stages.3.blocks.1.norm1.weight\n",
      "221: stages.3.blocks.1.norm1.bias\n",
      "222: stages.3.blocks.1.ls1.gamma\n",
      "223: stages.3.blocks.1.mlp.fc1.weight\n",
      "224: stages.3.blocks.1.mlp.fc1.bias\n",
      "225: stages.3.blocks.1.mlp.fc2.weight\n",
      "226: stages.3.blocks.1.mlp.fc2.bias\n",
      "227: stages.3.blocks.1.norm2.weight\n",
      "228: stages.3.blocks.1.norm2.bias\n",
      "229: stages.3.blocks.1.ls2.gamma\n",
      "230: layernorm.weight\n",
      "231: layernorm.bias\n",
      "232: classifier.weight\n",
      "233: classifier.bias\n"
     ]
    }
   ],
   "source": [
    "layer_names = []\n",
    "for i, (name, params) in enumerate(model.named_parameters()):\n",
    "    lr = base_lr\n",
    "    print(f'{i}: {name}')\n",
    "    layer_names.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['classifier.bias',\n",
       " 'classifier.weight',\n",
       " 'layernorm.bias',\n",
       " 'layernorm.weight',\n",
       " 'stages.3.blocks.1.ls2.gamma']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_names.reverse()\n",
    "layer_names[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: classifier.bias's lr=0.0014\n",
      "1: classifier.weight's lr=0.0014\n",
      "2: layernorm.bias's lr=0.001218\n",
      "3: layernorm.weight's lr=0.001218\n",
      "4: stages.3.blocks.1.ls2.gamma's lr=0.0010596599999999998\n",
      "5: stages.3.blocks.1.norm2.bias's lr=0.0010596599999999998\n",
      "6: stages.3.blocks.1.norm2.weight's lr=0.0010596599999999998\n",
      "7: stages.3.blocks.1.mlp.fc2.bias's lr=0.0010596599999999998\n",
      "8: stages.3.blocks.1.mlp.fc2.weight's lr=0.0010596599999999998\n",
      "9: stages.3.blocks.1.mlp.fc1.bias's lr=0.0010596599999999998\n",
      "10: stages.3.blocks.1.mlp.fc1.weight's lr=0.0010596599999999998\n",
      "11: stages.3.blocks.1.ls1.gamma's lr=0.0010596599999999998\n",
      "12: stages.3.blocks.1.norm1.bias's lr=0.0010596599999999998\n",
      "13: stages.3.blocks.1.norm1.weight's lr=0.0010596599999999998\n",
      "14: stages.3.blocks.1.attn.proj.bias's lr=0.0010596599999999998\n",
      "15: stages.3.blocks.1.attn.proj.weight's lr=0.0010596599999999998\n",
      "16: stages.3.blocks.1.attn.qkv.bias's lr=0.0010596599999999998\n",
      "17: stages.3.blocks.1.attn.qkv.weight's lr=0.0010596599999999998\n",
      "18: stages.3.blocks.1.attn.crpb_mlp.3.weight's lr=0.0010596599999999998\n",
      "19: stages.3.blocks.1.attn.crpb_mlp.0.bias's lr=0.0010596599999999998\n",
      "20: stages.3.blocks.1.attn.crpb_mlp.0.weight's lr=0.0010596599999999998\n",
      "21: stages.3.blocks.1.attn.t_scale's lr=0.0010596599999999998\n",
      "22: stages.3.blocks.0.ls2.gamma's lr=0.0009219041999999998\n",
      "23: stages.3.blocks.0.norm2.bias's lr=0.0009219041999999998\n",
      "24: stages.3.blocks.0.norm2.weight's lr=0.0009219041999999998\n",
      "25: stages.3.blocks.0.mlp.fc2.bias's lr=0.0009219041999999998\n",
      "26: stages.3.blocks.0.mlp.fc2.weight's lr=0.0009219041999999998\n",
      "27: stages.3.blocks.0.mlp.fc1.bias's lr=0.0009219041999999998\n",
      "28: stages.3.blocks.0.mlp.fc1.weight's lr=0.0009219041999999998\n",
      "29: stages.3.blocks.0.ls1.gamma's lr=0.0009219041999999998\n",
      "30: stages.3.blocks.0.norm1.bias's lr=0.0009219041999999998\n",
      "31: stages.3.blocks.0.norm1.weight's lr=0.0009219041999999998\n",
      "32: stages.3.blocks.0.attn.proj.bias's lr=0.0009219041999999998\n",
      "33: stages.3.blocks.0.attn.proj.weight's lr=0.0009219041999999998\n",
      "34: stages.3.blocks.0.attn.qkv.bias's lr=0.0009219041999999998\n",
      "35: stages.3.blocks.0.attn.qkv.weight's lr=0.0009219041999999998\n",
      "36: stages.3.blocks.0.attn.crpb_mlp.3.weight's lr=0.0009219041999999998\n",
      "37: stages.3.blocks.0.attn.crpb_mlp.0.bias's lr=0.0009219041999999998\n",
      "38: stages.3.blocks.0.attn.crpb_mlp.0.weight's lr=0.0009219041999999998\n",
      "39: stages.3.blocks.0.attn.t_scale's lr=0.0009219041999999998\n",
      "40: stages.2.downsample.norm.bias's lr=0.0008020566539999999\n",
      "41: stages.2.downsample.norm.weight's lr=0.0008020566539999999\n",
      "42: stages.2.downsample.reduction.weight's lr=0.0006977892889799999\n",
      "43: stages.2.blocks.5.ls2.gamma's lr=0.0006070766814125999\n",
      "44: stages.2.blocks.5.norm2.bias's lr=0.0006070766814125999\n",
      "45: stages.2.blocks.5.norm2.weight's lr=0.0006070766814125999\n",
      "46: stages.2.blocks.5.mlp.fc2.bias's lr=0.0006070766814125999\n",
      "47: stages.2.blocks.5.mlp.fc2.weight's lr=0.0006070766814125999\n",
      "48: stages.2.blocks.5.mlp.fc1.bias's lr=0.0006070766814125999\n",
      "49: stages.2.blocks.5.mlp.fc1.weight's lr=0.0006070766814125999\n",
      "50: stages.2.blocks.5.ls1.gamma's lr=0.0006070766814125999\n",
      "51: stages.2.blocks.5.norm1.bias's lr=0.0006070766814125999\n",
      "52: stages.2.blocks.5.norm1.weight's lr=0.0006070766814125999\n",
      "53: stages.2.blocks.5.attn.proj.bias's lr=0.0006070766814125999\n",
      "54: stages.2.blocks.5.attn.proj.weight's lr=0.0006070766814125999\n",
      "55: stages.2.blocks.5.attn.qkv.bias's lr=0.0006070766814125999\n",
      "56: stages.2.blocks.5.attn.qkv.weight's lr=0.0006070766814125999\n",
      "57: stages.2.blocks.5.attn.crpb_mlp.3.weight's lr=0.0006070766814125999\n",
      "58: stages.2.blocks.5.attn.crpb_mlp.0.bias's lr=0.0006070766814125999\n",
      "59: stages.2.blocks.5.attn.crpb_mlp.0.weight's lr=0.0006070766814125999\n",
      "60: stages.2.blocks.5.attn.t_scale's lr=0.0006070766814125999\n",
      "61: stages.2.blocks.4.ls2.gamma's lr=0.0005281567128289619\n",
      "62: stages.2.blocks.4.norm2.bias's lr=0.0005281567128289619\n",
      "63: stages.2.blocks.4.norm2.weight's lr=0.0005281567128289619\n",
      "64: stages.2.blocks.4.mlp.fc2.bias's lr=0.0005281567128289619\n",
      "65: stages.2.blocks.4.mlp.fc2.weight's lr=0.0005281567128289619\n",
      "66: stages.2.blocks.4.mlp.fc1.bias's lr=0.0005281567128289619\n",
      "67: stages.2.blocks.4.mlp.fc1.weight's lr=0.0005281567128289619\n",
      "68: stages.2.blocks.4.ls1.gamma's lr=0.0005281567128289619\n",
      "69: stages.2.blocks.4.norm1.bias's lr=0.0005281567128289619\n",
      "70: stages.2.blocks.4.norm1.weight's lr=0.0005281567128289619\n",
      "71: stages.2.blocks.4.attn.proj.bias's lr=0.0005281567128289619\n",
      "72: stages.2.blocks.4.attn.proj.weight's lr=0.0005281567128289619\n",
      "73: stages.2.blocks.4.attn.qkv.bias's lr=0.0005281567128289619\n",
      "74: stages.2.blocks.4.attn.qkv.weight's lr=0.0005281567128289619\n",
      "75: stages.2.blocks.4.attn.crpb_mlp.3.weight's lr=0.0005281567128289619\n",
      "76: stages.2.blocks.4.attn.crpb_mlp.0.bias's lr=0.0005281567128289619\n",
      "77: stages.2.blocks.4.attn.crpb_mlp.0.weight's lr=0.0005281567128289619\n",
      "78: stages.2.blocks.4.attn.t_scale's lr=0.0005281567128289619\n",
      "79: stages.2.blocks.3.ls2.gamma's lr=0.00045949634016119684\n",
      "80: stages.2.blocks.3.norm2.bias's lr=0.00045949634016119684\n",
      "81: stages.2.blocks.3.norm2.weight's lr=0.00045949634016119684\n",
      "82: stages.2.blocks.3.mlp.fc2.bias's lr=0.00045949634016119684\n",
      "83: stages.2.blocks.3.mlp.fc2.weight's lr=0.00045949634016119684\n",
      "84: stages.2.blocks.3.mlp.fc1.bias's lr=0.00045949634016119684\n",
      "85: stages.2.blocks.3.mlp.fc1.weight's lr=0.00045949634016119684\n",
      "86: stages.2.blocks.3.ls1.gamma's lr=0.00045949634016119684\n",
      "87: stages.2.blocks.3.norm1.bias's lr=0.00045949634016119684\n",
      "88: stages.2.blocks.3.norm1.weight's lr=0.00045949634016119684\n",
      "89: stages.2.blocks.3.attn.proj.bias's lr=0.00045949634016119684\n",
      "90: stages.2.blocks.3.attn.proj.weight's lr=0.00045949634016119684\n",
      "91: stages.2.blocks.3.attn.qkv.bias's lr=0.00045949634016119684\n",
      "92: stages.2.blocks.3.attn.qkv.weight's lr=0.00045949634016119684\n",
      "93: stages.2.blocks.3.attn.crpb_mlp.3.weight's lr=0.00045949634016119684\n",
      "94: stages.2.blocks.3.attn.crpb_mlp.0.bias's lr=0.00045949634016119684\n",
      "95: stages.2.blocks.3.attn.crpb_mlp.0.weight's lr=0.00045949634016119684\n",
      "96: stages.2.blocks.3.attn.t_scale's lr=0.00045949634016119684\n",
      "97: stages.2.blocks.2.ls2.gamma's lr=0.00039976181594024126\n",
      "98: stages.2.blocks.2.norm2.bias's lr=0.00039976181594024126\n",
      "99: stages.2.blocks.2.norm2.weight's lr=0.00039976181594024126\n",
      "100: stages.2.blocks.2.mlp.fc2.bias's lr=0.00039976181594024126\n",
      "101: stages.2.blocks.2.mlp.fc2.weight's lr=0.00039976181594024126\n",
      "102: stages.2.blocks.2.mlp.fc1.bias's lr=0.00039976181594024126\n",
      "103: stages.2.blocks.2.mlp.fc1.weight's lr=0.00039976181594024126\n",
      "104: stages.2.blocks.2.ls1.gamma's lr=0.00039976181594024126\n",
      "105: stages.2.blocks.2.norm1.bias's lr=0.00039976181594024126\n",
      "106: stages.2.blocks.2.norm1.weight's lr=0.00039976181594024126\n",
      "107: stages.2.blocks.2.attn.proj.bias's lr=0.00039976181594024126\n",
      "108: stages.2.blocks.2.attn.proj.weight's lr=0.00039976181594024126\n",
      "109: stages.2.blocks.2.attn.qkv.bias's lr=0.00039976181594024126\n",
      "110: stages.2.blocks.2.attn.qkv.weight's lr=0.00039976181594024126\n",
      "111: stages.2.blocks.2.attn.crpb_mlp.3.weight's lr=0.00039976181594024126\n",
      "112: stages.2.blocks.2.attn.crpb_mlp.0.bias's lr=0.00039976181594024126\n",
      "113: stages.2.blocks.2.attn.crpb_mlp.0.weight's lr=0.00039976181594024126\n",
      "114: stages.2.blocks.2.attn.t_scale's lr=0.00039976181594024126\n",
      "115: stages.2.blocks.1.ls2.gamma's lr=0.0003477927798680099\n",
      "116: stages.2.blocks.1.norm2.bias's lr=0.0003477927798680099\n",
      "117: stages.2.blocks.1.norm2.weight's lr=0.0003477927798680099\n",
      "118: stages.2.blocks.1.mlp.fc2.bias's lr=0.0003477927798680099\n",
      "119: stages.2.blocks.1.mlp.fc2.weight's lr=0.0003477927798680099\n",
      "120: stages.2.blocks.1.mlp.fc1.bias's lr=0.0003477927798680099\n",
      "121: stages.2.blocks.1.mlp.fc1.weight's lr=0.0003477927798680099\n",
      "122: stages.2.blocks.1.ls1.gamma's lr=0.0003477927798680099\n",
      "123: stages.2.blocks.1.norm1.bias's lr=0.0003477927798680099\n",
      "124: stages.2.blocks.1.norm1.weight's lr=0.0003477927798680099\n",
      "125: stages.2.blocks.1.attn.proj.bias's lr=0.0003477927798680099\n",
      "126: stages.2.blocks.1.attn.proj.weight's lr=0.0003477927798680099\n",
      "127: stages.2.blocks.1.attn.qkv.bias's lr=0.0003477927798680099\n",
      "128: stages.2.blocks.1.attn.qkv.weight's lr=0.0003477927798680099\n",
      "129: stages.2.blocks.1.attn.crpb_mlp.3.weight's lr=0.0003477927798680099\n",
      "130: stages.2.blocks.1.attn.crpb_mlp.0.bias's lr=0.0003477927798680099\n",
      "131: stages.2.blocks.1.attn.crpb_mlp.0.weight's lr=0.0003477927798680099\n",
      "132: stages.2.blocks.1.attn.t_scale's lr=0.0003477927798680099\n",
      "133: stages.2.blocks.0.ls2.gamma's lr=0.0003025797184851686\n",
      "134: stages.2.blocks.0.norm2.bias's lr=0.0003025797184851686\n",
      "135: stages.2.blocks.0.norm2.weight's lr=0.0003025797184851686\n",
      "136: stages.2.blocks.0.mlp.fc2.bias's lr=0.0003025797184851686\n",
      "137: stages.2.blocks.0.mlp.fc2.weight's lr=0.0003025797184851686\n",
      "138: stages.2.blocks.0.mlp.fc1.bias's lr=0.0003025797184851686\n",
      "139: stages.2.blocks.0.mlp.fc1.weight's lr=0.0003025797184851686\n",
      "140: stages.2.blocks.0.ls1.gamma's lr=0.0003025797184851686\n",
      "141: stages.2.blocks.0.norm1.bias's lr=0.0003025797184851686\n",
      "142: stages.2.blocks.0.norm1.weight's lr=0.0003025797184851686\n",
      "143: stages.2.blocks.0.attn.proj.bias's lr=0.0003025797184851686\n",
      "144: stages.2.blocks.0.attn.proj.weight's lr=0.0003025797184851686\n",
      "145: stages.2.blocks.0.attn.qkv.bias's lr=0.0003025797184851686\n",
      "146: stages.2.blocks.0.attn.qkv.weight's lr=0.0003025797184851686\n",
      "147: stages.2.blocks.0.attn.crpb_mlp.3.weight's lr=0.0003025797184851686\n",
      "148: stages.2.blocks.0.attn.crpb_mlp.0.bias's lr=0.0003025797184851686\n",
      "149: stages.2.blocks.0.attn.crpb_mlp.0.weight's lr=0.0003025797184851686\n",
      "150: stages.2.blocks.0.attn.t_scale's lr=0.0003025797184851686\n",
      "151: stages.1.downsample.norm.bias's lr=0.0002632443550820967\n",
      "152: stages.1.downsample.norm.weight's lr=0.0002632443550820967\n",
      "153: stages.1.downsample.reduction.weight's lr=0.0002290225889214241\n",
      "154: stages.1.blocks.1.ls2.gamma's lr=0.00019924965236163897\n",
      "155: stages.1.blocks.1.norm2.bias's lr=0.00019924965236163897\n",
      "156: stages.1.blocks.1.norm2.weight's lr=0.00019924965236163897\n",
      "157: stages.1.blocks.1.mlp.fc2.bias's lr=0.00019924965236163897\n",
      "158: stages.1.blocks.1.mlp.fc2.weight's lr=0.00019924965236163897\n",
      "159: stages.1.blocks.1.mlp.fc1.bias's lr=0.00019924965236163897\n",
      "160: stages.1.blocks.1.mlp.fc1.weight's lr=0.00019924965236163897\n",
      "161: stages.1.blocks.1.ls1.gamma's lr=0.00019924965236163897\n",
      "162: stages.1.blocks.1.norm1.bias's lr=0.00019924965236163897\n",
      "163: stages.1.blocks.1.norm1.weight's lr=0.00019924965236163897\n",
      "164: stages.1.blocks.1.attn.proj.bias's lr=0.00019924965236163897\n",
      "165: stages.1.blocks.1.attn.proj.weight's lr=0.00019924965236163897\n",
      "166: stages.1.blocks.1.attn.qkv.bias's lr=0.00019924965236163897\n",
      "167: stages.1.blocks.1.attn.qkv.weight's lr=0.00019924965236163897\n",
      "168: stages.1.blocks.1.attn.crpb_mlp.3.weight's lr=0.00019924965236163897\n",
      "169: stages.1.blocks.1.attn.crpb_mlp.0.bias's lr=0.00019924965236163897\n",
      "170: stages.1.blocks.1.attn.crpb_mlp.0.weight's lr=0.00019924965236163897\n",
      "171: stages.1.blocks.1.attn.t_scale's lr=0.00019924965236163897\n",
      "172: stages.1.blocks.0.ls2.gamma's lr=0.0001733471975546259\n",
      "173: stages.1.blocks.0.norm2.bias's lr=0.0001733471975546259\n",
      "174: stages.1.blocks.0.norm2.weight's lr=0.0001733471975546259\n",
      "175: stages.1.blocks.0.mlp.fc2.bias's lr=0.0001733471975546259\n",
      "176: stages.1.blocks.0.mlp.fc2.weight's lr=0.0001733471975546259\n",
      "177: stages.1.blocks.0.mlp.fc1.bias's lr=0.0001733471975546259\n",
      "178: stages.1.blocks.0.mlp.fc1.weight's lr=0.0001733471975546259\n",
      "179: stages.1.blocks.0.ls1.gamma's lr=0.0001733471975546259\n",
      "180: stages.1.blocks.0.norm1.bias's lr=0.0001733471975546259\n",
      "181: stages.1.blocks.0.norm1.weight's lr=0.0001733471975546259\n",
      "182: stages.1.blocks.0.attn.proj.bias's lr=0.0001733471975546259\n",
      "183: stages.1.blocks.0.attn.proj.weight's lr=0.0001733471975546259\n",
      "184: stages.1.blocks.0.attn.qkv.bias's lr=0.0001733471975546259\n",
      "185: stages.1.blocks.0.attn.qkv.weight's lr=0.0001733471975546259\n",
      "186: stages.1.blocks.0.attn.crpb_mlp.3.weight's lr=0.0001733471975546259\n",
      "187: stages.1.blocks.0.attn.crpb_mlp.0.bias's lr=0.0001733471975546259\n",
      "188: stages.1.blocks.0.attn.crpb_mlp.0.weight's lr=0.0001733471975546259\n",
      "189: stages.1.blocks.0.attn.t_scale's lr=0.0001733471975546259\n",
      "190: stages.0.downsample.norm.bias's lr=0.00015081206187252452\n",
      "191: stages.0.downsample.norm.weight's lr=0.00015081206187252452\n",
      "192: stages.0.downsample.reduction.weight's lr=0.00013120649382909633\n",
      "193: stages.0.blocks.1.ls2.gamma's lr=0.0001141496496313138\n",
      "194: stages.0.blocks.1.norm2.bias's lr=0.0001141496496313138\n",
      "195: stages.0.blocks.1.norm2.weight's lr=0.0001141496496313138\n",
      "196: stages.0.blocks.1.mlp.fc2.bias's lr=0.0001141496496313138\n",
      "197: stages.0.blocks.1.mlp.fc2.weight's lr=0.0001141496496313138\n",
      "198: stages.0.blocks.1.mlp.fc1.bias's lr=0.0001141496496313138\n",
      "199: stages.0.blocks.1.mlp.fc1.weight's lr=0.0001141496496313138\n",
      "200: stages.0.blocks.1.ls1.gamma's lr=0.0001141496496313138\n",
      "201: stages.0.blocks.1.norm1.bias's lr=0.0001141496496313138\n",
      "202: stages.0.blocks.1.norm1.weight's lr=0.0001141496496313138\n",
      "203: stages.0.blocks.1.attn.proj.bias's lr=0.0001141496496313138\n",
      "204: stages.0.blocks.1.attn.proj.weight's lr=0.0001141496496313138\n",
      "205: stages.0.blocks.1.attn.qkv.bias's lr=0.0001141496496313138\n",
      "206: stages.0.blocks.1.attn.qkv.weight's lr=0.0001141496496313138\n",
      "207: stages.0.blocks.1.attn.crpb_mlp.3.weight's lr=0.0001141496496313138\n",
      "208: stages.0.blocks.1.attn.crpb_mlp.0.bias's lr=0.0001141496496313138\n",
      "209: stages.0.blocks.1.attn.crpb_mlp.0.weight's lr=0.0001141496496313138\n",
      "210: stages.0.blocks.1.attn.t_scale's lr=0.0001141496496313138\n",
      "211: stages.0.blocks.0.ls2.gamma's lr=9.9310195179243e-05\n",
      "212: stages.0.blocks.0.norm2.bias's lr=9.9310195179243e-05\n",
      "213: stages.0.blocks.0.norm2.weight's lr=9.9310195179243e-05\n",
      "214: stages.0.blocks.0.mlp.fc2.bias's lr=9.9310195179243e-05\n",
      "215: stages.0.blocks.0.mlp.fc2.weight's lr=9.9310195179243e-05\n",
      "216: stages.0.blocks.0.mlp.fc1.bias's lr=9.9310195179243e-05\n",
      "217: stages.0.blocks.0.mlp.fc1.weight's lr=9.9310195179243e-05\n",
      "218: stages.0.blocks.0.ls1.gamma's lr=9.9310195179243e-05\n",
      "219: stages.0.blocks.0.norm1.bias's lr=9.9310195179243e-05\n",
      "220: stages.0.blocks.0.norm1.weight's lr=9.9310195179243e-05\n",
      "221: stages.0.blocks.0.attn.proj.bias's lr=9.9310195179243e-05\n",
      "222: stages.0.blocks.0.attn.proj.weight's lr=9.9310195179243e-05\n",
      "223: stages.0.blocks.0.attn.qkv.bias's lr=9.9310195179243e-05\n",
      "224: stages.0.blocks.0.attn.qkv.weight's lr=9.9310195179243e-05\n",
      "225: stages.0.blocks.0.attn.crpb_mlp.3.weight's lr=9.9310195179243e-05\n",
      "226: stages.0.blocks.0.attn.crpb_mlp.0.bias's lr=9.9310195179243e-05\n",
      "227: stages.0.blocks.0.attn.crpb_mlp.0.weight's lr=9.9310195179243e-05\n",
      "228: stages.0.blocks.0.attn.t_scale's lr=9.9310195179243e-05\n",
      "229: embeddings.norm.bias's lr=8.639986980594141e-05\n",
      "230: embeddings.norm.weight's lr=8.639986980594141e-05\n",
      "231: embeddings.patch_embeddings.bias's lr=8.639986980594141e-05\n",
      "232: embeddings.patch_embeddings.weight's lr=8.639986980594141e-05\n",
      "233: absolute_pos_embed's lr=7.516788673116902e-05\n"
     ]
    }
   ],
   "source": [
    "lr      = 1.4e-3      # paper : 1.4e-3\n",
    "lr_mult = 0.87  # paper : 0.87\n",
    "weight_decay = 0.01 # paper : 0.1\n",
    "\n",
    "param_groups = []\n",
    "prev_group_name = layer_names[0].split('.')[0]\n",
    "\n",
    "for idx, name in enumerate(layer_names):\n",
    "    \n",
    "    cur_group_name = name.split('.')[0] if 'stage' not in name else name.split('.')[3]\n",
    "    \n",
    "    if cur_group_name != prev_group_name:\n",
    "        lr *= lr_mult\n",
    "    prev_group_name = cur_group_name\n",
    "    \n",
    "    print(f\"{idx}: {name}'s lr={lr}\")\n",
    "    \n",
    "    param_groups += [{'params': [ p for n, p in model.named_parameters() if n == name and p.requires_grad],\n",
    "                      'lr' : lr,\n",
    "                      'weight_decay': weight_decay}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 모델 레이어의 이름 추출\n",
    "# layer_names = []\n",
    "# for i, (name, params) in enumerate(model.named_parameters()):\n",
    "#     lr = base_lr\n",
    "#     print(f'{i}: {name}')\n",
    "#     layer_names.append(name)\n",
    "\n",
    "# # 뒷 레이어부터 시작하도록 뒤집기    \n",
    "# layer_names.reverse()\n",
    "\n",
    "# # 하이퍼 파라미터 정의\n",
    "# lr      = 1.4e-3      # paper : 1.4e-3\n",
    "# lr_mult = 0.87  # paper : 0.87\n",
    "# weight_decay = 0.01 # paper : 0.1\n",
    "\n",
    "# param_groups = []\n",
    "# prev_group_name = layer_names[0].split('.')[0] # 그룹명 초기화\n",
    "\n",
    "# for idx, name in enumerate(layer_names):    \n",
    "#     cur_group_name = name.split('.')[0]    \n",
    "#     if cur_group_name != prev_group_name: # 동일한 그룹에 속하면 동일한 학습율\n",
    "#         lr *= lr_mult\n",
    "#     prev_group_name = cur_group_name    \n",
    "    \n",
    "#     param_groups += [{'params': [ p for n, p in model.named_parameters() if n == name and p.requires_grad],\n",
    "#                       'lr' : lr,\n",
    "#                       'weight_decay': weight_decay}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(param_groups)\n",
    "warmup_steps = int(len(train_loader)*(epochs)*0.1)\n",
    "train_steps = len(train_loader)*(epochs)\n",
    "scheduler = transformers.get_cosine_schedule_with_warmup(optimizer, \n",
    "                                                        num_warmup_steps=warmup_steps, \n",
    "                                                        num_training_steps=train_steps,\n",
    "                                                        num_cycles=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Train\n",
    "- 100에포크 먼저 학습하며 결과 확인하고, 이후 10에포크 학습하며 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 15/15 [00:55<00:00,  3.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.639624277750651, Val Loss: 4.52731466293335, LR: 9.333333333333333e-05, Duration: 56.60 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 15/15 [00:52<00:00,  3.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.480323600769043, Val Loss: 4.236594200134277, LR: 0.00018666666666666666, Duration: 53.27 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 15/15 [00:51<00:00,  3.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.29904842376709, Val Loss: 3.9949002265930176, LR: 0.00028000000000000003, Duration: 52.70 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 15/15 [00:51<00:00,  3.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.160694535573324, Val Loss: 3.894865036010742, LR: 0.0003733333333333333, Duration: 52.34 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 15/15 [00:52<00:00,  3.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.132351557413737, Val Loss: 3.8135552406311035, LR: 0.00046666666666666666, Duration: 53.43 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 15/15 [00:51<00:00,  3.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.112626965840658, Val Loss: 3.6758129596710205, LR: 0.0005600000000000001, Duration: 52.55 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 15/15 [00:52<00:00,  3.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.038573487599691, Val Loss: 3.5036494731903076, LR: 0.0006533333333333333, Duration: 53.33 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 15/15 [00:50<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.9282424290974935, Val Loss: 3.470417022705078, LR: 0.0007466666666666666, Duration: 51.99 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 15/15 [00:51<00:00,  3.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.9768927892049155, Val Loss: 3.210340976715088, LR: 0.0008399999999999999, Duration: 52.19 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 15/15 [00:51<00:00,  3.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.7661957263946535, Val Loss: 3.1094565391540527, LR: 0.0009333333333333333, Duration: 52.92 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 15/15 [00:51<00:00,  3.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.7604698816935223, Val Loss: 2.9728660583496094, LR: 0.0010266666666666666, Duration: 52.49 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 15/15 [00:51<00:00,  3.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.6178701400756834, Val Loss: 2.8156075477600098, LR: 0.0011200000000000001, Duration: 52.24 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 15/15 [00:51<00:00,  3.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.7217746575673423, Val Loss: 3.1104984283447266, LR: 0.0012133333333333334, Duration: 52.51 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 15/15 [00:52<00:00,  3.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.871579647064209, Val Loss: 2.9658126831054688, LR: 0.0013066666666666667, Duration: 53.14 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 15/15 [00:50<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.6477537473042805, Val Loss: 2.8373024463653564, LR: 0.0014, Duration: 51.71 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 15/15 [00:50<00:00,  3.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.5775813897450766, Val Loss: 2.6074471473693848, LR: 0.001399810468825623, Duration: 51.53 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 15/15 [00:50<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.720430008570353, Val Loss: 2.6565752029418945, LR: 0.0013992419779369672, Duration: 51.63 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 15/15 [00:50<00:00,  3.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.5597028732299805, Val Loss: 2.668596029281616, LR: 0.001398294835181877, Duration: 51.87 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 15/15 [00:50<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.5700852076212564, Val Loss: 2.4890060424804688, LR: 0.001396969553454863, Duration: 51.92 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 15/15 [00:50<00:00,  3.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.486566988627116, Val Loss: 2.686659812927246, LR: 0.0013952668504193602, Duration: 51.40 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 15/15 [00:50<00:00,  3.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.441392437616984, Val Loss: 2.372102975845337, LR: 0.0013931876481190993, Duration: 51.59 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 15/15 [00:50<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.4252774079640704, Val Loss: 2.2048966884613037, LR: 0.0013907330724788056, Duration: 51.89 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 15/15 [00:51<00:00,  3.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.5753732204437254, Val Loss: 2.3361012935638428, LR: 0.0013879044526944892, Duration: 52.15 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 15/15 [00:50<00:00,  3.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.3498565991719564, Val Loss: 2.134786605834961, LR: 0.001384703320513664, Duration: 52.24 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 15/15 [00:50<00:00,  3.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.31205054918925, Val Loss: 1.994530439376831, LR: 0.0013811314094058767, Duration: 52.12 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 15/15 [00:50<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.365058422088623, Val Loss: 1.9667328596115112, LR: 0.0013771906536240047, Duration: 51.84 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 15/15 [00:51<00:00,  3.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.0973904927571616, Val Loss: 2.0071427822113037, LR: 0.0013728831871568231, Duration: 52.47 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 15/15 [00:50<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.2678685506184895, Val Loss: 1.972508430480957, LR: 0.0013682113425734124, Duration: 51.71 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 15/15 [00:49<00:00,  3.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.178713385264079, Val Loss: 1.9284074306488037, LR: 0.0013631776497600304, Duration: 51.13 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 15/15 [00:50<00:00,  3.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.3949069023132323, Val Loss: 1.9838560819625854, LR: 0.001357784834550136, Duration: 51.86 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|██████████| 15/15 [00:50<00:00,  3.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.1374642054239907, Val Loss: 1.7706830501556396, LR: 0.0013520358172482998, Duration: 51.63 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|██████████| 15/15 [00:51<00:00,  3.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.3798948605855306, Val Loss: 1.9449610710144043, LR: 0.0013459337110488096, Duration: 52.29 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|██████████| 15/15 [00:51<00:00,  3.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.917900625864665, Val Loss: 1.6323398351669312, LR: 0.0013394818203498204, Duration: 52.34 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|██████████| 15/15 [00:50<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.8336232821146647, Val Loss: 1.6418977975845337, LR: 0.0013326836389639645, Duration: 51.76 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|██████████| 15/15 [00:50<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.2018714904785157, Val Loss: 1.8383897542953491, LR: 0.0013255428482263885, Duration: 51.64 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|██████████| 15/15 [00:51<00:00,  3.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.897903633117676, Val Loss: 1.5669962167739868, LR: 0.0013180633150012488, Duration: 52.23 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37: 100%|██████████| 15/15 [00:50<00:00,  3.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.8518974939982096, Val Loss: 1.5411107540130615, LR: 0.0013102490895877336, Duration: 51.64 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|██████████| 15/15 [00:51<00:00,  3.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.093213939666748, Val Loss: 1.4993896484375, LR: 0.001302104403526756, Duration: 52.86 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|██████████| 15/15 [00:50<00:00,  3.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.0245630741119385, Val Loss: 1.5315135717391968, LR: 0.001293633667309498, Duration: 51.21 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40: 100%|██████████| 15/15 [00:50<00:00,  3.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.3337452093760174, Val Loss: 1.5367175340652466, LR: 0.0012848414679890556, Duration: 51.10 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41: 100%|██████████| 15/15 [00:51<00:00,  3.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.0750495910644533, Val Loss: 1.5404714345932007, LR: 0.0012757325666964635, Duration: 51.95 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42: 100%|██████████| 15/15 [00:50<00:00,  3.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.7122789065043134, Val Loss: 1.437600016593933, LR: 0.0012663118960624632, Duration: 51.54 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43: 100%|██████████| 15/15 [00:50<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.67657356262207, Val Loss: 1.3624628782272339, LR: 0.0012565845575463934, Duration: 52.06 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44: 100%|██████████| 15/15 [00:51<00:00,  3.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.9973755995432536, Val Loss: 1.4044240713119507, LR: 0.0012465558186736615, Duration: 52.30 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45: 100%|██████████| 15/15 [00:50<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.487544854482015, Val Loss: 1.2798149585723877, LR: 0.0012362311101832846, Duration: 52.02 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46: 100%|██████████| 15/15 [00:52<00:00,  3.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.821072498957316, Val Loss: 1.3653918504714966, LR: 0.0012256160230870495, Duration: 53.34 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47: 100%|██████████| 15/15 [00:50<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.8453665097554524, Val Loss: 1.2465218305587769, LR: 0.00121471630564188, Duration: 52.05 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48: 100%|██████████| 15/15 [00:52<00:00,  3.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5465858141581217, Val Loss: 1.2531335353851318, LR: 0.0012035378602370558, Duration: 52.94 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 15/15 [00:51<00:00,  3.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.868056535720825, Val Loss: 1.27102792263031, LR: 0.0011920867401979632, Duration: 52.26 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50: 100%|██████████| 15/15 [00:50<00:00,  3.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.708110253016154, Val Loss: 1.1863858699798584, LR: 0.0011803691465081135, Duration: 52.15 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51: 100%|██████████| 15/15 [00:51<00:00,  3.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.726731562614441, Val Loss: 1.1767187118530273, LR: 0.0011683914244512007, Duration: 52.53 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52: 100%|██████████| 15/15 [00:52<00:00,  3.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.0887450218200683, Val Loss: 1.3356021642684937, LR: 0.0011561600601750187, Duration: 53.40 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53: 100%|██████████| 15/15 [00:51<00:00,  3.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.418167742093404, Val Loss: 1.184178113937378, LR: 0.001143681677179097, Duration: 52.01 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54: 100%|██████████| 15/15 [00:50<00:00,  3.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.737472081184387, Val Loss: 1.2643237113952637, LR: 0.0011309630327279608, Duration: 51.56 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55: 100%|██████████| 15/15 [00:50<00:00,  3.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.7109407583872476, Val Loss: 1.1689482927322388, LR: 0.0011180110141919503, Duration: 51.78 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56: 100%|██████████| 15/15 [00:50<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.999515914916992, Val Loss: 1.2303063869476318, LR: 0.0011048326353175905, Duration: 51.82 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57: 100%|██████████| 15/15 [00:50<00:00,  3.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3718130826950072, Val Loss: 1.137321949005127, LR: 0.0010914350324295228, Duration: 51.66 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58: 100%|██████████| 15/15 [00:50<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5416166464487713, Val Loss: 1.114243984222412, LR: 0.0010778254605660592, Duration: 51.96 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59: 100%|██████████| 15/15 [00:50<00:00,  3.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6446069955825804, Val Loss: 1.0870097875595093, LR: 0.0010640112895504506, Duration: 51.77 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60: 100%|██████████| 15/15 [00:50<00:00,  3.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.8083463430404665, Val Loss: 1.1110329627990723, LR: 0.00105, Duration: 51.57 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61: 100%|██████████| 15/15 [00:51<00:00,  3.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.498761439323425, Val Loss: 1.0722063779830933, LR: 0.0010357991792751724, Duration: 52.31 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62: 100%|██████████| 15/15 [00:51<00:00,  3.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5879223108291627, Val Loss: 1.0815168619155884, LR: 0.001021416517370908, Duration: 52.40 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63: 100%|██████████| 15/15 [00:50<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.8875001271565757, Val Loss: 1.11161208152771, LR: 0.001006859802752354, Duration: 51.75 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64: 100%|██████████| 15/15 [00:51<00:00,  3.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4629178285598754, Val Loss: 1.0083096027374268, LR: 0.0009921369181372726, Duration: 52.39 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65: 100%|██████████| 15/15 [00:50<00:00,  3.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.8941696087519326, Val Loss: 1.0668854713439941, LR: 0.0009772558362274098, Duration: 51.91 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66: 100%|██████████| 15/15 [00:51<00:00,  3.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.38413032690684, Val Loss: 0.9688988924026489, LR: 0.0009622246153911386, Duration: 52.39 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67: 100%|██████████| 15/15 [00:50<00:00,  3.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.44362047513326, Val Loss: 1.034164309501648, LR: 0.0009470513952997081, Duration: 51.19 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68: 100%|██████████| 15/15 [00:51<00:00,  3.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5956239620844523, Val Loss: 1.0586718320846558, LR: 0.0009317443925194707, Duration: 52.63 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69: 100%|██████████| 15/15 [00:51<00:00,  3.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3108063697814942, Val Loss: 0.8803516626358032, LR: 0.0009163118960624632, Duration: 52.75 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70: 100%|██████████| 15/15 [00:51<00:00,  3.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.7512192646662395, Val Loss: 0.95628422498703, LR: 0.0009007622628977632, Duration: 52.55 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71: 100%|██████████| 15/15 [00:52<00:00,  3.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4602179288864137, Val Loss: 0.9536236524581909, LR: 0.0008851039134260417, Duration: 53.00 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72: 100%|██████████| 15/15 [00:51<00:00,  3.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4564571301142375, Val Loss: 0.9204540252685547, LR: 0.0008693453269197673, Duration: 52.34 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 73: 100%|██████████| 15/15 [00:50<00:00,  3.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4142754793167116, Val Loss: 0.8655402064323425, LR: 0.0008534950369315323, Duration: 51.73 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 74: 100%|██████████| 15/15 [00:50<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4369064887364704, Val Loss: 0.9492825865745544, LR: 0.0008375616266729811, Duration: 51.60 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75: 100%|██████████| 15/15 [00:51<00:00,  3.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6146244684855144, Val Loss: 0.9645176529884338, LR: 0.0008215537243668514, Duration: 52.03 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 76: 100%|██████████| 15/15 [00:50<00:00,  3.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3897628784179688, Val Loss: 0.8983685374259949, LR: 0.0008054799985746381, Duration: 51.48 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 77: 100%|██████████| 15/15 [00:51<00:00,  3.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.360036571820577, Val Loss: 0.9264922142028809, LR: 0.0007893491535024164, Duration: 52.18 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 78: 100%|██████████| 15/15 [00:50<00:00,  3.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.240149466196696, Val Loss: 0.8855813145637512, LR: 0.0007731699242873575, Duration: 51.88 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 79: 100%|██████████| 15/15 [00:50<00:00,  3.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.8719823996225993, Val Loss: 0.7970055937767029, LR: 0.0007569510722675008, Duration: 51.56 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 80: 100%|██████████| 15/15 [00:50<00:00,  3.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.550924714406331, Val Loss: 0.8653868436813354, LR: 0.000740701380237333, Duration: 51.51 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 81: 100%|██████████| 15/15 [00:51<00:00,  3.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.184614372253418, Val Loss: 0.8457915186882019, LR: 0.0007244296476917508, Duration: 52.39 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 82: 100%|██████████| 15/15 [00:52<00:00,  3.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1950034221013386, Val Loss: 0.8505644202232361, LR: 0.0007081446860609781, Duration: 53.95 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 83: 100%|██████████| 15/15 [00:52<00:00,  3.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3487814108530682, Val Loss: 0.8019826412200928, LR: 0.0006918553139390222, Duration: 53.91 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 84: 100%|██████████| 15/15 [00:51<00:00,  3.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.391483489672343, Val Loss: 0.8646469116210938, LR: 0.0006755703523082495, Duration: 52.09 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 85: 100%|██████████| 15/15 [00:51<00:00,  3.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.9610302925109864, Val Loss: 0.8560354113578796, LR: 0.000659298619762667, Duration: 52.08 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 86: 100%|██████████| 15/15 [00:50<00:00,  3.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.122304010391235, Val Loss: 0.7402864694595337, LR: 0.0006430489277324992, Duration: 51.52 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 87: 100%|██████████| 15/15 [00:50<00:00,  3.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3401155710220336, Val Loss: 0.8361548781394958, LR: 0.0006268300757126426, Duration: 51.43 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 88: 100%|██████████| 15/15 [00:50<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3885532061258954, Val Loss: 0.7829493880271912, LR: 0.0006106508464975837, Duration: 51.68 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 89: 100%|██████████| 15/15 [00:51<00:00,  3.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.415633050600688, Val Loss: 0.8555623292922974, LR: 0.0005945200014253619, Duration: 52.15 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 90: 100%|██████████| 15/15 [00:50<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.9219393491744996, Val Loss: 0.7818421721458435, LR: 0.0005784462756331488, Duration: 51.84 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 91: 100%|██████████| 15/15 [00:50<00:00,  3.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.055371125539144, Val Loss: 0.7281903028488159, LR: 0.0005624383733270188, Duration: 51.72 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 92: 100%|██████████| 15/15 [00:54<00:00,  3.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.285679332415263, Val Loss: 0.80321204662323, LR: 0.0005465049630684676, Duration: 55.14 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 93: 100%|██████████| 15/15 [00:59<00:00,  3.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.029432678222656, Val Loss: 0.7448874711990356, LR: 0.0005306546730802327, Duration: 60.77 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 94: 100%|██████████| 15/15 [01:14<00:00,  4.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.026088277498881, Val Loss: 0.7991520762443542, LR: 0.0005148960865739587, Duration: 76.65 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 95: 100%|██████████| 15/15 [01:04<00:00,  4.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0650129636128742, Val Loss: 0.7133557796478271, LR: 0.000499237737102237, Duration: 65.44 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 96: 100%|██████████| 15/15 [00:59<00:00,  3.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.063551115989685, Val Loss: 0.7366840839385986, LR: 0.0004836881039375369, Duration: 60.46 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 97: 100%|██████████| 15/15 [01:04<00:00,  4.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1047794183095294, Val Loss: 0.7533627152442932, LR: 0.0004682556074805294, Duration: 65.85 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 98: 100%|██████████| 15/15 [01:03<00:00,  4.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.8517841895421345, Val Loss: 0.7762777805328369, LR: 0.00045294860470029185, Duration: 64.89 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 15/15 [01:03<00:00,  4.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2797441403071086, Val Loss: 0.779818058013916, LR: 0.0004377753846088615, Duration: 65.66 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 100: 100%|██████████| 15/15 [00:58<00:00,  3.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.135902206103007, Val Loss: 0.7159838080406189, LR: 0.0004227441637725902, Duration: 59.28 sec\n",
      "Epoch 당 평균 소요시간 : 35.48초\n"
     ]
    }
   ],
   "source": [
    "training_time = 0\n",
    "losses = []\n",
    "val_losses = []\n",
    "lrs = []\n",
    "best_loss = float('inf')\n",
    "\n",
    "# GradScaler 초기화\n",
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch + 1}\")\n",
    "    \n",
    "    for _, data in pbar:\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        inputs, labels = mixup_fn(inputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # AutoCast 적용\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "        # 스케일링된 그라디언트 계산\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # 그라디언트 클리핑 전에 스케일링 제거\n",
    "        scaler.unscale_(optimizer)\n",
    "        clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "\n",
    "        # 옵티마이저 스텝 및 스케일러 업데이트\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "            \n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        lrs.append(lr)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)        \n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in valid_loader:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    val_loss /= len(valid_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # 모델 저장\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        model_save = True\n",
    "        if model_save:\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_duration = time.time() - start_time\n",
    "    training_time += epoch_duration\n",
    "    \n",
    "    text = f'\\tLoss: {epoch_loss}, Val Loss: {val_loss}, LR: {lr}, Duration: {epoch_duration:.2f} sec'\n",
    "    \n",
    "    if model_save:\n",
    "        text += f' - model saved!'\n",
    "        model_save = False\n",
    "\n",
    "    print(text)\n",
    "        \n",
    "text = f\"Epoch 당 평균 소요시간 : {training_time / epochs:.2f}초\"      \n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.856000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.871734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Recall</td>\n",
       "      <td>0.856000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F1 Score</td>\n",
       "      <td>0.846234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Metric     Value\n",
       "0   Accuracy  0.856000\n",
       "1  Precision  0.871734\n",
       "2     Recall  0.856000\n",
       "3   F1 Score  0.846234"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예측 수행 및 레이블 저장\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# 혼동 행렬 생성\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# 예측과 실제 레이블\n",
    "y_true = all_labels  # 실제 레이블\n",
    "y_pred = all_preds  # 모델에 의해 예측된 레이블\n",
    "\n",
    "# 전체 데이터셋에 대한 정확도\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# 평균 정밀도, 리콜, F1-Score ('weighted')\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "\n",
    "# 판다스 데이터프레임으로 결과 정리\n",
    "performance_metrics = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "    'Value': [accuracy, precision, recall, f1_score]\n",
    "})\n",
    "\n",
    "# 데이터프레임 출력\n",
    "performance_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 110 Epoch Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 15/15 [01:03<00:00,  4.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.8520782709121704, Val Loss: 0.7338253259658813, LR: 0.0004078630818627275, Duration: 64.75 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 15/15 [01:03<00:00,  4.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.9222884893417358, Val Loss: 0.6909153461456299, LR: 0.00039314019724764573, Duration: 65.18 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 15/15 [01:01<00:00,  4.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0877732197443644, Val Loss: 0.746603786945343, LR: 0.0003785834826290917, Duration: 62.12 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 15/15 [01:03<00:00,  4.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.250378060340881, Val Loss: 0.7421069741249084, LR: 0.00036420082072482785, Duration: 64.16 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 15/15 [01:04<00:00,  4.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.252704127629598, Val Loss: 0.7614971995353699, LR: 0.00035000000000000016, Duration: 65.14 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 15/15 [01:05<00:00,  4.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.212277340888977, Val Loss: 0.720110297203064, LR: 0.00033598871044954924, Duration: 66.52 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 15/15 [00:58<00:00,  3.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.8461142619450888, Val Loss: 0.7129815220832825, LR: 0.0003221745394339412, Duration: 59.30 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 15/15 [01:03<00:00,  4.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.9561599572499593, Val Loss: 0.7569743990898132, LR: 0.0003085649675704773, Duration: 64.97 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 15/15 [00:59<00:00,  3.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.299123199780782, Val Loss: 0.7535581588745117, LR: 0.0002951673646824094, Duration: 60.80 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 15/15 [01:03<00:00,  4.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.9305575052897135, Val Loss: 0.7583569884300232, LR: 0.0002819889858080498, Duration: 64.75 sec\n",
      "Epoch 당 평균 소요시간 : 39.74초\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.858000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.871222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Recall</td>\n",
       "      <td>0.858000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F1 Score</td>\n",
       "      <td>0.849921</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Metric     Value\n",
       "0   Accuracy  0.858000\n",
       "1  Precision  0.871222\n",
       "2     Recall  0.858000\n",
       "3   F1 Score  0.849921"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch + 1}\")\n",
    "    \n",
    "    for _, data in pbar:\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        inputs, labels = mixup_fn(inputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # AutoCast 적용\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "        # 스케일링된 그라디언트 계산\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # 그라디언트 클리핑 전에 스케일링 제거\n",
    "        scaler.unscale_(optimizer)\n",
    "        clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "\n",
    "        # 옵티마이저 스텝 및 스케일러 업데이트\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "            \n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        lrs.append(lr)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)        \n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in valid_loader:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    val_loss /= len(valid_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # 모델 저장\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        model_save = True\n",
    "        if model_save:\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_duration = time.time() - start_time\n",
    "    training_time += epoch_duration\n",
    "    \n",
    "    text = f'\\tLoss: {epoch_loss}, Val Loss: {val_loss}, LR: {lr}, Duration: {epoch_duration:.2f} sec'\n",
    "    \n",
    "    if model_save:\n",
    "        text += f' - model saved!'\n",
    "        model_save = False\n",
    "\n",
    "    print(text)\n",
    "        \n",
    "text = f\"Epoch 당 평균 소요시간 : {training_time / epochs:.2f}초\"      \n",
    "print(text)\n",
    "\n",
    "# 예측 수행 및 레이블 저장\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# 혼동 행렬 생성\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# 예측과 실제 레이블\n",
    "y_true = all_labels  # 실제 레이블\n",
    "y_pred = all_preds  # 모델에 의해 예측된 레이블\n",
    "\n",
    "# 전체 데이터셋에 대한 정확도\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# 평균 정밀도, 리콜, F1-Score ('weighted')\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "\n",
    "# 판다스 데이터프레임으로 결과 정리\n",
    "performance_metrics = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "    'Value': [accuracy, precision, recall, f1_score]\n",
    "})\n",
    "\n",
    "# 데이터프레임 출력\n",
    "performance_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 120 Epoch Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 15/15 [01:06<00:00,  4.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.8062772512435914, Val Loss: 0.6907758712768555, LR: 0.0002690369672720392, Duration: 67.53 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 15/15 [01:04<00:00,  4.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.082922848065694, Val Loss: 0.7309015393257141, LR: 0.0002563183228209027, Duration: 65.57 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 15/15 [01:03<00:00,  4.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0087637901306152, Val Loss: 0.7283623218536377, LR: 0.00024383993982498138, Duration: 64.20 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 15/15 [01:01<00:00,  4.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.259898130098979, Val Loss: 0.7253285646438599, LR: 0.00023160857554879947, Duration: 62.66 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 15/15 [01:05<00:00,  4.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4554635842641193, Val Loss: 0.7483454942703247, LR: 0.00021963085349188655, Duration: 66.53 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 15/15 [00:57<00:00,  3.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.089094106356303, Val Loss: 0.7393231391906738, LR: 0.00020791325980203676, Duration: 58.54 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 15/15 [01:05<00:00,  4.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.923408842086792, Val Loss: 0.7245625257492065, LR: 0.00019646213976294433, Duration: 67.20 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 15/15 [01:05<00:00,  4.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.525791390736898, Val Loss: 0.7524661421775818, LR: 0.00018528369435812, Duration: 66.78 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 15/15 [01:01<00:00,  4.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.9349135001500448, Val Loss: 0.7138493657112122, LR: 0.00017438397691295056, Duration: 62.72 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 15/15 [01:03<00:00,  4.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.668072803815206, Val Loss: 0.7394352555274963, LR: 0.00016376888981671546, Duration: 64.41 sec\n",
      "Epoch 당 평균 소요시간 : 44.04초\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.876000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.890544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Recall</td>\n",
       "      <td>0.876000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F1 Score</td>\n",
       "      <td>0.869555</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Metric     Value\n",
       "0   Accuracy  0.876000\n",
       "1  Precision  0.890544\n",
       "2     Recall  0.876000\n",
       "3   F1 Score  0.869555"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch + 1}\")\n",
    "    \n",
    "    for _, data in pbar:\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        inputs, labels = mixup_fn(inputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # AutoCast 적용\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "        # 스케일링된 그라디언트 계산\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # 그라디언트 클리핑 전에 스케일링 제거\n",
    "        scaler.unscale_(optimizer)\n",
    "        clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "\n",
    "        # 옵티마이저 스텝 및 스케일러 업데이트\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "            \n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        lrs.append(lr)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)        \n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in valid_loader:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    val_loss /= len(valid_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # 모델 저장\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        model_save = True\n",
    "        if model_save:\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_duration = time.time() - start_time\n",
    "    training_time += epoch_duration\n",
    "    \n",
    "    text = f'\\tLoss: {epoch_loss}, Val Loss: {val_loss}, LR: {lr}, Duration: {epoch_duration:.2f} sec'\n",
    "    \n",
    "    if model_save:\n",
    "        text += f' - model saved!'\n",
    "        model_save = False\n",
    "\n",
    "    print(text)\n",
    "        \n",
    "text = f\"Epoch 당 평균 소요시간 : {training_time / epochs:.2f}초\"      \n",
    "print(text)\n",
    "\n",
    "# 예측 수행 및 레이블 저장\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# 혼동 행렬 생성\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# 예측과 실제 레이블\n",
    "y_true = all_labels  # 실제 레이블\n",
    "y_pred = all_preds  # 모델에 의해 예측된 레이블\n",
    "\n",
    "# 전체 데이터셋에 대한 정확도\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# 평균 정밀도, 리콜, F1-Score ('weighted')\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "\n",
    "# 판다스 데이터프레임으로 결과 정리\n",
    "performance_metrics = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "    'Value': [accuracy, precision, recall, f1_score]\n",
    "})\n",
    "\n",
    "# 데이터프레임 출력\n",
    "performance_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 130 Epoch Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 15/15 [00:56<00:00,  3.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.9854470094045003, Val Loss: 0.7712658047676086, LR: 0.00015344418132633855, Duration: 57.76 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 15/15 [01:05<00:00,  4.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2352643807729087, Val Loss: 0.761027991771698, LR: 0.00014341544245360648, Duration: 68.45 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 15/15 [01:07<00:00,  4.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.8876005729039511, Val Loss: 0.7365458011627197, LR: 0.00013368810393753685, Duration: 69.52 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 15/15 [01:00<00:00,  4.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.8177283525466919, Val Loss: 0.7450873255729675, LR: 0.00012426743330353637, Duration: 62.42 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 15/15 [01:06<00:00,  4.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.5608513593673705, Val Loss: 0.719188928604126, LR: 0.00011515853201094461, Duration: 67.82 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 15/15 [01:06<00:00,  4.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0344080130259194, Val Loss: 0.7367914319038391, LR: 0.00010636633269050183, Duration: 67.86 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 15/15 [01:01<00:00,  4.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.9761215368906657, Val Loss: 0.7360169291496277, LR: 9.789559647324434e-05, Duration: 62.14 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 15/15 [00:59<00:00,  3.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.9502652724583944, Val Loss: 0.7438703179359436, LR: 8.97509104122664e-05, Duration: 60.36 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 15/15 [01:01<00:00,  4.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.268953307469686, Val Loss: 0.7554949522018433, LR: 8.19366849987511e-05, Duration: 62.09 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 15/15 [01:02<00:00,  4.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.7179184754689534, Val Loss: 0.7154285907745361, LR: 7.445715177361148e-05, Duration: 63.59 sec\n",
      "Epoch 당 평균 소요시간 : 48.32초\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.882000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.892627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Recall</td>\n",
       "      <td>0.882000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F1 Score</td>\n",
       "      <td>0.874773</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Metric     Value\n",
       "0   Accuracy  0.882000\n",
       "1  Precision  0.892627\n",
       "2     Recall  0.882000\n",
       "3   F1 Score  0.874773"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch + 1}\")\n",
    "    \n",
    "    for _, data in pbar:\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        inputs, labels = mixup_fn(inputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # AutoCast 적용\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "        # 스케일링된 그라디언트 계산\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # 그라디언트 클리핑 전에 스케일링 제거\n",
    "        scaler.unscale_(optimizer)\n",
    "        clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "\n",
    "        # 옵티마이저 스텝 및 스케일러 업데이트\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "            \n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        lrs.append(lr)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)        \n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in valid_loader:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    val_loss /= len(valid_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # 모델 저장\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        model_save = True\n",
    "        if model_save:\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_duration = time.time() - start_time\n",
    "    training_time += epoch_duration\n",
    "    \n",
    "    text = f'\\tLoss: {epoch_loss}, Val Loss: {val_loss}, LR: {lr}, Duration: {epoch_duration:.2f} sec'\n",
    "    \n",
    "    if model_save:\n",
    "        text += f' - model saved!'\n",
    "        model_save = False\n",
    "\n",
    "    print(text)\n",
    "        \n",
    "text = f\"Epoch 당 평균 소요시간 : {training_time / epochs:.2f}초\"      \n",
    "print(text)\n",
    "\n",
    "# 예측 수행 및 레이블 저장\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# 혼동 행렬 생성\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# 예측과 실제 레이블\n",
    "y_true = all_labels  # 실제 레이블\n",
    "y_pred = all_preds  # 모델에 의해 예측된 레이블\n",
    "\n",
    "# 전체 데이터셋에 대한 정확도\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# 평균 정밀도, 리콜, F1-Score ('weighted')\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "\n",
    "# 판다스 데이터프레임으로 결과 정리\n",
    "performance_metrics = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "    'Value': [accuracy, precision, recall, f1_score]\n",
    "})\n",
    "\n",
    "# 데이터프레임 출력\n",
    "performance_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 140 Epoch Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 15/15 [01:03<00:00,  4.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.8992615302403768, Val Loss: 0.7198853492736816, LR: 6.731636103603565e-05, Duration: 64.87 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 15/15 [01:01<00:00,  4.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.879345432917277, Val Loss: 0.761381208896637, LR: 6.0518179650179314e-05, Duration: 63.22 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 15/15 [01:04<00:00,  4.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.8444550911585489, Val Loss: 0.7338782548904419, LR: 5.406628895119039e-05, Duration: 66.28 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 15/15 [01:04<00:00,  4.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.9799134492874146, Val Loss: 0.7396116256713867, LR: 4.796418275170029e-05, Duration: 68.44 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 15/15 [01:00<00:00,  4.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.5977686166763305, Val Loss: 0.7258949875831604, LR: 4.221516544986418e-05, Duration: 62.22 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 15/15 [01:00<00:00,  4.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.8323163588841755, Val Loss: 0.74171382188797, LR: 3.682235023996956e-05, Duration: 61.88 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 15/15 [01:02<00:00,  4.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.437954139709473, Val Loss: 0.7508115172386169, LR: 3.1788657426587664e-05, Duration: 63.38 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 15/15 [01:02<00:00,  4.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.7611775636672973, Val Loss: 0.7385635375976562, LR: 2.7116812843176773e-05, Duration: 63.15 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 15/15 [01:00<00:00,  4.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.7915082613627116, Val Loss: 0.7334794998168945, LR: 2.280934637599534e-05, Duration: 61.69 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 15/15 [01:03<00:00,  4.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0829975446065268, Val Loss: 0.7404515147209167, LR: 1.8868590594123336e-05, Duration: 64.30 sec\n",
      "Epoch 당 평균 소요시간 : 52.59초\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.876000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.886349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Recall</td>\n",
       "      <td>0.876000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F1 Score</td>\n",
       "      <td>0.867652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Metric     Value\n",
       "0   Accuracy  0.876000\n",
       "1  Precision  0.886349\n",
       "2     Recall  0.876000\n",
       "3   F1 Score  0.867652"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch + 1}\")\n",
    "    \n",
    "    for _, data in pbar:\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        inputs, labels = mixup_fn(inputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # AutoCast 적용\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "        # 스케일링된 그라디언트 계산\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # 그라디언트 클리핑 전에 스케일링 제거\n",
    "        scaler.unscale_(optimizer)\n",
    "        clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "\n",
    "        # 옵티마이저 스텝 및 스케일러 업데이트\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "            \n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        lrs.append(lr)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)        \n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in valid_loader:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    val_loss /= len(valid_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # 모델 저장\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        model_save = True\n",
    "        if model_save:\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_duration = time.time() - start_time\n",
    "    training_time += epoch_duration\n",
    "    \n",
    "    text = f'\\tLoss: {epoch_loss}, Val Loss: {val_loss}, LR: {lr}, Duration: {epoch_duration:.2f} sec'\n",
    "    \n",
    "    if model_save:\n",
    "        text += f' - model saved!'\n",
    "        model_save = False\n",
    "\n",
    "    print(text)\n",
    "        \n",
    "text = f\"Epoch 당 평균 소요시간 : {training_time / epochs:.2f}초\"      \n",
    "print(text)\n",
    "\n",
    "# 예측 수행 및 레이블 저장\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# 혼동 행렬 생성\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# 예측과 실제 레이블\n",
    "y_true = all_labels  # 실제 레이블\n",
    "y_pred = all_preds  # 모델에 의해 예측된 레이블\n",
    "\n",
    "# 전체 데이터셋에 대한 정확도\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# 평균 정밀도, 리콜, F1-Score ('weighted')\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "\n",
    "# 판다스 데이터프레임으로 결과 정리\n",
    "performance_metrics = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "    'Value': [accuracy, precision, recall, f1_score]\n",
    "})\n",
    "\n",
    "# 데이터프레임 출력\n",
    "performance_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 150 Epoch Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 15/15 [01:04<00:00,  4.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.7545771678288777, Val Loss: 0.7444794774055481, LR: 1.5296679486336016e-05, Duration: 65.38 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 15/15 [00:56<00:00,  3.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.7159150203069051, Val Loss: 0.7417320609092712, LR: 1.2095547305510733e-05, Duration: 58.93 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 15/15 [01:05<00:00,  4.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.7683289607365926, Val Loss: 0.7387528419494629, LR: 9.266927521194546e-06, Duration: 66.79 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 15/15 [01:04<00:00,  4.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.068185877799988, Val Loss: 0.7412886023521423, LR: 6.812351880900747e-06, Duration: 65.90 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 15/15 [01:03<00:00,  4.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0443437417348225, Val Loss: 0.7431501746177673, LR: 4.733149580639884e-06, Duration: 66.35 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 15/15 [01:09<00:00,  4.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.8944033225377401, Val Loss: 0.7439755797386169, LR: 3.0304465451369555e-06, Duration: 70.91 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 15/15 [01:03<00:00,  4.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.31347819964091, Val Loss: 0.7437183856964111, LR: 1.7051648181230617e-06, Duration: 64.65 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 15/15 [01:02<00:00,  4.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.9482012828191122, Val Loss: 0.7438963055610657, LR: 7.580220630328039e-07, Duration: 63.57 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 15/15 [01:03<00:00,  4.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0819226503372192, Val Loss: 0.7439556121826172, LR: 1.8953117437680023e-07, Duration: 64.65 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 15/15 [01:04<00:00,  4.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.7390932083129882, Val Loss: 0.7439295649528503, LR: 0.0, Duration: 65.35 sec\n",
      "Epoch 당 평균 소요시간 : 56.94초\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.874000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.887730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Recall</td>\n",
       "      <td>0.874000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F1 Score</td>\n",
       "      <td>0.867575</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Metric     Value\n",
       "0   Accuracy  0.874000\n",
       "1  Precision  0.887730\n",
       "2     Recall  0.874000\n",
       "3   F1 Score  0.867575"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch + 1}\")\n",
    "    \n",
    "    for _, data in pbar:\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        inputs, labels = mixup_fn(inputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # AutoCast 적용\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "        # 스케일링된 그라디언트 계산\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # 그라디언트 클리핑 전에 스케일링 제거\n",
    "        scaler.unscale_(optimizer)\n",
    "        clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "\n",
    "        # 옵티마이저 스텝 및 스케일러 업데이트\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "            \n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        lrs.append(lr)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)        \n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in valid_loader:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    val_loss /= len(valid_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # 모델 저장\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        model_save = True\n",
    "        if model_save:\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_duration = time.time() - start_time\n",
    "    training_time += epoch_duration\n",
    "    \n",
    "    text = f'\\tLoss: {epoch_loss}, Val Loss: {val_loss}, LR: {lr}, Duration: {epoch_duration:.2f} sec'\n",
    "    \n",
    "    if model_save:\n",
    "        text += f' - model saved!'\n",
    "        model_save = False\n",
    "\n",
    "    print(text)\n",
    "        \n",
    "text = f\"Epoch 당 평균 소요시간 : {training_time / epochs:.2f}초\"      \n",
    "print(text)\n",
    "\n",
    "# 예측 수행 및 레이블 저장\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# 혼동 행렬 생성\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# 예측과 실제 레이블\n",
    "y_true = all_labels  # 실제 레이블\n",
    "y_pred = all_preds  # 모델에 의해 예측된 레이블\n",
    "\n",
    "# 전체 데이터셋에 대한 정확도\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# 평균 정밀도, 리콜, F1-Score ('weighted')\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "\n",
    "# 판다스 데이터프레임으로 결과 정리\n",
    "performance_metrics = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "    'Value': [accuracy, precision, recall, f1_score]\n",
    "})\n",
    "\n",
    "# 데이터프레임 출력\n",
    "performance_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
