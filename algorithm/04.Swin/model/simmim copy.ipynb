{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from box import Box\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import simmim\n",
    "from swin_v2 import SwinTransformerV2\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "from timm.data import Mixup\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MODEL': {'TYPE': 'swinv2',\n",
       "  'NAME': 'simmim_pretrain',\n",
       "  'DROP_PATH_RATE': 0.0,\n",
       "  'SWIN': {'EMBED_DIM': 96,\n",
       "   'DEPTHS': [2, 2, 6, 2],\n",
       "   'NUM_HEADS': [3, 6, 12, 24],\n",
       "   'WINDOW_SIZE': 6,\n",
       "   'PATCH_SIZE': 4}},\n",
       " 'DATA': {'IMG_SIZE': 192,\n",
       "  'MASK_PATCH_SIZE': 32,\n",
       "  'MASK_RATIO': 0.6,\n",
       "  'BATCH_SIZE': 1024,\n",
       "  'NUM_WORKERS': 24,\n",
       "  'DATA_PATH': '../../data/sports'},\n",
       " 'TRAIN': {'EPOCHS': 100,\n",
       "  'WARMUP_EPOCHS': 10,\n",
       "  'BASE_LR': 0.0014,\n",
       "  'WEIGHT_DECAY': 0.05,\n",
       "  'CLIP_GRAD': 5}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simmim_config = yaml.load(open('config/pretrain.yaml'), Loader=yaml.FullLoader)\n",
    "simmim_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_config = {'img_size':simmim_config['DATA']['IMG_SIZE'], \n",
    "                'patch_size':simmim_config['MODEL']['SWIN']['PATCH_SIZE'], \n",
    "                'in_chans':3, \n",
    "                'num_classes':100,\n",
    "                'embed_dim':simmim_config['MODEL']['SWIN']['EMBED_DIM'], \n",
    "                'depths':simmim_config['MODEL']['SWIN']['DEPTHS'], \n",
    "                'num_heads':simmim_config['MODEL']['SWIN']['NUM_HEADS'],           \n",
    "                'window_size':simmim_config['MODEL']['SWIN']['WINDOW_SIZE'], \n",
    "                'mlp_ratio':4., \n",
    "                'qkv_bias':True, \n",
    "                'qk_scale':None,\n",
    "                'drop_rate':0., \n",
    "                'attn_drop_rate':0., \n",
    "                'drop_path_rate':simmim_config['MODEL']['DROP_PATH_RATE'],\n",
    "                'norm_layer':nn.LayerNorm, \n",
    "                'patch_norm':True, \n",
    "                'pretrained_window_sizes':[0,0,0,0],\n",
    "                'ape':True}\n",
    "\n",
    "encoder_stride = 32\n",
    "in_chans = encoder_config['in_chans']\n",
    "patch_size = encoder_config['patch_size']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Load SimMIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = simmim.SwinTransformerV2ForSimMIM(**encoder_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = simmim.SimMIM( encoder=encoder, \n",
    "                       encoder_stride=encoder_stride, \n",
    "                       in_chans=in_chans, \n",
    "                       patch_size=patch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask Generator Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 0, 0, 0, 0, 1],\n",
       "       [1, 0, 1, 1, 1, 1, 0, 0],\n",
       "       [1, 1, 0, 1, 0, 0, 1, 1],\n",
       "       [1, 1, 0, 1, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 1, 1, 0, 1, 0],\n",
       "       [1, 0, 0, 0, 1, 1, 1, 1],\n",
       "       [1, 0, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_generator = simmim.MaskGenerator(input_size=224,\n",
    "                                      mask_patch_size=28,\n",
    "                                      model_patch_size=28,\n",
    "                                      mask_ratio=.6)\n",
    "mask = mask_generator()\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생성된 mask의 비율은 60.9375%\n"
     ]
    }
   ],
   "source": [
    "print(f\"생성된 mask의 비율은 {mask.sum() / (mask.shape[0]*mask.shape[1])*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SimMIM DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simmim_config = Box(simmim_config)\n",
    "dataloader = simmim.build_loader_simmim(simmim_config)\n",
    "\n",
    "samples = next(iter(dataloader))\n",
    "len(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1024, 3, 192, 192]),\n",
       " torch.Size([1024, 48, 48]),\n",
       " torch.Size([1024]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples[0].shape, samples[1].shape, samples[2].shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Parameters and etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_lr = float(simmim_config.TRAIN.BASE_LR)\n",
    "weight_decay = simmim_config.TRAIN.WEIGHT_DECAY\n",
    "optimizer = optim.AdamW(model.parameters(), lr=base_lr, weight_decay=weight_decay)\n",
    "warmup_epochs = simmim_config.TRAIN.WARMUP_EPOCHS\n",
    "train_epochs = simmim_config.TRAIN.EPOCHS\n",
    "\n",
    "scheduler = transformers.get_cosine_schedule_with_warmup(optimizer, \n",
    "                                                        num_warmup_steps=warmup_epochs*len(dataloader), \n",
    "                                                        num_training_steps=train_epochs*len(dataloader),\n",
    "                                                        num_cycles=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:3'\n",
    "model.to(device)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "model_save = True\n",
    "simmim_path = '../../models/swin2/simmim.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.Train SimMIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 14/14 [00:19<00:00,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.1412, LR: 0.00014000000000000001, Duration: 20.75 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 14/14 [00:17<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.0940, LR: 0.00028000000000000003, Duration: 18.91 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 14/14 [00:17<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.0501, LR: 0.00041999999999999996, Duration: 18.29 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 14/14 [00:17<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.9446, LR: 0.0005600000000000001, Duration: 18.56 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 14/14 [00:16<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.8564, LR: 0.0007, Duration: 17.95 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 14/14 [00:16<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.7880, LR: 0.0008399999999999999, Duration: 18.15 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 14/14 [00:18<00:00,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.7477, LR: 0.00098, Duration: 19.33 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8: 100%|██████████| 14/14 [00:17<00:00,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.7619, LR: 0.0011200000000000001, Duration: 18.10 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9: 100%|██████████| 14/14 [00:16<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.7224, LR: 0.00126, Duration: 18.15 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 14/14 [00:17<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.7048, LR: 0.0014, Duration: 18.47 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 14/14 [00:17<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.6859, LR: 0.001399573578913367, Duration: 18.98 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 14/14 [00:17<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.6772, LR: 0.001398294835181877, Duration: 18.69 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 14/14 [00:17<00:00,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.6765, LR: 0.0013961653267577914, Duration: 18.86 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14: 100%|██████████| 14/14 [00:17<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.6503, LR: 0.0013931876481190993, Duration: 18.33 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 14/14 [00:17<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.6469, LR: 0.0013893654271085456, Duration: 18.99 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 14/14 [00:17<00:00,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.6423, LR: 0.001384703320513664, Duration: 18.76 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17: 100%|██████████| 14/14 [00:16<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.6183, LR: 0.0013792070083931975, Duration: 18.29 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 14/14 [00:17<00:00,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.6149, LR: 0.0013728831871568231, Duration: 18.60 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19: 100%|██████████| 14/14 [00:17<00:00,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.6085, LR: 0.0013657395614066075, Duration: 18.57 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20: 100%|██████████| 14/14 [00:17<00:00,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.6131, LR: 0.001357784834550136, Duration: 18.79 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21: 100%|██████████| 14/14 [00:17<00:00,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.6166, LR: 0.0013490286981967512, Duration: 18.53 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22: 100%|██████████| 14/14 [00:17<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5999, LR: 0.0013394818203498204, Duration: 19.14 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 14/14 [00:17<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5846, LR: 0.0013291558324094168, Duration: 18.39 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 14/14 [00:18<00:00,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5908, LR: 0.0013180633150012488, Duration: 19.11 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 25: 100%|██████████| 14/14 [00:17<00:00,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5852, LR: 0.0013062177826491071, Duration: 18.63 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26: 100%|██████████| 14/14 [00:17<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5818, LR: 0.001293633667309498, Duration: 18.30 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 14/14 [00:17<00:00,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5862, LR: 0.001280326300788529, Duration: 18.44 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28: 100%|██████████| 14/14 [00:17<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5788, LR: 0.0012663118960624632, Duration: 18.68 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 14/14 [00:17<00:00,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5782, LR: 0.0012516075275247052, Duration: 18.90 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 30: 100%|██████████| 14/14 [00:18<00:00,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5743, LR: 0.0012362311101832846, Duration: 19.69 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 31: 100%|██████████| 14/14 [00:17<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5682, LR: 0.001220201377834176, Duration: 18.34 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|██████████| 14/14 [00:17<00:00,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5837, LR: 0.0012035378602370558, Duration: 18.71 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 33: 100%|██████████| 14/14 [00:17<00:00,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5727, LR: 0.0011862608593212981, Duration: 18.65 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 34: 100%|██████████| 14/14 [00:17<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5614, LR: 0.0011683914244512007, Duration: 18.79 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|██████████| 14/14 [00:16<00:00,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5787, LR: 0.0011499513267805774, Duration: 17.97 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 36: 100%|██████████| 14/14 [00:16<00:00,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5751, LR: 0.0011309630327279608, Duration: 17.58 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 37: 100%|██████████| 14/14 [00:17<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5596, LR: 0.0011114496766047313, Duration: 18.60 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|██████████| 14/14 [00:16<00:00,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5613, LR: 0.0010914350324295228, Duration: 17.95 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 39: 100%|██████████| 14/14 [00:18<00:00,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5569, LR: 0.0010709434849632434, Duration: 19.36 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 40: 100%|██████████| 14/14 [00:17<00:00,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5572, LR: 0.00105, Duration: 18.37 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 41: 100%|██████████| 14/14 [00:17<00:00,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5528, LR: 0.0010286300939501235, Duration: 18.62 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 42: 100%|██████████| 14/14 [00:17<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5473, LR: 0.001006859802752354, Duration: 18.62 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43: 100%|██████████| 14/14 [00:16<00:00,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5503, LR: 0.0009847156501530602, Duration: 17.88 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 44: 100%|██████████| 14/14 [00:16<00:00,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5486, LR: 0.0009622246153911386, Duration: 17.83 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 45: 100%|██████████| 14/14 [00:16<00:00,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5493, LR: 0.0009394141003279682, Duration: 17.83 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 46: 100%|██████████| 14/14 [00:17<00:00,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5475, LR: 0.0009163118960624632, Duration: 18.51 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 47: 100%|██████████| 14/14 [00:17<00:00,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5476, LR: 0.0008929461490718994, Duration: 18.50 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 48: 100%|██████████| 14/14 [00:17<00:00,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5477, LR: 0.0008693453269197673, Duration: 18.44 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 49: 100%|██████████| 14/14 [00:17<00:00,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5512, LR: 0.0008455381835724314, Duration: 18.99 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 50: 100%|██████████| 14/14 [00:17<00:00,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5548, LR: 0.0008215537243668514, Duration: 18.38 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 51: 100%|██████████| 14/14 [00:17<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5438, LR: 0.0007974211706720458, Duration: 18.36 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52: 100%|██████████| 14/14 [00:18<00:00,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5403, LR: 0.0007731699242873575, Duration: 19.33 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 53: 100%|██████████| 14/14 [00:16<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5366, LR: 0.0007488295316208876, Duration: 18.01 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54: 100%|██████████| 14/14 [00:17<00:00,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5438, LR: 0.0007244296476917508, Duration: 18.74 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 55: 100%|██████████| 14/14 [00:16<00:00,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5468, LR: 0.0007, Duration: 17.91 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 56: 100%|██████████| 14/14 [00:17<00:00,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5379, LR: 0.0006755703523082495, Duration: 18.97 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 57: 100%|██████████| 14/14 [00:16<00:00,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5400, LR: 0.0006511704683791123, Duration: 17.56 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 58: 100%|██████████| 14/14 [00:17<00:00,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5381, LR: 0.0006268300757126426, Duration: 18.30 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 59: 100%|██████████| 14/14 [00:16<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5354, LR: 0.0006025788293279544, Duration: 17.99 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60: 100%|██████████| 14/14 [00:17<00:00,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5373, LR: 0.0005784462756331488, Duration: 18.48 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 61: 100%|██████████| 14/14 [00:17<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5351, LR: 0.0005544618164275686, Duration: 18.51 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62: 100%|██████████| 14/14 [00:17<00:00,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5330, LR: 0.0005306546730802327, Duration: 18.65 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 63: 100%|██████████| 14/14 [00:16<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5302, LR: 0.0005070538509281006, Duration: 18.19 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64: 100%|██████████| 14/14 [00:17<00:00,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5310, LR: 0.0004836881039375369, Duration: 18.54 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 65: 100%|██████████| 14/14 [00:18<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5297, LR: 0.0004605858996720319, Duration: 19.31 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66: 100%|██████████| 14/14 [00:17<00:00,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5271, LR: 0.0004377753846088615, Duration: 18.36 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 67: 100%|██████████| 14/14 [00:17<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5254, LR: 0.00041528434984693997, Duration: 19.21 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68: 100%|██████████| 14/14 [00:16<00:00,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5255, LR: 0.00039314019724764573, Duration: 17.61 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 69: 100%|██████████| 14/14 [00:17<00:00,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5269, LR: 0.00037136990604987665, Duration: 18.73 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 70: 100%|██████████| 14/14 [00:17<00:00,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5258, LR: 0.00035000000000000016, Duration: 18.84 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 71: 100%|██████████| 14/14 [00:17<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5245, LR: 0.00032905651503675667, Duration: 19.08 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72: 100%|██████████| 14/14 [00:18<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5221, LR: 0.0003085649675704773, Duration: 19.50 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 73: 100%|██████████| 14/14 [00:17<00:00,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5225, LR: 0.0002885503233952689, Duration: 18.26 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 74: 100%|██████████| 14/14 [00:16<00:00,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5220, LR: 0.0002690369672720392, Duration: 18.19 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 75: 100%|██████████| 14/14 [00:16<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5209, LR: 0.00025004867321942243, Duration: 18.19 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 76: 100%|██████████| 14/14 [00:17<00:00,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5211, LR: 0.00023160857554879947, Duration: 19.04 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 77: 100%|██████████| 14/14 [00:17<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5208, LR: 0.00021373914067870185, Duration: 19.03 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 78: 100%|██████████| 14/14 [00:16<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5208, LR: 0.00019646213976294433, Duration: 18.18 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 79: 100%|██████████| 14/14 [00:18<00:00,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5192, LR: 0.00017979862216582396, Duration: 19.14 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 80: 100%|██████████| 14/14 [00:17<00:00,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5202, LR: 0.00016376888981671546, Duration: 18.32 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 81: 100%|██████████| 14/14 [00:16<00:00,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5193, LR: 0.00014839247247529466, Duration: 17.83 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 82: 100%|██████████| 14/14 [00:17<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5176, LR: 0.00013368810393753685, Duration: 18.31 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 83: 100%|██████████| 14/14 [00:18<00:00,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5186, LR: 0.00011967369921147086, Duration: 19.17 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 84: 100%|██████████| 14/14 [00:17<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5178, LR: 0.00010636633269050183, Duration: 18.02 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 85: 100%|██████████| 14/14 [00:18<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5174, LR: 9.37822173508929e-05, Duration: 19.45 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 86: 100%|██████████| 14/14 [00:18<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5170, LR: 8.19366849987511e-05, Duration: 19.52 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 87: 100%|██████████| 14/14 [00:17<00:00,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5158, LR: 7.084416759058323e-05, Duration: 18.71 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 88: 100%|██████████| 14/14 [00:17<00:00,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5173, LR: 6.0518179650179314e-05, Duration: 18.26 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 89: 100%|██████████| 14/14 [00:16<00:00,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5167, LR: 5.097130180324888e-05, Duration: 18.05 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 90: 100%|██████████| 14/14 [00:16<00:00,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5170, LR: 4.221516544986418e-05, Duration: 17.97 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 91: 100%|██████████| 14/14 [00:17<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5170, LR: 3.426043859339253e-05, Duration: 18.75 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 92: 100%|██████████| 14/14 [00:17<00:00,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5155, LR: 2.7116812843176773e-05, Duration: 19.20 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 93: 100%|██████████| 14/14 [00:17<00:00,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5159, LR: 2.0792991606802468e-05, Duration: 18.91 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 94: 100%|██████████| 14/14 [00:17<00:00,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5175, LR: 1.5296679486336016e-05, Duration: 18.66 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 95: 100%|██████████| 14/14 [00:16<00:00,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5159, LR: 1.0634572891454386e-05, Duration: 17.95 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 96: 100%|██████████| 14/14 [00:17<00:00,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5163, LR: 6.812351880900747e-06, Duration: 18.37 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 97: 100%|██████████| 14/14 [00:17<00:00,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5163, LR: 3.834673242208697e-06, Duration: 18.74 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 98: 100%|██████████| 14/14 [00:17<00:00,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5168, LR: 1.7051648181230617e-06, Duration: 18.51 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 99: 100%|██████████| 14/14 [00:18<00:00,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5165, LR: 4.264210866329665e-07, Duration: 19.19 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 100: 100%|██████████| 14/14 [00:17<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5154, LR: 0.0, Duration: 18.59 sec - model saved!\n"
     ]
    }
   ],
   "source": [
    "training_time = 0\n",
    "losses = []\n",
    "val_losses = []\n",
    "lrs = []\n",
    "best_loss = float('inf')\n",
    "\n",
    "# GradScaler 초기화\n",
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in range(train_epochs):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f\"Epoch {epoch + 1}\")\n",
    "    \n",
    "    for _, data in pbar:\n",
    "        image, mask = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # AutoCast 적용\n",
    "        with autocast():\n",
    "            loss = model(image, mask)\n",
    "            \n",
    "        # 스케일링된 그라디언트 계산\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # 그라디언트 클리핑 전에 스케일링 제거\n",
    "        scaler.unscale_(optimizer)\n",
    "        if simmim_config.TRAIN.CLIP_GRAD:\n",
    "            clip_grad_norm_(model.parameters(), max_norm=simmim_config.TRAIN.CLIP_GRAD)\n",
    "        else:\n",
    "            clip_grad_norm_(model.parameters())\n",
    "\n",
    "        # 옵티마이저 스텝 및 스케일러 업데이트\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "            \n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        lrs.append(lr)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    losses.append(epoch_loss)\n",
    "\n",
    "    # 모델 저장\n",
    "    if epoch_loss < best_loss:\n",
    "        \n",
    "        best_loss = epoch_loss\n",
    "        vit_save = model_save\n",
    "        if vit_save:\n",
    "            torch.save(model.state_dict(), simmim_path)\n",
    "        \n",
    "    epoch_duration = time.time() - start_time\n",
    "    training_time += epoch_duration\n",
    "    \n",
    "    text = f'\\tLoss: {epoch_loss:.4f}, LR: {lr}, Duration: {epoch_duration:.2f} sec'\n",
    "    \n",
    "    if vit_save:\n",
    "        text += f' - model saved!'\n",
    "        vit_save = False    \n",
    "        \n",
    "    print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Del SimMIM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cpu()\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.Load Swin V2 for stage-2 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['absolute_pos_embed', 'embeddings.patch_embeddings.weight', 'embeddings.patch_embeddings.bias', 'embeddings.norm.weight', 'embeddings.norm.bias', 'stages.0.blocks.0.attn_mask', 'stages.0.blocks.0.attn.t_scale', 'stages.0.blocks.0.attn.relative_coords_table', 'stages.0.blocks.0.attn.relative_position_index', 'stages.0.blocks.0.attn.crpb_mlp.0.weight', 'stages.0.blocks.0.attn.crpb_mlp.0.bias', 'stages.0.blocks.0.attn.crpb_mlp.3.weight', 'stages.0.blocks.0.attn.qkv.weight', 'stages.0.blocks.0.attn.qkv.bias', 'stages.0.blocks.0.attn.proj.weight', 'stages.0.blocks.0.attn.proj.bias', 'stages.0.blocks.0.norm1.weight', 'stages.0.blocks.0.norm1.bias', 'stages.0.blocks.0.mlp.fc1.weight', 'stages.0.blocks.0.mlp.fc1.bias', 'stages.0.blocks.0.mlp.fc2.weight', 'stages.0.blocks.0.mlp.fc2.bias', 'stages.0.blocks.0.norm2.weight', 'stages.0.blocks.0.norm2.bias', 'stages.0.blocks.1.attn_mask', 'stages.0.blocks.1.attn.t_scale', 'stages.0.blocks.1.attn.relative_coords_table', 'stages.0.blocks.1.attn.relative_position_index', 'stages.0.blocks.1.attn.crpb_mlp.0.weight', 'stages.0.blocks.1.attn.crpb_mlp.0.bias', 'stages.0.blocks.1.attn.crpb_mlp.3.weight', 'stages.0.blocks.1.attn.qkv.weight', 'stages.0.blocks.1.attn.qkv.bias', 'stages.0.blocks.1.attn.proj.weight', 'stages.0.blocks.1.attn.proj.bias', 'stages.0.blocks.1.norm1.weight', 'stages.0.blocks.1.norm1.bias', 'stages.0.blocks.1.mlp.fc1.weight', 'stages.0.blocks.1.mlp.fc1.bias', 'stages.0.blocks.1.mlp.fc2.weight', 'stages.0.blocks.1.mlp.fc2.bias', 'stages.0.blocks.1.norm2.weight', 'stages.0.blocks.1.norm2.bias', 'stages.0.downsample.reduction.weight', 'stages.0.downsample.norm.weight', 'stages.0.downsample.norm.bias', 'stages.1.blocks.0.attn_mask', 'stages.1.blocks.0.attn.t_scale', 'stages.1.blocks.0.attn.relative_coords_table', 'stages.1.blocks.0.attn.relative_position_index', 'stages.1.blocks.0.attn.crpb_mlp.0.weight', 'stages.1.blocks.0.attn.crpb_mlp.0.bias', 'stages.1.blocks.0.attn.crpb_mlp.3.weight', 'stages.1.blocks.0.attn.qkv.weight', 'stages.1.blocks.0.attn.qkv.bias', 'stages.1.blocks.0.attn.proj.weight', 'stages.1.blocks.0.attn.proj.bias', 'stages.1.blocks.0.norm1.weight', 'stages.1.blocks.0.norm1.bias', 'stages.1.blocks.0.mlp.fc1.weight', 'stages.1.blocks.0.mlp.fc1.bias', 'stages.1.blocks.0.mlp.fc2.weight', 'stages.1.blocks.0.mlp.fc2.bias', 'stages.1.blocks.0.norm2.weight', 'stages.1.blocks.0.norm2.bias', 'stages.1.blocks.1.attn_mask', 'stages.1.blocks.1.attn.t_scale', 'stages.1.blocks.1.attn.relative_coords_table', 'stages.1.blocks.1.attn.relative_position_index', 'stages.1.blocks.1.attn.crpb_mlp.0.weight', 'stages.1.blocks.1.attn.crpb_mlp.0.bias', 'stages.1.blocks.1.attn.crpb_mlp.3.weight', 'stages.1.blocks.1.attn.qkv.weight', 'stages.1.blocks.1.attn.qkv.bias', 'stages.1.blocks.1.attn.proj.weight', 'stages.1.blocks.1.attn.proj.bias', 'stages.1.blocks.1.norm1.weight', 'stages.1.blocks.1.norm1.bias', 'stages.1.blocks.1.mlp.fc1.weight', 'stages.1.blocks.1.mlp.fc1.bias', 'stages.1.blocks.1.mlp.fc2.weight', 'stages.1.blocks.1.mlp.fc2.bias', 'stages.1.blocks.1.norm2.weight', 'stages.1.blocks.1.norm2.bias', 'stages.1.downsample.reduction.weight', 'stages.1.downsample.norm.weight', 'stages.1.downsample.norm.bias', 'stages.2.blocks.0.attn_mask', 'stages.2.blocks.0.attn.t_scale', 'stages.2.blocks.0.attn.relative_coords_table', 'stages.2.blocks.0.attn.relative_position_index', 'stages.2.blocks.0.attn.crpb_mlp.0.weight', 'stages.2.blocks.0.attn.crpb_mlp.0.bias', 'stages.2.blocks.0.attn.crpb_mlp.3.weight', 'stages.2.blocks.0.attn.qkv.weight', 'stages.2.blocks.0.attn.qkv.bias', 'stages.2.blocks.0.attn.proj.weight', 'stages.2.blocks.0.attn.proj.bias', 'stages.2.blocks.0.norm1.weight', 'stages.2.blocks.0.norm1.bias', 'stages.2.blocks.0.mlp.fc1.weight', 'stages.2.blocks.0.mlp.fc1.bias', 'stages.2.blocks.0.mlp.fc2.weight', 'stages.2.blocks.0.mlp.fc2.bias', 'stages.2.blocks.0.norm2.weight', 'stages.2.blocks.0.norm2.bias', 'stages.2.blocks.1.attn_mask', 'stages.2.blocks.1.attn.t_scale', 'stages.2.blocks.1.attn.relative_coords_table', 'stages.2.blocks.1.attn.relative_position_index', 'stages.2.blocks.1.attn.crpb_mlp.0.weight', 'stages.2.blocks.1.attn.crpb_mlp.0.bias', 'stages.2.blocks.1.attn.crpb_mlp.3.weight', 'stages.2.blocks.1.attn.qkv.weight', 'stages.2.blocks.1.attn.qkv.bias', 'stages.2.blocks.1.attn.proj.weight', 'stages.2.blocks.1.attn.proj.bias', 'stages.2.blocks.1.norm1.weight', 'stages.2.blocks.1.norm1.bias', 'stages.2.blocks.1.mlp.fc1.weight', 'stages.2.blocks.1.mlp.fc1.bias', 'stages.2.blocks.1.mlp.fc2.weight', 'stages.2.blocks.1.mlp.fc2.bias', 'stages.2.blocks.1.norm2.weight', 'stages.2.blocks.1.norm2.bias', 'stages.2.blocks.2.attn_mask', 'stages.2.blocks.2.attn.t_scale', 'stages.2.blocks.2.attn.relative_coords_table', 'stages.2.blocks.2.attn.relative_position_index', 'stages.2.blocks.2.attn.crpb_mlp.0.weight', 'stages.2.blocks.2.attn.crpb_mlp.0.bias', 'stages.2.blocks.2.attn.crpb_mlp.3.weight', 'stages.2.blocks.2.attn.qkv.weight', 'stages.2.blocks.2.attn.qkv.bias', 'stages.2.blocks.2.attn.proj.weight', 'stages.2.blocks.2.attn.proj.bias', 'stages.2.blocks.2.norm1.weight', 'stages.2.blocks.2.norm1.bias', 'stages.2.blocks.2.mlp.fc1.weight', 'stages.2.blocks.2.mlp.fc1.bias', 'stages.2.blocks.2.mlp.fc2.weight', 'stages.2.blocks.2.mlp.fc2.bias', 'stages.2.blocks.2.norm2.weight', 'stages.2.blocks.2.norm2.bias', 'stages.2.blocks.3.attn_mask', 'stages.2.blocks.3.attn.t_scale', 'stages.2.blocks.3.attn.relative_coords_table', 'stages.2.blocks.3.attn.relative_position_index', 'stages.2.blocks.3.attn.crpb_mlp.0.weight', 'stages.2.blocks.3.attn.crpb_mlp.0.bias', 'stages.2.blocks.3.attn.crpb_mlp.3.weight', 'stages.2.blocks.3.attn.qkv.weight', 'stages.2.blocks.3.attn.qkv.bias', 'stages.2.blocks.3.attn.proj.weight', 'stages.2.blocks.3.attn.proj.bias', 'stages.2.blocks.3.norm1.weight', 'stages.2.blocks.3.norm1.bias', 'stages.2.blocks.3.mlp.fc1.weight', 'stages.2.blocks.3.mlp.fc1.bias', 'stages.2.blocks.3.mlp.fc2.weight', 'stages.2.blocks.3.mlp.fc2.bias', 'stages.2.blocks.3.norm2.weight', 'stages.2.blocks.3.norm2.bias', 'stages.2.blocks.4.attn_mask', 'stages.2.blocks.4.attn.t_scale', 'stages.2.blocks.4.attn.relative_coords_table', 'stages.2.blocks.4.attn.relative_position_index', 'stages.2.blocks.4.attn.crpb_mlp.0.weight', 'stages.2.blocks.4.attn.crpb_mlp.0.bias', 'stages.2.blocks.4.attn.crpb_mlp.3.weight', 'stages.2.blocks.4.attn.qkv.weight', 'stages.2.blocks.4.attn.qkv.bias', 'stages.2.blocks.4.attn.proj.weight', 'stages.2.blocks.4.attn.proj.bias', 'stages.2.blocks.4.norm1.weight', 'stages.2.blocks.4.norm1.bias', 'stages.2.blocks.4.mlp.fc1.weight', 'stages.2.blocks.4.mlp.fc1.bias', 'stages.2.blocks.4.mlp.fc2.weight', 'stages.2.blocks.4.mlp.fc2.bias', 'stages.2.blocks.4.norm2.weight', 'stages.2.blocks.4.norm2.bias', 'stages.2.blocks.5.attn_mask', 'stages.2.blocks.5.attn.t_scale', 'stages.2.blocks.5.attn.relative_coords_table', 'stages.2.blocks.5.attn.relative_position_index', 'stages.2.blocks.5.attn.crpb_mlp.0.weight', 'stages.2.blocks.5.attn.crpb_mlp.0.bias', 'stages.2.blocks.5.attn.crpb_mlp.3.weight', 'stages.2.blocks.5.attn.qkv.weight', 'stages.2.blocks.5.attn.qkv.bias', 'stages.2.blocks.5.attn.proj.weight', 'stages.2.blocks.5.attn.proj.bias', 'stages.2.blocks.5.norm1.weight', 'stages.2.blocks.5.norm1.bias', 'stages.2.blocks.5.mlp.fc1.weight', 'stages.2.blocks.5.mlp.fc1.bias', 'stages.2.blocks.5.mlp.fc2.weight', 'stages.2.blocks.5.mlp.fc2.bias', 'stages.2.blocks.5.norm2.weight', 'stages.2.blocks.5.norm2.bias', 'stages.2.downsample.reduction.weight', 'stages.2.downsample.norm.weight', 'stages.2.downsample.norm.bias', 'stages.3.blocks.0.attn_mask', 'stages.3.blocks.0.attn.t_scale', 'stages.3.blocks.0.attn.relative_coords_table', 'stages.3.blocks.0.attn.relative_position_index', 'stages.3.blocks.0.attn.crpb_mlp.0.weight', 'stages.3.blocks.0.attn.crpb_mlp.0.bias', 'stages.3.blocks.0.attn.crpb_mlp.3.weight', 'stages.3.blocks.0.attn.qkv.weight', 'stages.3.blocks.0.attn.qkv.bias', 'stages.3.blocks.0.attn.proj.weight', 'stages.3.blocks.0.attn.proj.bias', 'stages.3.blocks.0.norm1.weight', 'stages.3.blocks.0.norm1.bias', 'stages.3.blocks.0.mlp.fc1.weight', 'stages.3.blocks.0.mlp.fc1.bias', 'stages.3.blocks.0.mlp.fc2.weight', 'stages.3.blocks.0.mlp.fc2.bias', 'stages.3.blocks.0.norm2.weight', 'stages.3.blocks.0.norm2.bias', 'stages.3.blocks.1.attn_mask', 'stages.3.blocks.1.attn.t_scale', 'stages.3.blocks.1.attn.relative_coords_table', 'stages.3.blocks.1.attn.relative_position_index', 'stages.3.blocks.1.attn.crpb_mlp.0.weight', 'stages.3.blocks.1.attn.crpb_mlp.0.bias', 'stages.3.blocks.1.attn.crpb_mlp.3.weight', 'stages.3.blocks.1.attn.qkv.weight', 'stages.3.blocks.1.attn.qkv.bias', 'stages.3.blocks.1.attn.proj.weight', 'stages.3.blocks.1.attn.proj.bias', 'stages.3.blocks.1.norm1.weight', 'stages.3.blocks.1.norm1.bias', 'stages.3.blocks.1.mlp.fc1.weight', 'stages.3.blocks.1.mlp.fc1.bias', 'stages.3.blocks.1.mlp.fc2.weight', 'stages.3.blocks.1.mlp.fc2.bias', 'stages.3.blocks.1.norm2.weight', 'stages.3.blocks.1.norm2.bias', 'layernorm.weight', 'layernorm.bias', 'classifier.weight', 'classifier.bias'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SwinTransformerV2(pretrained_window_sizes=[6,6,6,6], ape=True, drop_path_rate=0.3)\n",
    "model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 96, 56, 56]           4,704\n",
      "         LayerNorm-2             [-1, 3136, 96]             192\n",
      "        embeddings-3             [-1, 3136, 96]               0\n",
      "           Dropout-4             [-1, 3136, 96]               0\n",
      "            Linear-5              [-1, 49, 288]          27,936\n",
      "            Linear-6          [-1, 13, 13, 384]           1,152\n",
      "              ReLU-7          [-1, 13, 13, 384]               0\n",
      "           Dropout-8          [-1, 13, 13, 384]               0\n",
      "            Linear-9            [-1, 13, 13, 3]           1,152\n",
      "          Softmax-10            [-1, 3, 49, 49]               0\n",
      "          Dropout-11            [-1, 3, 49, 49]               0\n",
      "           Linear-12               [-1, 49, 96]           9,312\n",
      "          Dropout-13               [-1, 49, 96]               0\n",
      "  WindowAttention-14               [-1, 49, 96]               0\n",
      "         DropPath-15             [-1, 3136, 96]               0\n",
      "        LayerNorm-16             [-1, 3136, 96]             192\n",
      "           Linear-17            [-1, 3136, 384]          37,248\n",
      "             GELU-18            [-1, 3136, 384]               0\n",
      "          Dropout-19            [-1, 3136, 384]               0\n",
      "           Linear-20             [-1, 3136, 96]          36,960\n",
      "          Dropout-21             [-1, 3136, 96]               0\n",
      "              Mlp-22             [-1, 3136, 96]               0\n",
      "         DropPath-23             [-1, 3136, 96]               0\n",
      "        LayerNorm-24             [-1, 3136, 96]             192\n",
      "SwinTransformerBlock-25             [-1, 3136, 96]               0\n",
      "           Linear-26              [-1, 49, 288]          27,936\n",
      "           Linear-27          [-1, 13, 13, 384]           1,152\n",
      "             ReLU-28          [-1, 13, 13, 384]               0\n",
      "          Dropout-29          [-1, 13, 13, 384]               0\n",
      "           Linear-30            [-1, 13, 13, 3]           1,152\n",
      "          Softmax-31            [-1, 3, 49, 49]               0\n",
      "          Dropout-32            [-1, 3, 49, 49]               0\n",
      "           Linear-33               [-1, 49, 96]           9,312\n",
      "          Dropout-34               [-1, 49, 96]               0\n",
      "  WindowAttention-35               [-1, 49, 96]               0\n",
      "         DropPath-36             [-1, 3136, 96]               0\n",
      "        LayerNorm-37             [-1, 3136, 96]             192\n",
      "           Linear-38            [-1, 3136, 384]          37,248\n",
      "             GELU-39            [-1, 3136, 384]               0\n",
      "          Dropout-40            [-1, 3136, 384]               0\n",
      "           Linear-41             [-1, 3136, 96]          36,960\n",
      "          Dropout-42             [-1, 3136, 96]               0\n",
      "              Mlp-43             [-1, 3136, 96]               0\n",
      "         DropPath-44             [-1, 3136, 96]               0\n",
      "        LayerNorm-45             [-1, 3136, 96]             192\n",
      "SwinTransformerBlock-46             [-1, 3136, 96]               0\n",
      "        LayerNorm-47             [-1, 784, 384]             768\n",
      "           Linear-48             [-1, 784, 192]          73,728\n",
      "     PatchMerging-49             [-1, 784, 192]               0\n",
      "       StageLayer-50             [-1, 784, 192]               0\n",
      "           Linear-51              [-1, 49, 576]         111,168\n",
      "           Linear-52          [-1, 13, 13, 384]           1,152\n",
      "             ReLU-53          [-1, 13, 13, 384]               0\n",
      "          Dropout-54          [-1, 13, 13, 384]               0\n",
      "           Linear-55            [-1, 13, 13, 6]           2,304\n",
      "          Softmax-56            [-1, 6, 49, 49]               0\n",
      "          Dropout-57            [-1, 6, 49, 49]               0\n",
      "           Linear-58              [-1, 49, 192]          37,056\n",
      "          Dropout-59              [-1, 49, 192]               0\n",
      "  WindowAttention-60              [-1, 49, 192]               0\n",
      "         DropPath-61             [-1, 784, 192]               0\n",
      "        LayerNorm-62             [-1, 784, 192]             384\n",
      "           Linear-63             [-1, 784, 768]         148,224\n",
      "             GELU-64             [-1, 784, 768]               0\n",
      "          Dropout-65             [-1, 784, 768]               0\n",
      "           Linear-66             [-1, 784, 192]         147,648\n",
      "          Dropout-67             [-1, 784, 192]               0\n",
      "              Mlp-68             [-1, 784, 192]               0\n",
      "         DropPath-69             [-1, 784, 192]               0\n",
      "        LayerNorm-70             [-1, 784, 192]             384\n",
      "SwinTransformerBlock-71             [-1, 784, 192]               0\n",
      "           Linear-72              [-1, 49, 576]         111,168\n",
      "           Linear-73          [-1, 13, 13, 384]           1,152\n",
      "             ReLU-74          [-1, 13, 13, 384]               0\n",
      "          Dropout-75          [-1, 13, 13, 384]               0\n",
      "           Linear-76            [-1, 13, 13, 6]           2,304\n",
      "          Softmax-77            [-1, 6, 49, 49]               0\n",
      "          Dropout-78            [-1, 6, 49, 49]               0\n",
      "           Linear-79              [-1, 49, 192]          37,056\n",
      "          Dropout-80              [-1, 49, 192]               0\n",
      "  WindowAttention-81              [-1, 49, 192]               0\n",
      "         DropPath-82             [-1, 784, 192]               0\n",
      "        LayerNorm-83             [-1, 784, 192]             384\n",
      "           Linear-84             [-1, 784, 768]         148,224\n",
      "             GELU-85             [-1, 784, 768]               0\n",
      "          Dropout-86             [-1, 784, 768]               0\n",
      "           Linear-87             [-1, 784, 192]         147,648\n",
      "          Dropout-88             [-1, 784, 192]               0\n",
      "              Mlp-89             [-1, 784, 192]               0\n",
      "         DropPath-90             [-1, 784, 192]               0\n",
      "        LayerNorm-91             [-1, 784, 192]             384\n",
      "SwinTransformerBlock-92             [-1, 784, 192]               0\n",
      "        LayerNorm-93             [-1, 196, 768]           1,536\n",
      "           Linear-94             [-1, 196, 384]         294,912\n",
      "     PatchMerging-95             [-1, 196, 384]               0\n",
      "       StageLayer-96             [-1, 196, 384]               0\n",
      "           Linear-97             [-1, 49, 1152]         443,520\n",
      "           Linear-98          [-1, 13, 13, 384]           1,152\n",
      "             ReLU-99          [-1, 13, 13, 384]               0\n",
      "         Dropout-100          [-1, 13, 13, 384]               0\n",
      "          Linear-101           [-1, 13, 13, 12]           4,608\n",
      "         Softmax-102           [-1, 12, 49, 49]               0\n",
      "         Dropout-103           [-1, 12, 49, 49]               0\n",
      "          Linear-104              [-1, 49, 384]         147,840\n",
      "         Dropout-105              [-1, 49, 384]               0\n",
      " WindowAttention-106              [-1, 49, 384]               0\n",
      "        DropPath-107             [-1, 196, 384]               0\n",
      "       LayerNorm-108             [-1, 196, 384]             768\n",
      "          Linear-109            [-1, 196, 1536]         591,360\n",
      "            GELU-110            [-1, 196, 1536]               0\n",
      "         Dropout-111            [-1, 196, 1536]               0\n",
      "          Linear-112             [-1, 196, 384]         590,208\n",
      "         Dropout-113             [-1, 196, 384]               0\n",
      "             Mlp-114             [-1, 196, 384]               0\n",
      "        DropPath-115             [-1, 196, 384]               0\n",
      "       LayerNorm-116             [-1, 196, 384]             768\n",
      "SwinTransformerBlock-117             [-1, 196, 384]               0\n",
      "          Linear-118             [-1, 49, 1152]         443,520\n",
      "          Linear-119          [-1, 13, 13, 384]           1,152\n",
      "            ReLU-120          [-1, 13, 13, 384]               0\n",
      "         Dropout-121          [-1, 13, 13, 384]               0\n",
      "          Linear-122           [-1, 13, 13, 12]           4,608\n",
      "         Softmax-123           [-1, 12, 49, 49]               0\n",
      "         Dropout-124           [-1, 12, 49, 49]               0\n",
      "          Linear-125              [-1, 49, 384]         147,840\n",
      "         Dropout-126              [-1, 49, 384]               0\n",
      " WindowAttention-127              [-1, 49, 384]               0\n",
      "        DropPath-128             [-1, 196, 384]               0\n",
      "       LayerNorm-129             [-1, 196, 384]             768\n",
      "          Linear-130            [-1, 196, 1536]         591,360\n",
      "            GELU-131            [-1, 196, 1536]               0\n",
      "         Dropout-132            [-1, 196, 1536]               0\n",
      "          Linear-133             [-1, 196, 384]         590,208\n",
      "         Dropout-134             [-1, 196, 384]               0\n",
      "             Mlp-135             [-1, 196, 384]               0\n",
      "        DropPath-136             [-1, 196, 384]               0\n",
      "       LayerNorm-137             [-1, 196, 384]             768\n",
      "SwinTransformerBlock-138             [-1, 196, 384]               0\n",
      "          Linear-139             [-1, 49, 1152]         443,520\n",
      "          Linear-140          [-1, 13, 13, 384]           1,152\n",
      "            ReLU-141          [-1, 13, 13, 384]               0\n",
      "         Dropout-142          [-1, 13, 13, 384]               0\n",
      "          Linear-143           [-1, 13, 13, 12]           4,608\n",
      "         Softmax-144           [-1, 12, 49, 49]               0\n",
      "         Dropout-145           [-1, 12, 49, 49]               0\n",
      "          Linear-146              [-1, 49, 384]         147,840\n",
      "         Dropout-147              [-1, 49, 384]               0\n",
      " WindowAttention-148              [-1, 49, 384]               0\n",
      "        DropPath-149             [-1, 196, 384]               0\n",
      "       LayerNorm-150             [-1, 196, 384]             768\n",
      "          Linear-151            [-1, 196, 1536]         591,360\n",
      "            GELU-152            [-1, 196, 1536]               0\n",
      "         Dropout-153            [-1, 196, 1536]               0\n",
      "          Linear-154             [-1, 196, 384]         590,208\n",
      "         Dropout-155             [-1, 196, 384]               0\n",
      "             Mlp-156             [-1, 196, 384]               0\n",
      "        DropPath-157             [-1, 196, 384]               0\n",
      "       LayerNorm-158             [-1, 196, 384]             768\n",
      "SwinTransformerBlock-159             [-1, 196, 384]               0\n",
      "          Linear-160             [-1, 49, 1152]         443,520\n",
      "          Linear-161          [-1, 13, 13, 384]           1,152\n",
      "            ReLU-162          [-1, 13, 13, 384]               0\n",
      "         Dropout-163          [-1, 13, 13, 384]               0\n",
      "          Linear-164           [-1, 13, 13, 12]           4,608\n",
      "         Softmax-165           [-1, 12, 49, 49]               0\n",
      "         Dropout-166           [-1, 12, 49, 49]               0\n",
      "          Linear-167              [-1, 49, 384]         147,840\n",
      "         Dropout-168              [-1, 49, 384]               0\n",
      " WindowAttention-169              [-1, 49, 384]               0\n",
      "        DropPath-170             [-1, 196, 384]               0\n",
      "       LayerNorm-171             [-1, 196, 384]             768\n",
      "          Linear-172            [-1, 196, 1536]         591,360\n",
      "            GELU-173            [-1, 196, 1536]               0\n",
      "         Dropout-174            [-1, 196, 1536]               0\n",
      "          Linear-175             [-1, 196, 384]         590,208\n",
      "         Dropout-176             [-1, 196, 384]               0\n",
      "             Mlp-177             [-1, 196, 384]               0\n",
      "        DropPath-178             [-1, 196, 384]               0\n",
      "       LayerNorm-179             [-1, 196, 384]             768\n",
      "SwinTransformerBlock-180             [-1, 196, 384]               0\n",
      "          Linear-181             [-1, 49, 1152]         443,520\n",
      "          Linear-182          [-1, 13, 13, 384]           1,152\n",
      "            ReLU-183          [-1, 13, 13, 384]               0\n",
      "         Dropout-184          [-1, 13, 13, 384]               0\n",
      "          Linear-185           [-1, 13, 13, 12]           4,608\n",
      "         Softmax-186           [-1, 12, 49, 49]               0\n",
      "         Dropout-187           [-1, 12, 49, 49]               0\n",
      "          Linear-188              [-1, 49, 384]         147,840\n",
      "         Dropout-189              [-1, 49, 384]               0\n",
      " WindowAttention-190              [-1, 49, 384]               0\n",
      "        DropPath-191             [-1, 196, 384]               0\n",
      "       LayerNorm-192             [-1, 196, 384]             768\n",
      "          Linear-193            [-1, 196, 1536]         591,360\n",
      "            GELU-194            [-1, 196, 1536]               0\n",
      "         Dropout-195            [-1, 196, 1536]               0\n",
      "          Linear-196             [-1, 196, 384]         590,208\n",
      "         Dropout-197             [-1, 196, 384]               0\n",
      "             Mlp-198             [-1, 196, 384]               0\n",
      "        DropPath-199             [-1, 196, 384]               0\n",
      "       LayerNorm-200             [-1, 196, 384]             768\n",
      "SwinTransformerBlock-201             [-1, 196, 384]               0\n",
      "          Linear-202             [-1, 49, 1152]         443,520\n",
      "          Linear-203          [-1, 13, 13, 384]           1,152\n",
      "            ReLU-204          [-1, 13, 13, 384]               0\n",
      "         Dropout-205          [-1, 13, 13, 384]               0\n",
      "          Linear-206           [-1, 13, 13, 12]           4,608\n",
      "         Softmax-207           [-1, 12, 49, 49]               0\n",
      "         Dropout-208           [-1, 12, 49, 49]               0\n",
      "          Linear-209              [-1, 49, 384]         147,840\n",
      "         Dropout-210              [-1, 49, 384]               0\n",
      " WindowAttention-211              [-1, 49, 384]               0\n",
      "        DropPath-212             [-1, 196, 384]               0\n",
      "       LayerNorm-213             [-1, 196, 384]             768\n",
      "          Linear-214            [-1, 196, 1536]         591,360\n",
      "            GELU-215            [-1, 196, 1536]               0\n",
      "         Dropout-216            [-1, 196, 1536]               0\n",
      "          Linear-217             [-1, 196, 384]         590,208\n",
      "         Dropout-218             [-1, 196, 384]               0\n",
      "             Mlp-219             [-1, 196, 384]               0\n",
      "        DropPath-220             [-1, 196, 384]               0\n",
      "       LayerNorm-221             [-1, 196, 384]             768\n",
      "SwinTransformerBlock-222             [-1, 196, 384]               0\n",
      "       LayerNorm-223             [-1, 49, 1536]           3,072\n",
      "          Linear-224              [-1, 49, 768]       1,179,648\n",
      "    PatchMerging-225              [-1, 49, 768]               0\n",
      "      StageLayer-226              [-1, 49, 768]               0\n",
      "          Linear-227             [-1, 49, 2304]       1,771,776\n",
      "          Linear-228          [-1, 13, 13, 384]           1,152\n",
      "            ReLU-229          [-1, 13, 13, 384]               0\n",
      "         Dropout-230          [-1, 13, 13, 384]               0\n",
      "          Linear-231           [-1, 13, 13, 24]           9,216\n",
      "         Softmax-232           [-1, 24, 49, 49]               0\n",
      "         Dropout-233           [-1, 24, 49, 49]               0\n",
      "          Linear-234              [-1, 49, 768]         590,592\n",
      "         Dropout-235              [-1, 49, 768]               0\n",
      " WindowAttention-236              [-1, 49, 768]               0\n",
      "        DropPath-237              [-1, 49, 768]               0\n",
      "       LayerNorm-238              [-1, 49, 768]           1,536\n",
      "          Linear-239             [-1, 49, 3072]       2,362,368\n",
      "            GELU-240             [-1, 49, 3072]               0\n",
      "         Dropout-241             [-1, 49, 3072]               0\n",
      "          Linear-242              [-1, 49, 768]       2,360,064\n",
      "         Dropout-243              [-1, 49, 768]               0\n",
      "             Mlp-244              [-1, 49, 768]               0\n",
      "        DropPath-245              [-1, 49, 768]               0\n",
      "       LayerNorm-246              [-1, 49, 768]           1,536\n",
      "SwinTransformerBlock-247              [-1, 49, 768]               0\n",
      "          Linear-248             [-1, 49, 2304]       1,771,776\n",
      "          Linear-249          [-1, 13, 13, 384]           1,152\n",
      "            ReLU-250          [-1, 13, 13, 384]               0\n",
      "         Dropout-251          [-1, 13, 13, 384]               0\n",
      "          Linear-252           [-1, 13, 13, 24]           9,216\n",
      "         Softmax-253           [-1, 24, 49, 49]               0\n",
      "         Dropout-254           [-1, 24, 49, 49]               0\n",
      "          Linear-255              [-1, 49, 768]         590,592\n",
      "         Dropout-256              [-1, 49, 768]               0\n",
      " WindowAttention-257              [-1, 49, 768]               0\n",
      "        DropPath-258              [-1, 49, 768]               0\n",
      "       LayerNorm-259              [-1, 49, 768]           1,536\n",
      "          Linear-260             [-1, 49, 3072]       2,362,368\n",
      "            GELU-261             [-1, 49, 3072]               0\n",
      "         Dropout-262             [-1, 49, 3072]               0\n",
      "          Linear-263              [-1, 49, 768]       2,360,064\n",
      "         Dropout-264              [-1, 49, 768]               0\n",
      "             Mlp-265              [-1, 49, 768]               0\n",
      "        DropPath-266              [-1, 49, 768]               0\n",
      "       LayerNorm-267              [-1, 49, 768]           1,536\n",
      "SwinTransformerBlock-268              [-1, 49, 768]               0\n",
      "        Identity-269              [-1, 49, 768]               0\n",
      "      StageLayer-270              [-1, 49, 768]               0\n",
      "       LayerNorm-271              [-1, 49, 768]           1,536\n",
      "AdaptiveAvgPool1d-272               [-1, 768, 1]               0\n",
      "          Linear-273                  [-1, 100]          76,900\n",
      "================================================================\n",
      "Total params: 27,639,748\n",
      "Trainable params: 27,639,748\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 271.27\n",
      "Params size (MB): 105.44\n",
      "Estimated Total Size (MB): 377.28\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(model.to('cuda'), (3,224,224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default Parameter(weight) Check\n",
    "- 추후 SimMIM 가중치가 제대로 불러와졌는지 확인용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0249, -0.1357,  0.0652, -0.0484],\n",
       "         [-0.0892,  0.0055,  0.0608, -0.0454],\n",
       "         [-0.0204,  0.0156, -0.1238,  0.0323],\n",
       "         [ 0.1443,  0.0266,  0.1002, -0.0322]],\n",
       "\n",
       "        [[ 0.0634, -0.1199,  0.1377,  0.1177],\n",
       "         [ 0.0953, -0.0080,  0.0917,  0.1258],\n",
       "         [ 0.0817, -0.0437,  0.0734, -0.0057],\n",
       "         [ 0.0237,  0.0038, -0.0514,  0.0629]],\n",
       "\n",
       "        [[-0.1387, -0.0589,  0.0992, -0.0170],\n",
       "         [ 0.0031,  0.0119, -0.1375, -0.1187],\n",
       "         [ 0.0408, -0.0498, -0.0778,  0.0416],\n",
       "         [ 0.0169, -0.1076,  0.1107,  0.0740]]], device='cuda:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['embeddings.patch_embeddings.weight'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-7.6564e-03, -3.0947e-03,  3.9520e-03,  9.8743e-05, -9.1680e-03,\n",
       "        -6.5241e-03, -2.9101e-02,  2.5674e-02, -1.5266e-02, -2.6604e-03,\n",
       "        -2.3008e-02,  1.6138e-02, -5.8916e-03,  2.3412e-03,  2.2706e-02,\n",
       "         6.3729e-04, -3.1659e-02, -7.6400e-03, -1.1566e-02, -3.8812e-02,\n",
       "        -1.4659e-02,  2.2244e-02, -6.0563e-03, -9.8120e-03,  2.1949e-02,\n",
       "        -3.3376e-02, -5.6768e-03, -4.7501e-02, -2.0839e-02,  1.1640e-02,\n",
       "         2.9832e-02, -5.1324e-02,  4.6017e-03,  1.2906e-02, -1.7035e-02,\n",
       "        -4.3480e-04,  2.3710e-02, -1.1125e-02,  1.0401e-02, -1.4300e-02,\n",
       "         6.3200e-03, -2.8566e-02,  3.1112e-03,  3.4539e-02,  4.0399e-02,\n",
       "         7.7253e-03, -5.3018e-03,  3.3467e-02, -2.1451e-02,  2.1735e-02,\n",
       "        -1.0947e-04, -1.8737e-02,  3.7710e-02,  2.7607e-02,  8.8451e-03,\n",
       "        -2.7601e-02, -1.2940e-03, -5.7506e-03, -2.1240e-02, -1.0631e-02,\n",
       "        -1.7865e-02,  2.1000e-02,  3.1405e-02, -7.4189e-03, -3.4847e-03,\n",
       "         2.2864e-02, -1.7541e-02, -2.3306e-02,  1.2626e-02, -1.8306e-02,\n",
       "         1.0926e-02,  7.4274e-03, -3.0814e-02,  1.5466e-04, -1.7189e-02,\n",
       "         1.7189e-02,  2.2436e-02,  1.2370e-02, -2.7745e-02, -2.8139e-02,\n",
       "         8.9007e-03, -1.5564e-02, -1.1289e-02,  1.5466e-03, -5.9692e-03,\n",
       "        -2.5774e-02,  3.0941e-03, -2.1367e-02,  4.1660e-02, -2.0263e-02,\n",
       "        -7.5627e-03,  4.8187e-02,  8.9067e-03,  2.0825e-02, -3.6610e-02,\n",
       "         1.2072e-02,  1.0796e-02,  5.5592e-03, -3.9304e-03,  3.5707e-02,\n",
       "         2.3316e-02, -1.1590e-02, -2.7836e-02, -1.3288e-02, -2.5346e-02,\n",
       "         3.3489e-02,  8.4588e-03,  3.3314e-02,  3.2901e-02, -2.3503e-03,\n",
       "        -8.8058e-03,  3.6027e-02, -7.7764e-03,  1.0042e-02, -2.5165e-02,\n",
       "         4.8135e-03, -1.0902e-02, -1.3601e-03,  5.9943e-03,  1.5547e-02,\n",
       "         2.7771e-02, -2.8706e-02, -4.6901e-03,  2.6447e-02,  2.6052e-02,\n",
       "        -5.8457e-03, -1.6265e-02, -2.9496e-02, -3.6512e-03, -2.8758e-02,\n",
       "        -9.2153e-03, -3.0255e-02,  7.5736e-03, -5.0357e-03,  7.9906e-04,\n",
       "         5.8906e-04,  2.9947e-02, -1.3670e-02,  1.1911e-02, -2.3883e-02,\n",
       "        -6.1295e-03,  2.0480e-02,  1.2221e-02,  1.5152e-02,  7.4564e-03,\n",
       "         1.7043e-02,  1.4427e-03, -2.9686e-02,  1.9104e-02, -1.4771e-03,\n",
       "        -1.8218e-02,  1.8111e-02,  1.0867e-02,  2.4669e-04, -2.1531e-02,\n",
       "         1.6376e-02,  1.7965e-02, -2.6243e-02, -3.1823e-02, -1.8422e-02,\n",
       "         1.2802e-02, -2.8300e-02, -7.9526e-03, -3.6536e-05, -2.6823e-02,\n",
       "         1.8183e-02, -1.7910e-02,  3.1506e-03, -1.0250e-02,  4.5622e-02,\n",
       "         3.8131e-02,  1.0210e-02,  1.3424e-02,  8.0762e-03, -5.6518e-03,\n",
       "         2.6087e-02, -2.7371e-02,  1.6949e-02, -9.4112e-03, -8.1005e-03,\n",
       "         1.3523e-02, -3.8450e-02, -1.2077e-02, -1.2864e-02, -1.4554e-03,\n",
       "        -3.2153e-02, -1.1701e-02,  2.8542e-02, -2.0868e-02, -3.6436e-03,\n",
       "        -1.7096e-02, -7.4933e-03,  8.0328e-03,  4.5572e-03, -1.9924e-02,\n",
       "        -1.1903e-02,  2.6819e-02,  2.3200e-02,  3.8906e-03, -2.5075e-02,\n",
       "         2.1668e-02, -5.2983e-03,  8.2482e-03, -2.1979e-02,  1.9923e-03,\n",
       "        -3.7402e-02,  2.0721e-02, -2.8575e-02, -4.4426e-03,  9.4339e-04,\n",
       "        -1.7971e-03, -8.4006e-04,  7.1920e-03,  1.3832e-02,  1.7613e-02,\n",
       "         1.6514e-03,  1.0735e-02, -1.8789e-03, -3.4510e-03, -3.0138e-03,\n",
       "        -5.1564e-03, -3.8525e-02, -2.1962e-02,  2.5481e-02,  1.2764e-02,\n",
       "        -1.3959e-02, -2.1240e-02,  5.3025e-04, -3.6202e-02,  1.4028e-02,\n",
       "         6.7206e-05, -1.6164e-02,  1.4103e-02, -6.5673e-02, -6.5279e-03,\n",
       "         6.3688e-03,  8.6203e-03, -5.1516e-03, -2.5124e-03,  2.7610e-02,\n",
       "         6.3860e-04, -3.7224e-02,  4.1570e-03,  1.7188e-05, -1.7642e-02,\n",
       "        -1.4518e-02, -2.5306e-02,  2.0864e-02,  1.0237e-03,  1.0095e-02,\n",
       "         3.6072e-02,  2.1733e-02,  1.4877e-03, -6.8936e-03, -1.6509e-05,\n",
       "         1.4657e-02, -2.3665e-02,  1.0011e-02, -2.1998e-02, -9.0330e-03,\n",
       "        -8.7878e-03,  4.7825e-02,  7.9461e-03, -2.4416e-02, -3.2746e-02,\n",
       "         1.9952e-03, -1.8966e-02,  3.7067e-02,  2.5403e-02,  2.4043e-02,\n",
       "         7.0124e-04, -7.6953e-03, -1.0487e-02, -4.3391e-02,  9.4784e-03,\n",
       "         3.6514e-02, -5.9058e-03, -1.2345e-02, -6.6345e-03, -5.9812e-03,\n",
       "         2.3654e-02, -9.6053e-03, -3.9611e-03, -4.1437e-03, -1.1655e-02,\n",
       "         1.7303e-02,  1.7238e-02, -2.0185e-02, -1.3976e-02,  5.9628e-03,\n",
       "        -6.1396e-03, -2.9736e-02, -2.7944e-03, -1.5529e-02, -1.0963e-02,\n",
       "         7.9686e-03,  1.4793e-03,  1.4110e-02,  2.2163e-02, -1.0262e-02,\n",
       "         4.7540e-03, -9.8687e-03,  6.0591e-03,  1.3017e-02, -3.7330e-03,\n",
       "         2.6097e-02, -2.3325e-02, -1.1123e-02,  1.3520e-02,  2.7837e-02,\n",
       "        -5.2178e-02, -2.0581e-02,  2.3919e-02, -3.2782e-04,  1.8907e-02,\n",
       "        -1.1163e-02, -5.8838e-03, -3.7900e-02, -1.7956e-03, -1.7594e-02,\n",
       "         3.2614e-02, -4.2055e-03, -9.8105e-03, -1.3419e-02,  1.7278e-02,\n",
       "        -1.0824e-02, -1.2489e-02,  2.6207e-02,  7.1820e-03, -2.0175e-02,\n",
       "         1.8350e-02, -5.3852e-02,  3.1005e-02,  2.1461e-02,  3.1780e-02,\n",
       "         4.4655e-03, -6.7937e-03, -1.5470e-02, -5.4609e-03,  7.5114e-03,\n",
       "        -4.6963e-03,  2.9627e-03,  3.2924e-02, -2.1050e-02,  3.1289e-02,\n",
       "        -7.8982e-03, -1.4545e-02,  3.7392e-02,  1.2511e-02, -7.7886e-04,\n",
       "         1.9253e-02,  1.1670e-02, -2.1112e-03, -2.3409e-02, -1.1039e-02,\n",
       "        -4.5424e-03, -1.1889e-02, -6.6153e-03, -1.2827e-02, -3.2397e-03,\n",
       "        -2.2968e-02,  2.4120e-02, -2.9228e-02, -9.3828e-04, -1.4081e-02,\n",
       "        -5.6503e-03,  2.7491e-02,  1.3383e-02,  9.6698e-03, -3.5809e-03,\n",
       "        -7.8520e-03,  1.8249e-03,  3.0228e-03,  1.3948e-02,  2.8885e-02,\n",
       "        -2.7640e-02, -2.0998e-02, -2.6022e-02,  1.5739e-02,  3.1727e-03,\n",
       "         1.6382e-02,  1.1036e-02, -3.6182e-02, -8.2992e-03], device='cuda:0')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['stages.3.blocks.1.attn.crpb_mlp.3.weight'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Swin v2 config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MODEL': {'TYPE': 'swinv2',\n",
       "  'NAME': 'simmim_train',\n",
       "  'PRETRAINED': '../../models/swin2/simmim.pth',\n",
       "  'DROP_PATH_RATE': 0.2,\n",
       "  'SWIN': {'EMBED_DIM': 96,\n",
       "   'DEPTHS': [2, 2, 6, 2],\n",
       "   'NUM_HEADS': [3, 6, 12, 24],\n",
       "   'WINDOW_SIZE': 7,\n",
       "   'PATCH_SIZE': 4}},\n",
       " 'DATA': {'IMG_SIZE': 224,\n",
       "  'MASK_PATCH_SIZE': 32,\n",
       "  'MASK_RATIO': 0.6,\n",
       "  'BATCH_SIZE': 960,\n",
       "  'NUM_WORKERS': 24,\n",
       "  'DATA_PATH': '../../data/sports'},\n",
       " 'TRAIN': {'EPOCHS': 20,\n",
       "  'WARMUP_EPOCHS': 10,\n",
       "  'BASE_LR': '1e-4',\n",
       "  'WEIGHT_DECAY': 0.05,\n",
       "  'CLIP_GRAD': 5}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swin_config = yaml.load(open('config/train.yaml'), Loader=yaml.FullLoader)\n",
    "swin_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load weight from SimMIM Model\n",
    "- Different Image/Window Size\n",
    "- Image와 Window 사이즈의 비율은 맞춰야함\n",
    "  ex) 192÷6 = 224÷7 = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained(config, model):\n",
    "    print(f\"==============> Loading weight {config.MODEL.PRETRAINED} for fine-tuning......\")\n",
    "    state_dict = torch.load(config.MODEL.PRETRAINED, map_location='cpu')\n",
    "\n",
    "    # remain encoder only\n",
    "    not_encoder_keys = [k for k in state_dict.keys() if 'encoder' not in k]\n",
    "    for k in not_encoder_keys:\n",
    "        del state_dict[k]\n",
    "        \n",
    "    # remove prefix encoder.\n",
    "    state_dict = {k.replace('encoder.', ''):v for k, v in state_dict.items()}\n",
    "\n",
    "    # delete relative_position_index since we always re-init it\n",
    "    relative_position_index_keys = [k for k in state_dict.keys() if \"relative_position_index\" in k]\n",
    "    for k in relative_position_index_keys:\n",
    "        del state_dict[k]\n",
    "\n",
    "    # delete relative_coords_table since we always re-init it\n",
    "    relative_position_index_keys = [k for k in state_dict.keys() if \"relative_coords_table\" in k]\n",
    "    for k in relative_position_index_keys:\n",
    "        del state_dict[k]\n",
    "\n",
    "    # delete attn_mask since we always re-init it\n",
    "    attn_mask_keys = [k for k in state_dict.keys() if \"attn_mask\" in k]\n",
    "    for k in attn_mask_keys:\n",
    "        del state_dict[k]\n",
    "\n",
    "    # bicubic interpolate relative_position_bias_table if not match\n",
    "    relative_position_bias_table_keys = [k for k in state_dict.keys() if \"relative_position_bias_table\" in k]\n",
    "    for k in relative_position_bias_table_keys:\n",
    "        relative_position_bias_table_pretrained = state_dict[k]\n",
    "        relative_position_bias_table_current = model.state_dict()[k]\n",
    "        L1, nH1 = relative_position_bias_table_pretrained.size()\n",
    "        L2, nH2 = relative_position_bias_table_current.size()\n",
    "        if nH1 != nH2:\n",
    "            print(f\"Error in loading {k}, passing......\")\n",
    "        else:\n",
    "            if L1 != L2:\n",
    "                # bicubic interpolate relative_position_bias_table if not match\n",
    "                S1 = int(L1 ** 0.5)\n",
    "                S2 = int(L2 ** 0.5)\n",
    "                relative_position_bias_table_pretrained_resized = torch.nn.functional.interpolate(\n",
    "                    relative_position_bias_table_pretrained.permute(1, 0).view(1, nH1, S1, S1), size=(S2, S2),\n",
    "                    mode='bicubic')\n",
    "                state_dict[k] = relative_position_bias_table_pretrained_resized.view(nH2, L2).permute(1, 0)\n",
    "\n",
    "    # bicubic interpolate absolute_pos_embed if not match\n",
    "    absolute_pos_embed_keys = [k for k in state_dict.keys() if \"absolute_pos_embed\" in k]\n",
    "    for k in absolute_pos_embed_keys:\n",
    "        # dpe\n",
    "        absolute_pos_embed_pretrained = state_dict[k]\n",
    "        absolute_pos_embed_current = model.state_dict()[k.replace('encoder.','')]\n",
    "        _, L1, C1 = absolute_pos_embed_pretrained.size()\n",
    "        _, L2, C2 = absolute_pos_embed_current.size()\n",
    "        if C1 != C1:\n",
    "            print(f\"Error in loading {k}, passing......\")\n",
    "        else:\n",
    "            if L1 != L2:\n",
    "                S1 = int(L1 ** 0.5)\n",
    "                S2 = int(L2 ** 0.5)\n",
    "                absolute_pos_embed_pretrained = absolute_pos_embed_pretrained.reshape(-1, S1, S1, C1)\n",
    "                absolute_pos_embed_pretrained = absolute_pos_embed_pretrained.permute(0, 3, 1, 2)\n",
    "                absolute_pos_embed_pretrained_resized = torch.nn.functional.interpolate(\n",
    "                    absolute_pos_embed_pretrained, size=(S2, S2), mode='bicubic')\n",
    "                absolute_pos_embed_pretrained_resized = absolute_pos_embed_pretrained_resized.permute(0, 2, 3, 1)\n",
    "                absolute_pos_embed_pretrained_resized = absolute_pos_embed_pretrained_resized.flatten(1, 2)\n",
    "                state_dict[k] = absolute_pos_embed_pretrained_resized\n",
    "\n",
    "    # check classifier, if not match, then re-init classifier to zero\n",
    "    head_bias_pretrained = state_dict['classifier.bias']\n",
    "    Nc1 = head_bias_pretrained.shape[0]\n",
    "    Nc2 = model.classifier.bias.shape[0]\n",
    "    if (Nc1 != Nc2):\n",
    "        torch.nn.init.constant_(model.classifier.bias, 0.)\n",
    "        torch.nn.init.constant_(model.classifier.weight, 0.)\n",
    "        del state_dict['classifier.weight']\n",
    "        del state_dict['classifier.bias']\n",
    "        print(f\"Error in loading classifier head, re-init classifier head to 0\")\n",
    "\n",
    "    msg = model.load_state_dict(state_dict, strict=False)\n",
    "    print(msg)\n",
    "\n",
    "    print(f\"=> loaded successfully '{config.MODEL.PRETRAINED}'\")\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============> Loading weight ../../models/swin2/simmim.pth for fine-tuning......\n",
      "_IncompatibleKeys(missing_keys=['stages.0.blocks.0.attn_mask', 'stages.0.blocks.0.attn.relative_coords_table', 'stages.0.blocks.0.attn.relative_position_index', 'stages.0.blocks.1.attn_mask', 'stages.0.blocks.1.attn.relative_coords_table', 'stages.0.blocks.1.attn.relative_position_index', 'stages.1.blocks.0.attn_mask', 'stages.1.blocks.0.attn.relative_coords_table', 'stages.1.blocks.0.attn.relative_position_index', 'stages.1.blocks.1.attn_mask', 'stages.1.blocks.1.attn.relative_coords_table', 'stages.1.blocks.1.attn.relative_position_index', 'stages.2.blocks.0.attn_mask', 'stages.2.blocks.0.attn.relative_coords_table', 'stages.2.blocks.0.attn.relative_position_index', 'stages.2.blocks.1.attn_mask', 'stages.2.blocks.1.attn.relative_coords_table', 'stages.2.blocks.1.attn.relative_position_index', 'stages.2.blocks.2.attn_mask', 'stages.2.blocks.2.attn.relative_coords_table', 'stages.2.blocks.2.attn.relative_position_index', 'stages.2.blocks.3.attn_mask', 'stages.2.blocks.3.attn.relative_coords_table', 'stages.2.blocks.3.attn.relative_position_index', 'stages.2.blocks.4.attn_mask', 'stages.2.blocks.4.attn.relative_coords_table', 'stages.2.blocks.4.attn.relative_position_index', 'stages.2.blocks.5.attn_mask', 'stages.2.blocks.5.attn.relative_coords_table', 'stages.2.blocks.5.attn.relative_position_index', 'stages.3.blocks.0.attn_mask', 'stages.3.blocks.0.attn.relative_coords_table', 'stages.3.blocks.0.attn.relative_position_index', 'stages.3.blocks.1.attn_mask', 'stages.3.blocks.1.attn.relative_coords_table', 'stages.3.blocks.1.attn.relative_position_index'], unexpected_keys=['mask_token'])\n",
      "=> loaded successfully '../../models/swin2/simmim.pth'\n"
     ]
    }
   ],
   "source": [
    "swin_config = Box(swin_config)\n",
    "load_pretrained(swin_config, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Loading Weight Results\n",
    "- 정상적으로 불러와졌는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0885, -0.0692, -0.0402, -0.0523],\n",
       "         [-0.1125,  0.1111,  0.0967, -0.0874],\n",
       "         [-0.0227, -0.0821,  0.0540, -0.0839],\n",
       "         [-0.0902,  0.0955,  0.1533,  0.0951]],\n",
       "\n",
       "        [[-0.0110, -0.0767, -0.0311, -0.0313],\n",
       "         [-0.0856, -0.0221, -0.0438,  0.0515],\n",
       "         [ 0.0448,  0.0269, -0.0029, -0.0253],\n",
       "         [ 0.1158,  0.0870,  0.0266,  0.0537]],\n",
       "\n",
       "        [[-0.0808, -0.0060,  0.0093, -0.1139],\n",
       "         [ 0.0042,  0.0294,  0.0802, -0.1004],\n",
       "         [-0.0022, -0.1235, -0.0276, -0.0003],\n",
       "         [-0.0660, -0.1091,  0.0580,  0.0781]]], device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['embeddings.patch_embeddings.weight'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1812,  0.1375, -0.1465,  0.0959, -0.2070, -0.1725,  0.2529,  0.1662,\n",
       "        -0.1603, -0.1488,  0.2225, -0.1905,  0.0231, -0.1829,  0.3743, -0.1590,\n",
       "        -0.0029,  0.2117, -0.2130, -0.1355, -0.1971, -0.1558, -0.0692, -0.1649,\n",
       "        -0.0777, -0.0042,  0.1551,  0.1437,  0.1677, -0.1359, -0.1575, -0.1934,\n",
       "        -0.1672,  0.2042, -0.1687,  0.3358, -0.2298,  0.1301, -0.2281, -0.2121,\n",
       "        -0.1469,  0.2563, -0.2390, -0.1649, -0.1861, -0.1773,  0.0715,  0.1244,\n",
       "        -0.1361, -0.1702, -0.1773,  0.1325,  0.2626,  0.3273, -0.1300, -0.1592,\n",
       "         0.2470,  0.2190, -0.1812, -0.1574, -0.1818, -0.1866, -0.1661,  0.1727,\n",
       "        -0.0497,  0.3164, -0.1375, -0.1660,  0.1463, -0.1910,  0.2380, -0.1886,\n",
       "        -0.2017, -0.1808,  0.2015, -0.1037, -0.1745,  0.1759,  0.2304, -0.1498,\n",
       "        -0.0723,  0.1504, -0.1591, -0.1455,  0.1189,  0.0184, -0.1670,  0.2948,\n",
       "        -0.1489, -0.2176, -0.1764, -0.0125,  0.2113, -0.1551,  0.1681, -0.1744,\n",
       "        -0.1858, -0.1692, -0.1872, -0.1580,  0.2405,  0.2308, -0.1587,  0.1788,\n",
       "        -0.1866, -0.1770, -0.1539,  0.2118, -0.2446, -0.1907, -0.0284, -0.1313,\n",
       "        -0.2034,  0.3835, -0.1855, -0.1749,  0.3426,  0.1893,  0.1848, -0.0154,\n",
       "         0.1313, -0.2335, -0.1670,  0.1556,  0.1965,  0.0036,  0.1906,  0.2082,\n",
       "        -0.1520,  0.2414, -0.2112, -0.1658,  0.0171, -0.2305,  0.1532,  0.1707,\n",
       "        -0.2005, -0.1411, -0.2320, -0.2040, -0.1791, -0.1458,  0.1867, -0.1777,\n",
       "         0.1421,  0.1356, -0.1730, -0.1933, -0.1319,  0.2559,  0.1508,  0.1806,\n",
       "         0.2894, -0.0922,  0.1728, -0.2044, -0.2282, -0.2104, -0.1945, -0.1975,\n",
       "        -0.0151,  0.1734, -0.1734,  0.2104, -0.2185, -0.1880, -0.1702, -0.1370,\n",
       "        -0.0786,  0.2260,  0.1753, -0.1504,  0.1708, -0.1804, -0.1998, -0.1364,\n",
       "        -0.0019, -0.1709,  0.1891, -0.1596, -0.1487,  0.2631,  0.0089,  0.1562,\n",
       "        -0.1359,  0.1356, -0.1594, -0.1825,  0.2428, -0.2178, -0.1942,  0.1869,\n",
       "         0.1755, -0.1768, -0.1856,  0.2496,  0.2654,  0.2833, -0.1151, -0.1745,\n",
       "        -0.1999,  0.2164, -0.2295, -0.1745, -0.0999, -0.0205, -0.1335,  0.1825,\n",
       "        -0.2108, -0.1828, -0.2074,  0.2275, -0.1765, -0.0909, -0.1004, -0.1775,\n",
       "        -0.1640, -0.1352, -0.1547,  0.1295,  0.1141, -0.1669, -0.1464,  0.1613,\n",
       "         0.0080, -0.1767,  0.1813, -0.2142,  0.0553,  0.2954,  0.1825,  0.2650,\n",
       "         0.2456,  0.1869,  0.2847, -0.1525, -0.1904,  0.1727, -0.1402,  0.1187,\n",
       "         0.1974,  0.2492,  0.2789,  0.1590, -0.1895, -0.1065,  0.1706, -0.2100,\n",
       "         0.1794, -0.1606,  0.1724, -0.0088,  0.2081, -0.1725,  0.1522,  0.3278,\n",
       "        -0.2312,  0.1814, -0.2075, -0.1229, -0.1739,  0.1989,  0.2522,  0.1718,\n",
       "         0.2722,  0.2311,  0.0046, -0.2011,  0.2396, -0.1701, -0.1709,  0.1654,\n",
       "         0.1050, -0.1539, -0.2152,  0.3122,  0.1946,  0.1728, -0.0153, -0.1626,\n",
       "        -0.0090, -0.1320, -0.0060,  0.2010,  0.2664, -0.2075, -0.1713, -0.1629,\n",
       "        -0.1810, -0.0692, -0.1757,  0.1669, -0.2075, -0.1833,  0.1379, -0.2140,\n",
       "        -0.1792,  0.2229, -0.1625, -0.1612,  0.2314, -0.1431,  0.1769, -0.1447,\n",
       "         0.1879, -0.1836, -0.1592,  0.1494,  0.1684,  0.1605,  0.2320, -0.1707,\n",
       "        -0.1623, -0.1742,  0.1138,  0.2409,  0.1608, -0.2044,  0.1628, -0.1745,\n",
       "         0.1918,  0.3339, -0.1641, -0.1872, -0.1616, -0.1336, -0.1587, -0.2116,\n",
       "        -0.1572, -0.1463,  0.1591, -0.2076, -0.0095, -0.1610,  0.1468, -0.2043,\n",
       "        -0.1867, -0.1435,  0.2082, -0.1701, -0.1799, -0.1409, -0.0077, -0.1668,\n",
       "        -0.1802,  0.2739,  0.1507, -0.2072,  0.2881, -0.1539,  0.1669,  0.2107,\n",
       "         0.1501,  0.1953,  0.3251,  0.3184, -0.0182, -0.1860, -0.1588, -0.1606,\n",
       "         0.2667, -0.1596, -0.1434, -0.1560, -0.0927, -0.1587,  0.0197,  0.1585,\n",
       "         0.1921,  0.1512, -0.1888, -0.2014, -0.2140, -0.1693,  0.0991, -0.1914,\n",
       "         0.2450, -0.2016, -0.1351, -0.1601, -0.1961,  0.2451, -0.0789, -0.2082],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['stages.3.blocks.1.attn.crpb_mlp.3.weight'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.Stage-2 Traing\n",
    "- Supervised pre-training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Transform, Loss, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms 정의하기\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8,1), interpolation=transforms.InterpolationMode.LANCZOS),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    transforms.RandomErasing(p=0.9, scale=(0.02, 0.33)),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "data_dir = '../../data/sports'\n",
    "batch_size = 960\n",
    "\n",
    "train_path = data_dir+'/train'\n",
    "valid_path = data_dir+'/valid'\n",
    "test_path = data_dir+'/test'\n",
    "\n",
    "# dataset load\n",
    "train_data = ImageFolder(train_path, transform=train_transform)\n",
    "valid_data = ImageFolder(valid_path, transform=test_transform)\n",
    "test_data = ImageFolder(test_path, transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_norm = 1.0 # paper : 100 with G variants\n",
    "\n",
    "model.to(device)\n",
    "model_path = '../../models/swin2/model_w_simmim.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixup_fn = Mixup(mixup_alpha=.7, \n",
    "                cutmix_alpha=.7, \n",
    "                prob=.7, \n",
    "                switch_prob=0.5, \n",
    "                mode='batch',\n",
    "                label_smoothing=.1,\n",
    "                num_classes=100)\n",
    "\n",
    "epochs = 150\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer-Wise Learning Rate Decay ★"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: absolute_pos_embed\n",
      "1: embeddings.patch_embeddings.weight\n",
      "2: embeddings.patch_embeddings.bias\n",
      "3: embeddings.norm.weight\n",
      "4: embeddings.norm.bias\n",
      "5: stages.0.blocks.0.attn.t_scale\n",
      "6: stages.0.blocks.0.attn.crpb_mlp.0.weight\n",
      "7: stages.0.blocks.0.attn.crpb_mlp.0.bias\n",
      "8: stages.0.blocks.0.attn.crpb_mlp.3.weight\n",
      "9: stages.0.blocks.0.attn.qkv.weight\n",
      "10: stages.0.blocks.0.attn.qkv.bias\n",
      "11: stages.0.blocks.0.attn.proj.weight\n",
      "12: stages.0.blocks.0.attn.proj.bias\n",
      "13: stages.0.blocks.0.norm1.weight\n",
      "14: stages.0.blocks.0.norm1.bias\n",
      "15: stages.0.blocks.0.mlp.fc1.weight\n",
      "16: stages.0.blocks.0.mlp.fc1.bias\n",
      "17: stages.0.blocks.0.mlp.fc2.weight\n",
      "18: stages.0.blocks.0.mlp.fc2.bias\n",
      "19: stages.0.blocks.0.norm2.weight\n",
      "20: stages.0.blocks.0.norm2.bias\n",
      "21: stages.0.blocks.1.attn.t_scale\n",
      "22: stages.0.blocks.1.attn.crpb_mlp.0.weight\n",
      "23: stages.0.blocks.1.attn.crpb_mlp.0.bias\n",
      "24: stages.0.blocks.1.attn.crpb_mlp.3.weight\n",
      "25: stages.0.blocks.1.attn.qkv.weight\n",
      "26: stages.0.blocks.1.attn.qkv.bias\n",
      "27: stages.0.blocks.1.attn.proj.weight\n",
      "28: stages.0.blocks.1.attn.proj.bias\n",
      "29: stages.0.blocks.1.norm1.weight\n",
      "30: stages.0.blocks.1.norm1.bias\n",
      "31: stages.0.blocks.1.mlp.fc1.weight\n",
      "32: stages.0.blocks.1.mlp.fc1.bias\n",
      "33: stages.0.blocks.1.mlp.fc2.weight\n",
      "34: stages.0.blocks.1.mlp.fc2.bias\n",
      "35: stages.0.blocks.1.norm2.weight\n",
      "36: stages.0.blocks.1.norm2.bias\n",
      "37: stages.0.downsample.reduction.weight\n",
      "38: stages.0.downsample.norm.weight\n",
      "39: stages.0.downsample.norm.bias\n",
      "40: stages.1.blocks.0.attn.t_scale\n",
      "41: stages.1.blocks.0.attn.crpb_mlp.0.weight\n",
      "42: stages.1.blocks.0.attn.crpb_mlp.0.bias\n",
      "43: stages.1.blocks.0.attn.crpb_mlp.3.weight\n",
      "44: stages.1.blocks.0.attn.qkv.weight\n",
      "45: stages.1.blocks.0.attn.qkv.bias\n",
      "46: stages.1.blocks.0.attn.proj.weight\n",
      "47: stages.1.blocks.0.attn.proj.bias\n",
      "48: stages.1.blocks.0.norm1.weight\n",
      "49: stages.1.blocks.0.norm1.bias\n",
      "50: stages.1.blocks.0.mlp.fc1.weight\n",
      "51: stages.1.blocks.0.mlp.fc1.bias\n",
      "52: stages.1.blocks.0.mlp.fc2.weight\n",
      "53: stages.1.blocks.0.mlp.fc2.bias\n",
      "54: stages.1.blocks.0.norm2.weight\n",
      "55: stages.1.blocks.0.norm2.bias\n",
      "56: stages.1.blocks.1.attn.t_scale\n",
      "57: stages.1.blocks.1.attn.crpb_mlp.0.weight\n",
      "58: stages.1.blocks.1.attn.crpb_mlp.0.bias\n",
      "59: stages.1.blocks.1.attn.crpb_mlp.3.weight\n",
      "60: stages.1.blocks.1.attn.qkv.weight\n",
      "61: stages.1.blocks.1.attn.qkv.bias\n",
      "62: stages.1.blocks.1.attn.proj.weight\n",
      "63: stages.1.blocks.1.attn.proj.bias\n",
      "64: stages.1.blocks.1.norm1.weight\n",
      "65: stages.1.blocks.1.norm1.bias\n",
      "66: stages.1.blocks.1.mlp.fc1.weight\n",
      "67: stages.1.blocks.1.mlp.fc1.bias\n",
      "68: stages.1.blocks.1.mlp.fc2.weight\n",
      "69: stages.1.blocks.1.mlp.fc2.bias\n",
      "70: stages.1.blocks.1.norm2.weight\n",
      "71: stages.1.blocks.1.norm2.bias\n",
      "72: stages.1.downsample.reduction.weight\n",
      "73: stages.1.downsample.norm.weight\n",
      "74: stages.1.downsample.norm.bias\n",
      "75: stages.2.blocks.0.attn.t_scale\n",
      "76: stages.2.blocks.0.attn.crpb_mlp.0.weight\n",
      "77: stages.2.blocks.0.attn.crpb_mlp.0.bias\n",
      "78: stages.2.blocks.0.attn.crpb_mlp.3.weight\n",
      "79: stages.2.blocks.0.attn.qkv.weight\n",
      "80: stages.2.blocks.0.attn.qkv.bias\n",
      "81: stages.2.blocks.0.attn.proj.weight\n",
      "82: stages.2.blocks.0.attn.proj.bias\n",
      "83: stages.2.blocks.0.norm1.weight\n",
      "84: stages.2.blocks.0.norm1.bias\n",
      "85: stages.2.blocks.0.mlp.fc1.weight\n",
      "86: stages.2.blocks.0.mlp.fc1.bias\n",
      "87: stages.2.blocks.0.mlp.fc2.weight\n",
      "88: stages.2.blocks.0.mlp.fc2.bias\n",
      "89: stages.2.blocks.0.norm2.weight\n",
      "90: stages.2.blocks.0.norm2.bias\n",
      "91: stages.2.blocks.1.attn.t_scale\n",
      "92: stages.2.blocks.1.attn.crpb_mlp.0.weight\n",
      "93: stages.2.blocks.1.attn.crpb_mlp.0.bias\n",
      "94: stages.2.blocks.1.attn.crpb_mlp.3.weight\n",
      "95: stages.2.blocks.1.attn.qkv.weight\n",
      "96: stages.2.blocks.1.attn.qkv.bias\n",
      "97: stages.2.blocks.1.attn.proj.weight\n",
      "98: stages.2.blocks.1.attn.proj.bias\n",
      "99: stages.2.blocks.1.norm1.weight\n",
      "100: stages.2.blocks.1.norm1.bias\n",
      "101: stages.2.blocks.1.mlp.fc1.weight\n",
      "102: stages.2.blocks.1.mlp.fc1.bias\n",
      "103: stages.2.blocks.1.mlp.fc2.weight\n",
      "104: stages.2.blocks.1.mlp.fc2.bias\n",
      "105: stages.2.blocks.1.norm2.weight\n",
      "106: stages.2.blocks.1.norm2.bias\n",
      "107: stages.2.blocks.2.attn.t_scale\n",
      "108: stages.2.blocks.2.attn.crpb_mlp.0.weight\n",
      "109: stages.2.blocks.2.attn.crpb_mlp.0.bias\n",
      "110: stages.2.blocks.2.attn.crpb_mlp.3.weight\n",
      "111: stages.2.blocks.2.attn.qkv.weight\n",
      "112: stages.2.blocks.2.attn.qkv.bias\n",
      "113: stages.2.blocks.2.attn.proj.weight\n",
      "114: stages.2.blocks.2.attn.proj.bias\n",
      "115: stages.2.blocks.2.norm1.weight\n",
      "116: stages.2.blocks.2.norm1.bias\n",
      "117: stages.2.blocks.2.mlp.fc1.weight\n",
      "118: stages.2.blocks.2.mlp.fc1.bias\n",
      "119: stages.2.blocks.2.mlp.fc2.weight\n",
      "120: stages.2.blocks.2.mlp.fc2.bias\n",
      "121: stages.2.blocks.2.norm2.weight\n",
      "122: stages.2.blocks.2.norm2.bias\n",
      "123: stages.2.blocks.3.attn.t_scale\n",
      "124: stages.2.blocks.3.attn.crpb_mlp.0.weight\n",
      "125: stages.2.blocks.3.attn.crpb_mlp.0.bias\n",
      "126: stages.2.blocks.3.attn.crpb_mlp.3.weight\n",
      "127: stages.2.blocks.3.attn.qkv.weight\n",
      "128: stages.2.blocks.3.attn.qkv.bias\n",
      "129: stages.2.blocks.3.attn.proj.weight\n",
      "130: stages.2.blocks.3.attn.proj.bias\n",
      "131: stages.2.blocks.3.norm1.weight\n",
      "132: stages.2.blocks.3.norm1.bias\n",
      "133: stages.2.blocks.3.mlp.fc1.weight\n",
      "134: stages.2.blocks.3.mlp.fc1.bias\n",
      "135: stages.2.blocks.3.mlp.fc2.weight\n",
      "136: stages.2.blocks.3.mlp.fc2.bias\n",
      "137: stages.2.blocks.3.norm2.weight\n",
      "138: stages.2.blocks.3.norm2.bias\n",
      "139: stages.2.blocks.4.attn.t_scale\n",
      "140: stages.2.blocks.4.attn.crpb_mlp.0.weight\n",
      "141: stages.2.blocks.4.attn.crpb_mlp.0.bias\n",
      "142: stages.2.blocks.4.attn.crpb_mlp.3.weight\n",
      "143: stages.2.blocks.4.attn.qkv.weight\n",
      "144: stages.2.blocks.4.attn.qkv.bias\n",
      "145: stages.2.blocks.4.attn.proj.weight\n",
      "146: stages.2.blocks.4.attn.proj.bias\n",
      "147: stages.2.blocks.4.norm1.weight\n",
      "148: stages.2.blocks.4.norm1.bias\n",
      "149: stages.2.blocks.4.mlp.fc1.weight\n",
      "150: stages.2.blocks.4.mlp.fc1.bias\n",
      "151: stages.2.blocks.4.mlp.fc2.weight\n",
      "152: stages.2.blocks.4.mlp.fc2.bias\n",
      "153: stages.2.blocks.4.norm2.weight\n",
      "154: stages.2.blocks.4.norm2.bias\n",
      "155: stages.2.blocks.5.attn.t_scale\n",
      "156: stages.2.blocks.5.attn.crpb_mlp.0.weight\n",
      "157: stages.2.blocks.5.attn.crpb_mlp.0.bias\n",
      "158: stages.2.blocks.5.attn.crpb_mlp.3.weight\n",
      "159: stages.2.blocks.5.attn.qkv.weight\n",
      "160: stages.2.blocks.5.attn.qkv.bias\n",
      "161: stages.2.blocks.5.attn.proj.weight\n",
      "162: stages.2.blocks.5.attn.proj.bias\n",
      "163: stages.2.blocks.5.norm1.weight\n",
      "164: stages.2.blocks.5.norm1.bias\n",
      "165: stages.2.blocks.5.mlp.fc1.weight\n",
      "166: stages.2.blocks.5.mlp.fc1.bias\n",
      "167: stages.2.blocks.5.mlp.fc2.weight\n",
      "168: stages.2.blocks.5.mlp.fc2.bias\n",
      "169: stages.2.blocks.5.norm2.weight\n",
      "170: stages.2.blocks.5.norm2.bias\n",
      "171: stages.2.downsample.reduction.weight\n",
      "172: stages.2.downsample.norm.weight\n",
      "173: stages.2.downsample.norm.bias\n",
      "174: stages.3.blocks.0.attn.t_scale\n",
      "175: stages.3.blocks.0.attn.crpb_mlp.0.weight\n",
      "176: stages.3.blocks.0.attn.crpb_mlp.0.bias\n",
      "177: stages.3.blocks.0.attn.crpb_mlp.3.weight\n",
      "178: stages.3.blocks.0.attn.qkv.weight\n",
      "179: stages.3.blocks.0.attn.qkv.bias\n",
      "180: stages.3.blocks.0.attn.proj.weight\n",
      "181: stages.3.blocks.0.attn.proj.bias\n",
      "182: stages.3.blocks.0.norm1.weight\n",
      "183: stages.3.blocks.0.norm1.bias\n",
      "184: stages.3.blocks.0.mlp.fc1.weight\n",
      "185: stages.3.blocks.0.mlp.fc1.bias\n",
      "186: stages.3.blocks.0.mlp.fc2.weight\n",
      "187: stages.3.blocks.0.mlp.fc2.bias\n",
      "188: stages.3.blocks.0.norm2.weight\n",
      "189: stages.3.blocks.0.norm2.bias\n",
      "190: stages.3.blocks.1.attn.t_scale\n",
      "191: stages.3.blocks.1.attn.crpb_mlp.0.weight\n",
      "192: stages.3.blocks.1.attn.crpb_mlp.0.bias\n",
      "193: stages.3.blocks.1.attn.crpb_mlp.3.weight\n",
      "194: stages.3.blocks.1.attn.qkv.weight\n",
      "195: stages.3.blocks.1.attn.qkv.bias\n",
      "196: stages.3.blocks.1.attn.proj.weight\n",
      "197: stages.3.blocks.1.attn.proj.bias\n",
      "198: stages.3.blocks.1.norm1.weight\n",
      "199: stages.3.blocks.1.norm1.bias\n",
      "200: stages.3.blocks.1.mlp.fc1.weight\n",
      "201: stages.3.blocks.1.mlp.fc1.bias\n",
      "202: stages.3.blocks.1.mlp.fc2.weight\n",
      "203: stages.3.blocks.1.mlp.fc2.bias\n",
      "204: stages.3.blocks.1.norm2.weight\n",
      "205: stages.3.blocks.1.norm2.bias\n",
      "206: layernorm.weight\n",
      "207: layernorm.bias\n",
      "208: classifier.weight\n",
      "209: classifier.bias\n"
     ]
    }
   ],
   "source": [
    "layer_names = []\n",
    "for i, (name, params) in enumerate(model.named_parameters()):\n",
    "    lr = base_lr\n",
    "    print(f'{i}: {name}')\n",
    "    layer_names.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['classifier.bias',\n",
       " 'classifier.weight',\n",
       " 'layernorm.bias',\n",
       " 'layernorm.weight',\n",
       " 'stages.3.blocks.1.norm2.bias']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_names.reverse()\n",
    "layer_names[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: classifier.bias's lr=0.0014, weight_decay=0\n",
      "1: classifier.weight's lr=0.0014, weight_decay=0.01\n",
      "2: layernorm.bias's lr=0.001218, weight_decay=0\n",
      "3: layernorm.weight's lr=0.001218, weight_decay=0\n",
      "4: stages.3.blocks.1.norm2.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "5: stages.3.blocks.1.norm2.weight's lr=0.0010596599999999998, weight_decay=0\n",
      "6: stages.3.blocks.1.mlp.fc2.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "7: stages.3.blocks.1.mlp.fc2.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "8: stages.3.blocks.1.mlp.fc1.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "9: stages.3.blocks.1.mlp.fc1.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "10: stages.3.blocks.1.norm1.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "11: stages.3.blocks.1.norm1.weight's lr=0.0010596599999999998, weight_decay=0\n",
      "12: stages.3.blocks.1.attn.proj.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "13: stages.3.blocks.1.attn.proj.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "14: stages.3.blocks.1.attn.qkv.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "15: stages.3.blocks.1.attn.qkv.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "16: stages.3.blocks.1.attn.crpb_mlp.3.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "17: stages.3.blocks.1.attn.crpb_mlp.0.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "18: stages.3.blocks.1.attn.crpb_mlp.0.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "19: stages.3.blocks.1.attn.t_scale's lr=0.0010596599999999998, weight_decay=0\n",
      "20: stages.3.blocks.0.norm2.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "21: stages.3.blocks.0.norm2.weight's lr=0.0010596599999999998, weight_decay=0\n",
      "22: stages.3.blocks.0.mlp.fc2.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "23: stages.3.blocks.0.mlp.fc2.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "24: stages.3.blocks.0.mlp.fc1.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "25: stages.3.blocks.0.mlp.fc1.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "26: stages.3.blocks.0.norm1.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "27: stages.3.blocks.0.norm1.weight's lr=0.0010596599999999998, weight_decay=0\n",
      "28: stages.3.blocks.0.attn.proj.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "29: stages.3.blocks.0.attn.proj.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "30: stages.3.blocks.0.attn.qkv.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "31: stages.3.blocks.0.attn.qkv.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "32: stages.3.blocks.0.attn.crpb_mlp.3.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "33: stages.3.blocks.0.attn.crpb_mlp.0.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "34: stages.3.blocks.0.attn.crpb_mlp.0.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "35: stages.3.blocks.0.attn.t_scale's lr=0.0010596599999999998, weight_decay=0\n",
      "36: stages.2.downsample.norm.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "37: stages.2.downsample.norm.weight's lr=0.0010596599999999998, weight_decay=0\n",
      "38: stages.2.downsample.reduction.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "39: stages.2.blocks.5.norm2.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "40: stages.2.blocks.5.norm2.weight's lr=0.0010596599999999998, weight_decay=0\n",
      "41: stages.2.blocks.5.mlp.fc2.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "42: stages.2.blocks.5.mlp.fc2.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "43: stages.2.blocks.5.mlp.fc1.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "44: stages.2.blocks.5.mlp.fc1.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "45: stages.2.blocks.5.norm1.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "46: stages.2.blocks.5.norm1.weight's lr=0.0010596599999999998, weight_decay=0\n",
      "47: stages.2.blocks.5.attn.proj.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "48: stages.2.blocks.5.attn.proj.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "49: stages.2.blocks.5.attn.qkv.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "50: stages.2.blocks.5.attn.qkv.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "51: stages.2.blocks.5.attn.crpb_mlp.3.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "52: stages.2.blocks.5.attn.crpb_mlp.0.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "53: stages.2.blocks.5.attn.crpb_mlp.0.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "54: stages.2.blocks.5.attn.t_scale's lr=0.0010596599999999998, weight_decay=0\n",
      "55: stages.2.blocks.4.norm2.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "56: stages.2.blocks.4.norm2.weight's lr=0.0010596599999999998, weight_decay=0\n",
      "57: stages.2.blocks.4.mlp.fc2.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "58: stages.2.blocks.4.mlp.fc2.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "59: stages.2.blocks.4.mlp.fc1.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "60: stages.2.blocks.4.mlp.fc1.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "61: stages.2.blocks.4.norm1.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "62: stages.2.blocks.4.norm1.weight's lr=0.0010596599999999998, weight_decay=0\n",
      "63: stages.2.blocks.4.attn.proj.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "64: stages.2.blocks.4.attn.proj.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "65: stages.2.blocks.4.attn.qkv.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "66: stages.2.blocks.4.attn.qkv.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "67: stages.2.blocks.4.attn.crpb_mlp.3.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "68: stages.2.blocks.4.attn.crpb_mlp.0.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "69: stages.2.blocks.4.attn.crpb_mlp.0.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "70: stages.2.blocks.4.attn.t_scale's lr=0.0010596599999999998, weight_decay=0\n",
      "71: stages.2.blocks.3.norm2.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "72: stages.2.blocks.3.norm2.weight's lr=0.0010596599999999998, weight_decay=0\n",
      "73: stages.2.blocks.3.mlp.fc2.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "74: stages.2.blocks.3.mlp.fc2.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "75: stages.2.blocks.3.mlp.fc1.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "76: stages.2.blocks.3.mlp.fc1.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "77: stages.2.blocks.3.norm1.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "78: stages.2.blocks.3.norm1.weight's lr=0.0010596599999999998, weight_decay=0\n",
      "79: stages.2.blocks.3.attn.proj.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "80: stages.2.blocks.3.attn.proj.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "81: stages.2.blocks.3.attn.qkv.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "82: stages.2.blocks.3.attn.qkv.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "83: stages.2.blocks.3.attn.crpb_mlp.3.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "84: stages.2.blocks.3.attn.crpb_mlp.0.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "85: stages.2.blocks.3.attn.crpb_mlp.0.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "86: stages.2.blocks.3.attn.t_scale's lr=0.0010596599999999998, weight_decay=0\n",
      "87: stages.2.blocks.2.norm2.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "88: stages.2.blocks.2.norm2.weight's lr=0.0010596599999999998, weight_decay=0\n",
      "89: stages.2.blocks.2.mlp.fc2.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "90: stages.2.blocks.2.mlp.fc2.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "91: stages.2.blocks.2.mlp.fc1.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "92: stages.2.blocks.2.mlp.fc1.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "93: stages.2.blocks.2.norm1.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "94: stages.2.blocks.2.norm1.weight's lr=0.0010596599999999998, weight_decay=0\n",
      "95: stages.2.blocks.2.attn.proj.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "96: stages.2.blocks.2.attn.proj.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "97: stages.2.blocks.2.attn.qkv.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "98: stages.2.blocks.2.attn.qkv.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "99: stages.2.blocks.2.attn.crpb_mlp.3.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "100: stages.2.blocks.2.attn.crpb_mlp.0.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "101: stages.2.blocks.2.attn.crpb_mlp.0.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "102: stages.2.blocks.2.attn.t_scale's lr=0.0010596599999999998, weight_decay=0\n",
      "103: stages.2.blocks.1.norm2.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "104: stages.2.blocks.1.norm2.weight's lr=0.0010596599999999998, weight_decay=0\n",
      "105: stages.2.blocks.1.mlp.fc2.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "106: stages.2.blocks.1.mlp.fc2.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "107: stages.2.blocks.1.mlp.fc1.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "108: stages.2.blocks.1.mlp.fc1.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "109: stages.2.blocks.1.norm1.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "110: stages.2.blocks.1.norm1.weight's lr=0.0010596599999999998, weight_decay=0\n",
      "111: stages.2.blocks.1.attn.proj.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "112: stages.2.blocks.1.attn.proj.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "113: stages.2.blocks.1.attn.qkv.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "114: stages.2.blocks.1.attn.qkv.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "115: stages.2.blocks.1.attn.crpb_mlp.3.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "116: stages.2.blocks.1.attn.crpb_mlp.0.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "117: stages.2.blocks.1.attn.crpb_mlp.0.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "118: stages.2.blocks.1.attn.t_scale's lr=0.0010596599999999998, weight_decay=0\n",
      "119: stages.2.blocks.0.norm2.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "120: stages.2.blocks.0.norm2.weight's lr=0.0010596599999999998, weight_decay=0\n",
      "121: stages.2.blocks.0.mlp.fc2.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "122: stages.2.blocks.0.mlp.fc2.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "123: stages.2.blocks.0.mlp.fc1.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "124: stages.2.blocks.0.mlp.fc1.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "125: stages.2.blocks.0.norm1.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "126: stages.2.blocks.0.norm1.weight's lr=0.0010596599999999998, weight_decay=0\n",
      "127: stages.2.blocks.0.attn.proj.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "128: stages.2.blocks.0.attn.proj.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "129: stages.2.blocks.0.attn.qkv.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "130: stages.2.blocks.0.attn.qkv.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "131: stages.2.blocks.0.attn.crpb_mlp.3.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "132: stages.2.blocks.0.attn.crpb_mlp.0.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "133: stages.2.blocks.0.attn.crpb_mlp.0.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "134: stages.2.blocks.0.attn.t_scale's lr=0.0010596599999999998, weight_decay=0\n",
      "135: stages.1.downsample.norm.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "136: stages.1.downsample.norm.weight's lr=0.0010596599999999998, weight_decay=0\n",
      "137: stages.1.downsample.reduction.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "138: stages.1.blocks.1.norm2.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "139: stages.1.blocks.1.norm2.weight's lr=0.0010596599999999998, weight_decay=0\n",
      "140: stages.1.blocks.1.mlp.fc2.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "141: stages.1.blocks.1.mlp.fc2.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "142: stages.1.blocks.1.mlp.fc1.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "143: stages.1.blocks.1.mlp.fc1.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "144: stages.1.blocks.1.norm1.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "145: stages.1.blocks.1.norm1.weight's lr=0.0010596599999999998, weight_decay=0\n",
      "146: stages.1.blocks.1.attn.proj.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "147: stages.1.blocks.1.attn.proj.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "148: stages.1.blocks.1.attn.qkv.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "149: stages.1.blocks.1.attn.qkv.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "150: stages.1.blocks.1.attn.crpb_mlp.3.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "151: stages.1.blocks.1.attn.crpb_mlp.0.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "152: stages.1.blocks.1.attn.crpb_mlp.0.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "153: stages.1.blocks.1.attn.t_scale's lr=0.0010596599999999998, weight_decay=0\n",
      "154: stages.1.blocks.0.norm2.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "155: stages.1.blocks.0.norm2.weight's lr=0.0010596599999999998, weight_decay=0\n",
      "156: stages.1.blocks.0.mlp.fc2.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "157: stages.1.blocks.0.mlp.fc2.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "158: stages.1.blocks.0.mlp.fc1.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "159: stages.1.blocks.0.mlp.fc1.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "160: stages.1.blocks.0.norm1.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "161: stages.1.blocks.0.norm1.weight's lr=0.0010596599999999998, weight_decay=0\n",
      "162: stages.1.blocks.0.attn.proj.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "163: stages.1.blocks.0.attn.proj.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "164: stages.1.blocks.0.attn.qkv.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "165: stages.1.blocks.0.attn.qkv.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "166: stages.1.blocks.0.attn.crpb_mlp.3.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "167: stages.1.blocks.0.attn.crpb_mlp.0.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "168: stages.1.blocks.0.attn.crpb_mlp.0.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "169: stages.1.blocks.0.attn.t_scale's lr=0.0010596599999999998, weight_decay=0\n",
      "170: stages.0.downsample.norm.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "171: stages.0.downsample.norm.weight's lr=0.0010596599999999998, weight_decay=0\n",
      "172: stages.0.downsample.reduction.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "173: stages.0.blocks.1.norm2.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "174: stages.0.blocks.1.norm2.weight's lr=0.0010596599999999998, weight_decay=0\n",
      "175: stages.0.blocks.1.mlp.fc2.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "176: stages.0.blocks.1.mlp.fc2.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "177: stages.0.blocks.1.mlp.fc1.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "178: stages.0.blocks.1.mlp.fc1.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "179: stages.0.blocks.1.norm1.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "180: stages.0.blocks.1.norm1.weight's lr=0.0010596599999999998, weight_decay=0\n",
      "181: stages.0.blocks.1.attn.proj.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "182: stages.0.blocks.1.attn.proj.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "183: stages.0.blocks.1.attn.qkv.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "184: stages.0.blocks.1.attn.qkv.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "185: stages.0.blocks.1.attn.crpb_mlp.3.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "186: stages.0.blocks.1.attn.crpb_mlp.0.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "187: stages.0.blocks.1.attn.crpb_mlp.0.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "188: stages.0.blocks.1.attn.t_scale's lr=0.0010596599999999998, weight_decay=0\n",
      "189: stages.0.blocks.0.norm2.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "190: stages.0.blocks.0.norm2.weight's lr=0.0010596599999999998, weight_decay=0\n",
      "191: stages.0.blocks.0.mlp.fc2.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "192: stages.0.blocks.0.mlp.fc2.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "193: stages.0.blocks.0.mlp.fc1.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "194: stages.0.blocks.0.mlp.fc1.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "195: stages.0.blocks.0.norm1.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "196: stages.0.blocks.0.norm1.weight's lr=0.0010596599999999998, weight_decay=0\n",
      "197: stages.0.blocks.0.attn.proj.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "198: stages.0.blocks.0.attn.proj.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "199: stages.0.blocks.0.attn.qkv.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "200: stages.0.blocks.0.attn.qkv.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "201: stages.0.blocks.0.attn.crpb_mlp.3.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "202: stages.0.blocks.0.attn.crpb_mlp.0.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "203: stages.0.blocks.0.attn.crpb_mlp.0.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "204: stages.0.blocks.0.attn.t_scale's lr=0.0010596599999999998, weight_decay=0\n",
      "205: embeddings.norm.bias's lr=0.0009219041999999998, weight_decay=0\n",
      "206: embeddings.norm.weight's lr=0.0009219041999999998, weight_decay=0\n",
      "207: embeddings.patch_embeddings.bias's lr=0.0009219041999999998, weight_decay=0\n",
      "208: embeddings.patch_embeddings.weight's lr=0.0009219041999999998, weight_decay=0.01\n",
      "209: absolute_pos_embed's lr=0.0008020566539999999, weight_decay=0\n"
     ]
    }
   ],
   "source": [
    "lr      = 1.4e-3      # paper : 1.4e-3\n",
    "lr_mult = 0.87  # paper : 0.87\n",
    "weight_decay = 0.01 # paper : 0.1\n",
    "\n",
    "param_groups = []\n",
    "prev_group_name = layer_names[0].split('.')[0]\n",
    "\n",
    "for idx, name in enumerate(layer_names):\n",
    "    \n",
    "    cur_group_name = name.split('.')[0]\n",
    "    \n",
    "    if cur_group_name != prev_group_name:\n",
    "        lr *= lr_mult\n",
    "    prev_group_name = cur_group_name\n",
    "    weight_decay = 0.01 if ('weight' in name) and ('norm' not in name) else 0\n",
    "    \n",
    "    print(f\"{idx}: {name}'s lr={lr}, weight_decay={weight_decay}\")\n",
    "    \n",
    "    param_groups += [{'params': [ p for n, p in model.named_parameters() if n == name and p.requires_grad],\n",
    "                      'lr' : lr,\n",
    "                      'weight_decay': weight_decay}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 모델 레이어의 이름 추출\n",
    "# layer_names = []\n",
    "# for i, (name, params) in enumerate(model.named_parameters()):\n",
    "#     lr = base_lr\n",
    "#     print(f'{i}: {name}')\n",
    "#     layer_names.append(name)\n",
    "\n",
    "# # 뒷 레이어부터 시작하도록 뒤집기    \n",
    "# layer_names.reverse()\n",
    "\n",
    "# # 하이퍼 파라미터 정의\n",
    "# lr      = 1.4e-3      # paper : 1.4e-3\n",
    "# lr_mult = 0.87  # paper : 0.87\n",
    "# weight_decay = 0.01 # paper : 0.1\n",
    "\n",
    "# param_groups = []\n",
    "# prev_group_name = layer_names[0].split('.')[0] # 그룹명 초기화\n",
    "\n",
    "# for idx, name in enumerate(layer_names):    \n",
    "#     cur_group_name = name.split('.')[0]    \n",
    "#     if cur_group_name != prev_group_name: # 동일한 그룹에 속하면 동일한 학습율\n",
    "#         lr *= lr_mult\n",
    "#     prev_group_name = cur_group_name    \n",
    "    \n",
    "#     param_groups += [{'params': [ p for n, p in model.named_parameters() if n == name and p.requires_grad],\n",
    "#                       'lr' : lr,\n",
    "#                       'weight_decay': weight_decay}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(param_groups)\n",
    "warmup_steps = int(len(train_loader)*(epochs)*0.1)\n",
    "train_steps = len(train_loader)*(epochs)\n",
    "scheduler = transformers.get_cosine_schedule_with_warmup(optimizer, \n",
    "                                                        num_warmup_steps=warmup_steps, \n",
    "                                                        num_training_steps=train_steps,\n",
    "                                                        num_cycles=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Train\n",
    "- 100에포크 먼저 학습하며 결과 확인하고, 이후 10에포크 학습하며 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 15/15 [00:58<00:00,  3.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.626991844177246, Val Loss: 4.506292343139648, LR: 9.333333333333333e-05, Duration: 59.55 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 15/15 [01:00<00:00,  4.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.460748704274495, Val Loss: 4.2201457023620605, LR: 0.00018666666666666666, Duration: 62.18 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 15/15 [00:52<00:00,  3.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.340983549753825, Val Loss: 4.004014015197754, LR: 0.00028000000000000003, Duration: 53.71 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 15/15 [00:52<00:00,  3.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.222826862335205, Val Loss: 3.8812460899353027, LR: 0.0003733333333333333, Duration: 53.87 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 15/15 [01:01<00:00,  4.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.161698865890503, Val Loss: 3.8016414642333984, LR: 0.00046666666666666666, Duration: 63.24 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 15/15 [01:00<00:00,  4.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.084051465988159, Val Loss: 3.6472675800323486, LR: 0.0005600000000000001, Duration: 62.18 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 15/15 [01:01<00:00,  4.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.062760845820109, Val Loss: 3.4813220500946045, LR: 0.0006533333333333333, Duration: 62.23 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 15/15 [01:00<00:00,  4.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.024969832102458, Val Loss: 3.3530468940734863, LR: 0.0007466666666666666, Duration: 61.69 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 15/15 [00:58<00:00,  3.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.831586503982544, Val Loss: 3.214343547821045, LR: 0.0008399999999999999, Duration: 60.13 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 15/15 [01:04<00:00,  4.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.815111176172892, Val Loss: 3.2054519653320312, LR: 0.0009333333333333333, Duration: 66.16 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 15/15 [00:58<00:00,  3.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.8399160067240397, Val Loss: 3.1536366939544678, LR: 0.0010266666666666666, Duration: 59.63 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 15/15 [00:58<00:00,  3.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.725848372777303, Val Loss: 3.017932176589966, LR: 0.0011200000000000001, Duration: 60.04 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 15/15 [01:02<00:00,  4.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.7307978947957356, Val Loss: 2.867175340652466, LR: 0.0012133333333333334, Duration: 63.92 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 15/15 [00:57<00:00,  3.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.8719071706136066, Val Loss: 2.814711332321167, LR: 0.0013066666666666667, Duration: 59.20 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 15/15 [01:03<00:00,  4.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.724492899576823, Val Loss: 2.7849462032318115, LR: 0.0014, Duration: 64.66 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 15/15 [01:02<00:00,  4.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.608774677912394, Val Loss: 2.673262596130371, LR: 0.001399810468825623, Duration: 64.79 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 15/15 [01:02<00:00,  4.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.6665937105814614, Val Loss: 2.7115185260772705, LR: 0.0013992419779369672, Duration: 63.71 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 15/15 [01:03<00:00,  4.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.524884796142578, Val Loss: 2.4193263053894043, LR: 0.001398294835181877, Duration: 64.85 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 15/15 [01:02<00:00,  4.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.4447612285614015, Val Loss: 2.377565860748291, LR: 0.001396969553454863, Duration: 64.13 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 15/15 [01:03<00:00,  4.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.2468255678812663, Val Loss: 2.2881226539611816, LR: 0.0013952668504193602, Duration: 64.48 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 15/15 [00:59<00:00,  3.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.4544591108957925, Val Loss: 2.360004425048828, LR: 0.0013931876481190993, Duration: 61.16 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 15/15 [00:59<00:00,  3.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.494812234242757, Val Loss: 2.245044231414795, LR: 0.0013907330724788056, Duration: 61.15 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 15/15 [01:00<00:00,  4.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.2503856499989827, Val Loss: 2.3022923469543457, LR: 0.0013879044526944892, Duration: 61.61 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 15/15 [00:54<00:00,  3.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.3364601771036786, Val Loss: 2.1666336059570312, LR: 0.001384703320513664, Duration: 55.75 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 15/15 [00:56<00:00,  3.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.233853832880656, Val Loss: 2.0786757469177246, LR: 0.0013811314094058767, Duration: 58.14 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 15/15 [01:02<00:00,  4.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.43223287264506, Val Loss: 2.0300819873809814, LR: 0.0013771906536240047, Duration: 64.10 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 15/15 [01:00<00:00,  4.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.29380095799764, Val Loss: 2.0296013355255127, LR: 0.0013728831871568231, Duration: 62.17 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 15/15 [00:58<00:00,  3.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.1586191018422443, Val Loss: 1.8927744626998901, LR: 0.0013682113425734124, Duration: 59.76 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 15/15 [00:59<00:00,  3.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.1102041562398273, Val Loss: 1.8319021463394165, LR: 0.0013631776497600304, Duration: 60.25 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 15/15 [00:57<00:00,  3.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.1117082913716634, Val Loss: 1.7852400541305542, LR: 0.001357784834550136, Duration: 58.36 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|██████████| 15/15 [00:57<00:00,  3.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.0635855833689374, Val Loss: 1.693884015083313, LR: 0.0013520358172482998, Duration: 58.68 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|██████████| 15/15 [00:59<00:00,  3.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.1664939085642496, Val Loss: 1.762235403060913, LR: 0.0013459337110488096, Duration: 61.02 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|██████████| 15/15 [00:57<00:00,  3.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.368815231323242, Val Loss: 1.71271550655365, LR: 0.0013394818203498204, Duration: 58.63 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|██████████| 15/15 [00:59<00:00,  3.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.126810598373413, Val Loss: 1.6290462017059326, LR: 0.0013326836389639645, Duration: 60.48 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|██████████| 15/15 [01:03<00:00,  4.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.947221803665161, Val Loss: 1.6011756658554077, LR: 0.0013255428482263885, Duration: 64.78 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|██████████| 15/15 [00:59<00:00,  3.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.201627318064372, Val Loss: 1.6126904487609863, LR: 0.0013180633150012488, Duration: 61.40 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37: 100%|██████████| 15/15 [00:59<00:00,  4.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.1338157176971437, Val Loss: 1.625458002090454, LR: 0.0013102490895877336, Duration: 60.92 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|██████████| 15/15 [01:01<00:00,  4.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.950371201833089, Val Loss: 1.5287359952926636, LR: 0.001302104403526756, Duration: 62.59 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|██████████| 15/15 [01:02<00:00,  4.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.7992828845977784, Val Loss: 1.4854750633239746, LR: 0.001293633667309498, Duration: 63.30 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40: 100%|██████████| 15/15 [00:58<00:00,  3.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.7886632919311523, Val Loss: 1.4712761640548706, LR: 0.0012848414679890556, Duration: 59.94 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41: 100%|██████████| 15/15 [00:58<00:00,  3.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.024151007334391, Val Loss: 1.4806206226348877, LR: 0.0012757325666964635, Duration: 59.79 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42: 100%|██████████| 15/15 [00:56<00:00,  3.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.7476744492848715, Val Loss: 1.4383769035339355, LR: 0.0012663118960624632, Duration: 57.53 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43: 100%|██████████| 15/15 [01:00<00:00,  4.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.203972053527832, Val Loss: 1.5764886140823364, LR: 0.0012565845575463934, Duration: 61.67 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44: 100%|██████████| 15/15 [01:01<00:00,  4.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6780605634053547, Val Loss: 1.3299708366394043, LR: 0.0012465558186736615, Duration: 63.58 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45: 100%|██████████| 15/15 [01:03<00:00,  4.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.7449944098790486, Val Loss: 1.327117919921875, LR: 0.0012362311101832846, Duration: 65.14 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46: 100%|██████████| 15/15 [01:00<00:00,  4.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.141310199101766, Val Loss: 1.4603554010391235, LR: 0.0012256160230870495, Duration: 61.64 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47: 100%|██████████| 15/15 [01:00<00:00,  4.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.9438440640767416, Val Loss: 1.4267041683197021, LR: 0.00121471630564188, Duration: 61.33 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48: 100%|██████████| 15/15 [01:00<00:00,  4.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.9078289826711017, Val Loss: 1.4213134050369263, LR: 0.0012035378602370558, Duration: 61.24 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 15/15 [01:01<00:00,  4.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.999505074818929, Val Loss: 1.4604978561401367, LR: 0.0011920867401979632, Duration: 62.20 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50: 100%|██████████| 15/15 [00:59<00:00,  3.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.783182080586751, Val Loss: 1.2845427989959717, LR: 0.0011803691465081135, Duration: 60.91 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51: 100%|██████████| 15/15 [00:59<00:00,  3.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.024645932515462, Val Loss: 1.3165875673294067, LR: 0.0011683914244512007, Duration: 60.40 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52: 100%|██████████| 15/15 [01:06<00:00,  4.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5351617336273193, Val Loss: 1.229169487953186, LR: 0.0011561600601750187, Duration: 67.41 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53: 100%|██████████| 15/15 [01:06<00:00,  4.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.484326791763306, Val Loss: 1.1407653093338013, LR: 0.001143681677179097, Duration: 67.32 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54: 100%|██████████| 15/15 [00:56<00:00,  3.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.9471678098042804, Val Loss: 1.2126445770263672, LR: 0.0011309630327279608, Duration: 58.29 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55: 100%|██████████| 15/15 [01:00<00:00,  4.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.9026586691538494, Val Loss: 1.2953814268112183, LR: 0.0011180110141919503, Duration: 61.76 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56: 100%|██████████| 15/15 [00:56<00:00,  3.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.8830923159917194, Val Loss: 1.2230854034423828, LR: 0.0011048326353175905, Duration: 57.68 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57: 100%|██████████| 15/15 [00:59<00:00,  3.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5572444279988606, Val Loss: 1.1186017990112305, LR: 0.0010914350324295228, Duration: 60.78 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58: 100%|██████████| 15/15 [00:57<00:00,  3.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4161037683486937, Val Loss: 1.1070709228515625, LR: 0.0010778254605660592, Duration: 60.95 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59: 100%|██████████| 15/15 [01:02<00:00,  4.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.070221034685771, Val Loss: 1.1393178701400757, LR: 0.0010640112895504506, Duration: 63.95 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60: 100%|██████████| 15/15 [01:04<00:00,  4.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.817952561378479, Val Loss: 1.1211862564086914, LR: 0.00105, Duration: 65.35 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61: 100%|██████████| 15/15 [01:03<00:00,  4.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.7799264351526896, Val Loss: 1.0508002042770386, LR: 0.0010357991792751724, Duration: 64.47 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62: 100%|██████████| 15/15 [01:01<00:00,  4.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5487555901209515, Val Loss: 1.0566086769104004, LR: 0.001021416517370908, Duration: 62.77 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63: 100%|██████████| 15/15 [01:04<00:00,  4.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.059247899055481, Val Loss: 0.9606173038482666, LR: 0.001006859802752354, Duration: 65.80 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64: 100%|██████████| 15/15 [00:57<00:00,  3.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4399437506993613, Val Loss: 1.143599033355713, LR: 0.0009921369181372726, Duration: 57.97 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65: 100%|██████████| 15/15 [00:55<00:00,  3.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.580682571729024, Val Loss: 1.0343647003173828, LR: 0.0009772558362274098, Duration: 56.75 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66: 100%|██████████| 15/15 [01:03<00:00,  4.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5591801246007284, Val Loss: 0.950772762298584, LR: 0.0009622246153911386, Duration: 64.81 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67: 100%|██████████| 15/15 [00:59<00:00,  3.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.620518128077189, Val Loss: 0.9949902296066284, LR: 0.0009470513952997081, Duration: 60.84 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68: 100%|██████████| 15/15 [01:06<00:00,  4.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5110288143157957, Val Loss: 1.0437566041946411, LR: 0.0009317443925194707, Duration: 67.10 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69: 100%|██████████| 15/15 [01:05<00:00,  4.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5541162331899008, Val Loss: 0.9843952059745789, LR: 0.0009163118960624632, Duration: 66.84 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70: 100%|██████████| 15/15 [00:57<00:00,  3.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6537261883417766, Val Loss: 1.0147548913955688, LR: 0.0009007622628977632, Duration: 58.40 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71: 100%|██████████| 15/15 [01:02<00:00,  4.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.617645263671875, Val Loss: 0.990867555141449, LR: 0.0008851039134260417, Duration: 63.20 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72: 100%|██████████| 15/15 [01:01<00:00,  4.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.430638233820597, Val Loss: 0.9600546956062317, LR: 0.0008693453269197673, Duration: 62.62 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 73: 100%|██████████| 15/15 [00:57<00:00,  3.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5182886123657227, Val Loss: 0.968648374080658, LR: 0.0008534950369315323, Duration: 59.01 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 74: 100%|██████████| 15/15 [00:57<00:00,  3.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3815385421117146, Val Loss: 0.8900267481803894, LR: 0.0008375616266729811, Duration: 58.68 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75: 100%|██████████| 15/15 [00:59<00:00,  3.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1455774704615274, Val Loss: 0.8932757377624512, LR: 0.0008215537243668514, Duration: 60.39 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 76: 100%|██████████| 15/15 [01:01<00:00,  4.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3128055810928343, Val Loss: 0.8197294473648071, LR: 0.0008054799985746381, Duration: 62.43 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 77: 100%|██████████| 15/15 [01:00<00:00,  4.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.588639012972514, Val Loss: 0.9120055437088013, LR: 0.0007893491535024164, Duration: 61.80 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 78: 100%|██████████| 15/15 [01:02<00:00,  4.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.025572498639425, Val Loss: 0.8358860611915588, LR: 0.0007731699242873575, Duration: 63.14 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 79: 100%|██████████| 15/15 [01:01<00:00,  4.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.215488338470459, Val Loss: 0.8271394968032837, LR: 0.0007569510722675008, Duration: 62.45 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 80: 100%|██████████| 15/15 [00:58<00:00,  3.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.8983399311701457, Val Loss: 0.7774518728256226, LR: 0.000740701380237333, Duration: 59.39 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 81: 100%|██████████| 15/15 [00:58<00:00,  3.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1339569807052614, Val Loss: 0.7692208290100098, LR: 0.0007244296476917508, Duration: 59.44 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 82: 100%|██████████| 15/15 [01:08<00:00,  4.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.211329468091329, Val Loss: 0.8157359957695007, LR: 0.0007081446860609781, Duration: 69.62 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 83: 100%|██████████| 15/15 [01:02<00:00,  4.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3808172941207886, Val Loss: 0.8505873084068298, LR: 0.0006918553139390222, Duration: 64.92 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 84: 100%|██████████| 15/15 [01:03<00:00,  4.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.097442150115967, Val Loss: 0.7704806327819824, LR: 0.0006755703523082495, Duration: 64.65 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 85: 100%|██████████| 15/15 [01:05<00:00,  4.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5336355129877726, Val Loss: 0.798114001750946, LR: 0.000659298619762667, Duration: 66.59 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 86: 100%|██████████| 15/15 [00:59<00:00,  3.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.468687383333842, Val Loss: 0.8326034545898438, LR: 0.0006430489277324992, Duration: 60.48 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 87: 100%|██████████| 15/15 [00:59<00:00,  3.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0093989928563434, Val Loss: 0.7356250882148743, LR: 0.0006268300757126426, Duration: 60.96 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 88: 100%|██████████| 15/15 [00:58<00:00,  3.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.146946597099304, Val Loss: 0.7460469007492065, LR: 0.0006106508464975837, Duration: 59.55 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 89: 100%|██████████| 15/15 [01:00<00:00,  4.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3722941716512045, Val Loss: 0.8162336945533752, LR: 0.0005945200014253619, Duration: 61.88 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 90: 100%|██████████| 15/15 [01:01<00:00,  4.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0552615404129027, Val Loss: 0.741635262966156, LR: 0.0005784462756331488, Duration: 62.56 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 91: 100%|██████████| 15/15 [00:59<00:00,  3.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1116454521814982, Val Loss: 0.8096517324447632, LR: 0.0005624383733270188, Duration: 60.32 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 92: 100%|██████████| 15/15 [00:56<00:00,  3.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2188883145650227, Val Loss: 0.8021658658981323, LR: 0.0005465049630684676, Duration: 58.25 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 93: 100%|██████████| 15/15 [00:57<00:00,  3.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2783074378967285, Val Loss: 0.7974361777305603, LR: 0.0005306546730802327, Duration: 58.65 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 94: 100%|██████████| 15/15 [00:57<00:00,  3.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.255167826016744, Val Loss: 0.7705899477005005, LR: 0.0005148960865739587, Duration: 58.52 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 95: 100%|██████████| 15/15 [00:59<00:00,  3.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.312365436553955, Val Loss: 0.7779889702796936, LR: 0.000499237737102237, Duration: 60.59 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 96: 100%|██████████| 15/15 [01:00<00:00,  4.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0388551632563274, Val Loss: 0.75716632604599, LR: 0.0004836881039375369, Duration: 61.02 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 97: 100%|██████████| 15/15 [00:59<00:00,  3.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.162968667348226, Val Loss: 0.7820665240287781, LR: 0.0004682556074805294, Duration: 60.59 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 98: 100%|██████████| 15/15 [01:00<00:00,  4.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.307865786552429, Val Loss: 0.7429741024971008, LR: 0.00045294860470029185, Duration: 61.19 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 15/15 [00:58<00:00,  3.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.9684695959091187, Val Loss: 0.7502778172492981, LR: 0.0004377753846088615, Duration: 59.86 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 100: 100%|██████████| 15/15 [00:58<00:00,  3.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2147297620773316, Val Loss: 0.790279746055603, LR: 0.0004227441637725902, Duration: 59.84 sec\n",
      "Epoch 당 평균 소요시간 : 41.04초\n"
     ]
    }
   ],
   "source": [
    "training_time = 0\n",
    "losses = []\n",
    "val_losses = []\n",
    "lrs = []\n",
    "best_loss = float('inf')\n",
    "\n",
    "# GradScaler 초기화\n",
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch + 1}\")\n",
    "    \n",
    "    for _, data in pbar:\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        inputs, labels = mixup_fn(inputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # AutoCast 적용\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "        # 스케일링된 그라디언트 계산\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # 그라디언트 클리핑 전에 스케일링 제거\n",
    "        scaler.unscale_(optimizer)\n",
    "        clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "\n",
    "        # 옵티마이저 스텝 및 스케일러 업데이트\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "            \n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        lrs.append(lr)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)        \n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in valid_loader:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    val_loss /= len(valid_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # 모델 저장\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        vit_save = True\n",
    "        if vit_save:\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_duration = time.time() - start_time\n",
    "    training_time += epoch_duration\n",
    "    \n",
    "    text = f'\\tLoss: {epoch_loss}, Val Loss: {val_loss}, LR: {lr}, Duration: {epoch_duration:.2f} sec'\n",
    "    \n",
    "    if vit_save:\n",
    "        text += f' - model saved!'\n",
    "        vit_save = False\n",
    "\n",
    "    print(text)\n",
    "        \n",
    "text = f\"Epoch 당 평균 소요시간 : {training_time / epochs:.2f}초\"      \n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.836000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.856131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Recall</td>\n",
       "      <td>0.836000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F1 Score</td>\n",
       "      <td>0.826792</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Metric     Value\n",
       "0   Accuracy  0.836000\n",
       "1  Precision  0.856131\n",
       "2     Recall  0.836000\n",
       "3   F1 Score  0.826792"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예측 수행 및 레이블 저장\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# 혼동 행렬 생성\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# 예측과 실제 레이블\n",
    "y_true = all_labels  # 실제 레이블\n",
    "y_pred = all_preds  # 모델에 의해 예측된 레이블\n",
    "\n",
    "# 전체 데이터셋에 대한 정확도\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# 평균 정밀도, 리콜, F1-Score ('weighted')\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "\n",
    "# 판다스 데이터프레임으로 결과 정리\n",
    "performance_metrics = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "    'Value': [accuracy, precision, recall, f1_score]\n",
    "})\n",
    "\n",
    "# 데이터프레임 출력\n",
    "performance_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 110 Epoch Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 15/15 [00:58<00:00,  3.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.8434817949930826, Val Loss: 0.714182436466217, LR: 0.0004078630818627275, Duration: 60.17 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 15/15 [00:58<00:00,  3.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0573649326960246, Val Loss: 0.7710936665534973, LR: 0.00039314019724764573, Duration: 60.87 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 15/15 [01:02<00:00,  4.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2468320687611896, Val Loss: 0.770478367805481, LR: 0.0003785834826290917, Duration: 63.45 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 15/15 [00:58<00:00,  3.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.833013939857483, Val Loss: 0.7523660063743591, LR: 0.00036420082072482785, Duration: 58.97 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 15/15 [00:51<00:00,  3.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.9475464502970377, Val Loss: 0.7664220929145813, LR: 0.00035000000000000016, Duration: 51.98 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6:  87%|████████▋ | 13/15 [00:46<00:07,  3.57s/it]"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch + 1}\")\n",
    "    \n",
    "    for _, data in pbar:\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        inputs, labels = mixup_fn(inputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # AutoCast 적용\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "        # 스케일링된 그라디언트 계산\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # 그라디언트 클리핑 전에 스케일링 제거\n",
    "        scaler.unscale_(optimizer)\n",
    "        clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "\n",
    "        # 옵티마이저 스텝 및 스케일러 업데이트\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "            \n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        lrs.append(lr)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)        \n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in valid_loader:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    val_loss /= len(valid_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # 모델 저장\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        vit_save = True\n",
    "        if vit_save:\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_duration = time.time() - start_time\n",
    "    training_time += epoch_duration\n",
    "    \n",
    "    text = f'\\tLoss: {epoch_loss}, Val Loss: {val_loss}, LR: {lr}, Duration: {epoch_duration:.2f} sec'\n",
    "    \n",
    "    if vit_save:\n",
    "        text += f' - model saved!'\n",
    "        vit_save = False\n",
    "\n",
    "    print(text)\n",
    "        \n",
    "text = f\"Epoch 당 평균 소요시간 : {training_time / epochs:.2f}초\"      \n",
    "print(text)\n",
    "\n",
    "# 예측 수행 및 레이블 저장\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# 혼동 행렬 생성\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# 예측과 실제 레이블\n",
    "y_true = all_labels  # 실제 레이블\n",
    "y_pred = all_preds  # 모델에 의해 예측된 레이블\n",
    "\n",
    "# 전체 데이터셋에 대한 정확도\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# 평균 정밀도, 리콜, F1-Score ('weighted')\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "\n",
    "# 판다스 데이터프레임으로 결과 정리\n",
    "performance_metrics = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "    'Value': [accuracy, precision, recall, f1_score]\n",
    "})\n",
    "\n",
    "# 데이터프레임 출력\n",
    "performance_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 120 Epoch Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch + 1}\")\n",
    "    \n",
    "    for _, data in pbar:\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        inputs, labels = mixup_fn(inputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # AutoCast 적용\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "        # 스케일링된 그라디언트 계산\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # 그라디언트 클리핑 전에 스케일링 제거\n",
    "        scaler.unscale_(optimizer)\n",
    "        clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "\n",
    "        # 옵티마이저 스텝 및 스케일러 업데이트\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "            \n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        lrs.append(lr)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)        \n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in valid_loader:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    val_loss /= len(valid_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # 모델 저장\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        vit_save = True\n",
    "        if vit_save:\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_duration = time.time() - start_time\n",
    "    training_time += epoch_duration\n",
    "    \n",
    "    text = f'\\tLoss: {epoch_loss}, Val Loss: {val_loss}, LR: {lr}, Duration: {epoch_duration:.2f} sec'\n",
    "    \n",
    "    if vit_save:\n",
    "        text += f' - model saved!'\n",
    "        vit_save = False\n",
    "\n",
    "    print(text)\n",
    "        \n",
    "text = f\"Epoch 당 평균 소요시간 : {training_time / epochs:.2f}초\"      \n",
    "print(text)\n",
    "\n",
    "# 예측 수행 및 레이블 저장\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# 혼동 행렬 생성\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# 예측과 실제 레이블\n",
    "y_true = all_labels  # 실제 레이블\n",
    "y_pred = all_preds  # 모델에 의해 예측된 레이블\n",
    "\n",
    "# 전체 데이터셋에 대한 정확도\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# 평균 정밀도, 리콜, F1-Score ('weighted')\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "\n",
    "# 판다스 데이터프레임으로 결과 정리\n",
    "performance_metrics = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "    'Value': [accuracy, precision, recall, f1_score]\n",
    "})\n",
    "\n",
    "# 데이터프레임 출력\n",
    "performance_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 130 Epoch Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch + 1}\")\n",
    "    \n",
    "    for _, data in pbar:\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        inputs, labels = mixup_fn(inputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # AutoCast 적용\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "        # 스케일링된 그라디언트 계산\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # 그라디언트 클리핑 전에 스케일링 제거\n",
    "        scaler.unscale_(optimizer)\n",
    "        clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "\n",
    "        # 옵티마이저 스텝 및 스케일러 업데이트\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "            \n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        lrs.append(lr)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)        \n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in valid_loader:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    val_loss /= len(valid_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # 모델 저장\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        vit_save = True\n",
    "        if vit_save:\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_duration = time.time() - start_time\n",
    "    training_time += epoch_duration\n",
    "    \n",
    "    text = f'\\tLoss: {epoch_loss}, Val Loss: {val_loss}, LR: {lr}, Duration: {epoch_duration:.2f} sec'\n",
    "    \n",
    "    if vit_save:\n",
    "        text += f' - model saved!'\n",
    "        vit_save = False\n",
    "\n",
    "    print(text)\n",
    "        \n",
    "text = f\"Epoch 당 평균 소요시간 : {training_time / epochs:.2f}초\"      \n",
    "print(text)\n",
    "\n",
    "# 예측 수행 및 레이블 저장\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# 혼동 행렬 생성\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# 예측과 실제 레이블\n",
    "y_true = all_labels  # 실제 레이블\n",
    "y_pred = all_preds  # 모델에 의해 예측된 레이블\n",
    "\n",
    "# 전체 데이터셋에 대한 정확도\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# 평균 정밀도, 리콜, F1-Score ('weighted')\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "\n",
    "# 판다스 데이터프레임으로 결과 정리\n",
    "performance_metrics = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "    'Value': [accuracy, precision, recall, f1_score]\n",
    "})\n",
    "\n",
    "# 데이터프레임 출력\n",
    "performance_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 140 Epoch Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch + 1}\")\n",
    "    \n",
    "    for _, data in pbar:\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        inputs, labels = mixup_fn(inputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # AutoCast 적용\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "        # 스케일링된 그라디언트 계산\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # 그라디언트 클리핑 전에 스케일링 제거\n",
    "        scaler.unscale_(optimizer)\n",
    "        clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "\n",
    "        # 옵티마이저 스텝 및 스케일러 업데이트\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "            \n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        lrs.append(lr)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)        \n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in valid_loader:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    val_loss /= len(valid_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # 모델 저장\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        vit_save = True\n",
    "        if vit_save:\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_duration = time.time() - start_time\n",
    "    training_time += epoch_duration\n",
    "    \n",
    "    text = f'\\tLoss: {epoch_loss}, Val Loss: {val_loss}, LR: {lr}, Duration: {epoch_duration:.2f} sec'\n",
    "    \n",
    "    if vit_save:\n",
    "        text += f' - model saved!'\n",
    "        vit_save = False\n",
    "\n",
    "    print(text)\n",
    "        \n",
    "text = f\"Epoch 당 평균 소요시간 : {training_time / epochs:.2f}초\"      \n",
    "print(text)\n",
    "\n",
    "# 예측 수행 및 레이블 저장\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# 혼동 행렬 생성\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# 예측과 실제 레이블\n",
    "y_true = all_labels  # 실제 레이블\n",
    "y_pred = all_preds  # 모델에 의해 예측된 레이블\n",
    "\n",
    "# 전체 데이터셋에 대한 정확도\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# 평균 정밀도, 리콜, F1-Score ('weighted')\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "\n",
    "# 판다스 데이터프레임으로 결과 정리\n",
    "performance_metrics = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "    'Value': [accuracy, precision, recall, f1_score]\n",
    "})\n",
    "\n",
    "# 데이터프레임 출력\n",
    "performance_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 150 Epoch Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch + 1}\")\n",
    "    \n",
    "    for _, data in pbar:\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        inputs, labels = mixup_fn(inputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # AutoCast 적용\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "        # 스케일링된 그라디언트 계산\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # 그라디언트 클리핑 전에 스케일링 제거\n",
    "        scaler.unscale_(optimizer)\n",
    "        clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "\n",
    "        # 옵티마이저 스텝 및 스케일러 업데이트\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "            \n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        lrs.append(lr)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)        \n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in valid_loader:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    val_loss /= len(valid_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # 모델 저장\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        vit_save = True\n",
    "        if vit_save:\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_duration = time.time() - start_time\n",
    "    training_time += epoch_duration\n",
    "    \n",
    "    text = f'\\tLoss: {epoch_loss}, Val Loss: {val_loss}, LR: {lr}, Duration: {epoch_duration:.2f} sec'\n",
    "    \n",
    "    if vit_save:\n",
    "        text += f' - model saved!'\n",
    "        vit_save = False\n",
    "\n",
    "    print(text)\n",
    "        \n",
    "text = f\"Epoch 당 평균 소요시간 : {training_time / epochs:.2f}초\"      \n",
    "print(text)\n",
    "\n",
    "# 예측 수행 및 레이블 저장\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# 혼동 행렬 생성\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# 예측과 실제 레이블\n",
    "y_true = all_labels  # 실제 레이블\n",
    "y_pred = all_preds  # 모델에 의해 예측된 레이블\n",
    "\n",
    "# 전체 데이터셋에 대한 정확도\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# 평균 정밀도, 리콜, F1-Score ('weighted')\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "\n",
    "# 판다스 데이터프레임으로 결과 정리\n",
    "performance_metrics = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "    'Value': [accuracy, precision, recall, f1_score]\n",
    "})\n",
    "\n",
    "# 데이터프레임 출력\n",
    "performance_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
