{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "- Stage-2 학습 시 Paper에서 제시한 Parameter를 그대로 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from box import Box\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from swin_v2 import SwinTransformerV2\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "from timm.data import Mixup\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:4'\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "model_save = False\n",
    "simmim_path = '../../models/swin2/simmim.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['absolute_pos_embed', 'embeddings.patch_embeddings.weight', 'embeddings.patch_embeddings.bias', 'embeddings.norm.weight', 'embeddings.norm.bias', 'stages.0.blocks.0.attn_mask', 'stages.0.blocks.0.attn.t_scale', 'stages.0.blocks.0.attn.relative_coords_table', 'stages.0.blocks.0.attn.relative_position_index', 'stages.0.blocks.0.attn.crpb_mlp.0.weight', 'stages.0.blocks.0.attn.crpb_mlp.0.bias', 'stages.0.blocks.0.attn.crpb_mlp.3.weight', 'stages.0.blocks.0.attn.qkv.weight', 'stages.0.blocks.0.attn.qkv.bias', 'stages.0.blocks.0.attn.proj.weight', 'stages.0.blocks.0.attn.proj.bias', 'stages.0.blocks.0.norm1.weight', 'stages.0.blocks.0.norm1.bias', 'stages.0.blocks.0.mlp.fc1.weight', 'stages.0.blocks.0.mlp.fc1.bias', 'stages.0.blocks.0.mlp.fc2.weight', 'stages.0.blocks.0.mlp.fc2.bias', 'stages.0.blocks.0.norm2.weight', 'stages.0.blocks.0.norm2.bias', 'stages.0.blocks.1.attn_mask', 'stages.0.blocks.1.attn.t_scale', 'stages.0.blocks.1.attn.relative_coords_table', 'stages.0.blocks.1.attn.relative_position_index', 'stages.0.blocks.1.attn.crpb_mlp.0.weight', 'stages.0.blocks.1.attn.crpb_mlp.0.bias', 'stages.0.blocks.1.attn.crpb_mlp.3.weight', 'stages.0.blocks.1.attn.qkv.weight', 'stages.0.blocks.1.attn.qkv.bias', 'stages.0.blocks.1.attn.proj.weight', 'stages.0.blocks.1.attn.proj.bias', 'stages.0.blocks.1.norm1.weight', 'stages.0.blocks.1.norm1.bias', 'stages.0.blocks.1.mlp.fc1.weight', 'stages.0.blocks.1.mlp.fc1.bias', 'stages.0.blocks.1.mlp.fc2.weight', 'stages.0.blocks.1.mlp.fc2.bias', 'stages.0.blocks.1.norm2.weight', 'stages.0.blocks.1.norm2.bias', 'stages.0.downsample.reduction.weight', 'stages.0.downsample.norm.weight', 'stages.0.downsample.norm.bias', 'stages.1.blocks.0.attn_mask', 'stages.1.blocks.0.attn.t_scale', 'stages.1.blocks.0.attn.relative_coords_table', 'stages.1.blocks.0.attn.relative_position_index', 'stages.1.blocks.0.attn.crpb_mlp.0.weight', 'stages.1.blocks.0.attn.crpb_mlp.0.bias', 'stages.1.blocks.0.attn.crpb_mlp.3.weight', 'stages.1.blocks.0.attn.qkv.weight', 'stages.1.blocks.0.attn.qkv.bias', 'stages.1.blocks.0.attn.proj.weight', 'stages.1.blocks.0.attn.proj.bias', 'stages.1.blocks.0.norm1.weight', 'stages.1.blocks.0.norm1.bias', 'stages.1.blocks.0.mlp.fc1.weight', 'stages.1.blocks.0.mlp.fc1.bias', 'stages.1.blocks.0.mlp.fc2.weight', 'stages.1.blocks.0.mlp.fc2.bias', 'stages.1.blocks.0.norm2.weight', 'stages.1.blocks.0.norm2.bias', 'stages.1.blocks.1.attn_mask', 'stages.1.blocks.1.attn.t_scale', 'stages.1.blocks.1.attn.relative_coords_table', 'stages.1.blocks.1.attn.relative_position_index', 'stages.1.blocks.1.attn.crpb_mlp.0.weight', 'stages.1.blocks.1.attn.crpb_mlp.0.bias', 'stages.1.blocks.1.attn.crpb_mlp.3.weight', 'stages.1.blocks.1.attn.qkv.weight', 'stages.1.blocks.1.attn.qkv.bias', 'stages.1.blocks.1.attn.proj.weight', 'stages.1.blocks.1.attn.proj.bias', 'stages.1.blocks.1.norm1.weight', 'stages.1.blocks.1.norm1.bias', 'stages.1.blocks.1.mlp.fc1.weight', 'stages.1.blocks.1.mlp.fc1.bias', 'stages.1.blocks.1.mlp.fc2.weight', 'stages.1.blocks.1.mlp.fc2.bias', 'stages.1.blocks.1.norm2.weight', 'stages.1.blocks.1.norm2.bias', 'stages.1.downsample.reduction.weight', 'stages.1.downsample.norm.weight', 'stages.1.downsample.norm.bias', 'stages.2.blocks.0.attn_mask', 'stages.2.blocks.0.attn.t_scale', 'stages.2.blocks.0.attn.relative_coords_table', 'stages.2.blocks.0.attn.relative_position_index', 'stages.2.blocks.0.attn.crpb_mlp.0.weight', 'stages.2.blocks.0.attn.crpb_mlp.0.bias', 'stages.2.blocks.0.attn.crpb_mlp.3.weight', 'stages.2.blocks.0.attn.qkv.weight', 'stages.2.blocks.0.attn.qkv.bias', 'stages.2.blocks.0.attn.proj.weight', 'stages.2.blocks.0.attn.proj.bias', 'stages.2.blocks.0.norm1.weight', 'stages.2.blocks.0.norm1.bias', 'stages.2.blocks.0.mlp.fc1.weight', 'stages.2.blocks.0.mlp.fc1.bias', 'stages.2.blocks.0.mlp.fc2.weight', 'stages.2.blocks.0.mlp.fc2.bias', 'stages.2.blocks.0.norm2.weight', 'stages.2.blocks.0.norm2.bias', 'stages.2.blocks.1.attn_mask', 'stages.2.blocks.1.attn.t_scale', 'stages.2.blocks.1.attn.relative_coords_table', 'stages.2.blocks.1.attn.relative_position_index', 'stages.2.blocks.1.attn.crpb_mlp.0.weight', 'stages.2.blocks.1.attn.crpb_mlp.0.bias', 'stages.2.blocks.1.attn.crpb_mlp.3.weight', 'stages.2.blocks.1.attn.qkv.weight', 'stages.2.blocks.1.attn.qkv.bias', 'stages.2.blocks.1.attn.proj.weight', 'stages.2.blocks.1.attn.proj.bias', 'stages.2.blocks.1.norm1.weight', 'stages.2.blocks.1.norm1.bias', 'stages.2.blocks.1.mlp.fc1.weight', 'stages.2.blocks.1.mlp.fc1.bias', 'stages.2.blocks.1.mlp.fc2.weight', 'stages.2.blocks.1.mlp.fc2.bias', 'stages.2.blocks.1.norm2.weight', 'stages.2.blocks.1.norm2.bias', 'stages.2.blocks.2.attn_mask', 'stages.2.blocks.2.attn.t_scale', 'stages.2.blocks.2.attn.relative_coords_table', 'stages.2.blocks.2.attn.relative_position_index', 'stages.2.blocks.2.attn.crpb_mlp.0.weight', 'stages.2.blocks.2.attn.crpb_mlp.0.bias', 'stages.2.blocks.2.attn.crpb_mlp.3.weight', 'stages.2.blocks.2.attn.qkv.weight', 'stages.2.blocks.2.attn.qkv.bias', 'stages.2.blocks.2.attn.proj.weight', 'stages.2.blocks.2.attn.proj.bias', 'stages.2.blocks.2.norm1.weight', 'stages.2.blocks.2.norm1.bias', 'stages.2.blocks.2.mlp.fc1.weight', 'stages.2.blocks.2.mlp.fc1.bias', 'stages.2.blocks.2.mlp.fc2.weight', 'stages.2.blocks.2.mlp.fc2.bias', 'stages.2.blocks.2.norm2.weight', 'stages.2.blocks.2.norm2.bias', 'stages.2.blocks.3.attn_mask', 'stages.2.blocks.3.attn.t_scale', 'stages.2.blocks.3.attn.relative_coords_table', 'stages.2.blocks.3.attn.relative_position_index', 'stages.2.blocks.3.attn.crpb_mlp.0.weight', 'stages.2.blocks.3.attn.crpb_mlp.0.bias', 'stages.2.blocks.3.attn.crpb_mlp.3.weight', 'stages.2.blocks.3.attn.qkv.weight', 'stages.2.blocks.3.attn.qkv.bias', 'stages.2.blocks.3.attn.proj.weight', 'stages.2.blocks.3.attn.proj.bias', 'stages.2.blocks.3.norm1.weight', 'stages.2.blocks.3.norm1.bias', 'stages.2.blocks.3.mlp.fc1.weight', 'stages.2.blocks.3.mlp.fc1.bias', 'stages.2.blocks.3.mlp.fc2.weight', 'stages.2.blocks.3.mlp.fc2.bias', 'stages.2.blocks.3.norm2.weight', 'stages.2.blocks.3.norm2.bias', 'stages.2.blocks.4.attn_mask', 'stages.2.blocks.4.attn.t_scale', 'stages.2.blocks.4.attn.relative_coords_table', 'stages.2.blocks.4.attn.relative_position_index', 'stages.2.blocks.4.attn.crpb_mlp.0.weight', 'stages.2.blocks.4.attn.crpb_mlp.0.bias', 'stages.2.blocks.4.attn.crpb_mlp.3.weight', 'stages.2.blocks.4.attn.qkv.weight', 'stages.2.blocks.4.attn.qkv.bias', 'stages.2.blocks.4.attn.proj.weight', 'stages.2.blocks.4.attn.proj.bias', 'stages.2.blocks.4.norm1.weight', 'stages.2.blocks.4.norm1.bias', 'stages.2.blocks.4.mlp.fc1.weight', 'stages.2.blocks.4.mlp.fc1.bias', 'stages.2.blocks.4.mlp.fc2.weight', 'stages.2.blocks.4.mlp.fc2.bias', 'stages.2.blocks.4.norm2.weight', 'stages.2.blocks.4.norm2.bias', 'stages.2.blocks.5.attn_mask', 'stages.2.blocks.5.attn.t_scale', 'stages.2.blocks.5.attn.relative_coords_table', 'stages.2.blocks.5.attn.relative_position_index', 'stages.2.blocks.5.attn.crpb_mlp.0.weight', 'stages.2.blocks.5.attn.crpb_mlp.0.bias', 'stages.2.blocks.5.attn.crpb_mlp.3.weight', 'stages.2.blocks.5.attn.qkv.weight', 'stages.2.blocks.5.attn.qkv.bias', 'stages.2.blocks.5.attn.proj.weight', 'stages.2.blocks.5.attn.proj.bias', 'stages.2.blocks.5.norm1.weight', 'stages.2.blocks.5.norm1.bias', 'stages.2.blocks.5.mlp.fc1.weight', 'stages.2.blocks.5.mlp.fc1.bias', 'stages.2.blocks.5.mlp.fc2.weight', 'stages.2.blocks.5.mlp.fc2.bias', 'stages.2.blocks.5.norm2.weight', 'stages.2.blocks.5.norm2.bias', 'stages.2.downsample.reduction.weight', 'stages.2.downsample.norm.weight', 'stages.2.downsample.norm.bias', 'stages.3.blocks.0.attn_mask', 'stages.3.blocks.0.attn.t_scale', 'stages.3.blocks.0.attn.relative_coords_table', 'stages.3.blocks.0.attn.relative_position_index', 'stages.3.blocks.0.attn.crpb_mlp.0.weight', 'stages.3.blocks.0.attn.crpb_mlp.0.bias', 'stages.3.blocks.0.attn.crpb_mlp.3.weight', 'stages.3.blocks.0.attn.qkv.weight', 'stages.3.blocks.0.attn.qkv.bias', 'stages.3.blocks.0.attn.proj.weight', 'stages.3.blocks.0.attn.proj.bias', 'stages.3.blocks.0.norm1.weight', 'stages.3.blocks.0.norm1.bias', 'stages.3.blocks.0.mlp.fc1.weight', 'stages.3.blocks.0.mlp.fc1.bias', 'stages.3.blocks.0.mlp.fc2.weight', 'stages.3.blocks.0.mlp.fc2.bias', 'stages.3.blocks.0.norm2.weight', 'stages.3.blocks.0.norm2.bias', 'stages.3.blocks.1.attn_mask', 'stages.3.blocks.1.attn.t_scale', 'stages.3.blocks.1.attn.relative_coords_table', 'stages.3.blocks.1.attn.relative_position_index', 'stages.3.blocks.1.attn.crpb_mlp.0.weight', 'stages.3.blocks.1.attn.crpb_mlp.0.bias', 'stages.3.blocks.1.attn.crpb_mlp.3.weight', 'stages.3.blocks.1.attn.qkv.weight', 'stages.3.blocks.1.attn.qkv.bias', 'stages.3.blocks.1.attn.proj.weight', 'stages.3.blocks.1.attn.proj.bias', 'stages.3.blocks.1.norm1.weight', 'stages.3.blocks.1.norm1.bias', 'stages.3.blocks.1.mlp.fc1.weight', 'stages.3.blocks.1.mlp.fc1.bias', 'stages.3.blocks.1.mlp.fc2.weight', 'stages.3.blocks.1.mlp.fc2.bias', 'stages.3.blocks.1.norm2.weight', 'stages.3.blocks.1.norm2.bias', 'layernorm.weight', 'layernorm.bias', 'classifier.weight', 'classifier.bias'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SwinTransformerV2(pretrained_window_sizes=[6,6,6,6], ape=True, drop_path_rate=0.3)\n",
    "model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0270, -0.0150, -0.1292, -0.0200],\n",
       "         [-0.0695,  0.0042,  0.0692,  0.0476],\n",
       "         [ 0.1091, -0.1437,  0.0946, -0.0676],\n",
       "         [-0.0275,  0.0488,  0.1311,  0.0886]],\n",
       "\n",
       "        [[-0.0046,  0.0187, -0.1262, -0.0381],\n",
       "         [ 0.0380,  0.1383, -0.1189, -0.0180],\n",
       "         [-0.0150, -0.0903, -0.0621, -0.1111],\n",
       "         [ 0.1281, -0.1186,  0.0043,  0.0487]],\n",
       "\n",
       "        [[-0.0366,  0.0098, -0.0726, -0.1226],\n",
       "         [ 0.0872,  0.0253, -0.1398, -0.0425],\n",
       "         [ 0.1396,  0.0192,  0.0369,  0.1076],\n",
       "         [ 0.0522,  0.0779, -0.0197, -0.1169]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['embeddings.patch_embeddings.weight'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-5.7039e-03,  7.9231e-03,  1.6598e-03, -2.4443e-02, -2.0626e-03,\n",
       "        -1.3681e-02,  1.0584e-02, -2.8501e-02,  1.1764e-02,  5.9102e-02,\n",
       "         9.1187e-03, -1.2074e-02, -1.5805e-02, -4.2680e-02, -5.3424e-03,\n",
       "        -2.5962e-02,  2.2780e-02, -2.1294e-02, -2.6561e-02, -8.2797e-03,\n",
       "         2.1045e-02,  8.8609e-03, -2.0843e-02, -2.9074e-03, -1.2926e-02,\n",
       "        -2.2739e-03,  1.0008e-02, -3.0646e-02,  4.3791e-02, -6.7375e-04,\n",
       "        -3.3125e-03, -5.6427e-03, -1.5967e-02, -7.6297e-03,  2.1720e-02,\n",
       "         1.4745e-03, -9.9285e-03, -7.8507e-03,  2.8790e-02,  9.8445e-03,\n",
       "         4.7059e-03, -3.0176e-02, -9.3700e-03, -3.0868e-03, -1.0336e-02,\n",
       "        -3.1913e-02, -1.4835e-02,  3.4004e-02, -4.5951e-03,  1.1914e-02,\n",
       "         4.2241e-03,  2.3353e-02,  2.8223e-02,  1.3531e-02,  3.1149e-02,\n",
       "         1.9665e-02, -4.6744e-02, -9.7357e-05,  4.3877e-02, -5.9841e-03,\n",
       "        -2.7757e-02, -1.3494e-02, -1.7230e-02, -4.4924e-02, -1.8214e-02,\n",
       "        -3.3901e-02, -3.6728e-02,  4.3705e-02,  7.3543e-03,  5.5441e-03,\n",
       "         7.5948e-04, -4.2702e-03, -2.3576e-02,  1.3336e-02, -1.2800e-02,\n",
       "         2.8186e-03, -2.3427e-02,  7.7893e-03, -1.7930e-02,  9.3503e-03,\n",
       "         4.8659e-03,  4.1566e-02, -1.1205e-02, -9.9387e-03,  5.4963e-03,\n",
       "         2.8740e-02,  3.8810e-03,  2.2370e-02,  1.9076e-02, -1.2502e-03,\n",
       "         2.1695e-02,  1.6242e-02,  3.2562e-03,  1.2695e-03,  1.0902e-02,\n",
       "         4.3760e-02,  9.7554e-03, -1.5381e-02, -2.6734e-02, -3.7729e-03,\n",
       "         2.8187e-02, -1.0779e-02, -1.9097e-02, -4.0654e-02, -5.1996e-03,\n",
       "         3.7576e-02, -4.9245e-03, -1.7953e-02,  2.0849e-04,  5.2518e-03,\n",
       "         2.1507e-03,  3.3338e-02,  2.8605e-03,  2.8550e-02, -3.5302e-02,\n",
       "        -1.9004e-02,  1.1226e-05,  8.2117e-03, -4.2659e-02, -8.6667e-03,\n",
       "         3.0645e-02, -1.8047e-02, -1.7670e-03, -1.0452e-02,  2.2423e-03,\n",
       "        -2.6128e-02, -1.6391e-02,  8.4776e-04, -3.2299e-02,  1.6699e-02,\n",
       "         4.0970e-03, -1.3346e-02,  4.0147e-03,  3.7518e-02, -2.6472e-02,\n",
       "         2.4734e-02, -1.2977e-04,  1.2709e-03,  4.8747e-03,  2.8897e-03,\n",
       "         3.8824e-02, -1.1001e-02,  1.3627e-02,  2.8939e-02, -8.4520e-03,\n",
       "        -7.3054e-03, -2.0118e-02,  3.3879e-03,  2.0993e-02,  3.9191e-03,\n",
       "        -1.9692e-03,  8.1198e-04,  2.6118e-02,  1.1603e-02, -2.4066e-02,\n",
       "         2.9780e-02,  1.9877e-02,  1.7659e-02,  1.2013e-02,  1.0707e-02,\n",
       "        -2.3066e-04, -9.0367e-03, -3.0174e-02,  1.8523e-02,  4.1410e-05,\n",
       "        -4.9388e-02, -9.3171e-03, -1.0202e-02, -4.4846e-02, -3.9054e-03,\n",
       "        -1.8512e-02,  1.4555e-02,  3.2259e-02, -3.2774e-02,  2.9917e-03,\n",
       "        -2.6478e-03, -2.5297e-02,  5.9255e-05, -1.9756e-02,  1.6569e-02,\n",
       "        -5.2159e-04,  6.6689e-03,  1.5677e-02,  1.6611e-02,  9.4528e-03,\n",
       "        -2.6122e-02,  5.0354e-03, -1.6949e-02, -3.3761e-03,  2.2782e-04,\n",
       "        -1.1162e-03,  2.5856e-02,  1.9921e-02, -1.4968e-02, -1.7416e-02,\n",
       "         1.7095e-02, -2.9636e-03, -2.1754e-02, -2.8186e-02,  2.6293e-02,\n",
       "        -6.9988e-03, -2.0505e-02,  1.3450e-02, -4.7277e-03, -1.1661e-02,\n",
       "         1.0651e-02,  6.0167e-03,  3.6367e-03,  6.4970e-03, -2.0273e-02,\n",
       "         4.4796e-02, -2.3352e-04,  2.2503e-02,  2.5123e-02,  1.8530e-03,\n",
       "        -1.4185e-02,  2.6968e-02, -6.0989e-03, -4.9131e-02, -1.3366e-02,\n",
       "        -7.7564e-03,  7.5265e-03, -4.3869e-02,  1.7576e-02, -1.7981e-02,\n",
       "        -1.5682e-02, -1.7642e-02, -3.2323e-03, -1.2126e-05,  1.1672e-02,\n",
       "        -5.3369e-03, -1.1688e-02,  1.9348e-02,  1.6544e-02, -6.3561e-03,\n",
       "        -1.2330e-02,  1.0927e-03,  5.7639e-03, -3.0353e-02, -4.4022e-03,\n",
       "        -3.7156e-03,  1.7298e-02, -1.2142e-02, -1.2200e-02, -4.2164e-02,\n",
       "         3.4845e-02, -3.7266e-02, -1.8624e-02,  8.7555e-03,  7.8834e-03,\n",
       "         4.0317e-02,  2.4810e-03,  1.1246e-02, -8.4745e-03, -6.5173e-02,\n",
       "         1.0163e-02, -2.1035e-02,  1.0794e-02, -5.8246e-03, -1.7365e-02,\n",
       "         1.2889e-02,  9.7168e-03,  2.3013e-02,  3.3706e-02,  1.2623e-02,\n",
       "         2.6635e-02,  1.3356e-02, -1.5031e-02,  1.1641e-02,  3.5359e-02,\n",
       "        -3.0852e-02,  1.7665e-02, -5.5904e-03,  4.3509e-03, -2.5736e-02,\n",
       "         3.1835e-02,  4.6170e-02,  3.5594e-02, -2.5963e-02, -1.7056e-02,\n",
       "        -3.6423e-02, -9.6274e-03, -1.5897e-02, -3.1358e-02,  2.8636e-03,\n",
       "         3.1523e-02,  1.2739e-02, -7.8095e-03,  3.0641e-02,  2.4926e-02,\n",
       "         2.5128e-02,  2.5895e-02, -9.8948e-03,  3.9746e-04,  1.0386e-02,\n",
       "        -2.5060e-02, -9.6692e-03,  1.6958e-02,  3.7868e-02, -1.5266e-02,\n",
       "        -7.4353e-03,  3.0430e-02, -3.6591e-03,  2.1858e-03,  2.2160e-02,\n",
       "        -1.7399e-02, -1.3817e-02, -2.5269e-02,  6.6056e-03, -3.0325e-02,\n",
       "        -1.7202e-02, -6.1539e-04,  1.8357e-04, -2.8081e-03,  2.4897e-03,\n",
       "         9.8943e-03,  1.5525e-02, -1.1331e-02, -2.8095e-04,  1.3022e-02,\n",
       "        -2.4077e-02, -8.6441e-05,  1.1925e-02,  2.9119e-02, -1.0736e-02,\n",
       "        -1.6133e-02,  5.3169e-03,  4.1586e-02, -2.0760e-02, -1.1287e-02,\n",
       "        -6.7220e-03,  3.1268e-02,  1.5198e-02,  2.6911e-03,  1.3481e-02,\n",
       "         1.4035e-02, -1.2160e-02,  2.5663e-02, -1.8644e-02, -8.8077e-03,\n",
       "         1.5908e-02, -2.4165e-02,  2.4194e-02,  2.2593e-02, -3.5297e-02,\n",
       "        -2.4405e-03, -2.8903e-02, -1.6541e-03, -5.9620e-03,  1.9261e-02,\n",
       "        -1.7904e-04,  8.1806e-03, -3.7681e-02, -5.9809e-02,  7.9367e-04,\n",
       "        -1.4963e-02, -4.0960e-03,  4.8063e-02,  9.7890e-03,  1.9360e-02,\n",
       "         5.2412e-02, -1.3524e-02,  1.9956e-02,  1.4985e-02,  1.9233e-02,\n",
       "        -1.4084e-03, -1.9429e-02, -4.6110e-02, -3.8596e-03,  1.1606e-02,\n",
       "        -4.8537e-03, -1.3880e-02, -1.2013e-03, -1.4102e-02,  1.1873e-02,\n",
       "        -7.9864e-03, -1.5888e-02,  1.0139e-02,  2.1709e-02, -2.4012e-02,\n",
       "        -2.6527e-02,  3.6574e-02,  9.3414e-04,  7.7344e-03])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['stages.3.blocks.1.attn.crpb_mlp.3.weight'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MODEL': {'TYPE': 'swinv2',\n",
       "  'NAME': 'simmim_train',\n",
       "  'PRETRAINED': '../../models/swin2/simmim.pth',\n",
       "  'DROP_PATH_RATE': 0.2,\n",
       "  'SWIN': {'EMBED_DIM': 96,\n",
       "   'DEPTHS': [2, 2, 6, 2],\n",
       "   'NUM_HEADS': [3, 6, 12, 24],\n",
       "   'WINDOW_SIZE': 7,\n",
       "   'PATCH_SIZE': 4}},\n",
       " 'DATA': {'IMG_SIZE': 224,\n",
       "  'MASK_PATCH_SIZE': 32,\n",
       "  'MASK_RATIO': 0.6,\n",
       "  'BATCH_SIZE': 960,\n",
       "  'NUM_WORKERS': 24,\n",
       "  'DATA_PATH': '../../data/sports'},\n",
       " 'TRAIN': {'EPOCHS': 20,\n",
       "  'WARMUP_EPOCHS': 10,\n",
       "  'BASE_LR': '1e-4',\n",
       "  'WEIGHT_DECAY': 0.05,\n",
       "  'CLIP_GRAD': 5}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swin_config = yaml.load(open('config/train.yaml'), Loader=yaml.FullLoader)\n",
    "swin_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained(config, model):\n",
    "    print(f\"==============> Loading weight {config.MODEL.PRETRAINED} for fine-tuning......\")\n",
    "    state_dict = torch.load(config.MODEL.PRETRAINED, map_location='cpu')\n",
    "\n",
    "    # remain encoder only\n",
    "    not_encoder_keys = [k for k in state_dict.keys() if 'encoder' not in k]\n",
    "    for k in not_encoder_keys:\n",
    "        del state_dict[k]\n",
    "        \n",
    "    # remove prefix encoder.\n",
    "    state_dict = {k.replace('encoder.', ''):v for k, v in state_dict.items()}\n",
    "\n",
    "    # delete relative_position_index since we always re-init it\n",
    "    relative_position_index_keys = [k for k in state_dict.keys() if \"relative_position_index\" in k]\n",
    "    for k in relative_position_index_keys:\n",
    "        del state_dict[k]\n",
    "\n",
    "    # delete relative_coords_table since we always re-init it\n",
    "    relative_position_index_keys = [k for k in state_dict.keys() if \"relative_coords_table\" in k]\n",
    "    for k in relative_position_index_keys:\n",
    "        del state_dict[k]\n",
    "\n",
    "    # delete attn_mask since we always re-init it\n",
    "    attn_mask_keys = [k for k in state_dict.keys() if \"attn_mask\" in k]\n",
    "    for k in attn_mask_keys:\n",
    "        del state_dict[k]\n",
    "\n",
    "    # bicubic interpolate relative_position_bias_table if not match\n",
    "    relative_position_bias_table_keys = [k for k in state_dict.keys() if \"relative_position_bias_table\" in k]\n",
    "    for k in relative_position_bias_table_keys:\n",
    "        relative_position_bias_table_pretrained = state_dict[k]\n",
    "        relative_position_bias_table_current = model.state_dict()[k]\n",
    "        L1, nH1 = relative_position_bias_table_pretrained.size()\n",
    "        L2, nH2 = relative_position_bias_table_current.size()\n",
    "        if nH1 != nH2:\n",
    "            print(f\"Error in loading {k}, passing......\")\n",
    "        else:\n",
    "            if L1 != L2:\n",
    "                # bicubic interpolate relative_position_bias_table if not match\n",
    "                S1 = int(L1 ** 0.5)\n",
    "                S2 = int(L2 ** 0.5)\n",
    "                relative_position_bias_table_pretrained_resized = torch.nn.functional.interpolate(\n",
    "                    relative_position_bias_table_pretrained.permute(1, 0).view(1, nH1, S1, S1), size=(S2, S2),\n",
    "                    mode='bicubic')\n",
    "                state_dict[k] = relative_position_bias_table_pretrained_resized.view(nH2, L2).permute(1, 0)\n",
    "\n",
    "    # bicubic interpolate absolute_pos_embed if not match\n",
    "    absolute_pos_embed_keys = [k for k in state_dict.keys() if \"absolute_pos_embed\" in k]\n",
    "    for k in absolute_pos_embed_keys:\n",
    "        # dpe\n",
    "        absolute_pos_embed_pretrained = state_dict[k]\n",
    "        absolute_pos_embed_current = model.state_dict()[k.replace('encoder.','')]\n",
    "        _, L1, C1 = absolute_pos_embed_pretrained.size()\n",
    "        _, L2, C2 = absolute_pos_embed_current.size()\n",
    "        if C1 != C1:\n",
    "            print(f\"Error in loading {k}, passing......\")\n",
    "        else:\n",
    "            if L1 != L2:\n",
    "                S1 = int(L1 ** 0.5)\n",
    "                S2 = int(L2 ** 0.5)\n",
    "                absolute_pos_embed_pretrained = absolute_pos_embed_pretrained.reshape(-1, S1, S1, C1)\n",
    "                absolute_pos_embed_pretrained = absolute_pos_embed_pretrained.permute(0, 3, 1, 2)\n",
    "                absolute_pos_embed_pretrained_resized = torch.nn.functional.interpolate(\n",
    "                    absolute_pos_embed_pretrained, size=(S2, S2), mode='bicubic')\n",
    "                absolute_pos_embed_pretrained_resized = absolute_pos_embed_pretrained_resized.permute(0, 2, 3, 1)\n",
    "                absolute_pos_embed_pretrained_resized = absolute_pos_embed_pretrained_resized.flatten(1, 2)\n",
    "                state_dict[k] = absolute_pos_embed_pretrained_resized\n",
    "\n",
    "    # check classifier, if not match, then re-init classifier to zero\n",
    "    head_bias_pretrained = state_dict['classifier.bias']\n",
    "    Nc1 = head_bias_pretrained.shape[0]\n",
    "    Nc2 = model.classifier.bias.shape[0]\n",
    "    if (Nc1 != Nc2):\n",
    "        torch.nn.init.constant_(model.classifier.bias, 0.)\n",
    "        torch.nn.init.constant_(model.classifier.weight, 0.)\n",
    "        del state_dict['classifier.weight']\n",
    "        del state_dict['classifier.bias']\n",
    "        print(f\"Error in loading classifier head, re-init classifier head to 0\")\n",
    "\n",
    "    msg = model.load_state_dict(state_dict, strict=False)\n",
    "    print(msg)\n",
    "\n",
    "    print(f\"=> loaded successfully '{config.MODEL.PRETRAINED}'\")\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============> Loading weight ../../models/swin2/simmim.pth for fine-tuning......\n",
      "_IncompatibleKeys(missing_keys=['stages.0.blocks.0.attn_mask', 'stages.0.blocks.0.attn.relative_coords_table', 'stages.0.blocks.0.attn.relative_position_index', 'stages.0.blocks.1.attn_mask', 'stages.0.blocks.1.attn.relative_coords_table', 'stages.0.blocks.1.attn.relative_position_index', 'stages.1.blocks.0.attn_mask', 'stages.1.blocks.0.attn.relative_coords_table', 'stages.1.blocks.0.attn.relative_position_index', 'stages.1.blocks.1.attn_mask', 'stages.1.blocks.1.attn.relative_coords_table', 'stages.1.blocks.1.attn.relative_position_index', 'stages.2.blocks.0.attn_mask', 'stages.2.blocks.0.attn.relative_coords_table', 'stages.2.blocks.0.attn.relative_position_index', 'stages.2.blocks.1.attn_mask', 'stages.2.blocks.1.attn.relative_coords_table', 'stages.2.blocks.1.attn.relative_position_index', 'stages.2.blocks.2.attn_mask', 'stages.2.blocks.2.attn.relative_coords_table', 'stages.2.blocks.2.attn.relative_position_index', 'stages.2.blocks.3.attn_mask', 'stages.2.blocks.3.attn.relative_coords_table', 'stages.2.blocks.3.attn.relative_position_index', 'stages.2.blocks.4.attn_mask', 'stages.2.blocks.4.attn.relative_coords_table', 'stages.2.blocks.4.attn.relative_position_index', 'stages.2.blocks.5.attn_mask', 'stages.2.blocks.5.attn.relative_coords_table', 'stages.2.blocks.5.attn.relative_position_index', 'stages.3.blocks.0.attn_mask', 'stages.3.blocks.0.attn.relative_coords_table', 'stages.3.blocks.0.attn.relative_position_index', 'stages.3.blocks.1.attn_mask', 'stages.3.blocks.1.attn.relative_coords_table', 'stages.3.blocks.1.attn.relative_position_index'], unexpected_keys=['mask_token'])\n",
      "=> loaded successfully '../../models/swin2/simmim.pth'\n"
     ]
    }
   ],
   "source": [
    "swin_config = Box(swin_config)\n",
    "load_pretrained(swin_config, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0301,  0.0009,  0.0505, -0.0364],\n",
       "         [ 0.1105, -0.0453, -0.0787, -0.0376],\n",
       "         [-0.0682, -0.0893,  0.0535, -0.0659],\n",
       "         [-0.0380,  0.0854,  0.0976,  0.0287]],\n",
       "\n",
       "        [[-0.0771, -0.1077, -0.0139,  0.0576],\n",
       "         [-0.1156,  0.0058,  0.0420, -0.0442],\n",
       "         [-0.1327,  0.1288, -0.0719,  0.0688],\n",
       "         [ 0.0249,  0.1164, -0.0735, -0.0361]],\n",
       "\n",
       "        [[ 0.0094,  0.1149, -0.0851,  0.1097],\n",
       "         [ 0.1142, -0.1260,  0.1255, -0.0553],\n",
       "         [ 0.0574, -0.0123, -0.0088,  0.0854],\n",
       "         [-0.0981,  0.0671, -0.1177,  0.0822]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['embeddings.patch_embeddings.weight'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3441,  0.2157, -0.2093,  0.3680, -0.0128, -0.1047,  0.3628,  0.1783,\n",
       "        -0.1962, -0.0918,  0.2762,  0.3604, -0.1071,  0.3773, -0.0933, -0.0062,\n",
       "        -0.1436, -0.1425, -0.1067,  0.2658, -0.0970,  0.2234,  0.0165,  0.3350,\n",
       "        -0.1316, -0.1465,  0.3168, -0.1226, -0.1506, -0.0127,  0.3362, -0.1885,\n",
       "        -0.1618, -0.1210,  0.3268,  0.2339, -0.1220, -0.0818, -0.0878, -0.1294,\n",
       "        -0.0691,  0.1114, -0.0910, -0.1057,  0.2985,  0.3047, -0.1025, -0.0852,\n",
       "        -0.0855,  0.1312, -0.0273, -0.1521,  0.3055,  0.1763, -0.0781,  0.3170,\n",
       "        -0.0057, -0.0150, -0.0239, -0.1214, -0.0623, -0.2075, -0.0569,  0.2784,\n",
       "        -0.0833, -0.1258, -0.1253,  0.1136, -0.1609,  0.1525,  0.3013, -0.1182,\n",
       "         0.0469, -0.1002, -0.1370, -0.1209,  0.2787,  0.0903, -0.1388, -0.1924,\n",
       "        -0.1242,  0.0175, -0.1391, -0.0981,  0.3561,  0.3985, -0.0734,  0.3098,\n",
       "         0.0680, -0.1240,  0.2772, -0.0027,  0.2672,  0.3719, -0.1221,  0.2322,\n",
       "        -0.1118, -0.1924, -0.1657, -0.1465, -0.2035,  0.1239, -0.1328, -0.1246,\n",
       "        -0.1630, -0.1243,  0.0183,  0.1933,  0.3206,  0.0840, -0.1172, -0.1619,\n",
       "         0.2803, -0.1315, -0.0774, -0.1813, -0.1353, -0.1154, -0.1547,  0.0035,\n",
       "        -0.1676, -0.2131,  0.3817,  0.1613, -0.1180, -0.1891,  0.1556,  0.0457,\n",
       "        -0.1123,  0.1500, -0.1252, -0.1942,  0.0070,  0.3842, -0.1383, -0.1451,\n",
       "        -0.0104,  0.1070,  0.0060,  0.2915, -0.1389,  0.3873, -0.1191,  0.2119,\n",
       "         0.2249, -0.1321,  0.1351,  0.2725, -0.1091, -0.0455, -0.1145,  0.4053,\n",
       "         0.0386, -0.1343,  0.1865, -0.1044, -0.1041,  0.1070, -0.1304,  0.3231,\n",
       "         0.1755, -0.0995, -0.0674, -0.1109,  0.4288, -0.1018, -0.0100, -0.1207,\n",
       "        -0.1190, -0.1947, -0.1189,  0.1013, -0.0164, -0.1201,  0.3990, -0.0315,\n",
       "        -0.1000,  0.0497, -0.1145, -0.1840,  0.3667, -0.0871,  0.4341,  0.1278,\n",
       "        -0.1869,  0.2664, -0.1631,  0.3120,  0.2145, -0.1874, -0.1540, -0.0987,\n",
       "         0.1388, -0.0884,  0.0712, -0.1132, -0.1430,  0.1176, -0.1269,  0.1296,\n",
       "         0.1934,  0.2749, -0.1208, -0.1594, -0.0258, -0.0776, -0.1429,  0.1138,\n",
       "         0.1264,  0.2032,  0.2603, -0.0740, -0.2104, -0.0999,  0.0597, -0.1706,\n",
       "        -0.1895, -0.1468, -0.0890,  0.0882, -0.1409, -0.1187,  0.3585, -0.1258,\n",
       "         0.2729, -0.1978, -0.1840, -0.1468,  0.1739,  0.3891,  0.0774, -0.0752,\n",
       "         0.1035, -0.1300,  0.1101,  0.3737,  0.2278, -0.1651, -0.0667,  0.2701,\n",
       "         0.1460,  0.0951,  0.2819,  0.3445,  0.0975, -0.1205, -0.1011,  0.3742,\n",
       "        -0.1751, -0.2178,  0.0689, -0.0998, -0.1064,  0.1043, -0.0924, -0.0146,\n",
       "         0.1142, -0.1045, -0.1307, -0.1781, -0.1173, -0.1025,  0.2247,  0.0297,\n",
       "        -0.1058,  0.0530,  0.3087,  0.1489,  0.0973, -0.0852,  0.0206, -0.1205,\n",
       "        -0.0589, -0.1306,  0.3215,  0.0807, -0.1712,  0.2878, -0.1484,  0.2949,\n",
       "        -0.1277, -0.0067, -0.1076,  0.1325,  0.0985, -0.1223,  0.0184, -0.1417,\n",
       "        -0.1038, -0.1312,  0.1919, -0.1557,  0.2966,  0.3421,  0.2683,  0.3130,\n",
       "        -0.1660, -0.0919, -0.1215,  0.3667,  0.3266, -0.1250,  0.3586, -0.0910,\n",
       "        -0.1648, -0.1452, -0.1346, -0.1591, -0.1129, -0.0977, -0.0852,  0.3798,\n",
       "        -0.0844,  0.3037, -0.1081,  0.3366,  0.3138, -0.1073, -0.1729, -0.1409,\n",
       "         0.1763, -0.1172, -0.1060,  0.1426,  0.3322,  0.3639,  0.2781, -0.2158,\n",
       "        -0.1003,  0.3241, -0.0173, -0.1263, -0.1019,  0.3160,  0.1903, -0.1880,\n",
       "        -0.1185,  0.3448,  0.1412,  0.2618, -0.1933, -0.1846, -0.1656, -0.1857,\n",
       "        -0.1341, -0.1656, -0.0982, -0.0887,  0.3037, -0.1127,  0.0946,  0.2976,\n",
       "         0.3293,  0.3636,  0.1178,  0.1094, -0.0866,  0.2487,  0.1525,  0.2924,\n",
       "         0.1803, -0.1963, -0.0903, -0.1952,  0.0780, -0.1051, -0.0864, -0.1440,\n",
       "         0.1882,  0.2569, -0.1023, -0.1234, -0.1498,  0.0518, -0.1104, -0.0058,\n",
       "         0.3766,  0.2990, -0.0930, -0.0172, -0.1035,  0.1263,  0.1493, -0.1194])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['stages.3.blocks.1.attn.crpb_mlp.3.weight'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms 정의하기\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8,1), interpolation=transforms.InterpolationMode.LANCZOS),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    transforms.RandomErasing(p=0.9, scale=(0.02, 0.33)),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "data_dir = '../../data/sports'\n",
    "batch_size = 960\n",
    "\n",
    "train_path = data_dir+'/train'\n",
    "valid_path = data_dir+'/valid'\n",
    "test_path = data_dir+'/test'\n",
    "\n",
    "# dataset load\n",
    "train_data = ImageFolder(train_path, transform=train_transform)\n",
    "valid_data = ImageFolder(valid_path, transform=test_transform)\n",
    "test_data = ImageFolder(test_path, transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_norm = 5.0 # paper : 100 with G variants\n",
    "\n",
    "model.to(device)\n",
    "model_path = '../../models/swin2/model_w_simmim2.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixup_fn = Mixup(mixup_alpha=.7, \n",
    "                cutmix_alpha=.7, \n",
    "                prob=.7, \n",
    "                switch_prob=0.5, \n",
    "                mode='batch',\n",
    "                label_smoothing=.1,\n",
    "                num_classes=100)\n",
    "\n",
    "epochs = 150\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: absolute_pos_embed\n",
      "1: embeddings.patch_embeddings.weight\n",
      "2: embeddings.patch_embeddings.bias\n",
      "3: embeddings.norm.weight\n",
      "4: embeddings.norm.bias\n",
      "5: stages.0.blocks.0.attn.t_scale\n",
      "6: stages.0.blocks.0.attn.crpb_mlp.0.weight\n",
      "7: stages.0.blocks.0.attn.crpb_mlp.0.bias\n",
      "8: stages.0.blocks.0.attn.crpb_mlp.3.weight\n",
      "9: stages.0.blocks.0.attn.qkv.weight\n",
      "10: stages.0.blocks.0.attn.qkv.bias\n",
      "11: stages.0.blocks.0.attn.proj.weight\n",
      "12: stages.0.blocks.0.attn.proj.bias\n",
      "13: stages.0.blocks.0.norm1.weight\n",
      "14: stages.0.blocks.0.norm1.bias\n",
      "15: stages.0.blocks.0.mlp.fc1.weight\n",
      "16: stages.0.blocks.0.mlp.fc1.bias\n",
      "17: stages.0.blocks.0.mlp.fc2.weight\n",
      "18: stages.0.blocks.0.mlp.fc2.bias\n",
      "19: stages.0.blocks.0.norm2.weight\n",
      "20: stages.0.blocks.0.norm2.bias\n",
      "21: stages.0.blocks.1.attn.t_scale\n",
      "22: stages.0.blocks.1.attn.crpb_mlp.0.weight\n",
      "23: stages.0.blocks.1.attn.crpb_mlp.0.bias\n",
      "24: stages.0.blocks.1.attn.crpb_mlp.3.weight\n",
      "25: stages.0.blocks.1.attn.qkv.weight\n",
      "26: stages.0.blocks.1.attn.qkv.bias\n",
      "27: stages.0.blocks.1.attn.proj.weight\n",
      "28: stages.0.blocks.1.attn.proj.bias\n",
      "29: stages.0.blocks.1.norm1.weight\n",
      "30: stages.0.blocks.1.norm1.bias\n",
      "31: stages.0.blocks.1.mlp.fc1.weight\n",
      "32: stages.0.blocks.1.mlp.fc1.bias\n",
      "33: stages.0.blocks.1.mlp.fc2.weight\n",
      "34: stages.0.blocks.1.mlp.fc2.bias\n",
      "35: stages.0.blocks.1.norm2.weight\n",
      "36: stages.0.blocks.1.norm2.bias\n",
      "37: stages.0.downsample.reduction.weight\n",
      "38: stages.0.downsample.norm.weight\n",
      "39: stages.0.downsample.norm.bias\n",
      "40: stages.1.blocks.0.attn.t_scale\n",
      "41: stages.1.blocks.0.attn.crpb_mlp.0.weight\n",
      "42: stages.1.blocks.0.attn.crpb_mlp.0.bias\n",
      "43: stages.1.blocks.0.attn.crpb_mlp.3.weight\n",
      "44: stages.1.blocks.0.attn.qkv.weight\n",
      "45: stages.1.blocks.0.attn.qkv.bias\n",
      "46: stages.1.blocks.0.attn.proj.weight\n",
      "47: stages.1.blocks.0.attn.proj.bias\n",
      "48: stages.1.blocks.0.norm1.weight\n",
      "49: stages.1.blocks.0.norm1.bias\n",
      "50: stages.1.blocks.0.mlp.fc1.weight\n",
      "51: stages.1.blocks.0.mlp.fc1.bias\n",
      "52: stages.1.blocks.0.mlp.fc2.weight\n",
      "53: stages.1.blocks.0.mlp.fc2.bias\n",
      "54: stages.1.blocks.0.norm2.weight\n",
      "55: stages.1.blocks.0.norm2.bias\n",
      "56: stages.1.blocks.1.attn.t_scale\n",
      "57: stages.1.blocks.1.attn.crpb_mlp.0.weight\n",
      "58: stages.1.blocks.1.attn.crpb_mlp.0.bias\n",
      "59: stages.1.blocks.1.attn.crpb_mlp.3.weight\n",
      "60: stages.1.blocks.1.attn.qkv.weight\n",
      "61: stages.1.blocks.1.attn.qkv.bias\n",
      "62: stages.1.blocks.1.attn.proj.weight\n",
      "63: stages.1.blocks.1.attn.proj.bias\n",
      "64: stages.1.blocks.1.norm1.weight\n",
      "65: stages.1.blocks.1.norm1.bias\n",
      "66: stages.1.blocks.1.mlp.fc1.weight\n",
      "67: stages.1.blocks.1.mlp.fc1.bias\n",
      "68: stages.1.blocks.1.mlp.fc2.weight\n",
      "69: stages.1.blocks.1.mlp.fc2.bias\n",
      "70: stages.1.blocks.1.norm2.weight\n",
      "71: stages.1.blocks.1.norm2.bias\n",
      "72: stages.1.downsample.reduction.weight\n",
      "73: stages.1.downsample.norm.weight\n",
      "74: stages.1.downsample.norm.bias\n",
      "75: stages.2.blocks.0.attn.t_scale\n",
      "76: stages.2.blocks.0.attn.crpb_mlp.0.weight\n",
      "77: stages.2.blocks.0.attn.crpb_mlp.0.bias\n",
      "78: stages.2.blocks.0.attn.crpb_mlp.3.weight\n",
      "79: stages.2.blocks.0.attn.qkv.weight\n",
      "80: stages.2.blocks.0.attn.qkv.bias\n",
      "81: stages.2.blocks.0.attn.proj.weight\n",
      "82: stages.2.blocks.0.attn.proj.bias\n",
      "83: stages.2.blocks.0.norm1.weight\n",
      "84: stages.2.blocks.0.norm1.bias\n",
      "85: stages.2.blocks.0.mlp.fc1.weight\n",
      "86: stages.2.blocks.0.mlp.fc1.bias\n",
      "87: stages.2.blocks.0.mlp.fc2.weight\n",
      "88: stages.2.blocks.0.mlp.fc2.bias\n",
      "89: stages.2.blocks.0.norm2.weight\n",
      "90: stages.2.blocks.0.norm2.bias\n",
      "91: stages.2.blocks.1.attn.t_scale\n",
      "92: stages.2.blocks.1.attn.crpb_mlp.0.weight\n",
      "93: stages.2.blocks.1.attn.crpb_mlp.0.bias\n",
      "94: stages.2.blocks.1.attn.crpb_mlp.3.weight\n",
      "95: stages.2.blocks.1.attn.qkv.weight\n",
      "96: stages.2.blocks.1.attn.qkv.bias\n",
      "97: stages.2.blocks.1.attn.proj.weight\n",
      "98: stages.2.blocks.1.attn.proj.bias\n",
      "99: stages.2.blocks.1.norm1.weight\n",
      "100: stages.2.blocks.1.norm1.bias\n",
      "101: stages.2.blocks.1.mlp.fc1.weight\n",
      "102: stages.2.blocks.1.mlp.fc1.bias\n",
      "103: stages.2.blocks.1.mlp.fc2.weight\n",
      "104: stages.2.blocks.1.mlp.fc2.bias\n",
      "105: stages.2.blocks.1.norm2.weight\n",
      "106: stages.2.blocks.1.norm2.bias\n",
      "107: stages.2.blocks.2.attn.t_scale\n",
      "108: stages.2.blocks.2.attn.crpb_mlp.0.weight\n",
      "109: stages.2.blocks.2.attn.crpb_mlp.0.bias\n",
      "110: stages.2.blocks.2.attn.crpb_mlp.3.weight\n",
      "111: stages.2.blocks.2.attn.qkv.weight\n",
      "112: stages.2.blocks.2.attn.qkv.bias\n",
      "113: stages.2.blocks.2.attn.proj.weight\n",
      "114: stages.2.blocks.2.attn.proj.bias\n",
      "115: stages.2.blocks.2.norm1.weight\n",
      "116: stages.2.blocks.2.norm1.bias\n",
      "117: stages.2.blocks.2.mlp.fc1.weight\n",
      "118: stages.2.blocks.2.mlp.fc1.bias\n",
      "119: stages.2.blocks.2.mlp.fc2.weight\n",
      "120: stages.2.blocks.2.mlp.fc2.bias\n",
      "121: stages.2.blocks.2.norm2.weight\n",
      "122: stages.2.blocks.2.norm2.bias\n",
      "123: stages.2.blocks.3.attn.t_scale\n",
      "124: stages.2.blocks.3.attn.crpb_mlp.0.weight\n",
      "125: stages.2.blocks.3.attn.crpb_mlp.0.bias\n",
      "126: stages.2.blocks.3.attn.crpb_mlp.3.weight\n",
      "127: stages.2.blocks.3.attn.qkv.weight\n",
      "128: stages.2.blocks.3.attn.qkv.bias\n",
      "129: stages.2.blocks.3.attn.proj.weight\n",
      "130: stages.2.blocks.3.attn.proj.bias\n",
      "131: stages.2.blocks.3.norm1.weight\n",
      "132: stages.2.blocks.3.norm1.bias\n",
      "133: stages.2.blocks.3.mlp.fc1.weight\n",
      "134: stages.2.blocks.3.mlp.fc1.bias\n",
      "135: stages.2.blocks.3.mlp.fc2.weight\n",
      "136: stages.2.blocks.3.mlp.fc2.bias\n",
      "137: stages.2.blocks.3.norm2.weight\n",
      "138: stages.2.blocks.3.norm2.bias\n",
      "139: stages.2.blocks.4.attn.t_scale\n",
      "140: stages.2.blocks.4.attn.crpb_mlp.0.weight\n",
      "141: stages.2.blocks.4.attn.crpb_mlp.0.bias\n",
      "142: stages.2.blocks.4.attn.crpb_mlp.3.weight\n",
      "143: stages.2.blocks.4.attn.qkv.weight\n",
      "144: stages.2.blocks.4.attn.qkv.bias\n",
      "145: stages.2.blocks.4.attn.proj.weight\n",
      "146: stages.2.blocks.4.attn.proj.bias\n",
      "147: stages.2.blocks.4.norm1.weight\n",
      "148: stages.2.blocks.4.norm1.bias\n",
      "149: stages.2.blocks.4.mlp.fc1.weight\n",
      "150: stages.2.blocks.4.mlp.fc1.bias\n",
      "151: stages.2.blocks.4.mlp.fc2.weight\n",
      "152: stages.2.blocks.4.mlp.fc2.bias\n",
      "153: stages.2.blocks.4.norm2.weight\n",
      "154: stages.2.blocks.4.norm2.bias\n",
      "155: stages.2.blocks.5.attn.t_scale\n",
      "156: stages.2.blocks.5.attn.crpb_mlp.0.weight\n",
      "157: stages.2.blocks.5.attn.crpb_mlp.0.bias\n",
      "158: stages.2.blocks.5.attn.crpb_mlp.3.weight\n",
      "159: stages.2.blocks.5.attn.qkv.weight\n",
      "160: stages.2.blocks.5.attn.qkv.bias\n",
      "161: stages.2.blocks.5.attn.proj.weight\n",
      "162: stages.2.blocks.5.attn.proj.bias\n",
      "163: stages.2.blocks.5.norm1.weight\n",
      "164: stages.2.blocks.5.norm1.bias\n",
      "165: stages.2.blocks.5.mlp.fc1.weight\n",
      "166: stages.2.blocks.5.mlp.fc1.bias\n",
      "167: stages.2.blocks.5.mlp.fc2.weight\n",
      "168: stages.2.blocks.5.mlp.fc2.bias\n",
      "169: stages.2.blocks.5.norm2.weight\n",
      "170: stages.2.blocks.5.norm2.bias\n",
      "171: stages.2.downsample.reduction.weight\n",
      "172: stages.2.downsample.norm.weight\n",
      "173: stages.2.downsample.norm.bias\n",
      "174: stages.3.blocks.0.attn.t_scale\n",
      "175: stages.3.blocks.0.attn.crpb_mlp.0.weight\n",
      "176: stages.3.blocks.0.attn.crpb_mlp.0.bias\n",
      "177: stages.3.blocks.0.attn.crpb_mlp.3.weight\n",
      "178: stages.3.blocks.0.attn.qkv.weight\n",
      "179: stages.3.blocks.0.attn.qkv.bias\n",
      "180: stages.3.blocks.0.attn.proj.weight\n",
      "181: stages.3.blocks.0.attn.proj.bias\n",
      "182: stages.3.blocks.0.norm1.weight\n",
      "183: stages.3.blocks.0.norm1.bias\n",
      "184: stages.3.blocks.0.mlp.fc1.weight\n",
      "185: stages.3.blocks.0.mlp.fc1.bias\n",
      "186: stages.3.blocks.0.mlp.fc2.weight\n",
      "187: stages.3.blocks.0.mlp.fc2.bias\n",
      "188: stages.3.blocks.0.norm2.weight\n",
      "189: stages.3.blocks.0.norm2.bias\n",
      "190: stages.3.blocks.1.attn.t_scale\n",
      "191: stages.3.blocks.1.attn.crpb_mlp.0.weight\n",
      "192: stages.3.blocks.1.attn.crpb_mlp.0.bias\n",
      "193: stages.3.blocks.1.attn.crpb_mlp.3.weight\n",
      "194: stages.3.blocks.1.attn.qkv.weight\n",
      "195: stages.3.blocks.1.attn.qkv.bias\n",
      "196: stages.3.blocks.1.attn.proj.weight\n",
      "197: stages.3.blocks.1.attn.proj.bias\n",
      "198: stages.3.blocks.1.norm1.weight\n",
      "199: stages.3.blocks.1.norm1.bias\n",
      "200: stages.3.blocks.1.mlp.fc1.weight\n",
      "201: stages.3.blocks.1.mlp.fc1.bias\n",
      "202: stages.3.blocks.1.mlp.fc2.weight\n",
      "203: stages.3.blocks.1.mlp.fc2.bias\n",
      "204: stages.3.blocks.1.norm2.weight\n",
      "205: stages.3.blocks.1.norm2.bias\n",
      "206: layernorm.weight\n",
      "207: layernorm.bias\n",
      "208: classifier.weight\n",
      "209: classifier.bias\n"
     ]
    }
   ],
   "source": [
    "layer_names = []\n",
    "for i, (name, params) in enumerate(model.named_parameters()):\n",
    "    print(f'{i}: {name}')\n",
    "    layer_names.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['classifier.bias',\n",
       " 'classifier.weight',\n",
       " 'layernorm.bias',\n",
       " 'layernorm.weight',\n",
       " 'stages.3.blocks.1.norm2.bias']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_names.reverse()\n",
    "layer_names[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: classifier.bias's lr=0.0014\n",
      "1: classifier.weight's lr=0.0014\n",
      "2: layernorm.bias's lr=0.001218\n",
      "3: layernorm.weight's lr=0.001218\n",
      "4: stages.3.blocks.1.norm2.bias's lr=0.0010596599999999998\n",
      "5: stages.3.blocks.1.norm2.weight's lr=0.0010596599999999998\n",
      "6: stages.3.blocks.1.mlp.fc2.bias's lr=0.0010596599999999998\n",
      "7: stages.3.blocks.1.mlp.fc2.weight's lr=0.0010596599999999998\n",
      "8: stages.3.blocks.1.mlp.fc1.bias's lr=0.0010596599999999998\n",
      "9: stages.3.blocks.1.mlp.fc1.weight's lr=0.0010596599999999998\n",
      "10: stages.3.blocks.1.norm1.bias's lr=0.0010596599999999998\n",
      "11: stages.3.blocks.1.norm1.weight's lr=0.0010596599999999998\n",
      "12: stages.3.blocks.1.attn.proj.bias's lr=0.0010596599999999998\n",
      "13: stages.3.blocks.1.attn.proj.weight's lr=0.0010596599999999998\n",
      "14: stages.3.blocks.1.attn.qkv.bias's lr=0.0010596599999999998\n",
      "15: stages.3.blocks.1.attn.qkv.weight's lr=0.0010596599999999998\n",
      "16: stages.3.blocks.1.attn.crpb_mlp.3.weight's lr=0.0010596599999999998\n",
      "17: stages.3.blocks.1.attn.crpb_mlp.0.bias's lr=0.0010596599999999998\n",
      "18: stages.3.blocks.1.attn.crpb_mlp.0.weight's lr=0.0010596599999999998\n",
      "19: stages.3.blocks.1.attn.t_scale's lr=0.0010596599999999998\n",
      "20: stages.3.blocks.0.norm2.bias's lr=0.0010596599999999998\n",
      "21: stages.3.blocks.0.norm2.weight's lr=0.0010596599999999998\n",
      "22: stages.3.blocks.0.mlp.fc2.bias's lr=0.0010596599999999998\n",
      "23: stages.3.blocks.0.mlp.fc2.weight's lr=0.0010596599999999998\n",
      "24: stages.3.blocks.0.mlp.fc1.bias's lr=0.0010596599999999998\n",
      "25: stages.3.blocks.0.mlp.fc1.weight's lr=0.0010596599999999998\n",
      "26: stages.3.blocks.0.norm1.bias's lr=0.0010596599999999998\n",
      "27: stages.3.blocks.0.norm1.weight's lr=0.0010596599999999998\n",
      "28: stages.3.blocks.0.attn.proj.bias's lr=0.0010596599999999998\n",
      "29: stages.3.blocks.0.attn.proj.weight's lr=0.0010596599999999998\n",
      "30: stages.3.blocks.0.attn.qkv.bias's lr=0.0010596599999999998\n",
      "31: stages.3.blocks.0.attn.qkv.weight's lr=0.0010596599999999998\n",
      "32: stages.3.blocks.0.attn.crpb_mlp.3.weight's lr=0.0010596599999999998\n",
      "33: stages.3.blocks.0.attn.crpb_mlp.0.bias's lr=0.0010596599999999998\n",
      "34: stages.3.blocks.0.attn.crpb_mlp.0.weight's lr=0.0010596599999999998\n",
      "35: stages.3.blocks.0.attn.t_scale's lr=0.0010596599999999998\n",
      "36: stages.2.downsample.norm.bias's lr=0.0010596599999999998\n",
      "37: stages.2.downsample.norm.weight's lr=0.0010596599999999998\n",
      "38: stages.2.downsample.reduction.weight's lr=0.0010596599999999998\n",
      "39: stages.2.blocks.5.norm2.bias's lr=0.0010596599999999998\n",
      "40: stages.2.blocks.5.norm2.weight's lr=0.0010596599999999998\n",
      "41: stages.2.blocks.5.mlp.fc2.bias's lr=0.0010596599999999998\n",
      "42: stages.2.blocks.5.mlp.fc2.weight's lr=0.0010596599999999998\n",
      "43: stages.2.blocks.5.mlp.fc1.bias's lr=0.0010596599999999998\n",
      "44: stages.2.blocks.5.mlp.fc1.weight's lr=0.0010596599999999998\n",
      "45: stages.2.blocks.5.norm1.bias's lr=0.0010596599999999998\n",
      "46: stages.2.blocks.5.norm1.weight's lr=0.0010596599999999998\n",
      "47: stages.2.blocks.5.attn.proj.bias's lr=0.0010596599999999998\n",
      "48: stages.2.blocks.5.attn.proj.weight's lr=0.0010596599999999998\n",
      "49: stages.2.blocks.5.attn.qkv.bias's lr=0.0010596599999999998\n",
      "50: stages.2.blocks.5.attn.qkv.weight's lr=0.0010596599999999998\n",
      "51: stages.2.blocks.5.attn.crpb_mlp.3.weight's lr=0.0010596599999999998\n",
      "52: stages.2.blocks.5.attn.crpb_mlp.0.bias's lr=0.0010596599999999998\n",
      "53: stages.2.blocks.5.attn.crpb_mlp.0.weight's lr=0.0010596599999999998\n",
      "54: stages.2.blocks.5.attn.t_scale's lr=0.0010596599999999998\n",
      "55: stages.2.blocks.4.norm2.bias's lr=0.0010596599999999998\n",
      "56: stages.2.blocks.4.norm2.weight's lr=0.0010596599999999998\n",
      "57: stages.2.blocks.4.mlp.fc2.bias's lr=0.0010596599999999998\n",
      "58: stages.2.blocks.4.mlp.fc2.weight's lr=0.0010596599999999998\n",
      "59: stages.2.blocks.4.mlp.fc1.bias's lr=0.0010596599999999998\n",
      "60: stages.2.blocks.4.mlp.fc1.weight's lr=0.0010596599999999998\n",
      "61: stages.2.blocks.4.norm1.bias's lr=0.0010596599999999998\n",
      "62: stages.2.blocks.4.norm1.weight's lr=0.0010596599999999998\n",
      "63: stages.2.blocks.4.attn.proj.bias's lr=0.0010596599999999998\n",
      "64: stages.2.blocks.4.attn.proj.weight's lr=0.0010596599999999998\n",
      "65: stages.2.blocks.4.attn.qkv.bias's lr=0.0010596599999999998\n",
      "66: stages.2.blocks.4.attn.qkv.weight's lr=0.0010596599999999998\n",
      "67: stages.2.blocks.4.attn.crpb_mlp.3.weight's lr=0.0010596599999999998\n",
      "68: stages.2.blocks.4.attn.crpb_mlp.0.bias's lr=0.0010596599999999998\n",
      "69: stages.2.blocks.4.attn.crpb_mlp.0.weight's lr=0.0010596599999999998\n",
      "70: stages.2.blocks.4.attn.t_scale's lr=0.0010596599999999998\n",
      "71: stages.2.blocks.3.norm2.bias's lr=0.0010596599999999998\n",
      "72: stages.2.blocks.3.norm2.weight's lr=0.0010596599999999998\n",
      "73: stages.2.blocks.3.mlp.fc2.bias's lr=0.0010596599999999998\n",
      "74: stages.2.blocks.3.mlp.fc2.weight's lr=0.0010596599999999998\n",
      "75: stages.2.blocks.3.mlp.fc1.bias's lr=0.0010596599999999998\n",
      "76: stages.2.blocks.3.mlp.fc1.weight's lr=0.0010596599999999998\n",
      "77: stages.2.blocks.3.norm1.bias's lr=0.0010596599999999998\n",
      "78: stages.2.blocks.3.norm1.weight's lr=0.0010596599999999998\n",
      "79: stages.2.blocks.3.attn.proj.bias's lr=0.0010596599999999998\n",
      "80: stages.2.blocks.3.attn.proj.weight's lr=0.0010596599999999998\n",
      "81: stages.2.blocks.3.attn.qkv.bias's lr=0.0010596599999999998\n",
      "82: stages.2.blocks.3.attn.qkv.weight's lr=0.0010596599999999998\n",
      "83: stages.2.blocks.3.attn.crpb_mlp.3.weight's lr=0.0010596599999999998\n",
      "84: stages.2.blocks.3.attn.crpb_mlp.0.bias's lr=0.0010596599999999998\n",
      "85: stages.2.blocks.3.attn.crpb_mlp.0.weight's lr=0.0010596599999999998\n",
      "86: stages.2.blocks.3.attn.t_scale's lr=0.0010596599999999998\n",
      "87: stages.2.blocks.2.norm2.bias's lr=0.0010596599999999998\n",
      "88: stages.2.blocks.2.norm2.weight's lr=0.0010596599999999998\n",
      "89: stages.2.blocks.2.mlp.fc2.bias's lr=0.0010596599999999998\n",
      "90: stages.2.blocks.2.mlp.fc2.weight's lr=0.0010596599999999998\n",
      "91: stages.2.blocks.2.mlp.fc1.bias's lr=0.0010596599999999998\n",
      "92: stages.2.blocks.2.mlp.fc1.weight's lr=0.0010596599999999998\n",
      "93: stages.2.blocks.2.norm1.bias's lr=0.0010596599999999998\n",
      "94: stages.2.blocks.2.norm1.weight's lr=0.0010596599999999998\n",
      "95: stages.2.blocks.2.attn.proj.bias's lr=0.0010596599999999998\n",
      "96: stages.2.blocks.2.attn.proj.weight's lr=0.0010596599999999998\n",
      "97: stages.2.blocks.2.attn.qkv.bias's lr=0.0010596599999999998\n",
      "98: stages.2.blocks.2.attn.qkv.weight's lr=0.0010596599999999998\n",
      "99: stages.2.blocks.2.attn.crpb_mlp.3.weight's lr=0.0010596599999999998\n",
      "100: stages.2.blocks.2.attn.crpb_mlp.0.bias's lr=0.0010596599999999998\n",
      "101: stages.2.blocks.2.attn.crpb_mlp.0.weight's lr=0.0010596599999999998\n",
      "102: stages.2.blocks.2.attn.t_scale's lr=0.0010596599999999998\n",
      "103: stages.2.blocks.1.norm2.bias's lr=0.0010596599999999998\n",
      "104: stages.2.blocks.1.norm2.weight's lr=0.0010596599999999998\n",
      "105: stages.2.blocks.1.mlp.fc2.bias's lr=0.0010596599999999998\n",
      "106: stages.2.blocks.1.mlp.fc2.weight's lr=0.0010596599999999998\n",
      "107: stages.2.blocks.1.mlp.fc1.bias's lr=0.0010596599999999998\n",
      "108: stages.2.blocks.1.mlp.fc1.weight's lr=0.0010596599999999998\n",
      "109: stages.2.blocks.1.norm1.bias's lr=0.0010596599999999998\n",
      "110: stages.2.blocks.1.norm1.weight's lr=0.0010596599999999998\n",
      "111: stages.2.blocks.1.attn.proj.bias's lr=0.0010596599999999998\n",
      "112: stages.2.blocks.1.attn.proj.weight's lr=0.0010596599999999998\n",
      "113: stages.2.blocks.1.attn.qkv.bias's lr=0.0010596599999999998\n",
      "114: stages.2.blocks.1.attn.qkv.weight's lr=0.0010596599999999998\n",
      "115: stages.2.blocks.1.attn.crpb_mlp.3.weight's lr=0.0010596599999999998\n",
      "116: stages.2.blocks.1.attn.crpb_mlp.0.bias's lr=0.0010596599999999998\n",
      "117: stages.2.blocks.1.attn.crpb_mlp.0.weight's lr=0.0010596599999999998\n",
      "118: stages.2.blocks.1.attn.t_scale's lr=0.0010596599999999998\n",
      "119: stages.2.blocks.0.norm2.bias's lr=0.0010596599999999998\n",
      "120: stages.2.blocks.0.norm2.weight's lr=0.0010596599999999998\n",
      "121: stages.2.blocks.0.mlp.fc2.bias's lr=0.0010596599999999998\n",
      "122: stages.2.blocks.0.mlp.fc2.weight's lr=0.0010596599999999998\n",
      "123: stages.2.blocks.0.mlp.fc1.bias's lr=0.0010596599999999998\n",
      "124: stages.2.blocks.0.mlp.fc1.weight's lr=0.0010596599999999998\n",
      "125: stages.2.blocks.0.norm1.bias's lr=0.0010596599999999998\n",
      "126: stages.2.blocks.0.norm1.weight's lr=0.0010596599999999998\n",
      "127: stages.2.blocks.0.attn.proj.bias's lr=0.0010596599999999998\n",
      "128: stages.2.blocks.0.attn.proj.weight's lr=0.0010596599999999998\n",
      "129: stages.2.blocks.0.attn.qkv.bias's lr=0.0010596599999999998\n",
      "130: stages.2.blocks.0.attn.qkv.weight's lr=0.0010596599999999998\n",
      "131: stages.2.blocks.0.attn.crpb_mlp.3.weight's lr=0.0010596599999999998\n",
      "132: stages.2.blocks.0.attn.crpb_mlp.0.bias's lr=0.0010596599999999998\n",
      "133: stages.2.blocks.0.attn.crpb_mlp.0.weight's lr=0.0010596599999999998\n",
      "134: stages.2.blocks.0.attn.t_scale's lr=0.0010596599999999998\n",
      "135: stages.1.downsample.norm.bias's lr=0.0010596599999999998\n",
      "136: stages.1.downsample.norm.weight's lr=0.0010596599999999998\n",
      "137: stages.1.downsample.reduction.weight's lr=0.0010596599999999998\n",
      "138: stages.1.blocks.1.norm2.bias's lr=0.0010596599999999998\n",
      "139: stages.1.blocks.1.norm2.weight's lr=0.0010596599999999998\n",
      "140: stages.1.blocks.1.mlp.fc2.bias's lr=0.0010596599999999998\n",
      "141: stages.1.blocks.1.mlp.fc2.weight's lr=0.0010596599999999998\n",
      "142: stages.1.blocks.1.mlp.fc1.bias's lr=0.0010596599999999998\n",
      "143: stages.1.blocks.1.mlp.fc1.weight's lr=0.0010596599999999998\n",
      "144: stages.1.blocks.1.norm1.bias's lr=0.0010596599999999998\n",
      "145: stages.1.blocks.1.norm1.weight's lr=0.0010596599999999998\n",
      "146: stages.1.blocks.1.attn.proj.bias's lr=0.0010596599999999998\n",
      "147: stages.1.blocks.1.attn.proj.weight's lr=0.0010596599999999998\n",
      "148: stages.1.blocks.1.attn.qkv.bias's lr=0.0010596599999999998\n",
      "149: stages.1.blocks.1.attn.qkv.weight's lr=0.0010596599999999998\n",
      "150: stages.1.blocks.1.attn.crpb_mlp.3.weight's lr=0.0010596599999999998\n",
      "151: stages.1.blocks.1.attn.crpb_mlp.0.bias's lr=0.0010596599999999998\n",
      "152: stages.1.blocks.1.attn.crpb_mlp.0.weight's lr=0.0010596599999999998\n",
      "153: stages.1.blocks.1.attn.t_scale's lr=0.0010596599999999998\n",
      "154: stages.1.blocks.0.norm2.bias's lr=0.0010596599999999998\n",
      "155: stages.1.blocks.0.norm2.weight's lr=0.0010596599999999998\n",
      "156: stages.1.blocks.0.mlp.fc2.bias's lr=0.0010596599999999998\n",
      "157: stages.1.blocks.0.mlp.fc2.weight's lr=0.0010596599999999998\n",
      "158: stages.1.blocks.0.mlp.fc1.bias's lr=0.0010596599999999998\n",
      "159: stages.1.blocks.0.mlp.fc1.weight's lr=0.0010596599999999998\n",
      "160: stages.1.blocks.0.norm1.bias's lr=0.0010596599999999998\n",
      "161: stages.1.blocks.0.norm1.weight's lr=0.0010596599999999998\n",
      "162: stages.1.blocks.0.attn.proj.bias's lr=0.0010596599999999998\n",
      "163: stages.1.blocks.0.attn.proj.weight's lr=0.0010596599999999998\n",
      "164: stages.1.blocks.0.attn.qkv.bias's lr=0.0010596599999999998\n",
      "165: stages.1.blocks.0.attn.qkv.weight's lr=0.0010596599999999998\n",
      "166: stages.1.blocks.0.attn.crpb_mlp.3.weight's lr=0.0010596599999999998\n",
      "167: stages.1.blocks.0.attn.crpb_mlp.0.bias's lr=0.0010596599999999998\n",
      "168: stages.1.blocks.0.attn.crpb_mlp.0.weight's lr=0.0010596599999999998\n",
      "169: stages.1.blocks.0.attn.t_scale's lr=0.0010596599999999998\n",
      "170: stages.0.downsample.norm.bias's lr=0.0010596599999999998\n",
      "171: stages.0.downsample.norm.weight's lr=0.0010596599999999998\n",
      "172: stages.0.downsample.reduction.weight's lr=0.0010596599999999998\n",
      "173: stages.0.blocks.1.norm2.bias's lr=0.0010596599999999998\n",
      "174: stages.0.blocks.1.norm2.weight's lr=0.0010596599999999998\n",
      "175: stages.0.blocks.1.mlp.fc2.bias's lr=0.0010596599999999998\n",
      "176: stages.0.blocks.1.mlp.fc2.weight's lr=0.0010596599999999998\n",
      "177: stages.0.blocks.1.mlp.fc1.bias's lr=0.0010596599999999998\n",
      "178: stages.0.blocks.1.mlp.fc1.weight's lr=0.0010596599999999998\n",
      "179: stages.0.blocks.1.norm1.bias's lr=0.0010596599999999998\n",
      "180: stages.0.blocks.1.norm1.weight's lr=0.0010596599999999998\n",
      "181: stages.0.blocks.1.attn.proj.bias's lr=0.0010596599999999998\n",
      "182: stages.0.blocks.1.attn.proj.weight's lr=0.0010596599999999998\n",
      "183: stages.0.blocks.1.attn.qkv.bias's lr=0.0010596599999999998\n",
      "184: stages.0.blocks.1.attn.qkv.weight's lr=0.0010596599999999998\n",
      "185: stages.0.blocks.1.attn.crpb_mlp.3.weight's lr=0.0010596599999999998\n",
      "186: stages.0.blocks.1.attn.crpb_mlp.0.bias's lr=0.0010596599999999998\n",
      "187: stages.0.blocks.1.attn.crpb_mlp.0.weight's lr=0.0010596599999999998\n",
      "188: stages.0.blocks.1.attn.t_scale's lr=0.0010596599999999998\n",
      "189: stages.0.blocks.0.norm2.bias's lr=0.0010596599999999998\n",
      "190: stages.0.blocks.0.norm2.weight's lr=0.0010596599999999998\n",
      "191: stages.0.blocks.0.mlp.fc2.bias's lr=0.0010596599999999998\n",
      "192: stages.0.blocks.0.mlp.fc2.weight's lr=0.0010596599999999998\n",
      "193: stages.0.blocks.0.mlp.fc1.bias's lr=0.0010596599999999998\n",
      "194: stages.0.blocks.0.mlp.fc1.weight's lr=0.0010596599999999998\n",
      "195: stages.0.blocks.0.norm1.bias's lr=0.0010596599999999998\n",
      "196: stages.0.blocks.0.norm1.weight's lr=0.0010596599999999998\n",
      "197: stages.0.blocks.0.attn.proj.bias's lr=0.0010596599999999998\n",
      "198: stages.0.blocks.0.attn.proj.weight's lr=0.0010596599999999998\n",
      "199: stages.0.blocks.0.attn.qkv.bias's lr=0.0010596599999999998\n",
      "200: stages.0.blocks.0.attn.qkv.weight's lr=0.0010596599999999998\n",
      "201: stages.0.blocks.0.attn.crpb_mlp.3.weight's lr=0.0010596599999999998\n",
      "202: stages.0.blocks.0.attn.crpb_mlp.0.bias's lr=0.0010596599999999998\n",
      "203: stages.0.blocks.0.attn.crpb_mlp.0.weight's lr=0.0010596599999999998\n",
      "204: stages.0.blocks.0.attn.t_scale's lr=0.0010596599999999998\n",
      "205: embeddings.norm.bias's lr=0.0009219041999999998\n",
      "206: embeddings.norm.weight's lr=0.0009219041999999998\n",
      "207: embeddings.patch_embeddings.bias's lr=0.0009219041999999998\n",
      "208: embeddings.patch_embeddings.weight's lr=0.0009219041999999998\n",
      "209: absolute_pos_embed's lr=0.0008020566539999999\n"
     ]
    }
   ],
   "source": [
    "lr      = 1.4e-3   # paper : 1.4e-3\n",
    "lr_mult = 0.87     # paper : 0.87\n",
    "weight_decay = 0.1 # paper : 0.1\n",
    "\n",
    "param_groups = []\n",
    "prev_group_name = layer_names[0].split('.')[0]\n",
    "\n",
    "for idx, name in enumerate(layer_names):\n",
    "    \n",
    "    cur_group_name = name.split('.')[0]\n",
    "    \n",
    "    if cur_group_name != prev_group_name:\n",
    "        lr *= lr_mult\n",
    "    prev_group_name = cur_group_name\n",
    "    \n",
    "    print(f\"{idx}: {name}'s lr={lr}\")\n",
    "    \n",
    "    param_groups += [{'params': [ p for n, p in model.named_parameters() if n == name and p.requires_grad],\n",
    "                      'lr' : lr,\n",
    "                      'weight_decay': weight_decay}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-17 12:47:45.853747: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-17 12:47:45.853836: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-17 12:47:45.854726: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-17 12:47:45.860065: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-17 12:47:46.538634: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.AdamW(param_groups)\n",
    "warmup_steps = int(len(train_loader)*(epochs)*0.1)\n",
    "train_steps = len(train_loader)*(epochs)\n",
    "scheduler = transformers.get_cosine_schedule_with_warmup(optimizer, \n",
    "                                                        num_warmup_steps=warmup_steps, \n",
    "                                                        num_training_steps=train_steps,\n",
    "                                                        num_cycles=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 15/15 [01:02<00:00,  4.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.618670908610026, Val Loss: 4.50938081741333, LR: 9.333333333333333e-05, Duration: 63.63 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 15/15 [01:05<00:00,  4.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.475480238596599, Val Loss: 4.227418422698975, LR: 0.00018666666666666666, Duration: 66.46 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 15/15 [01:00<00:00,  4.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.2927678743998205, Val Loss: 3.9783058166503906, LR: 0.00028000000000000003, Duration: 62.49 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 15/15 [01:00<00:00,  4.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.185408624013265, Val Loss: 3.8510189056396484, LR: 0.0003733333333333333, Duration: 61.90 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 15/15 [01:01<00:00,  4.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.188698307673136, Val Loss: 3.777043104171753, LR: 0.00046666666666666666, Duration: 62.95 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 15/15 [00:59<00:00,  3.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.085565519332886, Val Loss: 3.612661123275757, LR: 0.0005600000000000001, Duration: 60.86 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 15/15 [00:59<00:00,  3.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.029887962341308, Val Loss: 3.502548933029175, LR: 0.0006533333333333333, Duration: 60.83 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 15/15 [01:01<00:00,  4.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.9596521059672036, Val Loss: 3.288982629776001, LR: 0.0007466666666666666, Duration: 62.93 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 15/15 [00:59<00:00,  3.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.8505250453948974, Val Loss: 3.460585832595825, LR: 0.0008399999999999999, Duration: 61.23 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 15/15 [00:58<00:00,  3.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.825633923212687, Val Loss: 3.17927622795105, LR: 0.0009333333333333333, Duration: 59.32 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 15/15 [01:00<00:00,  4.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.754436953862508, Val Loss: 3.032294750213623, LR: 0.0010266666666666666, Duration: 61.39 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 15/15 [01:03<00:00,  4.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.741052532196045, Val Loss: 2.830393075942993, LR: 0.0011200000000000001, Duration: 64.79 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 15/15 [01:01<00:00,  4.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.6179285685221356, Val Loss: 2.7576587200164795, LR: 0.0012133333333333334, Duration: 62.78 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 15/15 [01:01<00:00,  4.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.6987175941467285, Val Loss: 2.804995059967041, LR: 0.0013066666666666667, Duration: 62.87 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 15/15 [01:00<00:00,  4.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.7585471630096436, Val Loss: 2.7601146697998047, LR: 0.0014, Duration: 61.79 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 15/15 [00:59<00:00,  3.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.573187748591105, Val Loss: 2.6095640659332275, LR: 0.001399810468825623, Duration: 60.27 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 15/15 [01:04<00:00,  4.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.4878302574157716, Val Loss: 2.446174383163452, LR: 0.0013992419779369672, Duration: 65.48 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 15/15 [00:58<00:00,  3.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.732209348678589, Val Loss: 2.533973455429077, LR: 0.001398294835181877, Duration: 59.64 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 15/15 [01:04<00:00,  4.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.310075839360555, Val Loss: 2.334476947784424, LR: 0.001396969553454863, Duration: 65.74 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 15/15 [01:01<00:00,  4.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.242588011423747, Val Loss: 2.308940887451172, LR: 0.0013952668504193602, Duration: 63.01 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 15/15 [00:58<00:00,  3.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.5250309149424237, Val Loss: 2.233060121536255, LR: 0.0013931876481190993, Duration: 60.22 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 15/15 [01:00<00:00,  4.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.4456339200337727, Val Loss: 2.2011492252349854, LR: 0.0013907330724788056, Duration: 62.50 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 15/15 [01:00<00:00,  4.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.4249913374582928, Val Loss: 2.205648183822632, LR: 0.0013879044526944892, Duration: 61.55 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 15/15 [01:00<00:00,  4.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.2669747829437257, Val Loss: 2.1441659927368164, LR: 0.001384703320513664, Duration: 61.90 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 15/15 [00:57<00:00,  3.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.177278439203898, Val Loss: 2.045600175857544, LR: 0.0013811314094058767, Duration: 58.76 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 15/15 [00:56<00:00,  3.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.2295019467671713, Val Loss: 2.1202595233917236, LR: 0.0013771906536240047, Duration: 57.56 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 15/15 [00:54<00:00,  3.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.2944801330566404, Val Loss: 2.0036275386810303, LR: 0.0013728831871568231, Duration: 55.74 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 15/15 [00:56<00:00,  3.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.0105194250742593, Val Loss: 2.0051918029785156, LR: 0.0013682113425734124, Duration: 57.95 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 15/15 [00:56<00:00,  3.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.2194215297698974, Val Loss: 1.889381766319275, LR: 0.0013631776497600304, Duration: 58.06 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 15/15 [01:00<00:00,  4.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.0487961928049723, Val Loss: 1.8072423934936523, LR: 0.001357784834550136, Duration: 61.88 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|██████████| 15/15 [00:58<00:00,  3.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.8354305744171144, Val Loss: 1.7690962553024292, LR: 0.0013520358172482998, Duration: 59.22 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|██████████| 15/15 [00:55<00:00,  3.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.2498710632324217, Val Loss: 1.817293643951416, LR: 0.0013459337110488096, Duration: 56.33 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|██████████| 15/15 [01:00<00:00,  4.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.0212666829427084, Val Loss: 1.7167208194732666, LR: 0.0013394818203498204, Duration: 61.27 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|██████████| 15/15 [01:00<00:00,  4.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.2317893664042154, Val Loss: 1.742553472518921, LR: 0.0013326836389639645, Duration: 61.37 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|██████████| 15/15 [00:55<00:00,  3.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6526897112528482, Val Loss: 1.574305772781372, LR: 0.0013255428482263885, Duration: 56.83 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|██████████| 15/15 [01:02<00:00,  4.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.838868014017741, Val Loss: 1.608404278755188, LR: 0.0013180633150012488, Duration: 63.50 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37: 100%|██████████| 15/15 [00:58<00:00,  3.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.4213622570037843, Val Loss: 1.8732954263687134, LR: 0.0013102490895877336, Duration: 59.61 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|██████████| 15/15 [00:58<00:00,  3.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.3886288166046143, Val Loss: 1.7355008125305176, LR: 0.001302104403526756, Duration: 59.84 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|██████████| 15/15 [00:58<00:00,  3.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.9134287357330324, Val Loss: 1.5940375328063965, LR: 0.001293633667309498, Duration: 59.39 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40: 100%|██████████| 15/15 [00:58<00:00,  3.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.134367911020915, Val Loss: 1.6034857034683228, LR: 0.0012848414679890556, Duration: 59.26 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41: 100%|██████████| 15/15 [00:58<00:00,  3.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.9405723889668782, Val Loss: 1.6007782220840454, LR: 0.0012757325666964635, Duration: 60.16 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42: 100%|██████████| 15/15 [01:00<00:00,  4.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.133755620320638, Val Loss: 1.6268718242645264, LR: 0.0012663118960624632, Duration: 62.49 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43: 100%|██████████| 15/15 [01:01<00:00,  4.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.966562541325887, Val Loss: 1.5443663597106934, LR: 0.0012565845575463934, Duration: 62.85 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44: 100%|██████████| 15/15 [00:58<00:00,  3.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.1091596603393556, Val Loss: 1.4896876811981201, LR: 0.0012465558186736615, Duration: 59.47 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45: 100%|██████████| 15/15 [01:03<00:00,  4.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.047870127360026, Val Loss: 1.5520991086959839, LR: 0.0012362311101832846, Duration: 64.56 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46: 100%|██████████| 15/15 [00:59<00:00,  3.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.1140448888142904, Val Loss: 1.4458914995193481, LR: 0.0012256160230870495, Duration: 60.69 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47: 100%|██████████| 15/15 [00:54<00:00,  3.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.883157555262248, Val Loss: 1.4560774564743042, LR: 0.00121471630564188, Duration: 55.84 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48: 100%|██████████| 15/15 [01:00<00:00,  4.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.8926443735758465, Val Loss: 1.4176427125930786, LR: 0.0012035378602370558, Duration: 62.06 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 15/15 [00:59<00:00,  3.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.8409900029500323, Val Loss: 1.374856948852539, LR: 0.0011920867401979632, Duration: 62.40 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50: 100%|██████████| 15/15 [00:59<00:00,  3.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.8802491346995036, Val Loss: 1.3704078197479248, LR: 0.0011803691465081135, Duration: 61.13 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51: 100%|██████████| 15/15 [00:55<00:00,  3.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.9775190353393555, Val Loss: 1.3606295585632324, LR: 0.0011683914244512007, Duration: 57.38 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52: 100%|██████████| 15/15 [00:58<00:00,  3.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6950021107991535, Val Loss: 1.2675637006759644, LR: 0.0011561600601750187, Duration: 59.52 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53: 100%|██████████| 15/15 [00:58<00:00,  3.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.7900563716888427, Val Loss: 1.2465718984603882, LR: 0.001143681677179097, Duration: 60.60 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54: 100%|██████████| 15/15 [00:58<00:00,  3.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.9156020164489744, Val Loss: 1.2959985733032227, LR: 0.0011309630327279608, Duration: 60.07 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55: 100%|██████████| 15/15 [00:58<00:00,  3.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.8584819793701173, Val Loss: 1.2863301038742065, LR: 0.0011180110141919503, Duration: 59.19 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56: 100%|██████████| 15/15 [01:05<00:00,  4.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.8342790285746258, Val Loss: 1.3014600276947021, LR: 0.0011048326353175905, Duration: 66.46 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57: 100%|██████████| 15/15 [01:00<00:00,  4.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.820316743850708, Val Loss: 1.2739380598068237, LR: 0.0010914350324295228, Duration: 61.86 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58: 100%|██████████| 15/15 [00:56<00:00,  3.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.7934648672739666, Val Loss: 1.204862117767334, LR: 0.0010778254605660592, Duration: 57.64 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59: 100%|██████████| 15/15 [00:59<00:00,  3.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.7818673054377236, Val Loss: 1.1861522197723389, LR: 0.0010640112895504506, Duration: 61.13 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60: 100%|██████████| 15/15 [01:02<00:00,  4.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.06317302385966, Val Loss: 1.2389298677444458, LR: 0.00105, Duration: 63.44 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61: 100%|██████████| 15/15 [00:58<00:00,  3.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.8664756615956626, Val Loss: 1.1812207698822021, LR: 0.0010357991792751724, Duration: 60.14 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62: 100%|██████████| 15/15 [00:58<00:00,  3.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.659439547856649, Val Loss: 1.1732105016708374, LR: 0.001021416517370908, Duration: 59.39 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63: 100%|██████████| 15/15 [00:51<00:00,  3.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5793521801630654, Val Loss: 1.1375582218170166, LR: 0.001006859802752354, Duration: 52.94 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64: 100%|██████████| 15/15 [00:50<00:00,  3.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4882713158925376, Val Loss: 1.1222496032714844, LR: 0.0009921369181372726, Duration: 51.58 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65: 100%|██████████| 15/15 [00:50<00:00,  3.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.393323795000712, Val Loss: 1.0877681970596313, LR: 0.0009772558362274098, Duration: 51.18 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66: 100%|██████████| 15/15 [00:49<00:00,  3.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.838650377591451, Val Loss: 1.1629525423049927, LR: 0.0009622246153911386, Duration: 50.83 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67: 100%|██████████| 15/15 [00:49<00:00,  3.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.66704204082489, Val Loss: 1.115017294883728, LR: 0.0009470513952997081, Duration: 50.85 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68: 100%|██████████| 15/15 [00:49<00:00,  3.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.486530327796936, Val Loss: 1.067845344543457, LR: 0.0009317443925194707, Duration: 50.95 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69: 100%|██████████| 15/15 [00:49<00:00,  3.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.429682795206706, Val Loss: 1.0365954637527466, LR: 0.0009163118960624632, Duration: 50.87 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70: 100%|██████████| 15/15 [00:49<00:00,  3.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4157555103302, Val Loss: 1.0680855512619019, LR: 0.0009007622628977632, Duration: 50.62 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71: 100%|██████████| 15/15 [00:49<00:00,  3.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2048722823460896, Val Loss: 1.0170879364013672, LR: 0.0008851039134260417, Duration: 50.84 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72: 100%|██████████| 15/15 [00:49<00:00,  3.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.002666393915812, Val Loss: 1.0887179374694824, LR: 0.0008693453269197673, Duration: 50.88 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 73: 100%|██████████| 15/15 [00:49<00:00,  3.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4390547116597494, Val Loss: 1.0237997770309448, LR: 0.0008534950369315323, Duration: 50.56 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 74: 100%|██████████| 15/15 [00:49<00:00,  3.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.710242676734924, Val Loss: 1.0486034154891968, LR: 0.0008375616266729811, Duration: 50.56 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75: 100%|██████████| 15/15 [00:49<00:00,  3.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.692782505353292, Val Loss: 1.0186266899108887, LR: 0.0008215537243668514, Duration: 50.49 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 76: 100%|██████████| 15/15 [00:49<00:00,  3.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.28188849290212, Val Loss: 0.950322151184082, LR: 0.0008054799985746381, Duration: 50.75 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 77: 100%|██████████| 15/15 [00:49<00:00,  3.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4247338930765787, Val Loss: 1.0238053798675537, LR: 0.0007893491535024164, Duration: 50.74 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 78: 100%|██████████| 15/15 [00:49<00:00,  3.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.255471436182658, Val Loss: 0.927241325378418, LR: 0.0007731699242873575, Duration: 50.96 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 79: 100%|██████████| 15/15 [00:49<00:00,  3.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.380099352200826, Val Loss: 0.9659157395362854, LR: 0.0007569510722675008, Duration: 50.56 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 80: 100%|██████████| 15/15 [00:49<00:00,  3.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3037455320358275, Val Loss: 0.9601109623908997, LR: 0.000740701380237333, Duration: 50.32 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 81: 100%|██████████| 15/15 [00:49<00:00,  3.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.806292017300924, Val Loss: 1.0403941869735718, LR: 0.0007244296476917508, Duration: 50.56 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 82: 100%|██████████| 15/15 [00:49<00:00,  3.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.9640923579533895, Val Loss: 1.0752451419830322, LR: 0.0007081446860609781, Duration: 50.32 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 83: 100%|██████████| 15/15 [00:49<00:00,  3.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.579941169420878, Val Loss: 1.0344582796096802, LR: 0.0006918553139390222, Duration: 50.51 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 84: 100%|██████████| 15/15 [00:49<00:00,  3.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.421908235549927, Val Loss: 1.0204849243164062, LR: 0.0006755703523082495, Duration: 50.39 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 85: 100%|██████████| 15/15 [00:49<00:00,  3.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6780601660410563, Val Loss: 0.9489096403121948, LR: 0.000659298619762667, Duration: 50.58 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 86: 100%|██████████| 15/15 [00:49<00:00,  3.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1804168383280436, Val Loss: 0.9087435007095337, LR: 0.0006430489277324992, Duration: 50.80 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 87: 100%|██████████| 15/15 [00:49<00:00,  3.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.655252448717753, Val Loss: 0.9199017286300659, LR: 0.0006268300757126426, Duration: 50.65 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 88: 100%|██████████| 15/15 [00:49<00:00,  3.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.232796859741211, Val Loss: 0.930530309677124, LR: 0.0006106508464975837, Duration: 50.53 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 89: 100%|██████████| 15/15 [00:49<00:00,  3.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2809815406799316, Val Loss: 0.9093874096870422, LR: 0.0005945200014253619, Duration: 50.55 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 90: 100%|██████████| 15/15 [00:49<00:00,  3.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.343953498204549, Val Loss: 0.858731746673584, LR: 0.0005784462756331488, Duration: 50.68 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 91: 100%|██████████| 15/15 [00:49<00:00,  3.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4771473884582518, Val Loss: 0.9284597039222717, LR: 0.0005624383733270188, Duration: 50.77 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 92: 100%|██████████| 15/15 [00:53<00:00,  3.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.144125763575236, Val Loss: 0.8835548758506775, LR: 0.0005465049630684676, Duration: 54.92 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 93: 100%|██████████| 15/15 [00:58<00:00,  3.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.482381478945414, Val Loss: 0.8624404072761536, LR: 0.0005306546730802327, Duration: 59.66 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 94: 100%|██████████| 15/15 [00:59<00:00,  3.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.499165097872416, Val Loss: 0.8871961236000061, LR: 0.0005148960865739587, Duration: 60.52 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 95: 100%|██████████| 15/15 [00:58<00:00,  3.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1978412230809528, Val Loss: 0.8428054451942444, LR: 0.000499237737102237, Duration: 60.67 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 96: 100%|██████████| 15/15 [00:58<00:00,  3.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0911384264628095, Val Loss: 0.8299172520637512, LR: 0.0004836881039375369, Duration: 59.75 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 97: 100%|██████████| 15/15 [00:58<00:00,  3.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.446674100557963, Val Loss: 0.8384301066398621, LR: 0.0004682556074805294, Duration: 59.37 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 98: 100%|██████████| 15/15 [00:58<00:00,  3.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.268972651163737, Val Loss: 0.899932861328125, LR: 0.00045294860470029185, Duration: 59.66 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 15/15 [00:58<00:00,  3.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2860596974690757, Val Loss: 0.8425471186637878, LR: 0.0004377753846088615, Duration: 59.63 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 100: 100%|██████████| 15/15 [00:57<00:00,  3.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3145973682403564, Val Loss: 0.831217885017395, LR: 0.0004227441637725902, Duration: 58.08 sec\n",
      "Epoch 당 평균 소요시간 : 38.58초\n"
     ]
    }
   ],
   "source": [
    "training_time = 0\n",
    "losses = []\n",
    "val_losses = []\n",
    "lrs = []\n",
    "best_loss = float('inf')\n",
    "\n",
    "# GradScaler 초기화\n",
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch + 1}\")\n",
    "    \n",
    "    for _, data in pbar:\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        inputs, labels = mixup_fn(inputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # AutoCast 적용\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "        # 스케일링된 그라디언트 계산\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # 그라디언트 클리핑 전에 스케일링 제거\n",
    "        scaler.unscale_(optimizer)\n",
    "        clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "\n",
    "        # 옵티마이저 스텝 및 스케일러 업데이트\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "            \n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        lrs.append(lr)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)        \n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in valid_loader:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    val_loss /= len(valid_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # 모델 저장\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        model_save = True\n",
    "        if model_save:\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_duration = time.time() - start_time\n",
    "    training_time += epoch_duration\n",
    "    \n",
    "    text = f'\\tLoss: {epoch_loss}, Val Loss: {val_loss}, LR: {lr}, Duration: {epoch_duration:.2f} sec'\n",
    "    \n",
    "    if model_save:\n",
    "        text += f' - model saved!'\n",
    "        model_save = False\n",
    "\n",
    "    print(text)\n",
    "        \n",
    "text = f\"Epoch 당 평균 소요시간 : {training_time / epochs:.2f}초\"      \n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.834000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.853119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Recall</td>\n",
       "      <td>0.834000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F1 Score</td>\n",
       "      <td>0.824243</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Metric     Value\n",
       "0   Accuracy  0.834000\n",
       "1  Precision  0.853119\n",
       "2     Recall  0.834000\n",
       "3   F1 Score  0.824243"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예측 수행 및 레이블 저장\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# 혼동 행렬 생성\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# 예측과 실제 레이블\n",
    "y_true = all_labels  # 실제 레이블\n",
    "y_pred = all_preds  # 모델에 의해 예측된 레이블\n",
    "\n",
    "# 전체 데이터셋에 대한 정확도\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# 평균 정밀도, 리콜, F1-Score ('weighted')\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "\n",
    "# 판다스 데이터프레임으로 결과 정리\n",
    "performance_metrics = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "    'Value': [accuracy, precision, recall, f1_score]\n",
    "})\n",
    "\n",
    "# 데이터프레임 출력\n",
    "performance_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 15/15 [00:58<00:00,  3.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.315361825625102, Val Loss: 0.8361165523529053, LR: 0.0004078630818627275, Duration: 59.24 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 15/15 [00:56<00:00,  3.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.142416755358378, Val Loss: 0.8118091225624084, LR: 0.00039314019724764573, Duration: 57.30 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 15/15 [01:00<00:00,  4.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.14756293296814, Val Loss: 0.7851738333702087, LR: 0.0003785834826290917, Duration: 62.14 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 15/15 [01:04<00:00,  4.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0908672491709392, Val Loss: 0.8175212144851685, LR: 0.00036420082072482785, Duration: 64.99 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 15/15 [00:56<00:00,  3.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1895643711090087, Val Loss: 0.7927206754684448, LR: 0.00035000000000000016, Duration: 57.64 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 15/15 [00:57<00:00,  3.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2824945131937664, Val Loss: 0.7657453417778015, LR: 0.00033598871044954924, Duration: 58.80 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 15/15 [01:01<00:00,  4.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5321172078450522, Val Loss: 0.8085688352584839, LR: 0.0003221745394339412, Duration: 63.01 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 15/15 [00:59<00:00,  3.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.132683841387431, Val Loss: 0.780788242816925, LR: 0.0003085649675704773, Duration: 60.19 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 15/15 [01:02<00:00,  4.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.361740795771281, Val Loss: 0.7864488363265991, LR: 0.0002951673646824094, Duration: 63.09 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 15/15 [00:57<00:00,  3.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1840132077534995, Val Loss: 0.8022369146347046, LR: 0.0002819889858080498, Duration: 58.38 sec\n",
      "Epoch 당 평균 소요시간 : 42.61초\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.855460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Recall</td>\n",
       "      <td>0.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F1 Score</td>\n",
       "      <td>0.829451</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Metric     Value\n",
       "0   Accuracy  0.840000\n",
       "1  Precision  0.855460\n",
       "2     Recall  0.840000\n",
       "3   F1 Score  0.829451"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch + 1}\")\n",
    "    \n",
    "    for _, data in pbar:\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        inputs, labels = mixup_fn(inputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # AutoCast 적용\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "        # 스케일링된 그라디언트 계산\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # 그라디언트 클리핑 전에 스케일링 제거\n",
    "        scaler.unscale_(optimizer)\n",
    "        clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "\n",
    "        # 옵티마이저 스텝 및 스케일러 업데이트\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "            \n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        lrs.append(lr)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)        \n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in valid_loader:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    val_loss /= len(valid_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # 모델 저장\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        model_save = True\n",
    "        if model_save:\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_duration = time.time() - start_time\n",
    "    training_time += epoch_duration\n",
    "    \n",
    "    text = f'\\tLoss: {epoch_loss}, Val Loss: {val_loss}, LR: {lr}, Duration: {epoch_duration:.2f} sec'\n",
    "    \n",
    "    if model_save:\n",
    "        text += f' - model saved!'\n",
    "        model_save = False\n",
    "\n",
    "    print(text)\n",
    "        \n",
    "text = f\"Epoch 당 평균 소요시간 : {training_time / epochs:.2f}초\"      \n",
    "print(text)\n",
    "\n",
    "# 예측 수행 및 레이블 저장\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# 혼동 행렬 생성\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# 예측과 실제 레이블\n",
    "y_true = all_labels  # 실제 레이블\n",
    "y_pred = all_preds  # 모델에 의해 예측된 레이블\n",
    "\n",
    "# 전체 데이터셋에 대한 정확도\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# 평균 정밀도, 리콜, F1-Score ('weighted')\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "\n",
    "# 판다스 데이터프레임으로 결과 정리\n",
    "performance_metrics = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "    'Value': [accuracy, precision, recall, f1_score]\n",
    "})\n",
    "\n",
    "# 데이터프레임 출력\n",
    "performance_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 15/15 [01:00<00:00,  4.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2435073614120484, Val Loss: 0.7829024195671082, LR: 0.0002690369672720392, Duration: 61.46 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 15/15 [01:02<00:00,  4.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.25953631401062, Val Loss: 0.7946351170539856, LR: 0.0002563183228209027, Duration: 63.61 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 15/15 [00:57<00:00,  3.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.305389920870463, Val Loss: 0.7772101759910583, LR: 0.00024383993982498138, Duration: 58.82 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 15/15 [01:01<00:00,  4.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.137585417429606, Val Loss: 0.7772945761680603, LR: 0.00023160857554879947, Duration: 62.73 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 15/15 [01:00<00:00,  4.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.368615468343099, Val Loss: 0.7883490324020386, LR: 0.00021963085349188655, Duration: 62.02 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 15/15 [00:58<00:00,  3.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.8953270355860392, Val Loss: 0.7483291029930115, LR: 0.00020791325980203676, Duration: 59.30 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 15/15 [01:03<00:00,  4.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.25655513604482, Val Loss: 0.7678532004356384, LR: 0.00019646213976294433, Duration: 64.27 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 15/15 [00:56<00:00,  3.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.063239057858785, Val Loss: 0.7575422525405884, LR: 0.00018528369435812, Duration: 58.90 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 15/15 [00:55<00:00,  3.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3562647104263306, Val Loss: 0.7664015889167786, LR: 0.00017438397691295056, Duration: 56.80 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 15/15 [00:59<00:00,  3.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.9999084313710531, Val Loss: 0.7568098902702332, LR: 0.00016376888981671546, Duration: 60.53 sec\n",
      "Epoch 당 평균 소요시간 : 46.67초\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.862000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.890222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Recall</td>\n",
       "      <td>0.862000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F1 Score</td>\n",
       "      <td>0.852728</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Metric     Value\n",
       "0   Accuracy  0.862000\n",
       "1  Precision  0.890222\n",
       "2     Recall  0.862000\n",
       "3   F1 Score  0.852728"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch + 1}\")\n",
    "    \n",
    "    for _, data in pbar:\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        inputs, labels = mixup_fn(inputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # AutoCast 적용\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "        # 스케일링된 그라디언트 계산\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # 그라디언트 클리핑 전에 스케일링 제거\n",
    "        scaler.unscale_(optimizer)\n",
    "        clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "\n",
    "        # 옵티마이저 스텝 및 스케일러 업데이트\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "            \n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        lrs.append(lr)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)        \n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in valid_loader:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    val_loss /= len(valid_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # 모델 저장\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        model_save = True\n",
    "        if model_save:\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_duration = time.time() - start_time\n",
    "    training_time += epoch_duration\n",
    "    \n",
    "    text = f'\\tLoss: {epoch_loss}, Val Loss: {val_loss}, LR: {lr}, Duration: {epoch_duration:.2f} sec'\n",
    "    \n",
    "    if model_save:\n",
    "        text += f' - model saved!'\n",
    "        model_save = False\n",
    "\n",
    "    print(text)\n",
    "        \n",
    "text = f\"Epoch 당 평균 소요시간 : {training_time / epochs:.2f}초\"      \n",
    "print(text)\n",
    "\n",
    "# 예측 수행 및 레이블 저장\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# 혼동 행렬 생성\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# 예측과 실제 레이블\n",
    "y_true = all_labels  # 실제 레이블\n",
    "y_pred = all_preds  # 모델에 의해 예측된 레이블\n",
    "\n",
    "# 전체 데이터셋에 대한 정확도\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# 평균 정밀도, 리콜, F1-Score ('weighted')\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "\n",
    "# 판다스 데이터프레임으로 결과 정리\n",
    "performance_metrics = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "    'Value': [accuracy, precision, recall, f1_score]\n",
    "})\n",
    "\n",
    "# 데이터프레임 출력\n",
    "performance_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 15/15 [00:59<00:00,  4.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2095502773920694, Val Loss: 0.7891548871994019, LR: 0.00015344418132633855, Duration: 61.00 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 15/15 [01:04<00:00,  4.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.330418340365092, Val Loss: 0.7789267897605896, LR: 0.00014341544245360648, Duration: 65.41 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 15/15 [00:57<00:00,  3.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.02625040213267, Val Loss: 0.760909914970398, LR: 0.00013368810393753685, Duration: 59.31 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 15/15 [00:58<00:00,  3.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.8167444864908855, Val Loss: 0.7409899234771729, LR: 0.00012426743330353637, Duration: 60.33 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 15/15 [00:58<00:00,  3.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1454599539438886, Val Loss: 0.7635921239852905, LR: 0.00011515853201094461, Duration: 59.82 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 15/15 [00:55<00:00,  3.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.549266878763835, Val Loss: 0.792511522769928, LR: 0.00010636633269050183, Duration: 56.96 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 15/15 [01:01<00:00,  4.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.214522385597229, Val Loss: 0.7527441382408142, LR: 9.789559647324434e-05, Duration: 62.39 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 15/15 [00:58<00:00,  3.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.8735572973887125, Val Loss: 0.7425175905227661, LR: 8.97509104122664e-05, Duration: 59.79 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 15/15 [01:03<00:00,  4.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.092120313644409, Val Loss: 0.7583897709846497, LR: 8.19366849987511e-05, Duration: 64.89 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 15/15 [01:04<00:00,  4.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.9113513549168906, Val Loss: 0.7489486932754517, LR: 7.445715177361148e-05, Duration: 65.37 sec\n",
      "Epoch 당 평균 소요시간 : 50.77초\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.870000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.896099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Recall</td>\n",
       "      <td>0.870000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F1 Score</td>\n",
       "      <td>0.861196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Metric     Value\n",
       "0   Accuracy  0.870000\n",
       "1  Precision  0.896099\n",
       "2     Recall  0.870000\n",
       "3   F1 Score  0.861196"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch + 1}\")\n",
    "    \n",
    "    for _, data in pbar:\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        inputs, labels = mixup_fn(inputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # AutoCast 적용\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "        # 스케일링된 그라디언트 계산\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # 그라디언트 클리핑 전에 스케일링 제거\n",
    "        scaler.unscale_(optimizer)\n",
    "        clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "\n",
    "        # 옵티마이저 스텝 및 스케일러 업데이트\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "            \n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        lrs.append(lr)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)        \n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in valid_loader:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    val_loss /= len(valid_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # 모델 저장\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        model_save = True\n",
    "        if model_save:\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_duration = time.time() - start_time\n",
    "    training_time += epoch_duration\n",
    "    \n",
    "    text = f'\\tLoss: {epoch_loss}, Val Loss: {val_loss}, LR: {lr}, Duration: {epoch_duration:.2f} sec'\n",
    "    \n",
    "    if model_save:\n",
    "        text += f' - model saved!'\n",
    "        model_save = False\n",
    "\n",
    "    print(text)\n",
    "        \n",
    "text = f\"Epoch 당 평균 소요시간 : {training_time / epochs:.2f}초\"      \n",
    "print(text)\n",
    "\n",
    "# 예측 수행 및 레이블 저장\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# 혼동 행렬 생성\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# 예측과 실제 레이블\n",
    "y_true = all_labels  # 실제 레이블\n",
    "y_pred = all_preds  # 모델에 의해 예측된 레이블\n",
    "\n",
    "# 전체 데이터셋에 대한 정확도\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# 평균 정밀도, 리콜, F1-Score ('weighted')\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "\n",
    "# 판다스 데이터프레임으로 결과 정리\n",
    "performance_metrics = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "    'Value': [accuracy, precision, recall, f1_score]\n",
    "})\n",
    "\n",
    "# 데이터프레임 출력\n",
    "performance_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 15/15 [01:01<00:00,  4.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1929554382960004, Val Loss: 0.760482132434845, LR: 6.731636103603565e-05, Duration: 62.27 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 15/15 [01:00<00:00,  4.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.006266164779663, Val Loss: 0.7502516508102417, LR: 6.0518179650179314e-05, Duration: 61.48 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 15/15 [00:58<00:00,  3.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0040521303812664, Val Loss: 0.747874915599823, LR: 5.406628895119039e-05, Duration: 59.96 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 15/15 [00:58<00:00,  3.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.281879774729411, Val Loss: 0.7516612410545349, LR: 4.796418275170029e-05, Duration: 59.38 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 15/15 [00:57<00:00,  3.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1801023562749227, Val Loss: 0.7568996548652649, LR: 4.221516544986418e-05, Duration: 58.47 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 15/15 [01:00<00:00,  4.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.412435706456502, Val Loss: 0.7568815350532532, LR: 3.682235023996956e-05, Duration: 61.47 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 15/15 [01:00<00:00,  4.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.21396918296814, Val Loss: 0.7594372630119324, LR: 3.1788657426587664e-05, Duration: 61.86 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 15/15 [01:01<00:00,  4.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.620674236615499, Val Loss: 0.7634710073471069, LR: 2.7116812843176773e-05, Duration: 63.03 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 15/15 [00:55<00:00,  3.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.9375934680302938, Val Loss: 0.7562308311462402, LR: 2.280934637599534e-05, Duration: 57.25 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 15/15 [00:58<00:00,  3.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.926137924194336, Val Loss: 0.7489646673202515, LR: 1.8868590594123336e-05, Duration: 59.65 sec\n",
      "Epoch 당 평균 소요시간 : 54.80초\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.854000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.869440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Recall</td>\n",
       "      <td>0.854000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F1 Score</td>\n",
       "      <td>0.840944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Metric     Value\n",
       "0   Accuracy  0.854000\n",
       "1  Precision  0.869440\n",
       "2     Recall  0.854000\n",
       "3   F1 Score  0.840944"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch + 1}\")\n",
    "    \n",
    "    for _, data in pbar:\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        inputs, labels = mixup_fn(inputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # AutoCast 적용\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "        # 스케일링된 그라디언트 계산\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # 그라디언트 클리핑 전에 스케일링 제거\n",
    "        scaler.unscale_(optimizer)\n",
    "        clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "\n",
    "        # 옵티마이저 스텝 및 스케일러 업데이트\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "            \n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        lrs.append(lr)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)        \n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in valid_loader:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    val_loss /= len(valid_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # 모델 저장\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        model_save = True\n",
    "        if model_save:\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_duration = time.time() - start_time\n",
    "    training_time += epoch_duration\n",
    "    \n",
    "    text = f'\\tLoss: {epoch_loss}, Val Loss: {val_loss}, LR: {lr}, Duration: {epoch_duration:.2f} sec'\n",
    "    \n",
    "    if model_save:\n",
    "        text += f' - model saved!'\n",
    "        model_save = False\n",
    "\n",
    "    print(text)\n",
    "        \n",
    "text = f\"Epoch 당 평균 소요시간 : {training_time / epochs:.2f}초\"      \n",
    "print(text)\n",
    "\n",
    "# 예측 수행 및 레이블 저장\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# 혼동 행렬 생성\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# 예측과 실제 레이블\n",
    "y_true = all_labels  # 실제 레이블\n",
    "y_pred = all_preds  # 모델에 의해 예측된 레이블\n",
    "\n",
    "# 전체 데이터셋에 대한 정확도\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# 평균 정밀도, 리콜, F1-Score ('weighted')\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "\n",
    "# 판다스 데이터프레임으로 결과 정리\n",
    "performance_metrics = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "    'Value': [accuracy, precision, recall, f1_score]\n",
    "})\n",
    "\n",
    "# 데이터프레임 출력\n",
    "performance_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 15/15 [00:59<00:00,  3.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2802740414937337, Val Loss: 0.7501404285430908, LR: 1.5296679486336016e-05, Duration: 60.10 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 15/15 [01:02<00:00,  4.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.7234619537989297, Val Loss: 0.7468369007110596, LR: 1.2095547305510733e-05, Duration: 63.02 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 15/15 [01:00<00:00,  4.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.332926615079244, Val Loss: 0.7438059449195862, LR: 9.266927521194546e-06, Duration: 61.31 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 15/15 [01:03<00:00,  4.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1915342887242635, Val Loss: 0.7457473874092102, LR: 6.812351880900747e-06, Duration: 64.10 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 15/15 [00:59<00:00,  3.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2751359860102336, Val Loss: 0.7452839016914368, LR: 4.733149580639884e-06, Duration: 60.90 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 15/15 [00:59<00:00,  3.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.9821059465408326, Val Loss: 0.7443011403083801, LR: 3.0304465451369555e-06, Duration: 61.26 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 15/15 [00:59<00:00,  3.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.409575843811035, Val Loss: 0.743391215801239, LR: 1.7051648181230617e-06, Duration: 60.04 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 15/15 [01:00<00:00,  4.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2470608154932656, Val Loss: 0.7438032627105713, LR: 7.580220630328039e-07, Duration: 61.65 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 15/15 [01:03<00:00,  4.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0547617197036745, Val Loss: 0.7437141537666321, LR: 1.8953117437680023e-07, Duration: 64.09 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 15/15 [01:01<00:00,  4.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0507237116495767, Val Loss: 0.7436926960945129, LR: 0.0, Duration: 62.97 sec\n",
      "Epoch 당 평균 소요시간 : 58.93초\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.862000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.876028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Recall</td>\n",
       "      <td>0.862000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F1 Score</td>\n",
       "      <td>0.849429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Metric     Value\n",
       "0   Accuracy  0.862000\n",
       "1  Precision  0.876028\n",
       "2     Recall  0.862000\n",
       "3   F1 Score  0.849429"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch + 1}\")\n",
    "    \n",
    "    for _, data in pbar:\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        inputs, labels = mixup_fn(inputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # AutoCast 적용\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "        # 스케일링된 그라디언트 계산\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # 그라디언트 클리핑 전에 스케일링 제거\n",
    "        scaler.unscale_(optimizer)\n",
    "        clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "\n",
    "        # 옵티마이저 스텝 및 스케일러 업데이트\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "            \n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        lrs.append(lr)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)        \n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in valid_loader:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    val_loss /= len(valid_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # 모델 저장\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        model_save = True\n",
    "        if model_save:\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_duration = time.time() - start_time\n",
    "    training_time += epoch_duration\n",
    "    \n",
    "    text = f'\\tLoss: {epoch_loss}, Val Loss: {val_loss}, LR: {lr}, Duration: {epoch_duration:.2f} sec'\n",
    "    \n",
    "    if model_save:\n",
    "        text += f' - model saved!'\n",
    "        model_save = False\n",
    "\n",
    "    print(text)\n",
    "        \n",
    "text = f\"Epoch 당 평균 소요시간 : {training_time / epochs:.2f}초\"      \n",
    "print(text)\n",
    "\n",
    "# 예측 수행 및 레이블 저장\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# 혼동 행렬 생성\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# 예측과 실제 레이블\n",
    "y_true = all_labels  # 실제 레이블\n",
    "y_pred = all_preds  # 모델에 의해 예측된 레이블\n",
    "\n",
    "# 전체 데이터셋에 대한 정확도\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# 평균 정밀도, 리콜, F1-Score ('weighted')\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "\n",
    "# 판다스 데이터프레임으로 결과 정리\n",
    "performance_metrics = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "    'Value': [accuracy, precision, recall, f1_score]\n",
    "})\n",
    "\n",
    "# 데이터프레임 출력\n",
    "performance_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
