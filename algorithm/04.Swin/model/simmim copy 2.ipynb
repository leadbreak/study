{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "- Stage-2 학습 시 Paper에서 제시한 Parameter와 달리\n",
    "  - 더 낮은 weight decay와 ReduceLROnPlateau Scheduler를 이용해 보다 높은 학습율을 끝까지 유지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from box import Box\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from swin_v2 import SwinTransformerV2\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "from timm.data import Mixup\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:3'\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "model_save = False\n",
    "simmim_path = '../../models/swin2/simmim.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "odict_keys(['absolute_pos_embed', 'embeddings.patch_embeddings.weight', 'embeddings.patch_embeddings.bias', 'embeddings.norm.weight', 'embeddings.norm.bias', 'stages.0.blocks.0.attn_mask', 'stages.0.blocks.0.attn.t_scale', 'stages.0.blocks.0.attn.relative_coords_table', 'stages.0.blocks.0.attn.relative_position_index', 'stages.0.blocks.0.attn.crpb_mlp.0.weight', 'stages.0.blocks.0.attn.crpb_mlp.0.bias', 'stages.0.blocks.0.attn.crpb_mlp.3.weight', 'stages.0.blocks.0.attn.qkv.weight', 'stages.0.blocks.0.attn.qkv.bias', 'stages.0.blocks.0.attn.proj.weight', 'stages.0.blocks.0.attn.proj.bias', 'stages.0.blocks.0.norm1.weight', 'stages.0.blocks.0.norm1.bias', 'stages.0.blocks.0.mlp.fc1.weight', 'stages.0.blocks.0.mlp.fc1.bias', 'stages.0.blocks.0.mlp.fc2.weight', 'stages.0.blocks.0.mlp.fc2.bias', 'stages.0.blocks.0.norm2.weight', 'stages.0.blocks.0.norm2.bias', 'stages.0.blocks.1.attn_mask', 'stages.0.blocks.1.attn.t_scale', 'stages.0.blocks.1.attn.relative_coords_table', 'stages.0.blocks.1.attn.relative_position_index', 'stages.0.blocks.1.attn.crpb_mlp.0.weight', 'stages.0.blocks.1.attn.crpb_mlp.0.bias', 'stages.0.blocks.1.attn.crpb_mlp.3.weight', 'stages.0.blocks.1.attn.qkv.weight', 'stages.0.blocks.1.attn.qkv.bias', 'stages.0.blocks.1.attn.proj.weight', 'stages.0.blocks.1.attn.proj.bias', 'stages.0.blocks.1.norm1.weight', 'stages.0.blocks.1.norm1.bias', 'stages.0.blocks.1.mlp.fc1.weight', 'stages.0.blocks.1.mlp.fc1.bias', 'stages.0.blocks.1.mlp.fc2.weight', 'stages.0.blocks.1.mlp.fc2.bias', 'stages.0.blocks.1.norm2.weight', 'stages.0.blocks.1.norm2.bias', 'stages.0.downsample.reduction.weight', 'stages.0.downsample.norm.weight', 'stages.0.downsample.norm.bias', 'stages.1.blocks.0.attn_mask', 'stages.1.blocks.0.attn.t_scale', 'stages.1.blocks.0.attn.relative_coords_table', 'stages.1.blocks.0.attn.relative_position_index', 'stages.1.blocks.0.attn.crpb_mlp.0.weight', 'stages.1.blocks.0.attn.crpb_mlp.0.bias', 'stages.1.blocks.0.attn.crpb_mlp.3.weight', 'stages.1.blocks.0.attn.qkv.weight', 'stages.1.blocks.0.attn.qkv.bias', 'stages.1.blocks.0.attn.proj.weight', 'stages.1.blocks.0.attn.proj.bias', 'stages.1.blocks.0.norm1.weight', 'stages.1.blocks.0.norm1.bias', 'stages.1.blocks.0.mlp.fc1.weight', 'stages.1.blocks.0.mlp.fc1.bias', 'stages.1.blocks.0.mlp.fc2.weight', 'stages.1.blocks.0.mlp.fc2.bias', 'stages.1.blocks.0.norm2.weight', 'stages.1.blocks.0.norm2.bias', 'stages.1.blocks.1.attn_mask', 'stages.1.blocks.1.attn.t_scale', 'stages.1.blocks.1.attn.relative_coords_table', 'stages.1.blocks.1.attn.relative_position_index', 'stages.1.blocks.1.attn.crpb_mlp.0.weight', 'stages.1.blocks.1.attn.crpb_mlp.0.bias', 'stages.1.blocks.1.attn.crpb_mlp.3.weight', 'stages.1.blocks.1.attn.qkv.weight', 'stages.1.blocks.1.attn.qkv.bias', 'stages.1.blocks.1.attn.proj.weight', 'stages.1.blocks.1.attn.proj.bias', 'stages.1.blocks.1.norm1.weight', 'stages.1.blocks.1.norm1.bias', 'stages.1.blocks.1.mlp.fc1.weight', 'stages.1.blocks.1.mlp.fc1.bias', 'stages.1.blocks.1.mlp.fc2.weight', 'stages.1.blocks.1.mlp.fc2.bias', 'stages.1.blocks.1.norm2.weight', 'stages.1.blocks.1.norm2.bias', 'stages.1.downsample.reduction.weight', 'stages.1.downsample.norm.weight', 'stages.1.downsample.norm.bias', 'stages.2.blocks.0.attn_mask', 'stages.2.blocks.0.attn.t_scale', 'stages.2.blocks.0.attn.relative_coords_table', 'stages.2.blocks.0.attn.relative_position_index', 'stages.2.blocks.0.attn.crpb_mlp.0.weight', 'stages.2.blocks.0.attn.crpb_mlp.0.bias', 'stages.2.blocks.0.attn.crpb_mlp.3.weight', 'stages.2.blocks.0.attn.qkv.weight', 'stages.2.blocks.0.attn.qkv.bias', 'stages.2.blocks.0.attn.proj.weight', 'stages.2.blocks.0.attn.proj.bias', 'stages.2.blocks.0.norm1.weight', 'stages.2.blocks.0.norm1.bias', 'stages.2.blocks.0.mlp.fc1.weight', 'stages.2.blocks.0.mlp.fc1.bias', 'stages.2.blocks.0.mlp.fc2.weight', 'stages.2.blocks.0.mlp.fc2.bias', 'stages.2.blocks.0.norm2.weight', 'stages.2.blocks.0.norm2.bias', 'stages.2.blocks.1.attn_mask', 'stages.2.blocks.1.attn.t_scale', 'stages.2.blocks.1.attn.relative_coords_table', 'stages.2.blocks.1.attn.relative_position_index', 'stages.2.blocks.1.attn.crpb_mlp.0.weight', 'stages.2.blocks.1.attn.crpb_mlp.0.bias', 'stages.2.blocks.1.attn.crpb_mlp.3.weight', 'stages.2.blocks.1.attn.qkv.weight', 'stages.2.blocks.1.attn.qkv.bias', 'stages.2.blocks.1.attn.proj.weight', 'stages.2.blocks.1.attn.proj.bias', 'stages.2.blocks.1.norm1.weight', 'stages.2.blocks.1.norm1.bias', 'stages.2.blocks.1.mlp.fc1.weight', 'stages.2.blocks.1.mlp.fc1.bias', 'stages.2.blocks.1.mlp.fc2.weight', 'stages.2.blocks.1.mlp.fc2.bias', 'stages.2.blocks.1.norm2.weight', 'stages.2.blocks.1.norm2.bias', 'stages.2.blocks.2.attn_mask', 'stages.2.blocks.2.attn.t_scale', 'stages.2.blocks.2.attn.relative_coords_table', 'stages.2.blocks.2.attn.relative_position_index', 'stages.2.blocks.2.attn.crpb_mlp.0.weight', 'stages.2.blocks.2.attn.crpb_mlp.0.bias', 'stages.2.blocks.2.attn.crpb_mlp.3.weight', 'stages.2.blocks.2.attn.qkv.weight', 'stages.2.blocks.2.attn.qkv.bias', 'stages.2.blocks.2.attn.proj.weight', 'stages.2.blocks.2.attn.proj.bias', 'stages.2.blocks.2.norm1.weight', 'stages.2.blocks.2.norm1.bias', 'stages.2.blocks.2.mlp.fc1.weight', 'stages.2.blocks.2.mlp.fc1.bias', 'stages.2.blocks.2.mlp.fc2.weight', 'stages.2.blocks.2.mlp.fc2.bias', 'stages.2.blocks.2.norm2.weight', 'stages.2.blocks.2.norm2.bias', 'stages.2.blocks.3.attn_mask', 'stages.2.blocks.3.attn.t_scale', 'stages.2.blocks.3.attn.relative_coords_table', 'stages.2.blocks.3.attn.relative_position_index', 'stages.2.blocks.3.attn.crpb_mlp.0.weight', 'stages.2.blocks.3.attn.crpb_mlp.0.bias', 'stages.2.blocks.3.attn.crpb_mlp.3.weight', 'stages.2.blocks.3.attn.qkv.weight', 'stages.2.blocks.3.attn.qkv.bias', 'stages.2.blocks.3.attn.proj.weight', 'stages.2.blocks.3.attn.proj.bias', 'stages.2.blocks.3.norm1.weight', 'stages.2.blocks.3.norm1.bias', 'stages.2.blocks.3.mlp.fc1.weight', 'stages.2.blocks.3.mlp.fc1.bias', 'stages.2.blocks.3.mlp.fc2.weight', 'stages.2.blocks.3.mlp.fc2.bias', 'stages.2.blocks.3.norm2.weight', 'stages.2.blocks.3.norm2.bias', 'stages.2.blocks.4.attn_mask', 'stages.2.blocks.4.attn.t_scale', 'stages.2.blocks.4.attn.relative_coords_table', 'stages.2.blocks.4.attn.relative_position_index', 'stages.2.blocks.4.attn.crpb_mlp.0.weight', 'stages.2.blocks.4.attn.crpb_mlp.0.bias', 'stages.2.blocks.4.attn.crpb_mlp.3.weight', 'stages.2.blocks.4.attn.qkv.weight', 'stages.2.blocks.4.attn.qkv.bias', 'stages.2.blocks.4.attn.proj.weight', 'stages.2.blocks.4.attn.proj.bias', 'stages.2.blocks.4.norm1.weight', 'stages.2.blocks.4.norm1.bias', 'stages.2.blocks.4.mlp.fc1.weight', 'stages.2.blocks.4.mlp.fc1.bias', 'stages.2.blocks.4.mlp.fc2.weight', 'stages.2.blocks.4.mlp.fc2.bias', 'stages.2.blocks.4.norm2.weight', 'stages.2.blocks.4.norm2.bias', 'stages.2.blocks.5.attn_mask', 'stages.2.blocks.5.attn.t_scale', 'stages.2.blocks.5.attn.relative_coords_table', 'stages.2.blocks.5.attn.relative_position_index', 'stages.2.blocks.5.attn.crpb_mlp.0.weight', 'stages.2.blocks.5.attn.crpb_mlp.0.bias', 'stages.2.blocks.5.attn.crpb_mlp.3.weight', 'stages.2.blocks.5.attn.qkv.weight', 'stages.2.blocks.5.attn.qkv.bias', 'stages.2.blocks.5.attn.proj.weight', 'stages.2.blocks.5.attn.proj.bias', 'stages.2.blocks.5.norm1.weight', 'stages.2.blocks.5.norm1.bias', 'stages.2.blocks.5.mlp.fc1.weight', 'stages.2.blocks.5.mlp.fc1.bias', 'stages.2.blocks.5.mlp.fc2.weight', 'stages.2.blocks.5.mlp.fc2.bias', 'stages.2.blocks.5.norm2.weight', 'stages.2.blocks.5.norm2.bias', 'stages.2.downsample.reduction.weight', 'stages.2.downsample.norm.weight', 'stages.2.downsample.norm.bias', 'stages.3.blocks.0.attn_mask', 'stages.3.blocks.0.attn.t_scale', 'stages.3.blocks.0.attn.relative_coords_table', 'stages.3.blocks.0.attn.relative_position_index', 'stages.3.blocks.0.attn.crpb_mlp.0.weight', 'stages.3.blocks.0.attn.crpb_mlp.0.bias', 'stages.3.blocks.0.attn.crpb_mlp.3.weight', 'stages.3.blocks.0.attn.qkv.weight', 'stages.3.blocks.0.attn.qkv.bias', 'stages.3.blocks.0.attn.proj.weight', 'stages.3.blocks.0.attn.proj.bias', 'stages.3.blocks.0.norm1.weight', 'stages.3.blocks.0.norm1.bias', 'stages.3.blocks.0.mlp.fc1.weight', 'stages.3.blocks.0.mlp.fc1.bias', 'stages.3.blocks.0.mlp.fc2.weight', 'stages.3.blocks.0.mlp.fc2.bias', 'stages.3.blocks.0.norm2.weight', 'stages.3.blocks.0.norm2.bias', 'stages.3.blocks.1.attn_mask', 'stages.3.blocks.1.attn.t_scale', 'stages.3.blocks.1.attn.relative_coords_table', 'stages.3.blocks.1.attn.relative_position_index', 'stages.3.blocks.1.attn.crpb_mlp.0.weight', 'stages.3.blocks.1.attn.crpb_mlp.0.bias', 'stages.3.blocks.1.attn.crpb_mlp.3.weight', 'stages.3.blocks.1.attn.qkv.weight', 'stages.3.blocks.1.attn.qkv.bias', 'stages.3.blocks.1.attn.proj.weight', 'stages.3.blocks.1.attn.proj.bias', 'stages.3.blocks.1.norm1.weight', 'stages.3.blocks.1.norm1.bias', 'stages.3.blocks.1.mlp.fc1.weight', 'stages.3.blocks.1.mlp.fc1.bias', 'stages.3.blocks.1.mlp.fc2.weight', 'stages.3.blocks.1.mlp.fc2.bias', 'stages.3.blocks.1.norm2.weight', 'stages.3.blocks.1.norm2.bias', 'layernorm.weight', 'layernorm.bias', 'classifier.weight', 'classifier.bias'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SwinTransformerV2(pretrained_window_sizes=[6,6,6,6], ape=True, drop_path_rate=0.3)\n",
    "model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0019,  0.0024, -0.0877,  0.0232],\n",
       "         [-0.0616, -0.0999,  0.1143,  0.0281],\n",
       "         [-0.0393,  0.1294, -0.0249, -0.1348],\n",
       "         [ 0.0730,  0.0998, -0.0284, -0.0161]],\n",
       "\n",
       "        [[-0.0978,  0.0726, -0.0799,  0.0543],\n",
       "         [-0.1438,  0.0822, -0.0775, -0.1056],\n",
       "         [-0.0805,  0.0969, -0.0282, -0.0547],\n",
       "         [ 0.1176,  0.0040, -0.0782,  0.0853]],\n",
       "\n",
       "        [[ 0.1063,  0.1135,  0.0167, -0.0749],\n",
       "         [ 0.0823,  0.1091, -0.1335,  0.0438],\n",
       "         [-0.0085, -0.0238, -0.0313,  0.0249],\n",
       "         [ 0.1132,  0.0351, -0.1046, -0.1386]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['embeddings.patch_embeddings.weight'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.2457e-04, -1.2653e-02,  6.0946e-03, -3.4141e-02, -2.6560e-02,\n",
       "        -3.2582e-03,  1.8496e-02, -1.0947e-02,  2.9805e-02,  1.3757e-02,\n",
       "         7.6427e-04,  1.0726e-02, -7.6469e-03,  8.2123e-05,  8.1061e-03,\n",
       "        -2.9124e-03,  1.8990e-02,  7.5993e-03, -2.1990e-03, -2.4065e-02,\n",
       "        -1.3613e-02,  3.3351e-03, -2.0378e-02, -9.5173e-03, -2.2484e-02,\n",
       "        -9.3888e-03, -1.0285e-02, -1.8410e-02, -4.8249e-02, -2.3254e-02,\n",
       "         3.2788e-03, -1.2797e-02,  1.3834e-02, -5.7485e-03,  4.7370e-04,\n",
       "        -9.6089e-03, -2.0568e-03,  1.5328e-02, -2.3599e-02,  9.4640e-03,\n",
       "         4.2732e-02,  3.0600e-02,  1.7369e-02,  4.2784e-02,  2.0393e-02,\n",
       "         3.8013e-02, -4.1347e-03,  1.7765e-02, -4.4233e-03, -3.1388e-03,\n",
       "         9.9567e-03, -1.0617e-02, -1.5000e-02, -1.6930e-02,  2.2355e-02,\n",
       "        -5.4080e-03,  3.6536e-02, -1.8940e-02, -1.3961e-02, -1.2759e-02,\n",
       "        -3.7565e-02, -3.1580e-02,  6.5368e-03, -1.6552e-02, -1.4581e-02,\n",
       "         1.2098e-02, -3.4367e-03,  1.7027e-02,  1.4529e-02,  7.2884e-03,\n",
       "         2.2712e-02, -4.3049e-03, -2.8125e-02, -1.4041e-05,  2.8062e-02,\n",
       "        -1.5749e-02, -4.6156e-04, -1.2368e-02,  9.0832e-03,  1.6046e-02,\n",
       "         8.4588e-03, -4.0221e-03,  3.0126e-02, -3.5335e-02,  3.9015e-02,\n",
       "         1.2275e-02,  2.0959e-05, -2.7405e-03,  2.8994e-02, -7.6913e-03,\n",
       "        -1.3574e-02,  1.7645e-02,  1.6590e-03,  1.2867e-02, -1.4562e-03,\n",
       "        -4.0181e-02,  9.3873e-03,  1.3914e-02, -6.3930e-03, -2.0256e-02,\n",
       "        -1.4810e-03,  2.3722e-02, -3.4520e-02, -1.6096e-02, -1.0221e-02,\n",
       "        -2.8824e-02, -2.3030e-02,  1.8795e-02, -1.3629e-02,  1.3266e-02,\n",
       "        -2.6284e-02, -5.8026e-02, -1.7127e-02,  8.1168e-03, -1.4706e-02,\n",
       "         1.6203e-02, -1.3728e-02,  3.8532e-02, -1.3577e-02,  4.4603e-02,\n",
       "         1.4118e-02, -2.7071e-02, -7.9787e-04, -1.5905e-02,  1.8575e-03,\n",
       "         2.6483e-02, -1.9825e-02, -1.6145e-02, -2.8715e-02,  4.0358e-03,\n",
       "         2.5718e-02,  1.4012e-02,  6.0212e-04,  2.9819e-02, -7.8351e-03,\n",
       "         3.3056e-03,  1.0480e-02,  6.4539e-03,  1.0848e-02, -5.7157e-03,\n",
       "        -1.7219e-02, -3.6223e-02, -2.8151e-03, -3.0387e-02, -7.3317e-03,\n",
       "        -4.2678e-03, -9.3219e-03,  1.6699e-02,  5.0582e-03, -1.3761e-02,\n",
       "         1.8148e-02, -3.0515e-03,  1.4296e-02, -2.3767e-04,  1.6002e-02,\n",
       "        -6.2946e-03,  2.1453e-02, -6.4195e-03, -6.4465e-03,  4.3668e-02,\n",
       "         3.0858e-03,  1.4420e-02,  8.7092e-04,  1.9666e-02, -1.3100e-02,\n",
       "        -9.7680e-03, -2.0224e-02,  2.0607e-02,  1.4385e-02, -1.7674e-02,\n",
       "         8.6055e-03, -2.9289e-03,  3.1090e-02,  1.1172e-02,  4.1231e-03,\n",
       "        -1.0516e-02, -1.2156e-02, -5.2432e-02, -1.9965e-02,  9.7922e-03,\n",
       "        -3.8178e-02,  3.5963e-02, -2.5953e-02, -2.1687e-02, -2.7314e-02,\n",
       "        -8.0309e-03,  8.5650e-03, -1.3028e-02, -5.7952e-03, -6.6521e-04,\n",
       "         7.2109e-03, -2.3052e-04, -6.6453e-03,  4.9781e-03, -6.3167e-03,\n",
       "         4.2716e-03,  4.1435e-02, -3.1183e-02,  2.6876e-03,  9.4107e-03,\n",
       "         3.5967e-02, -2.3921e-02,  9.9922e-03,  1.3463e-03,  2.7054e-03,\n",
       "         1.1428e-02, -1.6024e-02,  3.3039e-02, -3.4812e-02, -3.5598e-02,\n",
       "        -5.6826e-03, -1.8064e-02, -2.9759e-03,  2.4851e-02,  3.7164e-03,\n",
       "        -3.7387e-03,  1.5578e-03, -2.7351e-02,  3.2932e-04,  1.1317e-03,\n",
       "        -2.1773e-02,  9.9430e-03,  8.8255e-03, -2.4480e-02,  6.2745e-04,\n",
       "         2.6974e-02,  1.5512e-02,  1.1452e-02, -1.3475e-02, -4.0801e-02,\n",
       "         2.0092e-02, -2.7625e-02, -1.2408e-02, -9.9100e-03, -3.1986e-02,\n",
       "        -1.5839e-03,  2.1094e-02,  3.5334e-03,  1.9882e-03, -6.0579e-04,\n",
       "         9.4600e-04, -3.1779e-03, -1.5911e-02,  1.5311e-02,  2.2515e-02,\n",
       "        -2.8870e-02, -1.7920e-02,  8.1585e-03, -9.2126e-03,  1.2421e-02,\n",
       "        -1.7263e-02,  1.7696e-02, -1.4936e-02, -1.6258e-02, -4.2739e-02,\n",
       "         1.3015e-02,  3.2988e-03, -1.3057e-02,  4.6005e-03, -4.5387e-02,\n",
       "        -1.3075e-02,  6.0089e-03,  1.2257e-02,  9.8481e-03,  8.5299e-03,\n",
       "        -5.7286e-03,  2.7596e-02, -1.7087e-02, -5.8871e-03, -1.6263e-02,\n",
       "         1.3671e-02, -1.2118e-02,  2.4651e-02,  3.4856e-02, -4.8342e-02,\n",
       "        -2.0592e-02, -4.8863e-03, -1.4220e-02, -1.9918e-02,  2.1386e-02,\n",
       "        -1.1753e-03,  8.8511e-03,  4.6018e-03, -2.3352e-02, -3.8337e-03,\n",
       "        -1.1411e-02, -1.7966e-03,  1.5778e-02,  3.8653e-02,  1.8229e-02,\n",
       "         2.3758e-03,  2.9541e-02, -2.7484e-02,  2.6852e-02, -1.3123e-02,\n",
       "        -1.6657e-02,  1.4615e-03, -9.3471e-03,  2.2451e-03,  1.9091e-02,\n",
       "         8.6709e-03,  1.4927e-02,  6.5933e-03,  3.4618e-04, -1.3448e-02,\n",
       "        -3.5356e-03,  1.6706e-02, -1.3665e-02, -7.3060e-04, -1.3922e-02,\n",
       "        -1.0506e-02,  2.2043e-02,  2.1347e-02,  6.2378e-03, -1.0476e-02,\n",
       "        -9.5213e-03,  2.1633e-02,  2.0884e-03,  1.5087e-02, -3.6977e-02,\n",
       "        -9.4413e-05,  4.5611e-02,  1.6312e-02,  2.4015e-04, -3.0568e-02,\n",
       "         2.6782e-02,  1.2778e-02, -9.5175e-04,  1.3980e-02,  7.2754e-03,\n",
       "        -1.7059e-02,  1.3256e-02,  3.8648e-03,  8.0192e-04,  1.4720e-02,\n",
       "        -1.5649e-02,  1.7152e-02,  9.4503e-04, -1.7269e-03,  5.5546e-03,\n",
       "        -3.1443e-03, -2.7555e-02,  2.7634e-02, -6.8220e-03, -4.6804e-02,\n",
       "         1.1489e-02, -3.9782e-04,  3.0388e-02,  3.0481e-02, -1.0228e-02,\n",
       "        -1.5716e-02, -3.6523e-03,  1.6706e-02, -1.5105e-02,  1.5907e-03,\n",
       "        -1.5700e-02,  5.4145e-03,  2.5715e-02,  1.0163e-02, -1.3962e-02,\n",
       "         2.3238e-02, -1.1191e-02,  7.7818e-03,  5.3725e-03,  1.4624e-02,\n",
       "         1.4664e-02, -1.8237e-02,  7.4576e-03,  8.9794e-03,  2.7308e-03,\n",
       "         3.0813e-02, -1.5804e-02, -1.9572e-02,  1.1955e-02,  6.3504e-03,\n",
       "        -7.4515e-03, -2.3085e-03, -2.3355e-02, -2.3809e-02, -2.1798e-02,\n",
       "         4.8190e-03, -2.7319e-02,  2.9393e-02, -5.3629e-04])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['stages.3.blocks.1.attn.crpb_mlp.3.weight'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MODEL': {'TYPE': 'swinv2',\n",
       "  'NAME': 'simmim_train',\n",
       "  'PRETRAINED': '../../models/swin2/simmim.pth',\n",
       "  'DROP_PATH_RATE': 0.2,\n",
       "  'SWIN': {'EMBED_DIM': 96,\n",
       "   'DEPTHS': [2, 2, 6, 2],\n",
       "   'NUM_HEADS': [3, 6, 12, 24],\n",
       "   'WINDOW_SIZE': 7,\n",
       "   'PATCH_SIZE': 4}},\n",
       " 'DATA': {'IMG_SIZE': 224,\n",
       "  'MASK_PATCH_SIZE': 32,\n",
       "  'MASK_RATIO': 0.6,\n",
       "  'BATCH_SIZE': 960,\n",
       "  'NUM_WORKERS': 24,\n",
       "  'DATA_PATH': '../../data/sports'},\n",
       " 'TRAIN': {'EPOCHS': 20,\n",
       "  'WARMUP_EPOCHS': 10,\n",
       "  'BASE_LR': '1e-4',\n",
       "  'WEIGHT_DECAY': 0.05,\n",
       "  'CLIP_GRAD': 5}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swin_config = yaml.load(open('config/train.yaml'), Loader=yaml.FullLoader)\n",
    "swin_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained(config, model):\n",
    "    print(f\"==============> Loading weight {config.MODEL.PRETRAINED} for fine-tuning......\")\n",
    "    state_dict = torch.load(config.MODEL.PRETRAINED, map_location='cpu')\n",
    "\n",
    "    # remain encoder only\n",
    "    not_encoder_keys = [k for k in state_dict.keys() if 'encoder' not in k]\n",
    "    for k in not_encoder_keys:\n",
    "        del state_dict[k]\n",
    "        \n",
    "    # remove prefix encoder.\n",
    "    state_dict = {k.replace('encoder.', ''):v for k, v in state_dict.items()}\n",
    "\n",
    "    # delete relative_position_index since we always re-init it\n",
    "    relative_position_index_keys = [k for k in state_dict.keys() if \"relative_position_index\" in k]\n",
    "    for k in relative_position_index_keys:\n",
    "        del state_dict[k]\n",
    "\n",
    "    # delete relative_coords_table since we always re-init it\n",
    "    relative_position_index_keys = [k for k in state_dict.keys() if \"relative_coords_table\" in k]\n",
    "    for k in relative_position_index_keys:\n",
    "        del state_dict[k]\n",
    "\n",
    "    # delete attn_mask since we always re-init it\n",
    "    attn_mask_keys = [k for k in state_dict.keys() if \"attn_mask\" in k]\n",
    "    for k in attn_mask_keys:\n",
    "        del state_dict[k]\n",
    "\n",
    "    # bicubic interpolate relative_position_bias_table if not match\n",
    "    relative_position_bias_table_keys = [k for k in state_dict.keys() if \"relative_position_bias_table\" in k]\n",
    "    for k in relative_position_bias_table_keys:\n",
    "        relative_position_bias_table_pretrained = state_dict[k]\n",
    "        relative_position_bias_table_current = model.state_dict()[k]\n",
    "        L1, nH1 = relative_position_bias_table_pretrained.size()\n",
    "        L2, nH2 = relative_position_bias_table_current.size()\n",
    "        if nH1 != nH2:\n",
    "            print(f\"Error in loading {k}, passing......\")\n",
    "        else:\n",
    "            if L1 != L2:\n",
    "                # bicubic interpolate relative_position_bias_table if not match\n",
    "                S1 = int(L1 ** 0.5)\n",
    "                S2 = int(L2 ** 0.5)\n",
    "                relative_position_bias_table_pretrained_resized = torch.nn.functional.interpolate(\n",
    "                    relative_position_bias_table_pretrained.permute(1, 0).view(1, nH1, S1, S1), size=(S2, S2),\n",
    "                    mode='bicubic')\n",
    "                state_dict[k] = relative_position_bias_table_pretrained_resized.view(nH2, L2).permute(1, 0)\n",
    "\n",
    "    # bicubic interpolate absolute_pos_embed if not match\n",
    "    absolute_pos_embed_keys = [k for k in state_dict.keys() if \"absolute_pos_embed\" in k]\n",
    "    for k in absolute_pos_embed_keys:\n",
    "        # dpe\n",
    "        absolute_pos_embed_pretrained = state_dict[k]\n",
    "        absolute_pos_embed_current = model.state_dict()[k.replace('encoder.','')]\n",
    "        _, L1, C1 = absolute_pos_embed_pretrained.size()\n",
    "        _, L2, C2 = absolute_pos_embed_current.size()\n",
    "        if C1 != C1:\n",
    "            print(f\"Error in loading {k}, passing......\")\n",
    "        else:\n",
    "            if L1 != L2:\n",
    "                S1 = int(L1 ** 0.5)\n",
    "                S2 = int(L2 ** 0.5)\n",
    "                absolute_pos_embed_pretrained = absolute_pos_embed_pretrained.reshape(-1, S1, S1, C1)\n",
    "                absolute_pos_embed_pretrained = absolute_pos_embed_pretrained.permute(0, 3, 1, 2)\n",
    "                absolute_pos_embed_pretrained_resized = torch.nn.functional.interpolate(\n",
    "                    absolute_pos_embed_pretrained, size=(S2, S2), mode='bicubic')\n",
    "                absolute_pos_embed_pretrained_resized = absolute_pos_embed_pretrained_resized.permute(0, 2, 3, 1)\n",
    "                absolute_pos_embed_pretrained_resized = absolute_pos_embed_pretrained_resized.flatten(1, 2)\n",
    "                state_dict[k] = absolute_pos_embed_pretrained_resized\n",
    "\n",
    "    # check classifier, if not match, then re-init classifier to zero\n",
    "    head_bias_pretrained = state_dict['classifier.bias']\n",
    "    Nc1 = head_bias_pretrained.shape[0]\n",
    "    Nc2 = model.classifier.bias.shape[0]\n",
    "    if (Nc1 != Nc2):\n",
    "        torch.nn.init.constant_(model.classifier.bias, 0.)\n",
    "        torch.nn.init.constant_(model.classifier.weight, 0.)\n",
    "        del state_dict['classifier.weight']\n",
    "        del state_dict['classifier.bias']\n",
    "        print(f\"Error in loading classifier head, re-init classifier head to 0\")\n",
    "\n",
    "    msg = model.load_state_dict(state_dict, strict=False)\n",
    "    print(msg)\n",
    "\n",
    "    print(f\"=> loaded successfully '{config.MODEL.PRETRAINED}'\")\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============> Loading weight ../../models/swin2/simmim.pth for fine-tuning......\n",
      "_IncompatibleKeys(missing_keys=['stages.0.blocks.0.attn_mask', 'stages.0.blocks.0.attn.relative_coords_table', 'stages.0.blocks.0.attn.relative_position_index', 'stages.0.blocks.1.attn_mask', 'stages.0.blocks.1.attn.relative_coords_table', 'stages.0.blocks.1.attn.relative_position_index', 'stages.1.blocks.0.attn_mask', 'stages.1.blocks.0.attn.relative_coords_table', 'stages.1.blocks.0.attn.relative_position_index', 'stages.1.blocks.1.attn_mask', 'stages.1.blocks.1.attn.relative_coords_table', 'stages.1.blocks.1.attn.relative_position_index', 'stages.2.blocks.0.attn_mask', 'stages.2.blocks.0.attn.relative_coords_table', 'stages.2.blocks.0.attn.relative_position_index', 'stages.2.blocks.1.attn_mask', 'stages.2.blocks.1.attn.relative_coords_table', 'stages.2.blocks.1.attn.relative_position_index', 'stages.2.blocks.2.attn_mask', 'stages.2.blocks.2.attn.relative_coords_table', 'stages.2.blocks.2.attn.relative_position_index', 'stages.2.blocks.3.attn_mask', 'stages.2.blocks.3.attn.relative_coords_table', 'stages.2.blocks.3.attn.relative_position_index', 'stages.2.blocks.4.attn_mask', 'stages.2.blocks.4.attn.relative_coords_table', 'stages.2.blocks.4.attn.relative_position_index', 'stages.2.blocks.5.attn_mask', 'stages.2.blocks.5.attn.relative_coords_table', 'stages.2.blocks.5.attn.relative_position_index', 'stages.3.blocks.0.attn_mask', 'stages.3.blocks.0.attn.relative_coords_table', 'stages.3.blocks.0.attn.relative_position_index', 'stages.3.blocks.1.attn_mask', 'stages.3.blocks.1.attn.relative_coords_table', 'stages.3.blocks.1.attn.relative_position_index'], unexpected_keys=['mask_token'])\n",
      "=> loaded successfully '../../models/swin2/simmim.pth'\n"
     ]
    }
   ],
   "source": [
    "swin_config = Box(swin_config)\n",
    "load_pretrained(swin_config, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0301,  0.0009,  0.0505, -0.0364],\n",
       "         [ 0.1105, -0.0453, -0.0787, -0.0376],\n",
       "         [-0.0682, -0.0893,  0.0535, -0.0659],\n",
       "         [-0.0380,  0.0854,  0.0976,  0.0287]],\n",
       "\n",
       "        [[-0.0771, -0.1077, -0.0139,  0.0576],\n",
       "         [-0.1156,  0.0058,  0.0420, -0.0442],\n",
       "         [-0.1327,  0.1288, -0.0719,  0.0688],\n",
       "         [ 0.0249,  0.1164, -0.0735, -0.0361]],\n",
       "\n",
       "        [[ 0.0094,  0.1149, -0.0851,  0.1097],\n",
       "         [ 0.1142, -0.1260,  0.1255, -0.0553],\n",
       "         [ 0.0574, -0.0123, -0.0088,  0.0854],\n",
       "         [-0.0981,  0.0671, -0.1177,  0.0822]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['embeddings.patch_embeddings.weight'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3441,  0.2157, -0.2093,  0.3680, -0.0128, -0.1047,  0.3628,  0.1783,\n",
       "        -0.1962, -0.0918,  0.2762,  0.3604, -0.1071,  0.3773, -0.0933, -0.0062,\n",
       "        -0.1436, -0.1425, -0.1067,  0.2658, -0.0970,  0.2234,  0.0165,  0.3350,\n",
       "        -0.1316, -0.1465,  0.3168, -0.1226, -0.1506, -0.0127,  0.3362, -0.1885,\n",
       "        -0.1618, -0.1210,  0.3268,  0.2339, -0.1220, -0.0818, -0.0878, -0.1294,\n",
       "        -0.0691,  0.1114, -0.0910, -0.1057,  0.2985,  0.3047, -0.1025, -0.0852,\n",
       "        -0.0855,  0.1312, -0.0273, -0.1521,  0.3055,  0.1763, -0.0781,  0.3170,\n",
       "        -0.0057, -0.0150, -0.0239, -0.1214, -0.0623, -0.2075, -0.0569,  0.2784,\n",
       "        -0.0833, -0.1258, -0.1253,  0.1136, -0.1609,  0.1525,  0.3013, -0.1182,\n",
       "         0.0469, -0.1002, -0.1370, -0.1209,  0.2787,  0.0903, -0.1388, -0.1924,\n",
       "        -0.1242,  0.0175, -0.1391, -0.0981,  0.3561,  0.3985, -0.0734,  0.3098,\n",
       "         0.0680, -0.1240,  0.2772, -0.0027,  0.2672,  0.3719, -0.1221,  0.2322,\n",
       "        -0.1118, -0.1924, -0.1657, -0.1465, -0.2035,  0.1239, -0.1328, -0.1246,\n",
       "        -0.1630, -0.1243,  0.0183,  0.1933,  0.3206,  0.0840, -0.1172, -0.1619,\n",
       "         0.2803, -0.1315, -0.0774, -0.1813, -0.1353, -0.1154, -0.1547,  0.0035,\n",
       "        -0.1676, -0.2131,  0.3817,  0.1613, -0.1180, -0.1891,  0.1556,  0.0457,\n",
       "        -0.1123,  0.1500, -0.1252, -0.1942,  0.0070,  0.3842, -0.1383, -0.1451,\n",
       "        -0.0104,  0.1070,  0.0060,  0.2915, -0.1389,  0.3873, -0.1191,  0.2119,\n",
       "         0.2249, -0.1321,  0.1351,  0.2725, -0.1091, -0.0455, -0.1145,  0.4053,\n",
       "         0.0386, -0.1343,  0.1865, -0.1044, -0.1041,  0.1070, -0.1304,  0.3231,\n",
       "         0.1755, -0.0995, -0.0674, -0.1109,  0.4288, -0.1018, -0.0100, -0.1207,\n",
       "        -0.1190, -0.1947, -0.1189,  0.1013, -0.0164, -0.1201,  0.3990, -0.0315,\n",
       "        -0.1000,  0.0497, -0.1145, -0.1840,  0.3667, -0.0871,  0.4341,  0.1278,\n",
       "        -0.1869,  0.2664, -0.1631,  0.3120,  0.2145, -0.1874, -0.1540, -0.0987,\n",
       "         0.1388, -0.0884,  0.0712, -0.1132, -0.1430,  0.1176, -0.1269,  0.1296,\n",
       "         0.1934,  0.2749, -0.1208, -0.1594, -0.0258, -0.0776, -0.1429,  0.1138,\n",
       "         0.1264,  0.2032,  0.2603, -0.0740, -0.2104, -0.0999,  0.0597, -0.1706,\n",
       "        -0.1895, -0.1468, -0.0890,  0.0882, -0.1409, -0.1187,  0.3585, -0.1258,\n",
       "         0.2729, -0.1978, -0.1840, -0.1468,  0.1739,  0.3891,  0.0774, -0.0752,\n",
       "         0.1035, -0.1300,  0.1101,  0.3737,  0.2278, -0.1651, -0.0667,  0.2701,\n",
       "         0.1460,  0.0951,  0.2819,  0.3445,  0.0975, -0.1205, -0.1011,  0.3742,\n",
       "        -0.1751, -0.2178,  0.0689, -0.0998, -0.1064,  0.1043, -0.0924, -0.0146,\n",
       "         0.1142, -0.1045, -0.1307, -0.1781, -0.1173, -0.1025,  0.2247,  0.0297,\n",
       "        -0.1058,  0.0530,  0.3087,  0.1489,  0.0973, -0.0852,  0.0206, -0.1205,\n",
       "        -0.0589, -0.1306,  0.3215,  0.0807, -0.1712,  0.2878, -0.1484,  0.2949,\n",
       "        -0.1277, -0.0067, -0.1076,  0.1325,  0.0985, -0.1223,  0.0184, -0.1417,\n",
       "        -0.1038, -0.1312,  0.1919, -0.1557,  0.2966,  0.3421,  0.2683,  0.3130,\n",
       "        -0.1660, -0.0919, -0.1215,  0.3667,  0.3266, -0.1250,  0.3586, -0.0910,\n",
       "        -0.1648, -0.1452, -0.1346, -0.1591, -0.1129, -0.0977, -0.0852,  0.3798,\n",
       "        -0.0844,  0.3037, -0.1081,  0.3366,  0.3138, -0.1073, -0.1729, -0.1409,\n",
       "         0.1763, -0.1172, -0.1060,  0.1426,  0.3322,  0.3639,  0.2781, -0.2158,\n",
       "        -0.1003,  0.3241, -0.0173, -0.1263, -0.1019,  0.3160,  0.1903, -0.1880,\n",
       "        -0.1185,  0.3448,  0.1412,  0.2618, -0.1933, -0.1846, -0.1656, -0.1857,\n",
       "        -0.1341, -0.1656, -0.0982, -0.0887,  0.3037, -0.1127,  0.0946,  0.2976,\n",
       "         0.3293,  0.3636,  0.1178,  0.1094, -0.0866,  0.2487,  0.1525,  0.2924,\n",
       "         0.1803, -0.1963, -0.0903, -0.1952,  0.0780, -0.1051, -0.0864, -0.1440,\n",
       "         0.1882,  0.2569, -0.1023, -0.1234, -0.1498,  0.0518, -0.1104, -0.0058,\n",
       "         0.3766,  0.2990, -0.0930, -0.0172, -0.1035,  0.1263,  0.1493, -0.1194])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['stages.3.blocks.1.attn.crpb_mlp.3.weight'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms 정의하기\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8,1), interpolation=transforms.InterpolationMode.LANCZOS),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    transforms.RandomErasing(p=0.9, scale=(0.02, 0.33)),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "data_dir = '../../data/sports'\n",
    "batch_size = 960\n",
    "\n",
    "train_path = data_dir+'/train'\n",
    "valid_path = data_dir+'/valid'\n",
    "test_path = data_dir+'/test'\n",
    "\n",
    "# dataset load\n",
    "train_data = ImageFolder(train_path, transform=train_transform)\n",
    "valid_data = ImageFolder(valid_path, transform=test_transform)\n",
    "test_data = ImageFolder(test_path, transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_norm = 5.0 # paper : 100 with G variants\n",
    "\n",
    "model.to(device)\n",
    "model_path = '../../models/swin2/model_w_simmim2.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixup_fn = Mixup(mixup_alpha=.7, \n",
    "                cutmix_alpha=.7, \n",
    "                prob=.7, \n",
    "                switch_prob=0.5, \n",
    "                mode='batch',\n",
    "                label_smoothing=.1,\n",
    "                num_classes=100)\n",
    "\n",
    "epochs = 150\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: absolute_pos_embed\n",
      "1: embeddings.patch_embeddings.weight\n",
      "2: embeddings.patch_embeddings.bias\n",
      "3: embeddings.norm.weight\n",
      "4: embeddings.norm.bias\n",
      "5: stages.0.blocks.0.attn.t_scale\n",
      "6: stages.0.blocks.0.attn.crpb_mlp.0.weight\n",
      "7: stages.0.blocks.0.attn.crpb_mlp.0.bias\n",
      "8: stages.0.blocks.0.attn.crpb_mlp.3.weight\n",
      "9: stages.0.blocks.0.attn.qkv.weight\n",
      "10: stages.0.blocks.0.attn.qkv.bias\n",
      "11: stages.0.blocks.0.attn.proj.weight\n",
      "12: stages.0.blocks.0.attn.proj.bias\n",
      "13: stages.0.blocks.0.norm1.weight\n",
      "14: stages.0.blocks.0.norm1.bias\n",
      "15: stages.0.blocks.0.mlp.fc1.weight\n",
      "16: stages.0.blocks.0.mlp.fc1.bias\n",
      "17: stages.0.blocks.0.mlp.fc2.weight\n",
      "18: stages.0.blocks.0.mlp.fc2.bias\n",
      "19: stages.0.blocks.0.norm2.weight\n",
      "20: stages.0.blocks.0.norm2.bias\n",
      "21: stages.0.blocks.1.attn.t_scale\n",
      "22: stages.0.blocks.1.attn.crpb_mlp.0.weight\n",
      "23: stages.0.blocks.1.attn.crpb_mlp.0.bias\n",
      "24: stages.0.blocks.1.attn.crpb_mlp.3.weight\n",
      "25: stages.0.blocks.1.attn.qkv.weight\n",
      "26: stages.0.blocks.1.attn.qkv.bias\n",
      "27: stages.0.blocks.1.attn.proj.weight\n",
      "28: stages.0.blocks.1.attn.proj.bias\n",
      "29: stages.0.blocks.1.norm1.weight\n",
      "30: stages.0.blocks.1.norm1.bias\n",
      "31: stages.0.blocks.1.mlp.fc1.weight\n",
      "32: stages.0.blocks.1.mlp.fc1.bias\n",
      "33: stages.0.blocks.1.mlp.fc2.weight\n",
      "34: stages.0.blocks.1.mlp.fc2.bias\n",
      "35: stages.0.blocks.1.norm2.weight\n",
      "36: stages.0.blocks.1.norm2.bias\n",
      "37: stages.0.downsample.reduction.weight\n",
      "38: stages.0.downsample.norm.weight\n",
      "39: stages.0.downsample.norm.bias\n",
      "40: stages.1.blocks.0.attn.t_scale\n",
      "41: stages.1.blocks.0.attn.crpb_mlp.0.weight\n",
      "42: stages.1.blocks.0.attn.crpb_mlp.0.bias\n",
      "43: stages.1.blocks.0.attn.crpb_mlp.3.weight\n",
      "44: stages.1.blocks.0.attn.qkv.weight\n",
      "45: stages.1.blocks.0.attn.qkv.bias\n",
      "46: stages.1.blocks.0.attn.proj.weight\n",
      "47: stages.1.blocks.0.attn.proj.bias\n",
      "48: stages.1.blocks.0.norm1.weight\n",
      "49: stages.1.blocks.0.norm1.bias\n",
      "50: stages.1.blocks.0.mlp.fc1.weight\n",
      "51: stages.1.blocks.0.mlp.fc1.bias\n",
      "52: stages.1.blocks.0.mlp.fc2.weight\n",
      "53: stages.1.blocks.0.mlp.fc2.bias\n",
      "54: stages.1.blocks.0.norm2.weight\n",
      "55: stages.1.blocks.0.norm2.bias\n",
      "56: stages.1.blocks.1.attn.t_scale\n",
      "57: stages.1.blocks.1.attn.crpb_mlp.0.weight\n",
      "58: stages.1.blocks.1.attn.crpb_mlp.0.bias\n",
      "59: stages.1.blocks.1.attn.crpb_mlp.3.weight\n",
      "60: stages.1.blocks.1.attn.qkv.weight\n",
      "61: stages.1.blocks.1.attn.qkv.bias\n",
      "62: stages.1.blocks.1.attn.proj.weight\n",
      "63: stages.1.blocks.1.attn.proj.bias\n",
      "64: stages.1.blocks.1.norm1.weight\n",
      "65: stages.1.blocks.1.norm1.bias\n",
      "66: stages.1.blocks.1.mlp.fc1.weight\n",
      "67: stages.1.blocks.1.mlp.fc1.bias\n",
      "68: stages.1.blocks.1.mlp.fc2.weight\n",
      "69: stages.1.blocks.1.mlp.fc2.bias\n",
      "70: stages.1.blocks.1.norm2.weight\n",
      "71: stages.1.blocks.1.norm2.bias\n",
      "72: stages.1.downsample.reduction.weight\n",
      "73: stages.1.downsample.norm.weight\n",
      "74: stages.1.downsample.norm.bias\n",
      "75: stages.2.blocks.0.attn.t_scale\n",
      "76: stages.2.blocks.0.attn.crpb_mlp.0.weight\n",
      "77: stages.2.blocks.0.attn.crpb_mlp.0.bias\n",
      "78: stages.2.blocks.0.attn.crpb_mlp.3.weight\n",
      "79: stages.2.blocks.0.attn.qkv.weight\n",
      "80: stages.2.blocks.0.attn.qkv.bias\n",
      "81: stages.2.blocks.0.attn.proj.weight\n",
      "82: stages.2.blocks.0.attn.proj.bias\n",
      "83: stages.2.blocks.0.norm1.weight\n",
      "84: stages.2.blocks.0.norm1.bias\n",
      "85: stages.2.blocks.0.mlp.fc1.weight\n",
      "86: stages.2.blocks.0.mlp.fc1.bias\n",
      "87: stages.2.blocks.0.mlp.fc2.weight\n",
      "88: stages.2.blocks.0.mlp.fc2.bias\n",
      "89: stages.2.blocks.0.norm2.weight\n",
      "90: stages.2.blocks.0.norm2.bias\n",
      "91: stages.2.blocks.1.attn.t_scale\n",
      "92: stages.2.blocks.1.attn.crpb_mlp.0.weight\n",
      "93: stages.2.blocks.1.attn.crpb_mlp.0.bias\n",
      "94: stages.2.blocks.1.attn.crpb_mlp.3.weight\n",
      "95: stages.2.blocks.1.attn.qkv.weight\n",
      "96: stages.2.blocks.1.attn.qkv.bias\n",
      "97: stages.2.blocks.1.attn.proj.weight\n",
      "98: stages.2.blocks.1.attn.proj.bias\n",
      "99: stages.2.blocks.1.norm1.weight\n",
      "100: stages.2.blocks.1.norm1.bias\n",
      "101: stages.2.blocks.1.mlp.fc1.weight\n",
      "102: stages.2.blocks.1.mlp.fc1.bias\n",
      "103: stages.2.blocks.1.mlp.fc2.weight\n",
      "104: stages.2.blocks.1.mlp.fc2.bias\n",
      "105: stages.2.blocks.1.norm2.weight\n",
      "106: stages.2.blocks.1.norm2.bias\n",
      "107: stages.2.blocks.2.attn.t_scale\n",
      "108: stages.2.blocks.2.attn.crpb_mlp.0.weight\n",
      "109: stages.2.blocks.2.attn.crpb_mlp.0.bias\n",
      "110: stages.2.blocks.2.attn.crpb_mlp.3.weight\n",
      "111: stages.2.blocks.2.attn.qkv.weight\n",
      "112: stages.2.blocks.2.attn.qkv.bias\n",
      "113: stages.2.blocks.2.attn.proj.weight\n",
      "114: stages.2.blocks.2.attn.proj.bias\n",
      "115: stages.2.blocks.2.norm1.weight\n",
      "116: stages.2.blocks.2.norm1.bias\n",
      "117: stages.2.blocks.2.mlp.fc1.weight\n",
      "118: stages.2.blocks.2.mlp.fc1.bias\n",
      "119: stages.2.blocks.2.mlp.fc2.weight\n",
      "120: stages.2.blocks.2.mlp.fc2.bias\n",
      "121: stages.2.blocks.2.norm2.weight\n",
      "122: stages.2.blocks.2.norm2.bias\n",
      "123: stages.2.blocks.3.attn.t_scale\n",
      "124: stages.2.blocks.3.attn.crpb_mlp.0.weight\n",
      "125: stages.2.blocks.3.attn.crpb_mlp.0.bias\n",
      "126: stages.2.blocks.3.attn.crpb_mlp.3.weight\n",
      "127: stages.2.blocks.3.attn.qkv.weight\n",
      "128: stages.2.blocks.3.attn.qkv.bias\n",
      "129: stages.2.blocks.3.attn.proj.weight\n",
      "130: stages.2.blocks.3.attn.proj.bias\n",
      "131: stages.2.blocks.3.norm1.weight\n",
      "132: stages.2.blocks.3.norm1.bias\n",
      "133: stages.2.blocks.3.mlp.fc1.weight\n",
      "134: stages.2.blocks.3.mlp.fc1.bias\n",
      "135: stages.2.blocks.3.mlp.fc2.weight\n",
      "136: stages.2.blocks.3.mlp.fc2.bias\n",
      "137: stages.2.blocks.3.norm2.weight\n",
      "138: stages.2.blocks.3.norm2.bias\n",
      "139: stages.2.blocks.4.attn.t_scale\n",
      "140: stages.2.blocks.4.attn.crpb_mlp.0.weight\n",
      "141: stages.2.blocks.4.attn.crpb_mlp.0.bias\n",
      "142: stages.2.blocks.4.attn.crpb_mlp.3.weight\n",
      "143: stages.2.blocks.4.attn.qkv.weight\n",
      "144: stages.2.blocks.4.attn.qkv.bias\n",
      "145: stages.2.blocks.4.attn.proj.weight\n",
      "146: stages.2.blocks.4.attn.proj.bias\n",
      "147: stages.2.blocks.4.norm1.weight\n",
      "148: stages.2.blocks.4.norm1.bias\n",
      "149: stages.2.blocks.4.mlp.fc1.weight\n",
      "150: stages.2.blocks.4.mlp.fc1.bias\n",
      "151: stages.2.blocks.4.mlp.fc2.weight\n",
      "152: stages.2.blocks.4.mlp.fc2.bias\n",
      "153: stages.2.blocks.4.norm2.weight\n",
      "154: stages.2.blocks.4.norm2.bias\n",
      "155: stages.2.blocks.5.attn.t_scale\n",
      "156: stages.2.blocks.5.attn.crpb_mlp.0.weight\n",
      "157: stages.2.blocks.5.attn.crpb_mlp.0.bias\n",
      "158: stages.2.blocks.5.attn.crpb_mlp.3.weight\n",
      "159: stages.2.blocks.5.attn.qkv.weight\n",
      "160: stages.2.blocks.5.attn.qkv.bias\n",
      "161: stages.2.blocks.5.attn.proj.weight\n",
      "162: stages.2.blocks.5.attn.proj.bias\n",
      "163: stages.2.blocks.5.norm1.weight\n",
      "164: stages.2.blocks.5.norm1.bias\n",
      "165: stages.2.blocks.5.mlp.fc1.weight\n",
      "166: stages.2.blocks.5.mlp.fc1.bias\n",
      "167: stages.2.blocks.5.mlp.fc2.weight\n",
      "168: stages.2.blocks.5.mlp.fc2.bias\n",
      "169: stages.2.blocks.5.norm2.weight\n",
      "170: stages.2.blocks.5.norm2.bias\n",
      "171: stages.2.downsample.reduction.weight\n",
      "172: stages.2.downsample.norm.weight\n",
      "173: stages.2.downsample.norm.bias\n",
      "174: stages.3.blocks.0.attn.t_scale\n",
      "175: stages.3.blocks.0.attn.crpb_mlp.0.weight\n",
      "176: stages.3.blocks.0.attn.crpb_mlp.0.bias\n",
      "177: stages.3.blocks.0.attn.crpb_mlp.3.weight\n",
      "178: stages.3.blocks.0.attn.qkv.weight\n",
      "179: stages.3.blocks.0.attn.qkv.bias\n",
      "180: stages.3.blocks.0.attn.proj.weight\n",
      "181: stages.3.blocks.0.attn.proj.bias\n",
      "182: stages.3.blocks.0.norm1.weight\n",
      "183: stages.3.blocks.0.norm1.bias\n",
      "184: stages.3.blocks.0.mlp.fc1.weight\n",
      "185: stages.3.blocks.0.mlp.fc1.bias\n",
      "186: stages.3.blocks.0.mlp.fc2.weight\n",
      "187: stages.3.blocks.0.mlp.fc2.bias\n",
      "188: stages.3.blocks.0.norm2.weight\n",
      "189: stages.3.blocks.0.norm2.bias\n",
      "190: stages.3.blocks.1.attn.t_scale\n",
      "191: stages.3.blocks.1.attn.crpb_mlp.0.weight\n",
      "192: stages.3.blocks.1.attn.crpb_mlp.0.bias\n",
      "193: stages.3.blocks.1.attn.crpb_mlp.3.weight\n",
      "194: stages.3.blocks.1.attn.qkv.weight\n",
      "195: stages.3.blocks.1.attn.qkv.bias\n",
      "196: stages.3.blocks.1.attn.proj.weight\n",
      "197: stages.3.blocks.1.attn.proj.bias\n",
      "198: stages.3.blocks.1.norm1.weight\n",
      "199: stages.3.blocks.1.norm1.bias\n",
      "200: stages.3.blocks.1.mlp.fc1.weight\n",
      "201: stages.3.blocks.1.mlp.fc1.bias\n",
      "202: stages.3.blocks.1.mlp.fc2.weight\n",
      "203: stages.3.blocks.1.mlp.fc2.bias\n",
      "204: stages.3.blocks.1.norm2.weight\n",
      "205: stages.3.blocks.1.norm2.bias\n",
      "206: layernorm.weight\n",
      "207: layernorm.bias\n",
      "208: classifier.weight\n",
      "209: classifier.bias\n"
     ]
    }
   ],
   "source": [
    "layer_names = []\n",
    "for i, (name, params) in enumerate(model.named_parameters()):\n",
    "    print(f'{i}: {name}')\n",
    "    layer_names.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['classifier.bias',\n",
       " 'classifier.weight',\n",
       " 'layernorm.bias',\n",
       " 'layernorm.weight',\n",
       " 'stages.3.blocks.1.norm2.bias']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_names.reverse()\n",
    "layer_names[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: classifier.bias's lr=0.0014\n",
      "1: classifier.weight's lr=0.0014\n",
      "2: layernorm.bias's lr=0.001218\n",
      "3: layernorm.weight's lr=0.001218\n",
      "4: stages.3.blocks.1.norm2.bias's lr=0.0010596599999999998\n",
      "5: stages.3.blocks.1.norm2.weight's lr=0.0010596599999999998\n",
      "6: stages.3.blocks.1.mlp.fc2.bias's lr=0.0010596599999999998\n",
      "7: stages.3.blocks.1.mlp.fc2.weight's lr=0.0010596599999999998\n",
      "8: stages.3.blocks.1.mlp.fc1.bias's lr=0.0010596599999999998\n",
      "9: stages.3.blocks.1.mlp.fc1.weight's lr=0.0010596599999999998\n",
      "10: stages.3.blocks.1.norm1.bias's lr=0.0010596599999999998\n",
      "11: stages.3.blocks.1.norm1.weight's lr=0.0010596599999999998\n",
      "12: stages.3.blocks.1.attn.proj.bias's lr=0.0010596599999999998\n",
      "13: stages.3.blocks.1.attn.proj.weight's lr=0.0010596599999999998\n",
      "14: stages.3.blocks.1.attn.qkv.bias's lr=0.0010596599999999998\n",
      "15: stages.3.blocks.1.attn.qkv.weight's lr=0.0010596599999999998\n",
      "16: stages.3.blocks.1.attn.crpb_mlp.3.weight's lr=0.0010596599999999998\n",
      "17: stages.3.blocks.1.attn.crpb_mlp.0.bias's lr=0.0010596599999999998\n",
      "18: stages.3.blocks.1.attn.crpb_mlp.0.weight's lr=0.0010596599999999998\n",
      "19: stages.3.blocks.1.attn.t_scale's lr=0.0010596599999999998\n",
      "20: stages.3.blocks.0.norm2.bias's lr=0.0010596599999999998\n",
      "21: stages.3.blocks.0.norm2.weight's lr=0.0010596599999999998\n",
      "22: stages.3.blocks.0.mlp.fc2.bias's lr=0.0010596599999999998\n",
      "23: stages.3.blocks.0.mlp.fc2.weight's lr=0.0010596599999999998\n",
      "24: stages.3.blocks.0.mlp.fc1.bias's lr=0.0010596599999999998\n",
      "25: stages.3.blocks.0.mlp.fc1.weight's lr=0.0010596599999999998\n",
      "26: stages.3.blocks.0.norm1.bias's lr=0.0010596599999999998\n",
      "27: stages.3.blocks.0.norm1.weight's lr=0.0010596599999999998\n",
      "28: stages.3.blocks.0.attn.proj.bias's lr=0.0010596599999999998\n",
      "29: stages.3.blocks.0.attn.proj.weight's lr=0.0010596599999999998\n",
      "30: stages.3.blocks.0.attn.qkv.bias's lr=0.0010596599999999998\n",
      "31: stages.3.blocks.0.attn.qkv.weight's lr=0.0010596599999999998\n",
      "32: stages.3.blocks.0.attn.crpb_mlp.3.weight's lr=0.0010596599999999998\n",
      "33: stages.3.blocks.0.attn.crpb_mlp.0.bias's lr=0.0010596599999999998\n",
      "34: stages.3.blocks.0.attn.crpb_mlp.0.weight's lr=0.0010596599999999998\n",
      "35: stages.3.blocks.0.attn.t_scale's lr=0.0010596599999999998\n",
      "36: stages.2.downsample.norm.bias's lr=0.0010596599999999998\n",
      "37: stages.2.downsample.norm.weight's lr=0.0010596599999999998\n",
      "38: stages.2.downsample.reduction.weight's lr=0.0010596599999999998\n",
      "39: stages.2.blocks.5.norm2.bias's lr=0.0010596599999999998\n",
      "40: stages.2.blocks.5.norm2.weight's lr=0.0010596599999999998\n",
      "41: stages.2.blocks.5.mlp.fc2.bias's lr=0.0010596599999999998\n",
      "42: stages.2.blocks.5.mlp.fc2.weight's lr=0.0010596599999999998\n",
      "43: stages.2.blocks.5.mlp.fc1.bias's lr=0.0010596599999999998\n",
      "44: stages.2.blocks.5.mlp.fc1.weight's lr=0.0010596599999999998\n",
      "45: stages.2.blocks.5.norm1.bias's lr=0.0010596599999999998\n",
      "46: stages.2.blocks.5.norm1.weight's lr=0.0010596599999999998\n",
      "47: stages.2.blocks.5.attn.proj.bias's lr=0.0010596599999999998\n",
      "48: stages.2.blocks.5.attn.proj.weight's lr=0.0010596599999999998\n",
      "49: stages.2.blocks.5.attn.qkv.bias's lr=0.0010596599999999998\n",
      "50: stages.2.blocks.5.attn.qkv.weight's lr=0.0010596599999999998\n",
      "51: stages.2.blocks.5.attn.crpb_mlp.3.weight's lr=0.0010596599999999998\n",
      "52: stages.2.blocks.5.attn.crpb_mlp.0.bias's lr=0.0010596599999999998\n",
      "53: stages.2.blocks.5.attn.crpb_mlp.0.weight's lr=0.0010596599999999998\n",
      "54: stages.2.blocks.5.attn.t_scale's lr=0.0010596599999999998\n",
      "55: stages.2.blocks.4.norm2.bias's lr=0.0010596599999999998\n",
      "56: stages.2.blocks.4.norm2.weight's lr=0.0010596599999999998\n",
      "57: stages.2.blocks.4.mlp.fc2.bias's lr=0.0010596599999999998\n",
      "58: stages.2.blocks.4.mlp.fc2.weight's lr=0.0010596599999999998\n",
      "59: stages.2.blocks.4.mlp.fc1.bias's lr=0.0010596599999999998\n",
      "60: stages.2.blocks.4.mlp.fc1.weight's lr=0.0010596599999999998\n",
      "61: stages.2.blocks.4.norm1.bias's lr=0.0010596599999999998\n",
      "62: stages.2.blocks.4.norm1.weight's lr=0.0010596599999999998\n",
      "63: stages.2.blocks.4.attn.proj.bias's lr=0.0010596599999999998\n",
      "64: stages.2.blocks.4.attn.proj.weight's lr=0.0010596599999999998\n",
      "65: stages.2.blocks.4.attn.qkv.bias's lr=0.0010596599999999998\n",
      "66: stages.2.blocks.4.attn.qkv.weight's lr=0.0010596599999999998\n",
      "67: stages.2.blocks.4.attn.crpb_mlp.3.weight's lr=0.0010596599999999998\n",
      "68: stages.2.blocks.4.attn.crpb_mlp.0.bias's lr=0.0010596599999999998\n",
      "69: stages.2.blocks.4.attn.crpb_mlp.0.weight's lr=0.0010596599999999998\n",
      "70: stages.2.blocks.4.attn.t_scale's lr=0.0010596599999999998\n",
      "71: stages.2.blocks.3.norm2.bias's lr=0.0010596599999999998\n",
      "72: stages.2.blocks.3.norm2.weight's lr=0.0010596599999999998\n",
      "73: stages.2.blocks.3.mlp.fc2.bias's lr=0.0010596599999999998\n",
      "74: stages.2.blocks.3.mlp.fc2.weight's lr=0.0010596599999999998\n",
      "75: stages.2.blocks.3.mlp.fc1.bias's lr=0.0010596599999999998\n",
      "76: stages.2.blocks.3.mlp.fc1.weight's lr=0.0010596599999999998\n",
      "77: stages.2.blocks.3.norm1.bias's lr=0.0010596599999999998\n",
      "78: stages.2.blocks.3.norm1.weight's lr=0.0010596599999999998\n",
      "79: stages.2.blocks.3.attn.proj.bias's lr=0.0010596599999999998\n",
      "80: stages.2.blocks.3.attn.proj.weight's lr=0.0010596599999999998\n",
      "81: stages.2.blocks.3.attn.qkv.bias's lr=0.0010596599999999998\n",
      "82: stages.2.blocks.3.attn.qkv.weight's lr=0.0010596599999999998\n",
      "83: stages.2.blocks.3.attn.crpb_mlp.3.weight's lr=0.0010596599999999998\n",
      "84: stages.2.blocks.3.attn.crpb_mlp.0.bias's lr=0.0010596599999999998\n",
      "85: stages.2.blocks.3.attn.crpb_mlp.0.weight's lr=0.0010596599999999998\n",
      "86: stages.2.blocks.3.attn.t_scale's lr=0.0010596599999999998\n",
      "87: stages.2.blocks.2.norm2.bias's lr=0.0010596599999999998\n",
      "88: stages.2.blocks.2.norm2.weight's lr=0.0010596599999999998\n",
      "89: stages.2.blocks.2.mlp.fc2.bias's lr=0.0010596599999999998\n",
      "90: stages.2.blocks.2.mlp.fc2.weight's lr=0.0010596599999999998\n",
      "91: stages.2.blocks.2.mlp.fc1.bias's lr=0.0010596599999999998\n",
      "92: stages.2.blocks.2.mlp.fc1.weight's lr=0.0010596599999999998\n",
      "93: stages.2.blocks.2.norm1.bias's lr=0.0010596599999999998\n",
      "94: stages.2.blocks.2.norm1.weight's lr=0.0010596599999999998\n",
      "95: stages.2.blocks.2.attn.proj.bias's lr=0.0010596599999999998\n",
      "96: stages.2.blocks.2.attn.proj.weight's lr=0.0010596599999999998\n",
      "97: stages.2.blocks.2.attn.qkv.bias's lr=0.0010596599999999998\n",
      "98: stages.2.blocks.2.attn.qkv.weight's lr=0.0010596599999999998\n",
      "99: stages.2.blocks.2.attn.crpb_mlp.3.weight's lr=0.0010596599999999998\n",
      "100: stages.2.blocks.2.attn.crpb_mlp.0.bias's lr=0.0010596599999999998\n",
      "101: stages.2.blocks.2.attn.crpb_mlp.0.weight's lr=0.0010596599999999998\n",
      "102: stages.2.blocks.2.attn.t_scale's lr=0.0010596599999999998\n",
      "103: stages.2.blocks.1.norm2.bias's lr=0.0010596599999999998\n",
      "104: stages.2.blocks.1.norm2.weight's lr=0.0010596599999999998\n",
      "105: stages.2.blocks.1.mlp.fc2.bias's lr=0.0010596599999999998\n",
      "106: stages.2.blocks.1.mlp.fc2.weight's lr=0.0010596599999999998\n",
      "107: stages.2.blocks.1.mlp.fc1.bias's lr=0.0010596599999999998\n",
      "108: stages.2.blocks.1.mlp.fc1.weight's lr=0.0010596599999999998\n",
      "109: stages.2.blocks.1.norm1.bias's lr=0.0010596599999999998\n",
      "110: stages.2.blocks.1.norm1.weight's lr=0.0010596599999999998\n",
      "111: stages.2.blocks.1.attn.proj.bias's lr=0.0010596599999999998\n",
      "112: stages.2.blocks.1.attn.proj.weight's lr=0.0010596599999999998\n",
      "113: stages.2.blocks.1.attn.qkv.bias's lr=0.0010596599999999998\n",
      "114: stages.2.blocks.1.attn.qkv.weight's lr=0.0010596599999999998\n",
      "115: stages.2.blocks.1.attn.crpb_mlp.3.weight's lr=0.0010596599999999998\n",
      "116: stages.2.blocks.1.attn.crpb_mlp.0.bias's lr=0.0010596599999999998\n",
      "117: stages.2.blocks.1.attn.crpb_mlp.0.weight's lr=0.0010596599999999998\n",
      "118: stages.2.blocks.1.attn.t_scale's lr=0.0010596599999999998\n",
      "119: stages.2.blocks.0.norm2.bias's lr=0.0010596599999999998\n",
      "120: stages.2.blocks.0.norm2.weight's lr=0.0010596599999999998\n",
      "121: stages.2.blocks.0.mlp.fc2.bias's lr=0.0010596599999999998\n",
      "122: stages.2.blocks.0.mlp.fc2.weight's lr=0.0010596599999999998\n",
      "123: stages.2.blocks.0.mlp.fc1.bias's lr=0.0010596599999999998\n",
      "124: stages.2.blocks.0.mlp.fc1.weight's lr=0.0010596599999999998\n",
      "125: stages.2.blocks.0.norm1.bias's lr=0.0010596599999999998\n",
      "126: stages.2.blocks.0.norm1.weight's lr=0.0010596599999999998\n",
      "127: stages.2.blocks.0.attn.proj.bias's lr=0.0010596599999999998\n",
      "128: stages.2.blocks.0.attn.proj.weight's lr=0.0010596599999999998\n",
      "129: stages.2.blocks.0.attn.qkv.bias's lr=0.0010596599999999998\n",
      "130: stages.2.blocks.0.attn.qkv.weight's lr=0.0010596599999999998\n",
      "131: stages.2.blocks.0.attn.crpb_mlp.3.weight's lr=0.0010596599999999998\n",
      "132: stages.2.blocks.0.attn.crpb_mlp.0.bias's lr=0.0010596599999999998\n",
      "133: stages.2.blocks.0.attn.crpb_mlp.0.weight's lr=0.0010596599999999998\n",
      "134: stages.2.blocks.0.attn.t_scale's lr=0.0010596599999999998\n",
      "135: stages.1.downsample.norm.bias's lr=0.0010596599999999998\n",
      "136: stages.1.downsample.norm.weight's lr=0.0010596599999999998\n",
      "137: stages.1.downsample.reduction.weight's lr=0.0010596599999999998\n",
      "138: stages.1.blocks.1.norm2.bias's lr=0.0010596599999999998\n",
      "139: stages.1.blocks.1.norm2.weight's lr=0.0010596599999999998\n",
      "140: stages.1.blocks.1.mlp.fc2.bias's lr=0.0010596599999999998\n",
      "141: stages.1.blocks.1.mlp.fc2.weight's lr=0.0010596599999999998\n",
      "142: stages.1.blocks.1.mlp.fc1.bias's lr=0.0010596599999999998\n",
      "143: stages.1.blocks.1.mlp.fc1.weight's lr=0.0010596599999999998\n",
      "144: stages.1.blocks.1.norm1.bias's lr=0.0010596599999999998\n",
      "145: stages.1.blocks.1.norm1.weight's lr=0.0010596599999999998\n",
      "146: stages.1.blocks.1.attn.proj.bias's lr=0.0010596599999999998\n",
      "147: stages.1.blocks.1.attn.proj.weight's lr=0.0010596599999999998\n",
      "148: stages.1.blocks.1.attn.qkv.bias's lr=0.0010596599999999998\n",
      "149: stages.1.blocks.1.attn.qkv.weight's lr=0.0010596599999999998\n",
      "150: stages.1.blocks.1.attn.crpb_mlp.3.weight's lr=0.0010596599999999998\n",
      "151: stages.1.blocks.1.attn.crpb_mlp.0.bias's lr=0.0010596599999999998\n",
      "152: stages.1.blocks.1.attn.crpb_mlp.0.weight's lr=0.0010596599999999998\n",
      "153: stages.1.blocks.1.attn.t_scale's lr=0.0010596599999999998\n",
      "154: stages.1.blocks.0.norm2.bias's lr=0.0010596599999999998\n",
      "155: stages.1.blocks.0.norm2.weight's lr=0.0010596599999999998\n",
      "156: stages.1.blocks.0.mlp.fc2.bias's lr=0.0010596599999999998\n",
      "157: stages.1.blocks.0.mlp.fc2.weight's lr=0.0010596599999999998\n",
      "158: stages.1.blocks.0.mlp.fc1.bias's lr=0.0010596599999999998\n",
      "159: stages.1.blocks.0.mlp.fc1.weight's lr=0.0010596599999999998\n",
      "160: stages.1.blocks.0.norm1.bias's lr=0.0010596599999999998\n",
      "161: stages.1.blocks.0.norm1.weight's lr=0.0010596599999999998\n",
      "162: stages.1.blocks.0.attn.proj.bias's lr=0.0010596599999999998\n",
      "163: stages.1.blocks.0.attn.proj.weight's lr=0.0010596599999999998\n",
      "164: stages.1.blocks.0.attn.qkv.bias's lr=0.0010596599999999998\n",
      "165: stages.1.blocks.0.attn.qkv.weight's lr=0.0010596599999999998\n",
      "166: stages.1.blocks.0.attn.crpb_mlp.3.weight's lr=0.0010596599999999998\n",
      "167: stages.1.blocks.0.attn.crpb_mlp.0.bias's lr=0.0010596599999999998\n",
      "168: stages.1.blocks.0.attn.crpb_mlp.0.weight's lr=0.0010596599999999998\n",
      "169: stages.1.blocks.0.attn.t_scale's lr=0.0010596599999999998\n",
      "170: stages.0.downsample.norm.bias's lr=0.0010596599999999998\n",
      "171: stages.0.downsample.norm.weight's lr=0.0010596599999999998\n",
      "172: stages.0.downsample.reduction.weight's lr=0.0010596599999999998\n",
      "173: stages.0.blocks.1.norm2.bias's lr=0.0010596599999999998\n",
      "174: stages.0.blocks.1.norm2.weight's lr=0.0010596599999999998\n",
      "175: stages.0.blocks.1.mlp.fc2.bias's lr=0.0010596599999999998\n",
      "176: stages.0.blocks.1.mlp.fc2.weight's lr=0.0010596599999999998\n",
      "177: stages.0.blocks.1.mlp.fc1.bias's lr=0.0010596599999999998\n",
      "178: stages.0.blocks.1.mlp.fc1.weight's lr=0.0010596599999999998\n",
      "179: stages.0.blocks.1.norm1.bias's lr=0.0010596599999999998\n",
      "180: stages.0.blocks.1.norm1.weight's lr=0.0010596599999999998\n",
      "181: stages.0.blocks.1.attn.proj.bias's lr=0.0010596599999999998\n",
      "182: stages.0.blocks.1.attn.proj.weight's lr=0.0010596599999999998\n",
      "183: stages.0.blocks.1.attn.qkv.bias's lr=0.0010596599999999998\n",
      "184: stages.0.blocks.1.attn.qkv.weight's lr=0.0010596599999999998\n",
      "185: stages.0.blocks.1.attn.crpb_mlp.3.weight's lr=0.0010596599999999998\n",
      "186: stages.0.blocks.1.attn.crpb_mlp.0.bias's lr=0.0010596599999999998\n",
      "187: stages.0.blocks.1.attn.crpb_mlp.0.weight's lr=0.0010596599999999998\n",
      "188: stages.0.blocks.1.attn.t_scale's lr=0.0010596599999999998\n",
      "189: stages.0.blocks.0.norm2.bias's lr=0.0010596599999999998\n",
      "190: stages.0.blocks.0.norm2.weight's lr=0.0010596599999999998\n",
      "191: stages.0.blocks.0.mlp.fc2.bias's lr=0.0010596599999999998\n",
      "192: stages.0.blocks.0.mlp.fc2.weight's lr=0.0010596599999999998\n",
      "193: stages.0.blocks.0.mlp.fc1.bias's lr=0.0010596599999999998\n",
      "194: stages.0.blocks.0.mlp.fc1.weight's lr=0.0010596599999999998\n",
      "195: stages.0.blocks.0.norm1.bias's lr=0.0010596599999999998\n",
      "196: stages.0.blocks.0.norm1.weight's lr=0.0010596599999999998\n",
      "197: stages.0.blocks.0.attn.proj.bias's lr=0.0010596599999999998\n",
      "198: stages.0.blocks.0.attn.proj.weight's lr=0.0010596599999999998\n",
      "199: stages.0.blocks.0.attn.qkv.bias's lr=0.0010596599999999998\n",
      "200: stages.0.blocks.0.attn.qkv.weight's lr=0.0010596599999999998\n",
      "201: stages.0.blocks.0.attn.crpb_mlp.3.weight's lr=0.0010596599999999998\n",
      "202: stages.0.blocks.0.attn.crpb_mlp.0.bias's lr=0.0010596599999999998\n",
      "203: stages.0.blocks.0.attn.crpb_mlp.0.weight's lr=0.0010596599999999998\n",
      "204: stages.0.blocks.0.attn.t_scale's lr=0.0010596599999999998\n",
      "205: embeddings.norm.bias's lr=0.0009219041999999998\n",
      "206: embeddings.norm.weight's lr=0.0009219041999999998\n",
      "207: embeddings.patch_embeddings.bias's lr=0.0009219041999999998\n",
      "208: embeddings.patch_embeddings.weight's lr=0.0009219041999999998\n",
      "209: absolute_pos_embed's lr=0.0008020566539999999\n"
     ]
    }
   ],
   "source": [
    "lr      = 1.4e-3   # paper : 1.4e-3\n",
    "lr_mult = 0.87     # paper : 0.87\n",
    "weight_decay = 0.001 # paper : 0.1\n",
    "\n",
    "param_groups = []\n",
    "prev_group_name = layer_names[0].split('.')[0]\n",
    "\n",
    "for idx, name in enumerate(layer_names):\n",
    "    \n",
    "    cur_group_name = name.split('.')[0]\n",
    "    \n",
    "    if cur_group_name != prev_group_name:\n",
    "        lr *= lr_mult\n",
    "    prev_group_name = cur_group_name\n",
    "    \n",
    "    print(f\"{idx}: {name}'s lr={lr}\")\n",
    "    \n",
    "    param_groups += [{'params': [ p for n, p in model.named_parameters() if n == name and p.requires_grad],\n",
    "                      'lr' : lr,\n",
    "                      'weight_decay': weight_decay}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(param_groups)\n",
    "warmup_steps = int(len(train_loader)*(epochs)*0.1)\n",
    "train_steps = len(train_loader)*(epochs)\n",
    "# scheduler = transformers.get_cosine_schedule_with_warmup(optimizer, \n",
    "#                                                         num_warmup_steps=warmup_steps, \n",
    "#                                                         num_training_steps=train_steps,\n",
    "#                                                         num_cycles=0.5)\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.99, patience=30, cooldown=15, min_lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  13%|█▎        | 2/15 [00:08<00:54,  4.19s/it]"
     ]
    }
   ],
   "source": [
    "training_time = 0\n",
    "losses = []\n",
    "val_losses = []\n",
    "lrs = []\n",
    "best_loss = float('inf')\n",
    "\n",
    "# GradScaler 초기화\n",
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch + 1}\")\n",
    "    \n",
    "    for _, data in pbar:\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        inputs, labels = mixup_fn(inputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # AutoCast 적용\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "        # 스케일링된 그라디언트 계산\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # 그라디언트 클리핑 전에 스케일링 제거\n",
    "        scaler.unscale_(optimizer)\n",
    "        clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "\n",
    "        # 옵티마이저 스텝 및 스케일러 업데이트\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step(loss)\n",
    "            \n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        lrs.append(lr)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)        \n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in valid_loader:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    val_loss /= len(valid_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # 모델 저장\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        vit_save = True\n",
    "        if vit_save:\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_duration = time.time() - start_time\n",
    "    training_time += epoch_duration\n",
    "    \n",
    "    text = f'\\tLoss: {epoch_loss}, Val Loss: {val_loss}, LR: {lr}, Duration: {epoch_duration:.2f} sec'\n",
    "    \n",
    "    if vit_save:\n",
    "        text += f' - model saved!'\n",
    "        vit_save = False\n",
    "\n",
    "    print(text)\n",
    "        \n",
    "text = f\"Epoch 당 평균 소요시간 : {training_time / 100:.2f}초\"      \n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 수행 및 레이블 저장\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# 혼동 행렬 생성\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# 예측과 실제 레이블\n",
    "y_true = all_labels  # 실제 레이블\n",
    "y_pred = all_preds  # 모델에 의해 예측된 레이블\n",
    "\n",
    "# 전체 데이터셋에 대한 정확도\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# 평균 정밀도, 리콜, F1-Score ('weighted')\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "\n",
    "# 판다스 데이터프레임으로 결과 정리\n",
    "performance_metrics = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "    'Value': [accuracy, precision, recall, f1_score]\n",
    "})\n",
    "\n",
    "# 데이터프레임 출력\n",
    "performance_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch + 1}\")\n",
    "    \n",
    "    for _, data in pbar:\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        inputs, labels = mixup_fn(inputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # AutoCast 적용\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "        # 스케일링된 그라디언트 계산\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # 그라디언트 클리핑 전에 스케일링 제거\n",
    "        scaler.unscale_(optimizer)\n",
    "        clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "\n",
    "        # 옵티마이저 스텝 및 스케일러 업데이트\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step(loss)\n",
    "            \n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        lrs.append(lr)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)        \n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in valid_loader:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    val_loss /= len(valid_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # 모델 저장\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        vit_save = True\n",
    "        if vit_save:\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_duration = time.time() - start_time\n",
    "    training_time += epoch_duration\n",
    "    \n",
    "    text = f'\\tLoss: {epoch_loss}, Val Loss: {val_loss}, LR: {lr}, Duration: {epoch_duration:.2f} sec'\n",
    "    \n",
    "    if vit_save:\n",
    "        text += f' - model saved!'\n",
    "        vit_save = False\n",
    "\n",
    "    print(text)\n",
    "        \n",
    "text = f\"Epoch 당 평균 소요시간 : {training_time / 110:.2f}초\"      \n",
    "print(text)\n",
    "\n",
    "# 예측 수행 및 레이블 저장\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# 혼동 행렬 생성\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# 예측과 실제 레이블\n",
    "y_true = all_labels  # 실제 레이블\n",
    "y_pred = all_preds  # 모델에 의해 예측된 레이블\n",
    "\n",
    "# 전체 데이터셋에 대한 정확도\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# 평균 정밀도, 리콜, F1-Score ('weighted')\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "\n",
    "# 판다스 데이터프레임으로 결과 정리\n",
    "performance_metrics = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "    'Value': [accuracy, precision, recall, f1_score]\n",
    "})\n",
    "\n",
    "# 데이터프레임 출력\n",
    "performance_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch + 1}\")\n",
    "    \n",
    "    for _, data in pbar:\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        inputs, labels = mixup_fn(inputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # AutoCast 적용\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "        # 스케일링된 그라디언트 계산\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # 그라디언트 클리핑 전에 스케일링 제거\n",
    "        scaler.unscale_(optimizer)\n",
    "        clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "\n",
    "        # 옵티마이저 스텝 및 스케일러 업데이트\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step(loss)\n",
    "            \n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        lrs.append(lr)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)        \n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in valid_loader:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    val_loss /= len(valid_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # 모델 저장\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        vit_save = True\n",
    "        if vit_save:\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_duration = time.time() - start_time\n",
    "    training_time += epoch_duration\n",
    "    \n",
    "    text = f'\\tLoss: {epoch_loss}, Val Loss: {val_loss}, LR: {lr}, Duration: {epoch_duration:.2f} sec'\n",
    "    \n",
    "    if vit_save:\n",
    "        text += f' - model saved!'\n",
    "        vit_save = False\n",
    "\n",
    "    print(text)\n",
    "        \n",
    "text = f\"Epoch 당 평균 소요시간 : {training_time / 120:.2f}초\"      \n",
    "print(text)\n",
    "\n",
    "# 예측 수행 및 레이블 저장\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# 혼동 행렬 생성\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# 예측과 실제 레이블\n",
    "y_true = all_labels  # 실제 레이블\n",
    "y_pred = all_preds  # 모델에 의해 예측된 레이블\n",
    "\n",
    "# 전체 데이터셋에 대한 정확도\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# 평균 정밀도, 리콜, F1-Score ('weighted')\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "\n",
    "# 판다스 데이터프레임으로 결과 정리\n",
    "performance_metrics = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "    'Value': [accuracy, precision, recall, f1_score]\n",
    "})\n",
    "\n",
    "# 데이터프레임 출력\n",
    "performance_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch + 1}\")\n",
    "    \n",
    "    for _, data in pbar:\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        inputs, labels = mixup_fn(inputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # AutoCast 적용\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "        # 스케일링된 그라디언트 계산\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # 그라디언트 클리핑 전에 스케일링 제거\n",
    "        scaler.unscale_(optimizer)\n",
    "        clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "\n",
    "        # 옵티마이저 스텝 및 스케일러 업데이트\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step(loss)\n",
    "            \n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        lrs.append(lr)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)        \n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in valid_loader:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    val_loss /= len(valid_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # 모델 저장\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        vit_save = True\n",
    "        if vit_save:\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_duration = time.time() - start_time\n",
    "    training_time += epoch_duration\n",
    "    \n",
    "    text = f'\\tLoss: {epoch_loss}, Val Loss: {val_loss}, LR: {lr}, Duration: {epoch_duration:.2f} sec'\n",
    "    \n",
    "    if vit_save:\n",
    "        text += f' - model saved!'\n",
    "        vit_save = False\n",
    "\n",
    "    print(text)\n",
    "        \n",
    "text = f\"Epoch 당 평균 소요시간 : {training_time / 130:.2f}초\"      \n",
    "print(text)\n",
    "\n",
    "# 예측 수행 및 레이블 저장\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# 혼동 행렬 생성\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# 예측과 실제 레이블\n",
    "y_true = all_labels  # 실제 레이블\n",
    "y_pred = all_preds  # 모델에 의해 예측된 레이블\n",
    "\n",
    "# 전체 데이터셋에 대한 정확도\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# 평균 정밀도, 리콜, F1-Score ('weighted')\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "\n",
    "# 판다스 데이터프레임으로 결과 정리\n",
    "performance_metrics = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "    'Value': [accuracy, precision, recall, f1_score]\n",
    "})\n",
    "\n",
    "# 데이터프레임 출력\n",
    "performance_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch + 1}\")\n",
    "    \n",
    "    for _, data in pbar:\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        inputs, labels = mixup_fn(inputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # AutoCast 적용\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "        # 스케일링된 그라디언트 계산\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # 그라디언트 클리핑 전에 스케일링 제거\n",
    "        scaler.unscale_(optimizer)\n",
    "        clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "\n",
    "        # 옵티마이저 스텝 및 스케일러 업데이트\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step(loss)\n",
    "            \n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        lrs.append(lr)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)        \n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in valid_loader:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    val_loss /= len(valid_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # 모델 저장\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        vit_save = True\n",
    "        if vit_save:\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_duration = time.time() - start_time\n",
    "    training_time += epoch_duration\n",
    "    \n",
    "    text = f'\\tLoss: {epoch_loss}, Val Loss: {val_loss}, LR: {lr}, Duration: {epoch_duration:.2f} sec'\n",
    "    \n",
    "    if vit_save:\n",
    "        text += f' - model saved!'\n",
    "        vit_save = False\n",
    "\n",
    "    print(text)\n",
    "        \n",
    "text = f\"Epoch 당 평균 소요시간 : {training_time / 140:.2f}초\"      \n",
    "print(text)\n",
    "\n",
    "# 예측 수행 및 레이블 저장\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# 혼동 행렬 생성\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# 예측과 실제 레이블\n",
    "y_true = all_labels  # 실제 레이블\n",
    "y_pred = all_preds  # 모델에 의해 예측된 레이블\n",
    "\n",
    "# 전체 데이터셋에 대한 정확도\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# 평균 정밀도, 리콜, F1-Score ('weighted')\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "\n",
    "# 판다스 데이터프레임으로 결과 정리\n",
    "performance_metrics = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "    'Value': [accuracy, precision, recall, f1_score]\n",
    "})\n",
    "\n",
    "# 데이터프레임 출력\n",
    "performance_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch + 1}\")\n",
    "    \n",
    "    for _, data in pbar:\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        inputs, labels = mixup_fn(inputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # AutoCast 적용\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "        # 스케일링된 그라디언트 계산\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # 그라디언트 클리핑 전에 스케일링 제거\n",
    "        scaler.unscale_(optimizer)\n",
    "        clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "\n",
    "        # 옵티마이저 스텝 및 스케일러 업데이트\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step(loss)\n",
    "            \n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        lrs.append(lr)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)        \n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in valid_loader:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    val_loss /= len(valid_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # 모델 저장\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        vit_save = True\n",
    "        if vit_save:\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_duration = time.time() - start_time\n",
    "    training_time += epoch_duration\n",
    "    \n",
    "    text = f'\\tLoss: {epoch_loss}, Val Loss: {val_loss}, LR: {lr}, Duration: {epoch_duration:.2f} sec'\n",
    "    \n",
    "    if vit_save:\n",
    "        text += f' - model saved!'\n",
    "        vit_save = False\n",
    "\n",
    "    print(text)\n",
    "        \n",
    "text = f\"Epoch 당 평균 소요시간 : {training_time / 150:.2f}초\"      \n",
    "print(text)\n",
    "\n",
    "# 예측 수행 및 레이블 저장\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# 혼동 행렬 생성\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# 예측과 실제 레이블\n",
    "y_true = all_labels  # 실제 레이블\n",
    "y_pred = all_preds  # 모델에 의해 예측된 레이블\n",
    "\n",
    "# 전체 데이터셋에 대한 정확도\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# 평균 정밀도, 리콜, F1-Score ('weighted')\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "\n",
    "# 판다스 데이터프레임으로 결과 정리\n",
    "performance_metrics = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "    'Value': [accuracy, precision, recall, f1_score]\n",
    "})\n",
    "\n",
    "# 데이터프레임 출력\n",
    "performance_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
