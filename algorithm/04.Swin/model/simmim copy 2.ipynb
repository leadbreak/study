{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from box import Box\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import simmim\n",
    "from swin_v2 import SwinTransformerV2\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "from timm.data import Mixup\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MODEL': {'TYPE': 'swinv2',\n",
       "  'NAME': 'simmim_pretrain',\n",
       "  'DROP_PATH_RATE': 0.0,\n",
       "  'SWIN': {'EMBED_DIM': 96,\n",
       "   'DEPTHS': [2, 2, 6, 2],\n",
       "   'NUM_HEADS': [3, 6, 12, 24],\n",
       "   'WINDOW_SIZE': 6,\n",
       "   'PATCH_SIZE': 4}},\n",
       " 'DATA': {'IMG_SIZE': 192,\n",
       "  'MASK_PATCH_SIZE': 32,\n",
       "  'MASK_RATIO': 0.6,\n",
       "  'BATCH_SIZE': 1024,\n",
       "  'NUM_WORKERS': 24,\n",
       "  'DATA_PATH': '../../data/sports'},\n",
       " 'TRAIN': {'EPOCHS': 100,\n",
       "  'WARMUP_EPOCHS': 10,\n",
       "  'BASE_LR': 0.0014,\n",
       "  'WEIGHT_DECAY': 0.05,\n",
       "  'CLIP_GRAD': 5}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simmim_config = yaml.load(open('config/pretrain.yaml'), Loader=yaml.FullLoader)\n",
    "simmim_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_config = {'img_size':simmim_config['DATA']['IMG_SIZE'], \n",
    "                'patch_size':simmim_config['MODEL']['SWIN']['PATCH_SIZE'], \n",
    "                'in_chans':3, \n",
    "                'num_classes':100,\n",
    "                'embed_dim':192, \n",
    "                'depths':[2,2,18,4], \n",
    "                'num_heads':[6,12,24,48],           \n",
    "                'window_size':12, \n",
    "                'mlp_ratio':4., \n",
    "                'qkv_bias':True, \n",
    "                'qk_scale':None,\n",
    "                'drop_rate':0., \n",
    "                'attn_drop_rate':0., \n",
    "                'drop_path_rate':.2,\n",
    "                'norm_layer':nn.LayerNorm, \n",
    "                'patch_norm':True, \n",
    "                'pretrained_window_sizes':[0,0,0,0],\n",
    "                'ape':True}\n",
    "\n",
    "encoder_stride = 32\n",
    "in_chans = encoder_config['in_chans']\n",
    "patch_size = encoder_config['patch_size']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Load SimMIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "encoder = simmim.SwinTransformerV2ForSimMIM(**encoder_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = simmim.SimMIM( encoder=encoder, \n",
    "                       encoder_stride=encoder_stride, \n",
    "                       in_chans=in_chans, \n",
    "                       patch_size=patch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask Generator Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 0, 1, 0, 0, 1],\n",
       "       [1, 1, 0, 1, 0, 1, 1, 0],\n",
       "       [1, 1, 1, 1, 1, 0, 1, 0],\n",
       "       [1, 0, 1, 1, 0, 0, 1, 0],\n",
       "       [0, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 1, 1, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 0, 0, 0, 0],\n",
       "       [0, 1, 1, 1, 1, 1, 0, 1]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_generator = simmim.MaskGenerator(input_size=224,\n",
    "                                      mask_patch_size=28,\n",
    "                                      model_patch_size=28,\n",
    "                                      mask_ratio=.6)\n",
    "mask = mask_generator()\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생성된 mask의 비율은 60.9375%\n"
     ]
    }
   ],
   "source": [
    "print(f\"생성된 mask의 비율은 {mask.sum() / (mask.shape[0]*mask.shape[1])*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SimMIM DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simmim_config = Box(simmim_config)\n",
    "simmim_config.DATA.BATCH_SIZE = 256\n",
    "simmim_config.DATA.BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader = simmim.build_loader_simmim(simmim_config)\n",
    "\n",
    "samples = next(iter(dataloader))\n",
    "len(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256, 3, 192, 192]), torch.Size([256, 48, 48]), torch.Size([256]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples[0].shape, samples[1].shape, samples[2].shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Parameters and etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-19 14:05:20.294512: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-19 14:05:20.294587: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-19 14:05:20.295445: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-19 14:05:20.300970: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-19 14:05:21.096456: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "base_lr = float(simmim_config.TRAIN.BASE_LR)\n",
    "weight_decay = .1\n",
    "optimizer = optim.AdamW(model.parameters(), lr=base_lr, weight_decay=weight_decay)\n",
    "warmup_epochs = 20\n",
    "train_epochs = 100\n",
    "\n",
    "scheduler = transformers.get_cosine_schedule_with_warmup(optimizer, \n",
    "                                                        num_warmup_steps=warmup_epochs*len(dataloader), \n",
    "                                                        num_training_steps=train_epochs*len(dataloader),\n",
    "                                                        num_cycles=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:2'\n",
    "model.to(device)\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save = True\n",
    "simmim_path = '../../models/swin2/simmim_large.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.Train SimMIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 56/56 [01:05<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.1199, LR: 7.000000000000001e-05, Duration: 67.11 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 56/56 [01:01<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.9498, LR: 0.00014000000000000001, Duration: 64.58 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 56/56 [01:01<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.7927, LR: 0.00020999999999999998, Duration: 64.60 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 56/56 [01:02<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.7450, LR: 0.00028000000000000003, Duration: 65.73 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 56/56 [01:00<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.7209, LR: 0.00035, Duration: 64.21 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 56/56 [01:01<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.6993, LR: 0.00041999999999999996, Duration: 64.63 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 56/56 [01:01<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.6814, LR: 0.00049, Duration: 65.06 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 56/56 [01:00<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.6649, LR: 0.0005600000000000001, Duration: 64.20 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 56/56 [01:01<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.6505, LR: 0.00063, Duration: 65.32 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 56/56 [01:01<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.6422, LR: 0.0007, Duration: 65.27 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 56/56 [01:01<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.6378, LR: 0.0007700000000000001, Duration: 65.13 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 56/56 [01:02<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.6111, LR: 0.0008399999999999999, Duration: 65.38 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 56/56 [01:01<00:00,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.6135, LR: 0.00091, Duration: 62.78 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14: 100%|██████████| 56/56 [01:01<00:00,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.6209, LR: 0.00098, Duration: 62.91 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15: 100%|██████████| 56/56 [01:02<00:00,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.6176, LR: 0.00105, Duration: 63.75 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16: 100%|██████████| 56/56 [01:02<00:00,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.6133, LR: 0.0011200000000000001, Duration: 63.59 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17: 100%|██████████| 56/56 [01:02<00:00,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.6136, LR: 0.0011899999999999999, Duration: 63.89 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18: 100%|██████████| 56/56 [01:02<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5983, LR: 0.00126, Duration: 65.96 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 56/56 [01:01<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5936, LR: 0.00133, Duration: 64.66 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 56/56 [01:02<00:00,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.6053, LR: 0.0014, Duration: 63.45 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21: 100%|██████████| 56/56 [01:01<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5890, LR: 0.0013994603253685062, Duration: 64.91 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 56/56 [01:01<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5863, LR: 0.0013978421336131896, Duration: 64.71 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 56/56 [01:01<00:00,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5983, LR: 0.0013951479198684486, Duration: 62.91 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24: 100%|██████████| 56/56 [01:01<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5707, LR: 0.0013913818384165965, Duration: 64.81 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 56/56 [01:02<00:00,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5763, LR: 0.0013865496962822614, Duration: 63.69 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26: 100%|██████████| 56/56 [01:02<00:00,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5774, LR: 0.0013806589442783735, Duration: 63.59 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27: 100%|██████████| 56/56 [01:02<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.5624, LR: 0.001373718665517553, Duration: 65.70 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28:  14%|█▍        | 8/56 [00:10<00:54,  1.13s/it]"
     ]
    }
   ],
   "source": [
    "training_time = 0\n",
    "losses = []\n",
    "val_losses = []\n",
    "lrs = []\n",
    "best_loss = float('inf')\n",
    "\n",
    "# GradScaler 초기화\n",
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in range(train_epochs):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f\"Epoch {epoch + 1}\")\n",
    "    \n",
    "    for _, data in pbar:\n",
    "        image, mask = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # AutoCast 적용\n",
    "        with autocast():\n",
    "            loss = model(image, mask)\n",
    "            \n",
    "        # 스케일링된 그라디언트 계산\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # 그라디언트 클리핑 전에 스케일링 제거\n",
    "        scaler.unscale_(optimizer)\n",
    "        if simmim_config.TRAIN.CLIP_GRAD:\n",
    "            clip_grad_norm_(model.parameters(), max_norm=100)\n",
    "        else:\n",
    "            clip_grad_norm_(model.parameters())\n",
    "\n",
    "        # 옵티마이저 스텝 및 스케일러 업데이트\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "            \n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        lrs.append(lr)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    losses.append(epoch_loss)\n",
    "\n",
    "    # 모델 저장\n",
    "    if epoch_loss < best_loss:\n",
    "        \n",
    "        best_loss = epoch_loss\n",
    "        vit_save = model_save\n",
    "        if vit_save:\n",
    "            torch.save(model.state_dict(), simmim_path)\n",
    "        \n",
    "    epoch_duration = time.time() - start_time\n",
    "    training_time += epoch_duration\n",
    "    \n",
    "    text = f'\\tLoss: {epoch_loss:.4f}, LR: {lr}, Duration: {epoch_duration:.2f} sec'\n",
    "    \n",
    "    if vit_save:\n",
    "        text += f' - model saved!'\n",
    "        vit_save = False    \n",
    "        \n",
    "    print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Del SimMIM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cpu()\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.Load Swin V2 for stage-2 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['absolute_pos_embed', 'embeddings.patch_embeddings.weight', 'embeddings.patch_embeddings.bias', 'embeddings.norm.weight', 'embeddings.norm.bias', 'stages.0.blocks.0.attn_mask', 'stages.0.blocks.0.attn.t_scale', 'stages.0.blocks.0.attn.relative_coords_table', 'stages.0.blocks.0.attn.relative_position_index', 'stages.0.blocks.0.attn.crpb_mlp.0.weight', 'stages.0.blocks.0.attn.crpb_mlp.0.bias', 'stages.0.blocks.0.attn.crpb_mlp.3.weight', 'stages.0.blocks.0.attn.qkv.weight', 'stages.0.blocks.0.attn.qkv.bias', 'stages.0.blocks.0.attn.proj.weight', 'stages.0.blocks.0.attn.proj.bias', 'stages.0.blocks.0.norm1.weight', 'stages.0.blocks.0.norm1.bias', 'stages.0.blocks.0.mlp.fc1.weight', 'stages.0.blocks.0.mlp.fc1.bias', 'stages.0.blocks.0.mlp.fc2.weight', 'stages.0.blocks.0.mlp.fc2.bias', 'stages.0.blocks.0.norm2.weight', 'stages.0.blocks.0.norm2.bias', 'stages.0.blocks.1.attn_mask', 'stages.0.blocks.1.attn.t_scale', 'stages.0.blocks.1.attn.relative_coords_table', 'stages.0.blocks.1.attn.relative_position_index', 'stages.0.blocks.1.attn.crpb_mlp.0.weight', 'stages.0.blocks.1.attn.crpb_mlp.0.bias', 'stages.0.blocks.1.attn.crpb_mlp.3.weight', 'stages.0.blocks.1.attn.qkv.weight', 'stages.0.blocks.1.attn.qkv.bias', 'stages.0.blocks.1.attn.proj.weight', 'stages.0.blocks.1.attn.proj.bias', 'stages.0.blocks.1.norm1.weight', 'stages.0.blocks.1.norm1.bias', 'stages.0.blocks.1.mlp.fc1.weight', 'stages.0.blocks.1.mlp.fc1.bias', 'stages.0.blocks.1.mlp.fc2.weight', 'stages.0.blocks.1.mlp.fc2.bias', 'stages.0.blocks.1.norm2.weight', 'stages.0.blocks.1.norm2.bias', 'stages.0.downsample.reduction.weight', 'stages.0.downsample.norm.weight', 'stages.0.downsample.norm.bias', 'stages.1.blocks.0.attn_mask', 'stages.1.blocks.0.attn.t_scale', 'stages.1.blocks.0.attn.relative_coords_table', 'stages.1.blocks.0.attn.relative_position_index', 'stages.1.blocks.0.attn.crpb_mlp.0.weight', 'stages.1.blocks.0.attn.crpb_mlp.0.bias', 'stages.1.blocks.0.attn.crpb_mlp.3.weight', 'stages.1.blocks.0.attn.qkv.weight', 'stages.1.blocks.0.attn.qkv.bias', 'stages.1.blocks.0.attn.proj.weight', 'stages.1.blocks.0.attn.proj.bias', 'stages.1.blocks.0.norm1.weight', 'stages.1.blocks.0.norm1.bias', 'stages.1.blocks.0.mlp.fc1.weight', 'stages.1.blocks.0.mlp.fc1.bias', 'stages.1.blocks.0.mlp.fc2.weight', 'stages.1.blocks.0.mlp.fc2.bias', 'stages.1.blocks.0.norm2.weight', 'stages.1.blocks.0.norm2.bias', 'stages.1.blocks.1.attn_mask', 'stages.1.blocks.1.attn.t_scale', 'stages.1.blocks.1.attn.relative_coords_table', 'stages.1.blocks.1.attn.relative_position_index', 'stages.1.blocks.1.attn.crpb_mlp.0.weight', 'stages.1.blocks.1.attn.crpb_mlp.0.bias', 'stages.1.blocks.1.attn.crpb_mlp.3.weight', 'stages.1.blocks.1.attn.qkv.weight', 'stages.1.blocks.1.attn.qkv.bias', 'stages.1.blocks.1.attn.proj.weight', 'stages.1.blocks.1.attn.proj.bias', 'stages.1.blocks.1.norm1.weight', 'stages.1.blocks.1.norm1.bias', 'stages.1.blocks.1.mlp.fc1.weight', 'stages.1.blocks.1.mlp.fc1.bias', 'stages.1.blocks.1.mlp.fc2.weight', 'stages.1.blocks.1.mlp.fc2.bias', 'stages.1.blocks.1.norm2.weight', 'stages.1.blocks.1.norm2.bias', 'stages.1.downsample.reduction.weight', 'stages.1.downsample.norm.weight', 'stages.1.downsample.norm.bias', 'stages.2.blocks.0.attn_mask', 'stages.2.blocks.0.attn.t_scale', 'stages.2.blocks.0.attn.relative_coords_table', 'stages.2.blocks.0.attn.relative_position_index', 'stages.2.blocks.0.attn.crpb_mlp.0.weight', 'stages.2.blocks.0.attn.crpb_mlp.0.bias', 'stages.2.blocks.0.attn.crpb_mlp.3.weight', 'stages.2.blocks.0.attn.qkv.weight', 'stages.2.blocks.0.attn.qkv.bias', 'stages.2.blocks.0.attn.proj.weight', 'stages.2.blocks.0.attn.proj.bias', 'stages.2.blocks.0.norm1.weight', 'stages.2.blocks.0.norm1.bias', 'stages.2.blocks.0.mlp.fc1.weight', 'stages.2.blocks.0.mlp.fc1.bias', 'stages.2.blocks.0.mlp.fc2.weight', 'stages.2.blocks.0.mlp.fc2.bias', 'stages.2.blocks.0.norm2.weight', 'stages.2.blocks.0.norm2.bias', 'stages.2.blocks.1.attn_mask', 'stages.2.blocks.1.attn.t_scale', 'stages.2.blocks.1.attn.relative_coords_table', 'stages.2.blocks.1.attn.relative_position_index', 'stages.2.blocks.1.attn.crpb_mlp.0.weight', 'stages.2.blocks.1.attn.crpb_mlp.0.bias', 'stages.2.blocks.1.attn.crpb_mlp.3.weight', 'stages.2.blocks.1.attn.qkv.weight', 'stages.2.blocks.1.attn.qkv.bias', 'stages.2.blocks.1.attn.proj.weight', 'stages.2.blocks.1.attn.proj.bias', 'stages.2.blocks.1.norm1.weight', 'stages.2.blocks.1.norm1.bias', 'stages.2.blocks.1.mlp.fc1.weight', 'stages.2.blocks.1.mlp.fc1.bias', 'stages.2.blocks.1.mlp.fc2.weight', 'stages.2.blocks.1.mlp.fc2.bias', 'stages.2.blocks.1.norm2.weight', 'stages.2.blocks.1.norm2.bias', 'stages.2.blocks.2.attn_mask', 'stages.2.blocks.2.attn.t_scale', 'stages.2.blocks.2.attn.relative_coords_table', 'stages.2.blocks.2.attn.relative_position_index', 'stages.2.blocks.2.attn.crpb_mlp.0.weight', 'stages.2.blocks.2.attn.crpb_mlp.0.bias', 'stages.2.blocks.2.attn.crpb_mlp.3.weight', 'stages.2.blocks.2.attn.qkv.weight', 'stages.2.blocks.2.attn.qkv.bias', 'stages.2.blocks.2.attn.proj.weight', 'stages.2.blocks.2.attn.proj.bias', 'stages.2.blocks.2.norm1.weight', 'stages.2.blocks.2.norm1.bias', 'stages.2.blocks.2.mlp.fc1.weight', 'stages.2.blocks.2.mlp.fc1.bias', 'stages.2.blocks.2.mlp.fc2.weight', 'stages.2.blocks.2.mlp.fc2.bias', 'stages.2.blocks.2.norm2.weight', 'stages.2.blocks.2.norm2.bias', 'stages.2.blocks.3.attn_mask', 'stages.2.blocks.3.attn.t_scale', 'stages.2.blocks.3.attn.relative_coords_table', 'stages.2.blocks.3.attn.relative_position_index', 'stages.2.blocks.3.attn.crpb_mlp.0.weight', 'stages.2.blocks.3.attn.crpb_mlp.0.bias', 'stages.2.blocks.3.attn.crpb_mlp.3.weight', 'stages.2.blocks.3.attn.qkv.weight', 'stages.2.blocks.3.attn.qkv.bias', 'stages.2.blocks.3.attn.proj.weight', 'stages.2.blocks.3.attn.proj.bias', 'stages.2.blocks.3.norm1.weight', 'stages.2.blocks.3.norm1.bias', 'stages.2.blocks.3.mlp.fc1.weight', 'stages.2.blocks.3.mlp.fc1.bias', 'stages.2.blocks.3.mlp.fc2.weight', 'stages.2.blocks.3.mlp.fc2.bias', 'stages.2.blocks.3.norm2.weight', 'stages.2.blocks.3.norm2.bias', 'stages.2.blocks.4.attn_mask', 'stages.2.blocks.4.attn.t_scale', 'stages.2.blocks.4.attn.relative_coords_table', 'stages.2.blocks.4.attn.relative_position_index', 'stages.2.blocks.4.attn.crpb_mlp.0.weight', 'stages.2.blocks.4.attn.crpb_mlp.0.bias', 'stages.2.blocks.4.attn.crpb_mlp.3.weight', 'stages.2.blocks.4.attn.qkv.weight', 'stages.2.blocks.4.attn.qkv.bias', 'stages.2.blocks.4.attn.proj.weight', 'stages.2.blocks.4.attn.proj.bias', 'stages.2.blocks.4.norm1.weight', 'stages.2.blocks.4.norm1.bias', 'stages.2.blocks.4.mlp.fc1.weight', 'stages.2.blocks.4.mlp.fc1.bias', 'stages.2.blocks.4.mlp.fc2.weight', 'stages.2.blocks.4.mlp.fc2.bias', 'stages.2.blocks.4.norm2.weight', 'stages.2.blocks.4.norm2.bias', 'stages.2.blocks.5.attn_mask', 'stages.2.blocks.5.attn.t_scale', 'stages.2.blocks.5.attn.relative_coords_table', 'stages.2.blocks.5.attn.relative_position_index', 'stages.2.blocks.5.attn.crpb_mlp.0.weight', 'stages.2.blocks.5.attn.crpb_mlp.0.bias', 'stages.2.blocks.5.attn.crpb_mlp.3.weight', 'stages.2.blocks.5.attn.qkv.weight', 'stages.2.blocks.5.attn.qkv.bias', 'stages.2.blocks.5.attn.proj.weight', 'stages.2.blocks.5.attn.proj.bias', 'stages.2.blocks.5.norm1.weight', 'stages.2.blocks.5.norm1.bias', 'stages.2.blocks.5.mlp.fc1.weight', 'stages.2.blocks.5.mlp.fc1.bias', 'stages.2.blocks.5.mlp.fc2.weight', 'stages.2.blocks.5.mlp.fc2.bias', 'stages.2.blocks.5.norm2.weight', 'stages.2.blocks.5.norm2.bias', 'stages.2.downsample.reduction.weight', 'stages.2.downsample.norm.weight', 'stages.2.downsample.norm.bias', 'stages.3.blocks.0.attn_mask', 'stages.3.blocks.0.attn.t_scale', 'stages.3.blocks.0.attn.relative_coords_table', 'stages.3.blocks.0.attn.relative_position_index', 'stages.3.blocks.0.attn.crpb_mlp.0.weight', 'stages.3.blocks.0.attn.crpb_mlp.0.bias', 'stages.3.blocks.0.attn.crpb_mlp.3.weight', 'stages.3.blocks.0.attn.qkv.weight', 'stages.3.blocks.0.attn.qkv.bias', 'stages.3.blocks.0.attn.proj.weight', 'stages.3.blocks.0.attn.proj.bias', 'stages.3.blocks.0.norm1.weight', 'stages.3.blocks.0.norm1.bias', 'stages.3.blocks.0.mlp.fc1.weight', 'stages.3.blocks.0.mlp.fc1.bias', 'stages.3.blocks.0.mlp.fc2.weight', 'stages.3.blocks.0.mlp.fc2.bias', 'stages.3.blocks.0.norm2.weight', 'stages.3.blocks.0.norm2.bias', 'stages.3.blocks.1.attn_mask', 'stages.3.blocks.1.attn.t_scale', 'stages.3.blocks.1.attn.relative_coords_table', 'stages.3.blocks.1.attn.relative_position_index', 'stages.3.blocks.1.attn.crpb_mlp.0.weight', 'stages.3.blocks.1.attn.crpb_mlp.0.bias', 'stages.3.blocks.1.attn.crpb_mlp.3.weight', 'stages.3.blocks.1.attn.qkv.weight', 'stages.3.blocks.1.attn.qkv.bias', 'stages.3.blocks.1.attn.proj.weight', 'stages.3.blocks.1.attn.proj.bias', 'stages.3.blocks.1.norm1.weight', 'stages.3.blocks.1.norm1.bias', 'stages.3.blocks.1.mlp.fc1.weight', 'stages.3.blocks.1.mlp.fc1.bias', 'stages.3.blocks.1.mlp.fc2.weight', 'stages.3.blocks.1.mlp.fc2.bias', 'stages.3.blocks.1.norm2.weight', 'stages.3.blocks.1.norm2.bias', 'layernorm.weight', 'layernorm.bias', 'classifier.weight', 'classifier.bias'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SwinTransformerV2(pretrained_window_sizes=[12,12,12,12], \n",
    "                          ape=True, \n",
    "                          drop_path_rate=0.3,\n",
    "                          embed_dim=192,\n",
    "                          depths=[2,2,18,2],\n",
    "                          num_heads=[6,12,24,48],\n",
    "                          window_size=12)\n",
    "model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 96, 56, 56]           4,704\n",
      "         LayerNorm-2             [-1, 3136, 96]             192\n",
      "        embeddings-3             [-1, 3136, 96]               0\n",
      "           Dropout-4             [-1, 3136, 96]               0\n",
      "            Linear-5              [-1, 49, 288]          27,936\n",
      "            Linear-6          [-1, 13, 13, 384]           1,152\n",
      "              ReLU-7          [-1, 13, 13, 384]               0\n",
      "           Dropout-8          [-1, 13, 13, 384]               0\n",
      "            Linear-9            [-1, 13, 13, 3]           1,152\n",
      "          Softmax-10            [-1, 3, 49, 49]               0\n",
      "          Dropout-11            [-1, 3, 49, 49]               0\n",
      "           Linear-12               [-1, 49, 96]           9,312\n",
      "          Dropout-13               [-1, 49, 96]               0\n",
      "  WindowAttention-14               [-1, 49, 96]               0\n",
      "         DropPath-15             [-1, 3136, 96]               0\n",
      "        LayerNorm-16             [-1, 3136, 96]             192\n",
      "           Linear-17            [-1, 3136, 384]          37,248\n",
      "             GELU-18            [-1, 3136, 384]               0\n",
      "          Dropout-19            [-1, 3136, 384]               0\n",
      "           Linear-20             [-1, 3136, 96]          36,960\n",
      "          Dropout-21             [-1, 3136, 96]               0\n",
      "              Mlp-22             [-1, 3136, 96]               0\n",
      "         DropPath-23             [-1, 3136, 96]               0\n",
      "        LayerNorm-24             [-1, 3136, 96]             192\n",
      "SwinTransformerBlock-25             [-1, 3136, 96]               0\n",
      "           Linear-26              [-1, 49, 288]          27,936\n",
      "           Linear-27          [-1, 13, 13, 384]           1,152\n",
      "             ReLU-28          [-1, 13, 13, 384]               0\n",
      "          Dropout-29          [-1, 13, 13, 384]               0\n",
      "           Linear-30            [-1, 13, 13, 3]           1,152\n",
      "          Softmax-31            [-1, 3, 49, 49]               0\n",
      "          Dropout-32            [-1, 3, 49, 49]               0\n",
      "           Linear-33               [-1, 49, 96]           9,312\n",
      "          Dropout-34               [-1, 49, 96]               0\n",
      "  WindowAttention-35               [-1, 49, 96]               0\n",
      "         DropPath-36             [-1, 3136, 96]               0\n",
      "        LayerNorm-37             [-1, 3136, 96]             192\n",
      "           Linear-38            [-1, 3136, 384]          37,248\n",
      "             GELU-39            [-1, 3136, 384]               0\n",
      "          Dropout-40            [-1, 3136, 384]               0\n",
      "           Linear-41             [-1, 3136, 96]          36,960\n",
      "          Dropout-42             [-1, 3136, 96]               0\n",
      "              Mlp-43             [-1, 3136, 96]               0\n",
      "         DropPath-44             [-1, 3136, 96]               0\n",
      "        LayerNorm-45             [-1, 3136, 96]             192\n",
      "SwinTransformerBlock-46             [-1, 3136, 96]               0\n",
      "        LayerNorm-47             [-1, 784, 384]             768\n",
      "           Linear-48             [-1, 784, 192]          73,728\n",
      "     PatchMerging-49             [-1, 784, 192]               0\n",
      "       StageLayer-50             [-1, 784, 192]               0\n",
      "           Linear-51              [-1, 49, 576]         111,168\n",
      "           Linear-52          [-1, 13, 13, 384]           1,152\n",
      "             ReLU-53          [-1, 13, 13, 384]               0\n",
      "          Dropout-54          [-1, 13, 13, 384]               0\n",
      "           Linear-55            [-1, 13, 13, 6]           2,304\n",
      "          Softmax-56            [-1, 6, 49, 49]               0\n",
      "          Dropout-57            [-1, 6, 49, 49]               0\n",
      "           Linear-58              [-1, 49, 192]          37,056\n",
      "          Dropout-59              [-1, 49, 192]               0\n",
      "  WindowAttention-60              [-1, 49, 192]               0\n",
      "         DropPath-61             [-1, 784, 192]               0\n",
      "        LayerNorm-62             [-1, 784, 192]             384\n",
      "           Linear-63             [-1, 784, 768]         148,224\n",
      "             GELU-64             [-1, 784, 768]               0\n",
      "          Dropout-65             [-1, 784, 768]               0\n",
      "           Linear-66             [-1, 784, 192]         147,648\n",
      "          Dropout-67             [-1, 784, 192]               0\n",
      "              Mlp-68             [-1, 784, 192]               0\n",
      "         DropPath-69             [-1, 784, 192]               0\n",
      "        LayerNorm-70             [-1, 784, 192]             384\n",
      "SwinTransformerBlock-71             [-1, 784, 192]               0\n",
      "           Linear-72              [-1, 49, 576]         111,168\n",
      "           Linear-73          [-1, 13, 13, 384]           1,152\n",
      "             ReLU-74          [-1, 13, 13, 384]               0\n",
      "          Dropout-75          [-1, 13, 13, 384]               0\n",
      "           Linear-76            [-1, 13, 13, 6]           2,304\n",
      "          Softmax-77            [-1, 6, 49, 49]               0\n",
      "          Dropout-78            [-1, 6, 49, 49]               0\n",
      "           Linear-79              [-1, 49, 192]          37,056\n",
      "          Dropout-80              [-1, 49, 192]               0\n",
      "  WindowAttention-81              [-1, 49, 192]               0\n",
      "         DropPath-82             [-1, 784, 192]               0\n",
      "        LayerNorm-83             [-1, 784, 192]             384\n",
      "           Linear-84             [-1, 784, 768]         148,224\n",
      "             GELU-85             [-1, 784, 768]               0\n",
      "          Dropout-86             [-1, 784, 768]               0\n",
      "           Linear-87             [-1, 784, 192]         147,648\n",
      "          Dropout-88             [-1, 784, 192]               0\n",
      "              Mlp-89             [-1, 784, 192]               0\n",
      "         DropPath-90             [-1, 784, 192]               0\n",
      "        LayerNorm-91             [-1, 784, 192]             384\n",
      "SwinTransformerBlock-92             [-1, 784, 192]               0\n",
      "        LayerNorm-93             [-1, 196, 768]           1,536\n",
      "           Linear-94             [-1, 196, 384]         294,912\n",
      "     PatchMerging-95             [-1, 196, 384]               0\n",
      "       StageLayer-96             [-1, 196, 384]               0\n",
      "           Linear-97             [-1, 49, 1152]         443,520\n",
      "           Linear-98          [-1, 13, 13, 384]           1,152\n",
      "             ReLU-99          [-1, 13, 13, 384]               0\n",
      "         Dropout-100          [-1, 13, 13, 384]               0\n",
      "          Linear-101           [-1, 13, 13, 12]           4,608\n",
      "         Softmax-102           [-1, 12, 49, 49]               0\n",
      "         Dropout-103           [-1, 12, 49, 49]               0\n",
      "          Linear-104              [-1, 49, 384]         147,840\n",
      "         Dropout-105              [-1, 49, 384]               0\n",
      " WindowAttention-106              [-1, 49, 384]               0\n",
      "        DropPath-107             [-1, 196, 384]               0\n",
      "       LayerNorm-108             [-1, 196, 384]             768\n",
      "          Linear-109            [-1, 196, 1536]         591,360\n",
      "            GELU-110            [-1, 196, 1536]               0\n",
      "         Dropout-111            [-1, 196, 1536]               0\n",
      "          Linear-112             [-1, 196, 384]         590,208\n",
      "         Dropout-113             [-1, 196, 384]               0\n",
      "             Mlp-114             [-1, 196, 384]               0\n",
      "        DropPath-115             [-1, 196, 384]               0\n",
      "       LayerNorm-116             [-1, 196, 384]             768\n",
      "SwinTransformerBlock-117             [-1, 196, 384]               0\n",
      "          Linear-118             [-1, 49, 1152]         443,520\n",
      "          Linear-119          [-1, 13, 13, 384]           1,152\n",
      "            ReLU-120          [-1, 13, 13, 384]               0\n",
      "         Dropout-121          [-1, 13, 13, 384]               0\n",
      "          Linear-122           [-1, 13, 13, 12]           4,608\n",
      "         Softmax-123           [-1, 12, 49, 49]               0\n",
      "         Dropout-124           [-1, 12, 49, 49]               0\n",
      "          Linear-125              [-1, 49, 384]         147,840\n",
      "         Dropout-126              [-1, 49, 384]               0\n",
      " WindowAttention-127              [-1, 49, 384]               0\n",
      "        DropPath-128             [-1, 196, 384]               0\n",
      "       LayerNorm-129             [-1, 196, 384]             768\n",
      "          Linear-130            [-1, 196, 1536]         591,360\n",
      "            GELU-131            [-1, 196, 1536]               0\n",
      "         Dropout-132            [-1, 196, 1536]               0\n",
      "          Linear-133             [-1, 196, 384]         590,208\n",
      "         Dropout-134             [-1, 196, 384]               0\n",
      "             Mlp-135             [-1, 196, 384]               0\n",
      "        DropPath-136             [-1, 196, 384]               0\n",
      "       LayerNorm-137             [-1, 196, 384]             768\n",
      "SwinTransformerBlock-138             [-1, 196, 384]               0\n",
      "          Linear-139             [-1, 49, 1152]         443,520\n",
      "          Linear-140          [-1, 13, 13, 384]           1,152\n",
      "            ReLU-141          [-1, 13, 13, 384]               0\n",
      "         Dropout-142          [-1, 13, 13, 384]               0\n",
      "          Linear-143           [-1, 13, 13, 12]           4,608\n",
      "         Softmax-144           [-1, 12, 49, 49]               0\n",
      "         Dropout-145           [-1, 12, 49, 49]               0\n",
      "          Linear-146              [-1, 49, 384]         147,840\n",
      "         Dropout-147              [-1, 49, 384]               0\n",
      " WindowAttention-148              [-1, 49, 384]               0\n",
      "        DropPath-149             [-1, 196, 384]               0\n",
      "       LayerNorm-150             [-1, 196, 384]             768\n",
      "          Linear-151            [-1, 196, 1536]         591,360\n",
      "            GELU-152            [-1, 196, 1536]               0\n",
      "         Dropout-153            [-1, 196, 1536]               0\n",
      "          Linear-154             [-1, 196, 384]         590,208\n",
      "         Dropout-155             [-1, 196, 384]               0\n",
      "             Mlp-156             [-1, 196, 384]               0\n",
      "        DropPath-157             [-1, 196, 384]               0\n",
      "       LayerNorm-158             [-1, 196, 384]             768\n",
      "SwinTransformerBlock-159             [-1, 196, 384]               0\n",
      "          Linear-160             [-1, 49, 1152]         443,520\n",
      "          Linear-161          [-1, 13, 13, 384]           1,152\n",
      "            ReLU-162          [-1, 13, 13, 384]               0\n",
      "         Dropout-163          [-1, 13, 13, 384]               0\n",
      "          Linear-164           [-1, 13, 13, 12]           4,608\n",
      "         Softmax-165           [-1, 12, 49, 49]               0\n",
      "         Dropout-166           [-1, 12, 49, 49]               0\n",
      "          Linear-167              [-1, 49, 384]         147,840\n",
      "         Dropout-168              [-1, 49, 384]               0\n",
      " WindowAttention-169              [-1, 49, 384]               0\n",
      "        DropPath-170             [-1, 196, 384]               0\n",
      "       LayerNorm-171             [-1, 196, 384]             768\n",
      "          Linear-172            [-1, 196, 1536]         591,360\n",
      "            GELU-173            [-1, 196, 1536]               0\n",
      "         Dropout-174            [-1, 196, 1536]               0\n",
      "          Linear-175             [-1, 196, 384]         590,208\n",
      "         Dropout-176             [-1, 196, 384]               0\n",
      "             Mlp-177             [-1, 196, 384]               0\n",
      "        DropPath-178             [-1, 196, 384]               0\n",
      "       LayerNorm-179             [-1, 196, 384]             768\n",
      "SwinTransformerBlock-180             [-1, 196, 384]               0\n",
      "          Linear-181             [-1, 49, 1152]         443,520\n",
      "          Linear-182          [-1, 13, 13, 384]           1,152\n",
      "            ReLU-183          [-1, 13, 13, 384]               0\n",
      "         Dropout-184          [-1, 13, 13, 384]               0\n",
      "          Linear-185           [-1, 13, 13, 12]           4,608\n",
      "         Softmax-186           [-1, 12, 49, 49]               0\n",
      "         Dropout-187           [-1, 12, 49, 49]               0\n",
      "          Linear-188              [-1, 49, 384]         147,840\n",
      "         Dropout-189              [-1, 49, 384]               0\n",
      " WindowAttention-190              [-1, 49, 384]               0\n",
      "        DropPath-191             [-1, 196, 384]               0\n",
      "       LayerNorm-192             [-1, 196, 384]             768\n",
      "          Linear-193            [-1, 196, 1536]         591,360\n",
      "            GELU-194            [-1, 196, 1536]               0\n",
      "         Dropout-195            [-1, 196, 1536]               0\n",
      "          Linear-196             [-1, 196, 384]         590,208\n",
      "         Dropout-197             [-1, 196, 384]               0\n",
      "             Mlp-198             [-1, 196, 384]               0\n",
      "        DropPath-199             [-1, 196, 384]               0\n",
      "       LayerNorm-200             [-1, 196, 384]             768\n",
      "SwinTransformerBlock-201             [-1, 196, 384]               0\n",
      "          Linear-202             [-1, 49, 1152]         443,520\n",
      "          Linear-203          [-1, 13, 13, 384]           1,152\n",
      "            ReLU-204          [-1, 13, 13, 384]               0\n",
      "         Dropout-205          [-1, 13, 13, 384]               0\n",
      "          Linear-206           [-1, 13, 13, 12]           4,608\n",
      "         Softmax-207           [-1, 12, 49, 49]               0\n",
      "         Dropout-208           [-1, 12, 49, 49]               0\n",
      "          Linear-209              [-1, 49, 384]         147,840\n",
      "         Dropout-210              [-1, 49, 384]               0\n",
      " WindowAttention-211              [-1, 49, 384]               0\n",
      "        DropPath-212             [-1, 196, 384]               0\n",
      "       LayerNorm-213             [-1, 196, 384]             768\n",
      "          Linear-214            [-1, 196, 1536]         591,360\n",
      "            GELU-215            [-1, 196, 1536]               0\n",
      "         Dropout-216            [-1, 196, 1536]               0\n",
      "          Linear-217             [-1, 196, 384]         590,208\n",
      "         Dropout-218             [-1, 196, 384]               0\n",
      "             Mlp-219             [-1, 196, 384]               0\n",
      "        DropPath-220             [-1, 196, 384]               0\n",
      "       LayerNorm-221             [-1, 196, 384]             768\n",
      "SwinTransformerBlock-222             [-1, 196, 384]               0\n",
      "       LayerNorm-223             [-1, 49, 1536]           3,072\n",
      "          Linear-224              [-1, 49, 768]       1,179,648\n",
      "    PatchMerging-225              [-1, 49, 768]               0\n",
      "      StageLayer-226              [-1, 49, 768]               0\n",
      "          Linear-227             [-1, 49, 2304]       1,771,776\n",
      "          Linear-228          [-1, 13, 13, 384]           1,152\n",
      "            ReLU-229          [-1, 13, 13, 384]               0\n",
      "         Dropout-230          [-1, 13, 13, 384]               0\n",
      "          Linear-231           [-1, 13, 13, 24]           9,216\n",
      "         Softmax-232           [-1, 24, 49, 49]               0\n",
      "         Dropout-233           [-1, 24, 49, 49]               0\n",
      "          Linear-234              [-1, 49, 768]         590,592\n",
      "         Dropout-235              [-1, 49, 768]               0\n",
      " WindowAttention-236              [-1, 49, 768]               0\n",
      "        DropPath-237              [-1, 49, 768]               0\n",
      "       LayerNorm-238              [-1, 49, 768]           1,536\n",
      "          Linear-239             [-1, 49, 3072]       2,362,368\n",
      "            GELU-240             [-1, 49, 3072]               0\n",
      "         Dropout-241             [-1, 49, 3072]               0\n",
      "          Linear-242              [-1, 49, 768]       2,360,064\n",
      "         Dropout-243              [-1, 49, 768]               0\n",
      "             Mlp-244              [-1, 49, 768]               0\n",
      "        DropPath-245              [-1, 49, 768]               0\n",
      "       LayerNorm-246              [-1, 49, 768]           1,536\n",
      "SwinTransformerBlock-247              [-1, 49, 768]               0\n",
      "          Linear-248             [-1, 49, 2304]       1,771,776\n",
      "          Linear-249          [-1, 13, 13, 384]           1,152\n",
      "            ReLU-250          [-1, 13, 13, 384]               0\n",
      "         Dropout-251          [-1, 13, 13, 384]               0\n",
      "          Linear-252           [-1, 13, 13, 24]           9,216\n",
      "         Softmax-253           [-1, 24, 49, 49]               0\n",
      "         Dropout-254           [-1, 24, 49, 49]               0\n",
      "          Linear-255              [-1, 49, 768]         590,592\n",
      "         Dropout-256              [-1, 49, 768]               0\n",
      " WindowAttention-257              [-1, 49, 768]               0\n",
      "        DropPath-258              [-1, 49, 768]               0\n",
      "       LayerNorm-259              [-1, 49, 768]           1,536\n",
      "          Linear-260             [-1, 49, 3072]       2,362,368\n",
      "            GELU-261             [-1, 49, 3072]               0\n",
      "         Dropout-262             [-1, 49, 3072]               0\n",
      "          Linear-263              [-1, 49, 768]       2,360,064\n",
      "         Dropout-264              [-1, 49, 768]               0\n",
      "             Mlp-265              [-1, 49, 768]               0\n",
      "        DropPath-266              [-1, 49, 768]               0\n",
      "       LayerNorm-267              [-1, 49, 768]           1,536\n",
      "SwinTransformerBlock-268              [-1, 49, 768]               0\n",
      "        Identity-269              [-1, 49, 768]               0\n",
      "      StageLayer-270              [-1, 49, 768]               0\n",
      "       LayerNorm-271              [-1, 49, 768]           1,536\n",
      "AdaptiveAvgPool1d-272               [-1, 768, 1]               0\n",
      "          Linear-273                  [-1, 100]          76,900\n",
      "================================================================\n",
      "Total params: 27,639,748\n",
      "Trainable params: 27,639,748\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 271.27\n",
      "Params size (MB): 105.44\n",
      "Estimated Total Size (MB): 377.28\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(model.to('cuda'), (3,224,224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default Parameter(weight) Check\n",
    "- 추후 SimMIM 가중치가 제대로 불러와졌는지 확인용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0129, -0.1077, -0.0841,  0.0873],\n",
       "         [ 0.0951,  0.0538, -0.1324, -0.1053],\n",
       "         [ 0.1133,  0.0925, -0.0437, -0.1434],\n",
       "         [-0.0528, -0.1394, -0.0026,  0.0884]],\n",
       "\n",
       "        [[-0.0144,  0.0523, -0.0949,  0.0638],\n",
       "         [-0.0402,  0.0985,  0.0427,  0.0493],\n",
       "         [-0.0610,  0.0717,  0.0322,  0.1303],\n",
       "         [-0.0906, -0.0806,  0.0084,  0.1267]],\n",
       "\n",
       "        [[-0.0225,  0.0842,  0.0438,  0.0556],\n",
       "         [ 0.0726, -0.0971,  0.0465,  0.0960],\n",
       "         [ 0.0120,  0.0960,  0.0975,  0.1130],\n",
       "         [ 0.1074, -0.0488,  0.0056, -0.1428]]], device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['embeddings.patch_embeddings.weight'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.5615e-03,  1.8758e-02,  1.1253e-02,  3.1161e-02, -2.9341e-02,\n",
       "         1.6727e-02,  1.5575e-02, -6.6814e-03, -1.8727e-02,  3.3447e-02,\n",
       "        -1.0352e-02, -3.8013e-03,  6.1451e-03,  1.2605e-02,  1.4256e-02,\n",
       "         3.3592e-02, -1.0751e-02, -3.6015e-03,  2.7951e-02, -1.5929e-03,\n",
       "        -3.5530e-02,  3.4795e-02,  1.0762e-02, -7.6697e-03,  3.6423e-03,\n",
       "        -1.6669e-02,  7.6708e-03, -4.1405e-02,  6.0026e-04, -9.1808e-03,\n",
       "         1.3536e-03,  3.4790e-02, -5.9329e-02, -5.1861e-03, -1.4435e-02,\n",
       "         4.2382e-03,  2.3635e-03, -1.4914e-02,  1.2138e-02,  1.1349e-02,\n",
       "         1.9588e-02,  5.6263e-04,  1.7838e-02,  2.1391e-02,  3.3820e-02,\n",
       "        -1.7201e-02, -2.9766e-03, -3.0597e-03,  1.5437e-02, -2.3733e-02,\n",
       "         3.9303e-02,  6.1092e-03,  1.2487e-02, -1.5646e-02, -8.8374e-03,\n",
       "        -8.1489e-03, -3.6487e-03,  1.4805e-02,  3.5202e-02, -4.5491e-03,\n",
       "        -1.2150e-03,  1.3884e-02, -4.7024e-03, -5.4003e-03, -8.8659e-03,\n",
       "         6.6497e-03, -1.7442e-02, -1.0223e-02,  1.1088e-02,  1.1443e-02,\n",
       "         2.2838e-02, -9.0183e-03,  2.0556e-02, -6.3564e-03,  2.3337e-02,\n",
       "         2.8022e-02,  3.4619e-03,  2.4792e-02, -5.0694e-02, -1.1494e-02,\n",
       "        -2.7922e-02, -2.3964e-02, -1.0216e-02, -5.6411e-03, -4.2955e-02,\n",
       "         1.7579e-02, -5.3140e-03, -1.1091e-02,  3.5734e-03, -4.1647e-03,\n",
       "        -1.0574e-02, -1.3474e-02, -1.7878e-03, -2.5199e-02, -1.1615e-02,\n",
       "        -1.1542e-02,  9.3986e-03,  1.0682e-02, -1.7017e-03,  1.0985e-02,\n",
       "         2.9772e-02, -1.3774e-02,  3.6623e-02,  5.1770e-03, -4.0874e-03,\n",
       "         2.5827e-02,  1.5161e-02, -2.5656e-02,  3.8033e-03, -1.8584e-02,\n",
       "         6.4993e-03, -1.5399e-02, -8.9952e-03, -1.5752e-02,  2.6941e-02,\n",
       "         1.6716e-02,  1.8104e-02,  1.5029e-02,  1.2706e-02, -2.3770e-02,\n",
       "         1.8757e-02, -3.5239e-03,  2.6282e-02, -1.8035e-02,  9.5306e-03,\n",
       "        -3.7253e-02,  3.9982e-02, -5.6074e-03, -4.7924e-04, -5.2237e-03,\n",
       "        -2.5402e-02,  2.9975e-02,  8.9135e-03, -2.2850e-02, -2.1920e-02,\n",
       "         2.5607e-02,  5.2016e-04, -1.2962e-02,  3.2400e-02, -2.4801e-02,\n",
       "         2.2279e-02, -3.2449e-02,  1.9394e-02, -1.7532e-02, -1.8416e-02,\n",
       "         3.9659e-03,  2.6003e-02,  5.8304e-04,  2.5122e-02,  2.3388e-02,\n",
       "         3.2463e-03,  2.2363e-02, -1.7566e-02,  1.2151e-02,  1.5301e-02,\n",
       "        -1.1603e-02,  3.5617e-03, -4.1680e-02, -2.1422e-02,  3.0726e-02,\n",
       "         8.2574e-03,  3.0569e-02, -1.9336e-02,  3.1603e-03,  2.3603e-02,\n",
       "         1.0684e-03, -1.8033e-02,  8.3302e-03,  7.8012e-03, -1.9841e-02,\n",
       "        -8.8542e-03,  1.1275e-02,  7.5166e-03, -3.0752e-03, -3.6739e-02,\n",
       "         3.7614e-03,  1.3335e-02,  8.0896e-04, -2.9687e-03, -7.6584e-03,\n",
       "        -9.0517e-03,  1.0666e-02,  1.9439e-02,  2.0848e-03,  6.1731e-03,\n",
       "        -3.4749e-02, -1.4875e-02, -1.6298e-02,  1.4300e-03,  1.6517e-02,\n",
       "         3.4836e-02, -2.4897e-02, -1.8420e-02,  5.3721e-03,  5.1112e-02,\n",
       "        -1.3145e-02, -3.9322e-03, -1.8548e-02, -3.1281e-03,  1.2342e-03,\n",
       "        -1.9246e-02,  2.6977e-02,  2.6399e-02, -1.6186e-03, -1.8456e-02,\n",
       "        -1.0483e-02,  7.0149e-03,  2.3954e-02,  1.3194e-02,  1.3829e-02,\n",
       "         1.8657e-02, -2.1524e-02, -7.2991e-03, -3.4097e-02,  5.7127e-03,\n",
       "         9.3335e-03,  1.0473e-02,  6.1616e-03,  3.2411e-02, -7.8273e-03,\n",
       "        -3.3773e-02,  3.3599e-02, -2.0097e-02, -1.5736e-02,  2.5124e-02,\n",
       "        -1.1660e-02,  8.0221e-03,  4.6178e-03,  1.4855e-02, -6.4636e-03,\n",
       "        -4.4134e-03, -2.2564e-02, -1.3246e-02,  5.8797e-03, -1.4975e-02,\n",
       "        -1.2447e-03, -2.0340e-02,  4.9412e-02,  1.4735e-02, -4.3394e-02,\n",
       "        -1.8792e-02,  3.1935e-02, -3.7905e-02,  2.8306e-03, -2.3299e-03,\n",
       "         1.5279e-02,  3.6655e-03,  2.4777e-02, -3.6808e-02, -2.8458e-02,\n",
       "         1.6989e-02,  5.1625e-03,  2.0049e-02, -5.2289e-03, -1.1172e-02,\n",
       "         1.4190e-02,  6.7558e-03,  1.1504e-02,  1.3033e-02,  2.1102e-03,\n",
       "         6.9592e-04,  1.2365e-02,  4.3381e-03, -7.3706e-03, -7.0610e-05,\n",
       "         1.7724e-02,  1.7974e-02, -2.1048e-02, -9.9114e-04,  1.1607e-02,\n",
       "         2.1604e-02,  3.5209e-03, -1.0114e-02, -2.2089e-02, -8.8000e-03,\n",
       "         5.9595e-05,  2.7624e-03, -9.0763e-04, -1.8945e-02, -1.4327e-02,\n",
       "         3.3095e-04, -4.1328e-03,  2.6886e-02, -3.2741e-02,  2.6439e-02,\n",
       "        -4.9032e-04,  4.7680e-03,  2.5376e-02,  4.8642e-03, -8.9701e-03,\n",
       "         2.0121e-02, -9.6368e-03, -4.6084e-03, -2.7613e-02, -3.8959e-02,\n",
       "        -4.1883e-02,  2.5669e-02,  7.6107e-03, -5.0484e-03, -3.7441e-02,\n",
       "         1.8519e-02, -5.8844e-03,  6.6584e-03,  2.0022e-02,  4.1921e-02,\n",
       "         2.2195e-03, -3.8636e-03,  1.3128e-02,  5.7113e-03, -1.7031e-02,\n",
       "         9.5581e-03,  6.2951e-03,  1.5868e-03, -1.1878e-02, -2.8881e-02,\n",
       "         4.1967e-03, -4.1049e-02,  1.7913e-02, -2.5611e-02,  1.1383e-02,\n",
       "        -7.5825e-03,  1.2505e-02,  2.5889e-02,  6.6733e-03,  3.1285e-03,\n",
       "         1.4524e-02, -1.6832e-02,  1.5961e-02,  2.5858e-02, -2.0040e-02,\n",
       "         2.7974e-02, -6.8105e-03, -3.5004e-04, -1.6795e-02, -3.8028e-02,\n",
       "         1.4152e-02, -2.9289e-02, -4.9243e-03, -3.3917e-03, -2.2126e-02,\n",
       "        -3.6177e-04,  3.6739e-02, -1.7772e-02,  2.0691e-02, -3.0628e-06,\n",
       "         7.2938e-05,  1.7504e-02,  2.4400e-02, -2.5838e-02,  6.6199e-03,\n",
       "         2.4958e-02,  1.0910e-02,  5.5766e-03,  1.0443e-02,  3.4933e-03,\n",
       "        -2.4665e-02, -5.4144e-03, -8.6577e-03,  1.0762e-03,  8.9664e-03,\n",
       "        -2.9854e-02, -1.7849e-02,  2.3303e-02, -2.6041e-03, -2.5024e-02,\n",
       "         1.3579e-02,  3.0741e-03,  6.9263e-03, -3.1782e-02, -1.7823e-02,\n",
       "        -1.4173e-02,  1.0884e-02,  5.3759e-03, -4.1249e-04, -1.3797e-03,\n",
       "        -1.7479e-02,  5.8995e-03, -8.5976e-03, -4.9796e-02,  4.2011e-02,\n",
       "         1.2691e-02, -2.9495e-02, -1.0332e-02, -4.2063e-03], device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['stages.3.blocks.1.attn.crpb_mlp.3.weight'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Swin v2 config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MODEL': {'TYPE': 'swinv2',\n",
       "  'NAME': 'simmim_train',\n",
       "  'PRETRAINED': '../../models/swin2/simmim.pth',\n",
       "  'DROP_PATH_RATE': 0.2,\n",
       "  'SWIN': {'EMBED_DIM': 96,\n",
       "   'DEPTHS': [2, 2, 6, 2],\n",
       "   'NUM_HEADS': [3, 6, 12, 24],\n",
       "   'WINDOW_SIZE': 7,\n",
       "   'PATCH_SIZE': 4}},\n",
       " 'DATA': {'IMG_SIZE': 224,\n",
       "  'MASK_PATCH_SIZE': 32,\n",
       "  'MASK_RATIO': 0.6,\n",
       "  'BATCH_SIZE': 960,\n",
       "  'NUM_WORKERS': 24,\n",
       "  'DATA_PATH': '../../data/sports'},\n",
       " 'TRAIN': {'EPOCHS': 20,\n",
       "  'WARMUP_EPOCHS': 10,\n",
       "  'BASE_LR': '1e-4',\n",
       "  'WEIGHT_DECAY': 0.05,\n",
       "  'CLIP_GRAD': 5}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swin_config = yaml.load(open('config/train.yaml'), Loader=yaml.FullLoader)\n",
    "swin_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load weight from SimMIM Model\n",
    "- Different Image/Window Size\n",
    "- Image와 Window 사이즈의 비율은 맞춰야함\n",
    "  ex) 192÷6 = 224÷7 = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained(config, model):\n",
    "    print(f\"==============> Loading weight {config.MODEL.PRETRAINED} for fine-tuning......\")\n",
    "    state_dict = torch.load(config.MODEL.PRETRAINED, map_location='cpu')\n",
    "\n",
    "    # remain encoder only\n",
    "    not_encoder_keys = [k for k in state_dict.keys() if 'encoder' not in k]\n",
    "    for k in not_encoder_keys:\n",
    "        del state_dict[k]\n",
    "        \n",
    "    # remove prefix encoder.\n",
    "    state_dict = {k.replace('encoder.', ''):v for k, v in state_dict.items()}\n",
    "\n",
    "    # delete relative_position_index since we always re-init it\n",
    "    relative_position_index_keys = [k for k in state_dict.keys() if \"relative_position_index\" in k]\n",
    "    for k in relative_position_index_keys:\n",
    "        del state_dict[k]\n",
    "\n",
    "    # delete relative_coords_table since we always re-init it\n",
    "    relative_position_index_keys = [k for k in state_dict.keys() if \"relative_coords_table\" in k]\n",
    "    for k in relative_position_index_keys:\n",
    "        del state_dict[k]\n",
    "\n",
    "    # delete attn_mask since we always re-init it\n",
    "    attn_mask_keys = [k for k in state_dict.keys() if \"attn_mask\" in k]\n",
    "    for k in attn_mask_keys:\n",
    "        del state_dict[k]\n",
    "\n",
    "    # bicubic interpolate relative_position_bias_table if not match\n",
    "    relative_position_bias_table_keys = [k for k in state_dict.keys() if \"relative_position_bias_table\" in k]\n",
    "    for k in relative_position_bias_table_keys:\n",
    "        relative_position_bias_table_pretrained = state_dict[k]\n",
    "        relative_position_bias_table_current = model.state_dict()[k]\n",
    "        L1, nH1 = relative_position_bias_table_pretrained.size()\n",
    "        L2, nH2 = relative_position_bias_table_current.size()\n",
    "        if nH1 != nH2:\n",
    "            print(f\"Error in loading {k}, passing......\")\n",
    "        else:\n",
    "            if L1 != L2:\n",
    "                # bicubic interpolate relative_position_bias_table if not match\n",
    "                S1 = int(L1 ** 0.5)\n",
    "                S2 = int(L2 ** 0.5)\n",
    "                relative_position_bias_table_pretrained_resized = torch.nn.functional.interpolate(\n",
    "                    relative_position_bias_table_pretrained.permute(1, 0).view(1, nH1, S1, S1), size=(S2, S2),\n",
    "                    mode='bicubic')\n",
    "                state_dict[k] = relative_position_bias_table_pretrained_resized.view(nH2, L2).permute(1, 0)\n",
    "\n",
    "    # bicubic interpolate absolute_pos_embed if not match\n",
    "    absolute_pos_embed_keys = [k for k in state_dict.keys() if \"absolute_pos_embed\" in k]\n",
    "    for k in absolute_pos_embed_keys:\n",
    "        # dpe\n",
    "        absolute_pos_embed_pretrained = state_dict[k]\n",
    "        absolute_pos_embed_current = model.state_dict()[k.replace('encoder.','')]\n",
    "        _, L1, C1 = absolute_pos_embed_pretrained.size()\n",
    "        _, L2, C2 = absolute_pos_embed_current.size()\n",
    "        if C1 != C1:\n",
    "            print(f\"Error in loading {k}, passing......\")\n",
    "        else:\n",
    "            if L1 != L2:\n",
    "                S1 = int(L1 ** 0.5)\n",
    "                S2 = int(L2 ** 0.5)\n",
    "                absolute_pos_embed_pretrained = absolute_pos_embed_pretrained.reshape(-1, S1, S1, C1)\n",
    "                absolute_pos_embed_pretrained = absolute_pos_embed_pretrained.permute(0, 3, 1, 2)\n",
    "                absolute_pos_embed_pretrained_resized = torch.nn.functional.interpolate(\n",
    "                    absolute_pos_embed_pretrained, size=(S2, S2), mode='bicubic')\n",
    "                absolute_pos_embed_pretrained_resized = absolute_pos_embed_pretrained_resized.permute(0, 2, 3, 1)\n",
    "                absolute_pos_embed_pretrained_resized = absolute_pos_embed_pretrained_resized.flatten(1, 2)\n",
    "                state_dict[k] = absolute_pos_embed_pretrained_resized\n",
    "\n",
    "    # check classifier, if not match, then re-init classifier to zero\n",
    "    head_bias_pretrained = state_dict['classifier.bias']\n",
    "    Nc1 = head_bias_pretrained.shape[0]\n",
    "    Nc2 = model.classifier.bias.shape[0]\n",
    "    if (Nc1 != Nc2):\n",
    "        torch.nn.init.constant_(model.classifier.bias, 0.)\n",
    "        torch.nn.init.constant_(model.classifier.weight, 0.)\n",
    "        del state_dict['classifier.weight']\n",
    "        del state_dict['classifier.bias']\n",
    "        print(f\"Error in loading classifier head, re-init classifier head to 0\")\n",
    "\n",
    "    msg = model.load_state_dict(state_dict, strict=False)\n",
    "    print(msg)\n",
    "\n",
    "    print(f\"=> loaded successfully '{config.MODEL.PRETRAINED}'\")\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============> Loading weight ../../models/swin2/simmim.pth for fine-tuning......\n",
      "_IncompatibleKeys(missing_keys=['stages.0.blocks.0.attn_mask', 'stages.0.blocks.0.attn.relative_coords_table', 'stages.0.blocks.0.attn.relative_position_index', 'stages.0.blocks.1.attn_mask', 'stages.0.blocks.1.attn.relative_coords_table', 'stages.0.blocks.1.attn.relative_position_index', 'stages.1.blocks.0.attn_mask', 'stages.1.blocks.0.attn.relative_coords_table', 'stages.1.blocks.0.attn.relative_position_index', 'stages.1.blocks.1.attn_mask', 'stages.1.blocks.1.attn.relative_coords_table', 'stages.1.blocks.1.attn.relative_position_index', 'stages.2.blocks.0.attn_mask', 'stages.2.blocks.0.attn.relative_coords_table', 'stages.2.blocks.0.attn.relative_position_index', 'stages.2.blocks.1.attn_mask', 'stages.2.blocks.1.attn.relative_coords_table', 'stages.2.blocks.1.attn.relative_position_index', 'stages.2.blocks.2.attn_mask', 'stages.2.blocks.2.attn.relative_coords_table', 'stages.2.blocks.2.attn.relative_position_index', 'stages.2.blocks.3.attn_mask', 'stages.2.blocks.3.attn.relative_coords_table', 'stages.2.blocks.3.attn.relative_position_index', 'stages.2.blocks.4.attn_mask', 'stages.2.blocks.4.attn.relative_coords_table', 'stages.2.blocks.4.attn.relative_position_index', 'stages.2.blocks.5.attn_mask', 'stages.2.blocks.5.attn.relative_coords_table', 'stages.2.blocks.5.attn.relative_position_index', 'stages.3.blocks.0.attn_mask', 'stages.3.blocks.0.attn.relative_coords_table', 'stages.3.blocks.0.attn.relative_position_index', 'stages.3.blocks.1.attn_mask', 'stages.3.blocks.1.attn.relative_coords_table', 'stages.3.blocks.1.attn.relative_position_index'], unexpected_keys=['mask_token'])\n",
      "=> loaded successfully '../../models/swin2/simmim.pth'\n"
     ]
    }
   ],
   "source": [
    "swin_config = Box(swin_config)\n",
    "load_pretrained(swin_config, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Loading Weight Results\n",
    "- 정상적으로 불러와졌는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0460,  0.0211, -0.0460,  0.0576],\n",
       "         [ 0.1058,  0.0408,  0.1015, -0.0615],\n",
       "         [-0.0989, -0.1073, -0.0074, -0.0704],\n",
       "         [ 0.0441,  0.0788, -0.1280, -0.1076]],\n",
       "\n",
       "        [[ 0.0988, -0.0679, -0.0443,  0.0764],\n",
       "         [ 0.0506,  0.0706, -0.0776,  0.0877],\n",
       "         [ 0.0787, -0.0059, -0.0263,  0.0769],\n",
       "         [ 0.0604, -0.0411,  0.0631,  0.0655]],\n",
       "\n",
       "        [[ 0.0299,  0.0698,  0.0270, -0.0802],\n",
       "         [ 0.1213, -0.0450, -0.1119, -0.0372],\n",
       "         [-0.0303,  0.0959,  0.0342,  0.0791],\n",
       "         [ 0.0050,  0.0420, -0.1352, -0.1343]]], device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['embeddings.patch_embeddings.weight'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.4102,  0.0190, -0.1695, -0.0780,  0.1185,  0.0560, -0.1155, -0.1401,\n",
       "        -0.1512, -0.1898,  0.1406, -0.1439, -0.0065, -0.0821,  0.2524, -0.1698,\n",
       "        -0.2071, -0.1644, -0.0385,  0.2512, -0.1601, -0.0571,  0.0558, -0.1932,\n",
       "        -0.1750, -0.0531, -0.0358, -0.0926,  0.0338,  0.2469, -0.1665,  0.0107,\n",
       "        -0.1085, -0.0957, -0.1816, -0.2193, -0.1730,  0.1014, -0.1871,  0.1377,\n",
       "        -0.0585,  0.3111,  0.0440, -0.0796, -0.1696, -0.0710, -0.0333, -0.0791,\n",
       "         0.0836,  0.2392, -0.1803, -0.0753, -0.1639, -0.0884,  0.0031, -0.1653,\n",
       "        -0.0262,  0.2262, -0.1484, -0.1127, -0.0729, -0.1282, -0.0657,  0.1217,\n",
       "        -0.1038,  0.1075,  0.0501, -0.1895, -0.0205, -0.0921, -0.0430, -0.0467,\n",
       "        -0.2041, -0.1932,  0.2660,  0.1162,  0.0723,  0.0066,  0.0251, -0.1999,\n",
       "        -0.1775, -0.1588, -0.1684, -0.0295,  0.0416, -0.2120, -0.0950, -0.0661,\n",
       "         0.0866, -0.0630, -0.0387, -0.2299, -0.1044,  0.0472, -0.0744, -0.1581,\n",
       "        -0.1818,  0.0387,  0.1348, -0.2045,  0.0280,  0.1852, -0.1781,  0.0409,\n",
       "         0.0035, -0.2059, -0.1942, -0.1630,  0.2748,  0.0633,  0.1389, -0.0552,\n",
       "        -0.2019, -0.1783, -0.0675, -0.1994, -0.0105,  0.1170, -0.1600,  0.0550,\n",
       "         0.0968, -0.2032,  0.0029, -0.0610,  0.0747, -0.1634, -0.1968, -0.0680,\n",
       "         0.2615, -0.0598, -0.2015,  0.0342, -0.0538,  0.1656, -0.0702,  0.0047,\n",
       "        -0.0724, -0.1742,  0.1454, -0.0817, -0.0231, -0.1882,  0.1018, -0.0354,\n",
       "         0.1905, -0.0919,  0.0610, -0.1950, -0.0914,  0.0741,  0.0701, -0.1558,\n",
       "         0.1570,  0.0642,  0.0713, -0.1791, -0.1427, -0.0853, -0.1552,  0.2886,\n",
       "         0.0282, -0.0036,  0.0629,  0.0395, -0.1779,  0.2965, -0.1333,  0.4062,\n",
       "        -0.1608, -0.1914,  0.0319,  0.3345,  0.0849,  0.0322,  0.0964, -0.1841,\n",
       "        -0.1890, -0.2122,  0.0314, -0.2146, -0.0747,  0.0241,  0.0012, -0.0717,\n",
       "        -0.0164, -0.1652, -0.1749, -0.1653, -0.2059, -0.1840,  0.0482, -0.1817,\n",
       "        -0.1675, -0.1627,  0.0653, -0.0979, -0.1607, -0.1050, -0.0600,  0.0179,\n",
       "         0.0947, -0.1855, -0.1622,  0.0124, -0.1253,  0.2771,  0.4477, -0.1971,\n",
       "        -0.1869, -0.1995, -0.0928, -0.1209, -0.1834, -0.1810, -0.1677, -0.1770,\n",
       "        -0.1244,  0.1016,  0.0904, -0.1747,  0.0886,  0.0794,  0.3072, -0.1639,\n",
       "        -0.1648, -0.1812,  0.0852, -0.1715, -0.1522, -0.2107,  0.0203,  0.1314,\n",
       "         0.3117,  0.2793,  0.0646, -0.1663, -0.0734, -0.1817,  0.0489, -0.1806,\n",
       "        -0.0602, -0.0631, -0.1625,  0.3647, -0.0869,  0.1676, -0.1767, -0.0323,\n",
       "         0.0207,  0.3126, -0.0884, -0.0133, -0.1216,  0.3579, -0.1596,  0.0690,\n",
       "         0.1158, -0.0575,  0.0608,  0.0660,  0.0986,  0.1013, -0.1732, -0.0668,\n",
       "         0.0942, -0.0830, -0.1758, -0.2123, -0.1639,  0.0836, -0.1653, -0.1684,\n",
       "        -0.1407, -0.0716, -0.1819, -0.1927, -0.0639,  0.0720,  0.0314, -0.0736,\n",
       "        -0.1178, -0.1474, -0.0355, -0.1820,  0.0873, -0.1594, -0.0968, -0.0828,\n",
       "        -0.1368, -0.1822,  0.0026, -0.1918, -0.1615, -0.1656,  0.0639, -0.0764,\n",
       "         0.1011, -0.1753,  0.0737, -0.0388, -0.1503, -0.1794,  0.1544, -0.0008,\n",
       "        -0.1276, -0.1904, -0.1843,  0.2936, -0.0490, -0.1680, -0.1759, -0.1811,\n",
       "        -0.1871, -0.1836, -0.1413, -0.0979, -0.1666,  0.1850, -0.1728,  0.4047,\n",
       "         0.0367,  0.1197, -0.1782, -0.1661, -0.0788, -0.2086,  0.0861,  0.1003,\n",
       "        -0.1712,  0.0035, -0.1847, -0.1733,  0.0161, -0.1347,  0.2732, -0.1983,\n",
       "        -0.1632, -0.1763, -0.1844,  0.0086, -0.1771,  0.2863, -0.1878,  0.0807,\n",
       "         0.0407, -0.0990, -0.1799, -0.0993,  0.0705,  0.0822, -0.1899,  0.1199,\n",
       "         0.0014,  0.0199, -0.0847,  0.0268, -0.1019,  0.3854, -0.1857,  0.1173,\n",
       "        -0.1944, -0.0378,  0.0396, -0.1572, -0.0734,  0.3350,  0.0064, -0.0181,\n",
       "        -0.0826,  0.0549, -0.1961, -0.0563, -0.1029,  0.0084, -0.0944, -0.1627,\n",
       "        -0.0907,  0.0882, -0.2312,  0.0734, -0.1575, -0.0594, -0.0747,  0.0560],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['stages.3.blocks.1.attn.crpb_mlp.3.weight'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.Stage-2 Traing\n",
    "- Supervised pre-training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Transform, Loss, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms 정의하기\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8,1), interpolation=transforms.InterpolationMode.LANCZOS),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    transforms.RandomErasing(p=0.9, scale=(0.02, 0.33)),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "data_dir = '../../data/sports'\n",
    "batch_size = 960\n",
    "\n",
    "train_path = data_dir+'/train'\n",
    "valid_path = data_dir+'/valid'\n",
    "test_path = data_dir+'/test'\n",
    "\n",
    "# dataset load\n",
    "train_data = ImageFolder(train_path, transform=train_transform)\n",
    "valid_data = ImageFolder(valid_path, transform=test_transform)\n",
    "test_data = ImageFolder(test_path, transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_norm = 1.0 # paper : 100 with G variants\n",
    "\n",
    "model.to(device)\n",
    "model_path = '../../models/swin2/model_w_simmim.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixup_fn = Mixup(mixup_alpha=.7, \n",
    "                cutmix_alpha=.7, \n",
    "                prob=.7, \n",
    "                switch_prob=0.5, \n",
    "                mode='batch',\n",
    "                label_smoothing=.1,\n",
    "                num_classes=100)\n",
    "\n",
    "epochs = 200\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer-Wise Learning Rate Decay ★"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: absolute_pos_embed\n",
      "1: embeddings.patch_embeddings.weight\n",
      "2: embeddings.patch_embeddings.bias\n",
      "3: embeddings.norm.weight\n",
      "4: embeddings.norm.bias\n",
      "5: stages.0.blocks.0.attn.t_scale\n",
      "6: stages.0.blocks.0.attn.crpb_mlp.0.weight\n",
      "7: stages.0.blocks.0.attn.crpb_mlp.0.bias\n",
      "8: stages.0.blocks.0.attn.crpb_mlp.3.weight\n",
      "9: stages.0.blocks.0.attn.qkv.weight\n",
      "10: stages.0.blocks.0.attn.qkv.bias\n",
      "11: stages.0.blocks.0.attn.proj.weight\n",
      "12: stages.0.blocks.0.attn.proj.bias\n",
      "13: stages.0.blocks.0.norm1.weight\n",
      "14: stages.0.blocks.0.norm1.bias\n",
      "15: stages.0.blocks.0.mlp.fc1.weight\n",
      "16: stages.0.blocks.0.mlp.fc1.bias\n",
      "17: stages.0.blocks.0.mlp.fc2.weight\n",
      "18: stages.0.blocks.0.mlp.fc2.bias\n",
      "19: stages.0.blocks.0.norm2.weight\n",
      "20: stages.0.blocks.0.norm2.bias\n",
      "21: stages.0.blocks.1.attn.t_scale\n",
      "22: stages.0.blocks.1.attn.crpb_mlp.0.weight\n",
      "23: stages.0.blocks.1.attn.crpb_mlp.0.bias\n",
      "24: stages.0.blocks.1.attn.crpb_mlp.3.weight\n",
      "25: stages.0.blocks.1.attn.qkv.weight\n",
      "26: stages.0.blocks.1.attn.qkv.bias\n",
      "27: stages.0.blocks.1.attn.proj.weight\n",
      "28: stages.0.blocks.1.attn.proj.bias\n",
      "29: stages.0.blocks.1.norm1.weight\n",
      "30: stages.0.blocks.1.norm1.bias\n",
      "31: stages.0.blocks.1.mlp.fc1.weight\n",
      "32: stages.0.blocks.1.mlp.fc1.bias\n",
      "33: stages.0.blocks.1.mlp.fc2.weight\n",
      "34: stages.0.blocks.1.mlp.fc2.bias\n",
      "35: stages.0.blocks.1.norm2.weight\n",
      "36: stages.0.blocks.1.norm2.bias\n",
      "37: stages.0.downsample.reduction.weight\n",
      "38: stages.0.downsample.norm.weight\n",
      "39: stages.0.downsample.norm.bias\n",
      "40: stages.1.blocks.0.attn.t_scale\n",
      "41: stages.1.blocks.0.attn.crpb_mlp.0.weight\n",
      "42: stages.1.blocks.0.attn.crpb_mlp.0.bias\n",
      "43: stages.1.blocks.0.attn.crpb_mlp.3.weight\n",
      "44: stages.1.blocks.0.attn.qkv.weight\n",
      "45: stages.1.blocks.0.attn.qkv.bias\n",
      "46: stages.1.blocks.0.attn.proj.weight\n",
      "47: stages.1.blocks.0.attn.proj.bias\n",
      "48: stages.1.blocks.0.norm1.weight\n",
      "49: stages.1.blocks.0.norm1.bias\n",
      "50: stages.1.blocks.0.mlp.fc1.weight\n",
      "51: stages.1.blocks.0.mlp.fc1.bias\n",
      "52: stages.1.blocks.0.mlp.fc2.weight\n",
      "53: stages.1.blocks.0.mlp.fc2.bias\n",
      "54: stages.1.blocks.0.norm2.weight\n",
      "55: stages.1.blocks.0.norm2.bias\n",
      "56: stages.1.blocks.1.attn.t_scale\n",
      "57: stages.1.blocks.1.attn.crpb_mlp.0.weight\n",
      "58: stages.1.blocks.1.attn.crpb_mlp.0.bias\n",
      "59: stages.1.blocks.1.attn.crpb_mlp.3.weight\n",
      "60: stages.1.blocks.1.attn.qkv.weight\n",
      "61: stages.1.blocks.1.attn.qkv.bias\n",
      "62: stages.1.blocks.1.attn.proj.weight\n",
      "63: stages.1.blocks.1.attn.proj.bias\n",
      "64: stages.1.blocks.1.norm1.weight\n",
      "65: stages.1.blocks.1.norm1.bias\n",
      "66: stages.1.blocks.1.mlp.fc1.weight\n",
      "67: stages.1.blocks.1.mlp.fc1.bias\n",
      "68: stages.1.blocks.1.mlp.fc2.weight\n",
      "69: stages.1.blocks.1.mlp.fc2.bias\n",
      "70: stages.1.blocks.1.norm2.weight\n",
      "71: stages.1.blocks.1.norm2.bias\n",
      "72: stages.1.downsample.reduction.weight\n",
      "73: stages.1.downsample.norm.weight\n",
      "74: stages.1.downsample.norm.bias\n",
      "75: stages.2.blocks.0.attn.t_scale\n",
      "76: stages.2.blocks.0.attn.crpb_mlp.0.weight\n",
      "77: stages.2.blocks.0.attn.crpb_mlp.0.bias\n",
      "78: stages.2.blocks.0.attn.crpb_mlp.3.weight\n",
      "79: stages.2.blocks.0.attn.qkv.weight\n",
      "80: stages.2.blocks.0.attn.qkv.bias\n",
      "81: stages.2.blocks.0.attn.proj.weight\n",
      "82: stages.2.blocks.0.attn.proj.bias\n",
      "83: stages.2.blocks.0.norm1.weight\n",
      "84: stages.2.blocks.0.norm1.bias\n",
      "85: stages.2.blocks.0.mlp.fc1.weight\n",
      "86: stages.2.blocks.0.mlp.fc1.bias\n",
      "87: stages.2.blocks.0.mlp.fc2.weight\n",
      "88: stages.2.blocks.0.mlp.fc2.bias\n",
      "89: stages.2.blocks.0.norm2.weight\n",
      "90: stages.2.blocks.0.norm2.bias\n",
      "91: stages.2.blocks.1.attn.t_scale\n",
      "92: stages.2.blocks.1.attn.crpb_mlp.0.weight\n",
      "93: stages.2.blocks.1.attn.crpb_mlp.0.bias\n",
      "94: stages.2.blocks.1.attn.crpb_mlp.3.weight\n",
      "95: stages.2.blocks.1.attn.qkv.weight\n",
      "96: stages.2.blocks.1.attn.qkv.bias\n",
      "97: stages.2.blocks.1.attn.proj.weight\n",
      "98: stages.2.blocks.1.attn.proj.bias\n",
      "99: stages.2.blocks.1.norm1.weight\n",
      "100: stages.2.blocks.1.norm1.bias\n",
      "101: stages.2.blocks.1.mlp.fc1.weight\n",
      "102: stages.2.blocks.1.mlp.fc1.bias\n",
      "103: stages.2.blocks.1.mlp.fc2.weight\n",
      "104: stages.2.blocks.1.mlp.fc2.bias\n",
      "105: stages.2.blocks.1.norm2.weight\n",
      "106: stages.2.blocks.1.norm2.bias\n",
      "107: stages.2.blocks.2.attn.t_scale\n",
      "108: stages.2.blocks.2.attn.crpb_mlp.0.weight\n",
      "109: stages.2.blocks.2.attn.crpb_mlp.0.bias\n",
      "110: stages.2.blocks.2.attn.crpb_mlp.3.weight\n",
      "111: stages.2.blocks.2.attn.qkv.weight\n",
      "112: stages.2.blocks.2.attn.qkv.bias\n",
      "113: stages.2.blocks.2.attn.proj.weight\n",
      "114: stages.2.blocks.2.attn.proj.bias\n",
      "115: stages.2.blocks.2.norm1.weight\n",
      "116: stages.2.blocks.2.norm1.bias\n",
      "117: stages.2.blocks.2.mlp.fc1.weight\n",
      "118: stages.2.blocks.2.mlp.fc1.bias\n",
      "119: stages.2.blocks.2.mlp.fc2.weight\n",
      "120: stages.2.blocks.2.mlp.fc2.bias\n",
      "121: stages.2.blocks.2.norm2.weight\n",
      "122: stages.2.blocks.2.norm2.bias\n",
      "123: stages.2.blocks.3.attn.t_scale\n",
      "124: stages.2.blocks.3.attn.crpb_mlp.0.weight\n",
      "125: stages.2.blocks.3.attn.crpb_mlp.0.bias\n",
      "126: stages.2.blocks.3.attn.crpb_mlp.3.weight\n",
      "127: stages.2.blocks.3.attn.qkv.weight\n",
      "128: stages.2.blocks.3.attn.qkv.bias\n",
      "129: stages.2.blocks.3.attn.proj.weight\n",
      "130: stages.2.blocks.3.attn.proj.bias\n",
      "131: stages.2.blocks.3.norm1.weight\n",
      "132: stages.2.blocks.3.norm1.bias\n",
      "133: stages.2.blocks.3.mlp.fc1.weight\n",
      "134: stages.2.blocks.3.mlp.fc1.bias\n",
      "135: stages.2.blocks.3.mlp.fc2.weight\n",
      "136: stages.2.blocks.3.mlp.fc2.bias\n",
      "137: stages.2.blocks.3.norm2.weight\n",
      "138: stages.2.blocks.3.norm2.bias\n",
      "139: stages.2.blocks.4.attn.t_scale\n",
      "140: stages.2.blocks.4.attn.crpb_mlp.0.weight\n",
      "141: stages.2.blocks.4.attn.crpb_mlp.0.bias\n",
      "142: stages.2.blocks.4.attn.crpb_mlp.3.weight\n",
      "143: stages.2.blocks.4.attn.qkv.weight\n",
      "144: stages.2.blocks.4.attn.qkv.bias\n",
      "145: stages.2.blocks.4.attn.proj.weight\n",
      "146: stages.2.blocks.4.attn.proj.bias\n",
      "147: stages.2.blocks.4.norm1.weight\n",
      "148: stages.2.blocks.4.norm1.bias\n",
      "149: stages.2.blocks.4.mlp.fc1.weight\n",
      "150: stages.2.blocks.4.mlp.fc1.bias\n",
      "151: stages.2.blocks.4.mlp.fc2.weight\n",
      "152: stages.2.blocks.4.mlp.fc2.bias\n",
      "153: stages.2.blocks.4.norm2.weight\n",
      "154: stages.2.blocks.4.norm2.bias\n",
      "155: stages.2.blocks.5.attn.t_scale\n",
      "156: stages.2.blocks.5.attn.crpb_mlp.0.weight\n",
      "157: stages.2.blocks.5.attn.crpb_mlp.0.bias\n",
      "158: stages.2.blocks.5.attn.crpb_mlp.3.weight\n",
      "159: stages.2.blocks.5.attn.qkv.weight\n",
      "160: stages.2.blocks.5.attn.qkv.bias\n",
      "161: stages.2.blocks.5.attn.proj.weight\n",
      "162: stages.2.blocks.5.attn.proj.bias\n",
      "163: stages.2.blocks.5.norm1.weight\n",
      "164: stages.2.blocks.5.norm1.bias\n",
      "165: stages.2.blocks.5.mlp.fc1.weight\n",
      "166: stages.2.blocks.5.mlp.fc1.bias\n",
      "167: stages.2.blocks.5.mlp.fc2.weight\n",
      "168: stages.2.blocks.5.mlp.fc2.bias\n",
      "169: stages.2.blocks.5.norm2.weight\n",
      "170: stages.2.blocks.5.norm2.bias\n",
      "171: stages.2.downsample.reduction.weight\n",
      "172: stages.2.downsample.norm.weight\n",
      "173: stages.2.downsample.norm.bias\n",
      "174: stages.3.blocks.0.attn.t_scale\n",
      "175: stages.3.blocks.0.attn.crpb_mlp.0.weight\n",
      "176: stages.3.blocks.0.attn.crpb_mlp.0.bias\n",
      "177: stages.3.blocks.0.attn.crpb_mlp.3.weight\n",
      "178: stages.3.blocks.0.attn.qkv.weight\n",
      "179: stages.3.blocks.0.attn.qkv.bias\n",
      "180: stages.3.blocks.0.attn.proj.weight\n",
      "181: stages.3.blocks.0.attn.proj.bias\n",
      "182: stages.3.blocks.0.norm1.weight\n",
      "183: stages.3.blocks.0.norm1.bias\n",
      "184: stages.3.blocks.0.mlp.fc1.weight\n",
      "185: stages.3.blocks.0.mlp.fc1.bias\n",
      "186: stages.3.blocks.0.mlp.fc2.weight\n",
      "187: stages.3.blocks.0.mlp.fc2.bias\n",
      "188: stages.3.blocks.0.norm2.weight\n",
      "189: stages.3.blocks.0.norm2.bias\n",
      "190: stages.3.blocks.1.attn.t_scale\n",
      "191: stages.3.blocks.1.attn.crpb_mlp.0.weight\n",
      "192: stages.3.blocks.1.attn.crpb_mlp.0.bias\n",
      "193: stages.3.blocks.1.attn.crpb_mlp.3.weight\n",
      "194: stages.3.blocks.1.attn.qkv.weight\n",
      "195: stages.3.blocks.1.attn.qkv.bias\n",
      "196: stages.3.blocks.1.attn.proj.weight\n",
      "197: stages.3.blocks.1.attn.proj.bias\n",
      "198: stages.3.blocks.1.norm1.weight\n",
      "199: stages.3.blocks.1.norm1.bias\n",
      "200: stages.3.blocks.1.mlp.fc1.weight\n",
      "201: stages.3.blocks.1.mlp.fc1.bias\n",
      "202: stages.3.blocks.1.mlp.fc2.weight\n",
      "203: stages.3.blocks.1.mlp.fc2.bias\n",
      "204: stages.3.blocks.1.norm2.weight\n",
      "205: stages.3.blocks.1.norm2.bias\n",
      "206: layernorm.weight\n",
      "207: layernorm.bias\n",
      "208: classifier.weight\n",
      "209: classifier.bias\n"
     ]
    }
   ],
   "source": [
    "layer_names = []\n",
    "for i, (name, params) in enumerate(model.named_parameters()):\n",
    "    lr = base_lr\n",
    "    print(f'{i}: {name}')\n",
    "    layer_names.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['classifier.bias',\n",
       " 'classifier.weight',\n",
       " 'layernorm.bias',\n",
       " 'layernorm.weight',\n",
       " 'stages.3.blocks.1.norm2.bias']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_names.reverse()\n",
    "layer_names[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: classifier.bias's lr=0.0014, weight_decay=0\n",
      "1: classifier.weight's lr=0.0014, weight_decay=0.01\n",
      "2: layernorm.bias's lr=0.001218, weight_decay=0\n",
      "3: layernorm.weight's lr=0.001218, weight_decay=0\n",
      "4: stages.3.blocks.1.norm2.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "5: stages.3.blocks.1.norm2.weight's lr=0.0010596599999999998, weight_decay=0\n",
      "6: stages.3.blocks.1.mlp.fc2.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "7: stages.3.blocks.1.mlp.fc2.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "8: stages.3.blocks.1.mlp.fc1.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "9: stages.3.blocks.1.mlp.fc1.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "10: stages.3.blocks.1.norm1.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "11: stages.3.blocks.1.norm1.weight's lr=0.0010596599999999998, weight_decay=0\n",
      "12: stages.3.blocks.1.attn.proj.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "13: stages.3.blocks.1.attn.proj.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "14: stages.3.blocks.1.attn.qkv.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "15: stages.3.blocks.1.attn.qkv.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "16: stages.3.blocks.1.attn.crpb_mlp.3.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "17: stages.3.blocks.1.attn.crpb_mlp.0.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "18: stages.3.blocks.1.attn.crpb_mlp.0.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "19: stages.3.blocks.1.attn.t_scale's lr=0.0010596599999999998, weight_decay=0\n",
      "20: stages.3.blocks.0.norm2.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "21: stages.3.blocks.0.norm2.weight's lr=0.0010596599999999998, weight_decay=0\n",
      "22: stages.3.blocks.0.mlp.fc2.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "23: stages.3.blocks.0.mlp.fc2.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "24: stages.3.blocks.0.mlp.fc1.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "25: stages.3.blocks.0.mlp.fc1.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "26: stages.3.blocks.0.norm1.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "27: stages.3.blocks.0.norm1.weight's lr=0.0010596599999999998, weight_decay=0\n",
      "28: stages.3.blocks.0.attn.proj.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "29: stages.3.blocks.0.attn.proj.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "30: stages.3.blocks.0.attn.qkv.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "31: stages.3.blocks.0.attn.qkv.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "32: stages.3.blocks.0.attn.crpb_mlp.3.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "33: stages.3.blocks.0.attn.crpb_mlp.0.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "34: stages.3.blocks.0.attn.crpb_mlp.0.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "35: stages.3.blocks.0.attn.t_scale's lr=0.0010596599999999998, weight_decay=0\n",
      "36: stages.2.downsample.norm.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "37: stages.2.downsample.norm.weight's lr=0.0010596599999999998, weight_decay=0\n",
      "38: stages.2.downsample.reduction.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "39: stages.2.blocks.5.norm2.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "40: stages.2.blocks.5.norm2.weight's lr=0.0010596599999999998, weight_decay=0\n",
      "41: stages.2.blocks.5.mlp.fc2.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "42: stages.2.blocks.5.mlp.fc2.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "43: stages.2.blocks.5.mlp.fc1.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "44: stages.2.blocks.5.mlp.fc1.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "45: stages.2.blocks.5.norm1.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "46: stages.2.blocks.5.norm1.weight's lr=0.0010596599999999998, weight_decay=0\n",
      "47: stages.2.blocks.5.attn.proj.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "48: stages.2.blocks.5.attn.proj.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "49: stages.2.blocks.5.attn.qkv.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "50: stages.2.blocks.5.attn.qkv.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "51: stages.2.blocks.5.attn.crpb_mlp.3.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "52: stages.2.blocks.5.attn.crpb_mlp.0.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "53: stages.2.blocks.5.attn.crpb_mlp.0.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "54: stages.2.blocks.5.attn.t_scale's lr=0.0010596599999999998, weight_decay=0\n",
      "55: stages.2.blocks.4.norm2.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "56: stages.2.blocks.4.norm2.weight's lr=0.0010596599999999998, weight_decay=0\n",
      "57: stages.2.blocks.4.mlp.fc2.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "58: stages.2.blocks.4.mlp.fc2.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "59: stages.2.blocks.4.mlp.fc1.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "60: stages.2.blocks.4.mlp.fc1.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "61: stages.2.blocks.4.norm1.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "62: stages.2.blocks.4.norm1.weight's lr=0.0010596599999999998, weight_decay=0\n",
      "63: stages.2.blocks.4.attn.proj.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "64: stages.2.blocks.4.attn.proj.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "65: stages.2.blocks.4.attn.qkv.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "66: stages.2.blocks.4.attn.qkv.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "67: stages.2.blocks.4.attn.crpb_mlp.3.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "68: stages.2.blocks.4.attn.crpb_mlp.0.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "69: stages.2.blocks.4.attn.crpb_mlp.0.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "70: stages.2.blocks.4.attn.t_scale's lr=0.0010596599999999998, weight_decay=0\n",
      "71: stages.2.blocks.3.norm2.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "72: stages.2.blocks.3.norm2.weight's lr=0.0010596599999999998, weight_decay=0\n",
      "73: stages.2.blocks.3.mlp.fc2.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "74: stages.2.blocks.3.mlp.fc2.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "75: stages.2.blocks.3.mlp.fc1.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "76: stages.2.blocks.3.mlp.fc1.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "77: stages.2.blocks.3.norm1.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "78: stages.2.blocks.3.norm1.weight's lr=0.0010596599999999998, weight_decay=0\n",
      "79: stages.2.blocks.3.attn.proj.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "80: stages.2.blocks.3.attn.proj.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "81: stages.2.blocks.3.attn.qkv.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "82: stages.2.blocks.3.attn.qkv.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "83: stages.2.blocks.3.attn.crpb_mlp.3.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "84: stages.2.blocks.3.attn.crpb_mlp.0.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "85: stages.2.blocks.3.attn.crpb_mlp.0.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "86: stages.2.blocks.3.attn.t_scale's lr=0.0010596599999999998, weight_decay=0\n",
      "87: stages.2.blocks.2.norm2.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "88: stages.2.blocks.2.norm2.weight's lr=0.0010596599999999998, weight_decay=0\n",
      "89: stages.2.blocks.2.mlp.fc2.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "90: stages.2.blocks.2.mlp.fc2.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "91: stages.2.blocks.2.mlp.fc1.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "92: stages.2.blocks.2.mlp.fc1.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "93: stages.2.blocks.2.norm1.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "94: stages.2.blocks.2.norm1.weight's lr=0.0010596599999999998, weight_decay=0\n",
      "95: stages.2.blocks.2.attn.proj.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "96: stages.2.blocks.2.attn.proj.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "97: stages.2.blocks.2.attn.qkv.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "98: stages.2.blocks.2.attn.qkv.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "99: stages.2.blocks.2.attn.crpb_mlp.3.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "100: stages.2.blocks.2.attn.crpb_mlp.0.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "101: stages.2.blocks.2.attn.crpb_mlp.0.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "102: stages.2.blocks.2.attn.t_scale's lr=0.0010596599999999998, weight_decay=0\n",
      "103: stages.2.blocks.1.norm2.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "104: stages.2.blocks.1.norm2.weight's lr=0.0010596599999999998, weight_decay=0\n",
      "105: stages.2.blocks.1.mlp.fc2.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "106: stages.2.blocks.1.mlp.fc2.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "107: stages.2.blocks.1.mlp.fc1.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "108: stages.2.blocks.1.mlp.fc1.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "109: stages.2.blocks.1.norm1.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "110: stages.2.blocks.1.norm1.weight's lr=0.0010596599999999998, weight_decay=0\n",
      "111: stages.2.blocks.1.attn.proj.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "112: stages.2.blocks.1.attn.proj.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "113: stages.2.blocks.1.attn.qkv.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "114: stages.2.blocks.1.attn.qkv.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "115: stages.2.blocks.1.attn.crpb_mlp.3.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "116: stages.2.blocks.1.attn.crpb_mlp.0.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "117: stages.2.blocks.1.attn.crpb_mlp.0.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "118: stages.2.blocks.1.attn.t_scale's lr=0.0010596599999999998, weight_decay=0\n",
      "119: stages.2.blocks.0.norm2.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "120: stages.2.blocks.0.norm2.weight's lr=0.0010596599999999998, weight_decay=0\n",
      "121: stages.2.blocks.0.mlp.fc2.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "122: stages.2.blocks.0.mlp.fc2.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "123: stages.2.blocks.0.mlp.fc1.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "124: stages.2.blocks.0.mlp.fc1.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "125: stages.2.blocks.0.norm1.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "126: stages.2.blocks.0.norm1.weight's lr=0.0010596599999999998, weight_decay=0\n",
      "127: stages.2.blocks.0.attn.proj.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "128: stages.2.blocks.0.attn.proj.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "129: stages.2.blocks.0.attn.qkv.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "130: stages.2.blocks.0.attn.qkv.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "131: stages.2.blocks.0.attn.crpb_mlp.3.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "132: stages.2.blocks.0.attn.crpb_mlp.0.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "133: stages.2.blocks.0.attn.crpb_mlp.0.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "134: stages.2.blocks.0.attn.t_scale's lr=0.0010596599999999998, weight_decay=0\n",
      "135: stages.1.downsample.norm.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "136: stages.1.downsample.norm.weight's lr=0.0010596599999999998, weight_decay=0\n",
      "137: stages.1.downsample.reduction.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "138: stages.1.blocks.1.norm2.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "139: stages.1.blocks.1.norm2.weight's lr=0.0010596599999999998, weight_decay=0\n",
      "140: stages.1.blocks.1.mlp.fc2.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "141: stages.1.blocks.1.mlp.fc2.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "142: stages.1.blocks.1.mlp.fc1.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "143: stages.1.blocks.1.mlp.fc1.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "144: stages.1.blocks.1.norm1.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "145: stages.1.blocks.1.norm1.weight's lr=0.0010596599999999998, weight_decay=0\n",
      "146: stages.1.blocks.1.attn.proj.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "147: stages.1.blocks.1.attn.proj.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "148: stages.1.blocks.1.attn.qkv.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "149: stages.1.blocks.1.attn.qkv.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "150: stages.1.blocks.1.attn.crpb_mlp.3.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "151: stages.1.blocks.1.attn.crpb_mlp.0.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "152: stages.1.blocks.1.attn.crpb_mlp.0.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "153: stages.1.blocks.1.attn.t_scale's lr=0.0010596599999999998, weight_decay=0\n",
      "154: stages.1.blocks.0.norm2.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "155: stages.1.blocks.0.norm2.weight's lr=0.0010596599999999998, weight_decay=0\n",
      "156: stages.1.blocks.0.mlp.fc2.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "157: stages.1.blocks.0.mlp.fc2.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "158: stages.1.blocks.0.mlp.fc1.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "159: stages.1.blocks.0.mlp.fc1.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "160: stages.1.blocks.0.norm1.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "161: stages.1.blocks.0.norm1.weight's lr=0.0010596599999999998, weight_decay=0\n",
      "162: stages.1.blocks.0.attn.proj.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "163: stages.1.blocks.0.attn.proj.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "164: stages.1.blocks.0.attn.qkv.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "165: stages.1.blocks.0.attn.qkv.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "166: stages.1.blocks.0.attn.crpb_mlp.3.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "167: stages.1.blocks.0.attn.crpb_mlp.0.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "168: stages.1.blocks.0.attn.crpb_mlp.0.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "169: stages.1.blocks.0.attn.t_scale's lr=0.0010596599999999998, weight_decay=0\n",
      "170: stages.0.downsample.norm.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "171: stages.0.downsample.norm.weight's lr=0.0010596599999999998, weight_decay=0\n",
      "172: stages.0.downsample.reduction.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "173: stages.0.blocks.1.norm2.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "174: stages.0.blocks.1.norm2.weight's lr=0.0010596599999999998, weight_decay=0\n",
      "175: stages.0.blocks.1.mlp.fc2.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "176: stages.0.blocks.1.mlp.fc2.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "177: stages.0.blocks.1.mlp.fc1.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "178: stages.0.blocks.1.mlp.fc1.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "179: stages.0.blocks.1.norm1.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "180: stages.0.blocks.1.norm1.weight's lr=0.0010596599999999998, weight_decay=0\n",
      "181: stages.0.blocks.1.attn.proj.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "182: stages.0.blocks.1.attn.proj.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "183: stages.0.blocks.1.attn.qkv.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "184: stages.0.blocks.1.attn.qkv.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "185: stages.0.blocks.1.attn.crpb_mlp.3.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "186: stages.0.blocks.1.attn.crpb_mlp.0.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "187: stages.0.blocks.1.attn.crpb_mlp.0.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "188: stages.0.blocks.1.attn.t_scale's lr=0.0010596599999999998, weight_decay=0\n",
      "189: stages.0.blocks.0.norm2.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "190: stages.0.blocks.0.norm2.weight's lr=0.0010596599999999998, weight_decay=0\n",
      "191: stages.0.blocks.0.mlp.fc2.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "192: stages.0.blocks.0.mlp.fc2.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "193: stages.0.blocks.0.mlp.fc1.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "194: stages.0.blocks.0.mlp.fc1.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "195: stages.0.blocks.0.norm1.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "196: stages.0.blocks.0.norm1.weight's lr=0.0010596599999999998, weight_decay=0\n",
      "197: stages.0.blocks.0.attn.proj.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "198: stages.0.blocks.0.attn.proj.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "199: stages.0.blocks.0.attn.qkv.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "200: stages.0.blocks.0.attn.qkv.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "201: stages.0.blocks.0.attn.crpb_mlp.3.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "202: stages.0.blocks.0.attn.crpb_mlp.0.bias's lr=0.0010596599999999998, weight_decay=0\n",
      "203: stages.0.blocks.0.attn.crpb_mlp.0.weight's lr=0.0010596599999999998, weight_decay=0.01\n",
      "204: stages.0.blocks.0.attn.t_scale's lr=0.0010596599999999998, weight_decay=0\n",
      "205: embeddings.norm.bias's lr=0.0009219041999999998, weight_decay=0\n",
      "206: embeddings.norm.weight's lr=0.0009219041999999998, weight_decay=0\n",
      "207: embeddings.patch_embeddings.bias's lr=0.0009219041999999998, weight_decay=0\n",
      "208: embeddings.patch_embeddings.weight's lr=0.0009219041999999998, weight_decay=0.01\n",
      "209: absolute_pos_embed's lr=0.0008020566539999999, weight_decay=0\n"
     ]
    }
   ],
   "source": [
    "lr      = 1.4e-3      # paper : 1.4e-3\n",
    "lr_mult = 0.87  # paper : 0.87\n",
    "weight_decay = 0.01 # paper : 0.1\n",
    "\n",
    "param_groups = []\n",
    "prev_group_name = layer_names[0].split('.')[0]\n",
    "\n",
    "for idx, name in enumerate(layer_names):\n",
    "    \n",
    "    cur_group_name = name.split('.')[0]\n",
    "    \n",
    "    if cur_group_name != prev_group_name:\n",
    "        lr *= lr_mult\n",
    "    prev_group_name = cur_group_name\n",
    "    weight_decay = 0.01 if ('weight' in name) and ('norm' not in name) else 0\n",
    "    \n",
    "    print(f\"{idx}: {name}'s lr={lr}, weight_decay={weight_decay}\")\n",
    "    \n",
    "    param_groups += [{'params': [ p for n, p in model.named_parameters() if n == name and p.requires_grad],\n",
    "                      'lr' : lr,\n",
    "                      'weight_decay': weight_decay}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 모델 레이어의 이름 추출\n",
    "# layer_names = []\n",
    "# for i, (name, params) in enumerate(model.named_parameters()):\n",
    "#     lr = base_lr\n",
    "#     print(f'{i}: {name}')\n",
    "#     layer_names.append(name)\n",
    "\n",
    "# # 뒷 레이어부터 시작하도록 뒤집기    \n",
    "# layer_names.reverse()\n",
    "\n",
    "# # 하이퍼 파라미터 정의\n",
    "# lr      = 1.4e-3      # paper : 1.4e-3\n",
    "# lr_mult = 0.87  # paper : 0.87\n",
    "# weight_decay = 0.01 # paper : 0.1\n",
    "\n",
    "# param_groups = []\n",
    "# prev_group_name = layer_names[0].split('.')[0] # 그룹명 초기화\n",
    "\n",
    "# for idx, name in enumerate(layer_names):    \n",
    "#     cur_group_name = name.split('.')[0]    \n",
    "#     if cur_group_name != prev_group_name: # 동일한 그룹에 속하면 동일한 학습율\n",
    "#         lr *= lr_mult\n",
    "#     prev_group_name = cur_group_name    \n",
    "    \n",
    "#     param_groups += [{'params': [ p for n, p in model.named_parameters() if n == name and p.requires_grad],\n",
    "#                       'lr' : lr,\n",
    "#                       'weight_decay': weight_decay}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(param_groups)\n",
    "warmup_steps = int(len(train_loader)*(epochs)*0.1)\n",
    "train_steps = len(train_loader)*(epochs)\n",
    "scheduler = transformers.get_cosine_schedule_with_warmup(optimizer, \n",
    "                                                        num_warmup_steps=warmup_steps, \n",
    "                                                        num_training_steps=train_steps,\n",
    "                                                        num_cycles=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Train\n",
    "- 100에포크 먼저 학습하며 결과 확인하고, 이후 10에포크 학습하며 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 15/15 [00:53<00:00,  3.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 4.648767407735189, Val Loss: 4.551398277282715, Total Loss: 9.200165685017904, LR: 7.000000000000001e-05, Duration: 55.21 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 15/15 [00:51<00:00,  3.44s/it]\n"
     ]
    }
   ],
   "source": [
    "training_time = 0\n",
    "losses = []\n",
    "val_losses = []\n",
    "lrs = []\n",
    "best_loss = float('inf')\n",
    "\n",
    "# GradScaler 초기화\n",
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch + 1}\")\n",
    "    \n",
    "    for _, data in pbar:\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        inputs, labels = mixup_fn(inputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # AutoCast 적용\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "        # 스케일링된 그라디언트 계산\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # 그라디언트 클리핑 전에 스케일링 제거\n",
    "        scaler.unscale_(optimizer)\n",
    "        clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "\n",
    "        # 옵티마이저 스텝 및 스케일러 업데이트\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "            \n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        lrs.append(lr)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)        \n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in valid_loader:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    val_loss /= len(valid_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    total_loss = epoch_loss + val_loss\n",
    "    \n",
    "    # 모델 저장\n",
    "    if total_loss < best_loss:\n",
    "        best_loss = total_loss\n",
    "        vit_save = True\n",
    "        if vit_save:\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_duration = time.time() - start_time\n",
    "    training_time += epoch_duration\n",
    "    \n",
    "    text = f'\\tLoss: {epoch_loss}, Val Loss: {val_loss}, Total Loss: {total_loss}, LR: {lr}, Duration: {epoch_duration:.2f} sec'\n",
    "    \n",
    "    if vit_save:\n",
    "        text += f' - model saved!'\n",
    "        vit_save = False\n",
    "\n",
    "    print(text)\n",
    "        \n",
    "text = f\"Epoch 당 평균 소요시간 : {training_time / 50:.2f}초\"      \n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.730000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.761444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Recall</td>\n",
       "      <td>0.730000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F1 Score</td>\n",
       "      <td>0.720376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Metric     Value\n",
       "0   Accuracy  0.730000\n",
       "1  Precision  0.761444\n",
       "2     Recall  0.730000\n",
       "3   F1 Score  0.720376"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예측 수행 및 레이블 저장\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# 혼동 행렬 생성\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# 예측과 실제 레이블\n",
    "y_true = all_labels  # 실제 레이블\n",
    "y_pred = all_preds  # 모델에 의해 예측된 레이블\n",
    "\n",
    "# 전체 데이터셋에 대한 정확도\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# 평균 정밀도, 리콜, F1-Score ('weighted')\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "\n",
    "# 판다스 데이터프레임으로 결과 정리\n",
    "performance_metrics = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "    'Value': [accuracy, precision, recall, f1_score]\n",
    "})\n",
    "\n",
    "# 데이터프레임 출력\n",
    "performance_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 100 Epoch Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 15/15 [00:50<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.656868060429891, Val Loss: 1.0866397619247437, Total Loss: 3.7435078223546348, LR: 0.0013000171104914787, Duration: 52.05 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 15/15 [00:51<00:00,  3.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.7411521037419635, Val Loss: 1.1920576095581055, Total Loss: 3.933209713300069, LR: 0.001293633667309498, Duration: 52.05 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 15/15 [00:50<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6991294225056968, Val Loss: 1.190779685974121, Total Loss: 3.889909108479818, LR: 0.001287069397561797, Duration: 51.69 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 15/15 [00:50<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.269495209058126, Val Loss: 1.1089175939559937, Total Loss: 3.3784128030141196, LR: 0.001280326300788529, Duration: 51.91 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 15/15 [00:50<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.632977509498596, Val Loss: 1.0065799951553345, Total Loss: 3.6395575046539306, LR: 0.0012734064310022943, Duration: 51.72 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 15/15 [00:50<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4295517921447756, Val Loss: 0.9872391819953918, Total Loss: 3.4167909741401674, LR: 0.0012663118960624632, Duration: 51.63 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 15/15 [00:50<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.657050069173177, Val Loss: 1.0636014938354492, Total Loss: 3.720651563008626, LR: 0.001259044857033105, Duration: 51.78 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 15/15 [00:50<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.793322213490804, Val Loss: 1.0805952548980713, Total Loss: 3.8739174683888753, LR: 0.0012516075275247052, Duration: 51.65 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 15/15 [00:50<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4584624767303467, Val Loss: 0.9559261202812195, Total Loss: 3.414388597011566, LR: 0.0012440021730198796, Duration: 51.61 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 15/15 [00:51<00:00,  3.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 3.0136234680811564, Val Loss: 1.0421936511993408, Total Loss: 4.055817119280498, LR: 0.0012362311101832846, Duration: 52.00 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 15/15 [00:50<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4933032274246214, Val Loss: 0.9437807202339172, Total Loss: 3.4370839476585386, LR: 0.0012282967061559404, Duration: 51.65 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 15/15 [00:50<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4297483364741006, Val Loss: 1.0428630113601685, Total Loss: 3.472611347834269, LR: 0.001220201377834176, Duration: 51.58 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 15/15 [00:51<00:00,  3.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4320934375127155, Val Loss: 0.9670103192329407, Total Loss: 3.399103756745656, LR: 0.0012119475911334192, Duration: 52.07 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 15/15 [00:50<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5664584159851076, Val Loss: 0.9769777059555054, Total Loss: 3.543436121940613, LR: 0.0012035378602370558, Duration: 51.67 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 15/15 [00:50<00:00,  3.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6650139888127646, Val Loss: 0.9653957486152649, Total Loss: 3.6304097374280295, LR: 0.0011949747468305832, Duration: 51.50 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 15/15 [00:50<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.5750688632329304, Val Loss: 0.995498538017273, Total Loss: 3.5705674012502033, LR: 0.0011862608593212981, Duration: 51.75 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 15/15 [00:51<00:00,  3.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.7154326597849527, Val Loss: 0.9883191585540771, Total Loss: 3.70375181833903, LR: 0.001177398852043749, Duration: 52.04 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 15/15 [00:51<00:00,  3.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2634711186091105, Val Loss: 0.904066801071167, Total Loss: 3.1675379196802775, LR: 0.0011683914244512007, Duration: 52.37 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 15/15 [00:50<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.256103507677714, Val Loss: 0.8322899341583252, Total Loss: 3.0883934418360393, LR: 0.001159241320293355, Duration: 52.02 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 15/15 [00:50<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2764090061187745, Val Loss: 0.8936916589736938, Total Loss: 3.1701006650924684, LR: 0.0011499513267805774, Duration: 51.83 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 15/15 [00:50<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.439480749766032, Val Loss: 0.8417661190032959, Total Loss: 3.281246868769328, LR: 0.0011405242737348863, Duration: 51.84 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 15/15 [00:50<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.376136644681295, Val Loss: 0.8899009823799133, Total Loss: 3.266037627061208, LR: 0.0011309630327279608, Duration: 51.64 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 15/15 [00:50<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1310139020284016, Val Loss: 0.7905547022819519, Total Loss: 2.9215686043103535, LR: 0.0011212705162064339, Duration: 51.89 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 15/15 [00:50<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1650134166081747, Val Loss: 0.8339634537696838, Total Loss: 2.9989768703778585, LR: 0.0011114496766047313, Duration: 51.78 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 15/15 [00:51<00:00,  3.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.33322536945343, Val Loss: 0.9288654923439026, Total Loss: 3.2620908617973328, LR: 0.0011015035054457321, Duration: 51.91 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 15/15 [00:50<00:00,  3.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1799781719843545, Val Loss: 0.861397385597229, Total Loss: 3.0413755575815835, LR: 0.0010914350324295228, Duration: 51.87 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 15/15 [00:50<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.6036650419235228, Val Loss: 0.94874107837677, Total Loss: 3.552406120300293, LR: 0.001081247324510519, Duration: 51.68 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 15/15 [00:50<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4560523907343548, Val Loss: 0.8359626531600952, Total Loss: 3.29201504389445, LR: 0.0010709434849632434, Duration: 51.67 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 15/15 [00:50<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.9994967619578043, Val Loss: 0.7379708886146545, Total Loss: 2.7374676505724587, LR: 0.001060526652437038, Duration: 52.04 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 15/15 [00:50<00:00,  3.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3076162656148274, Val Loss: 0.811880350112915, Total Loss: 3.1194966157277424, LR: 0.00105, Duration: 51.86 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|██████████| 15/15 [00:50<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3585557381312054, Val Loss: 0.8148956298828125, Total Loss: 3.173451368014018, LR: 0.001039366734172436, Duration: 51.77 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|██████████| 15/15 [00:50<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.8906003713607789, Val Loss: 0.7121273279190063, Total Loss: 2.602727699279785, LR: 0.0010286300939501235, Duration: 51.96 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|██████████| 15/15 [00:50<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.45534127553304, Val Loss: 0.784798800945282, Total Loss: 3.240140076478322, LR: 0.0010177933498176828, Duration: 51.65 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|██████████| 15/15 [00:50<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.142392118771871, Val Loss: 0.744280219078064, Total Loss: 2.886672337849935, LR: 0.001006859802752354, Duration: 51.76 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|██████████| 15/15 [00:50<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0640012900034588, Val Loss: 0.7880378365516663, Total Loss: 2.852039126555125, LR: 0.0009958327832184897, Duration: 51.77 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|██████████| 15/15 [00:50<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1263386329015095, Val Loss: 0.7130172252655029, Total Loss: 2.8393558581670124, LR: 0.0009847156501530602, Duration: 51.69 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37: 100%|██████████| 15/15 [00:50<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.9871632655461628, Val Loss: 0.7495275735855103, Total Loss: 2.736690839131673, LR: 0.0009735117899424916, Duration: 51.67 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|██████████| 15/15 [00:50<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.319912830988566, Val Loss: 0.7525551319122314, Total Loss: 3.0724679629007974, LR: 0.0009622246153911386, Duration: 51.80 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|██████████| 15/15 [00:50<00:00,  3.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.8786864121754965, Val Loss: 0.7501755356788635, Total Loss: 2.62886194785436, LR: 0.0009508575646817101, Duration: 51.87 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40: 100%|██████████| 15/15 [00:51<00:00,  3.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.2865551551183065, Val Loss: 0.7636071443557739, Total Loss: 3.0501622994740805, LR: 0.0009394141003279682, Duration: 52.01 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41: 100%|██████████| 15/15 [00:50<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3690157016118367, Val Loss: 0.7316689491271973, Total Loss: 3.100684650739034, LR: 0.0009278977081200097, Duration: 51.68 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42: 100%|██████████| 15/15 [00:50<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.115553347269694, Val Loss: 0.7264184951782227, Total Loss: 2.841971842447917, LR: 0.0009163118960624632, Duration: 51.60 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43: 100%|██████████| 15/15 [00:50<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.7769123713175456, Val Loss: 0.6899837255477905, Total Loss: 2.4668960968653364, LR: 0.0009046601933059157, Duration: 52.02 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44: 100%|██████████| 15/15 [00:50<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.16112121740977, Val Loss: 0.6956101655960083, Total Loss: 2.856731383005778, LR: 0.0008929461490718994, Duration: 51.79 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45: 100%|██████████| 15/15 [00:51<00:00,  3.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3510693470637003, Val Loss: 0.7207252979278564, Total Loss: 3.0717946449915567, LR: 0.0008811733315717645, Duration: 51.92 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46: 100%|██████████| 15/15 [00:50<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1274606625239056, Val Loss: 0.7758668065071106, Total Loss: 2.9033274690310162, LR: 0.0008693453269197673, Duration: 51.56 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47: 100%|██████████| 15/15 [00:51<00:00,  3.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0673088312149046, Val Loss: 0.7239354252815247, Total Loss: 2.7912442564964293, LR: 0.0008574657380407056, Duration: 51.97 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48: 100%|██████████| 15/15 [00:50<00:00,  3.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.9367709716161092, Val Loss: 0.6858772039413452, Total Loss: 2.6226481755574547, LR: 0.0008455381835724314, Duration: 51.83 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 15/15 [00:50<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.8003018697102864, Val Loss: 0.664739727973938, Total Loss: 2.465041597684224, LR: 0.0008335662967635814, Duration: 51.82 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50: 100%|██████████| 15/15 [00:50<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.802888774871826, Val Loss: 0.7137707471847534, Total Loss: 2.5166595220565795, LR: 0.0008215537243668514, Duration: 51.75 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.838000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.862524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Recall</td>\n",
       "      <td>0.838000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F1 Score</td>\n",
       "      <td>0.829496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Metric     Value\n",
       "0   Accuracy  0.838000\n",
       "1  Precision  0.862524\n",
       "2     Recall  0.838000\n",
       "3   F1 Score  0.829496"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch + 1}\")\n",
    "    \n",
    "    for _, data in pbar:\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        inputs, labels = mixup_fn(inputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # AutoCast 적용\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "        # 스케일링된 그라디언트 계산\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # 그라디언트 클리핑 전에 스케일링 제거\n",
    "        scaler.unscale_(optimizer)\n",
    "        clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "\n",
    "        # 옵티마이저 스텝 및 스케일러 업데이트\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "            \n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        lrs.append(lr)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)        \n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in valid_loader:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    val_loss /= len(valid_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    total_loss = epoch_loss + val_loss\n",
    "    \n",
    "    # 모델 저장\n",
    "    if total_loss < best_loss:\n",
    "        best_loss = total_loss\n",
    "        vit_save = True\n",
    "        if vit_save:\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_duration = time.time() - start_time\n",
    "    training_time += epoch_duration\n",
    "    \n",
    "    text = f'\\tLoss: {epoch_loss}, Val Loss: {val_loss}, Total Loss: {total_loss}, LR: {lr}, Duration: {epoch_duration:.2f} sec'\n",
    "    \n",
    "    if vit_save:\n",
    "        text += f' - model saved!'\n",
    "        vit_save = False\n",
    "\n",
    "    print(text)\n",
    "\n",
    "# 예측 수행 및 레이블 저장\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# 혼동 행렬 생성\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# 예측과 실제 레이블\n",
    "y_true = all_labels  # 실제 레이블\n",
    "y_pred = all_preds  # 모델에 의해 예측된 레이블\n",
    "\n",
    "# 전체 데이터셋에 대한 정확도\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# 평균 정밀도, 리콜, F1-Score ('weighted')\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "\n",
    "# 판다스 데이터프레임으로 결과 정리\n",
    "performance_metrics = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "    'Value': [accuracy, precision, recall, f1_score]\n",
    "})\n",
    "\n",
    "# 데이터프레임 출력\n",
    "performance_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 15/15 [00:50<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0989944458007814, Val Loss: 0.7974388599395752, Total Loss: 2.8964333057403566, LR: 0.0008095041255281617, Duration: 51.54 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 15/15 [00:50<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1859948714574178, Val Loss: 0.7359203696250916, Total Loss: 2.9219152410825093, LR: 0.0007974211706720458, Duration: 51.77 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 15/15 [00:50<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0909013986587524, Val Loss: 0.7298261523246765, Total Loss: 2.820727550983429, LR: 0.0007853085403836032, Duration: 51.68 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 15/15 [00:50<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.980771509806315, Val Loss: 0.720022439956665, Total Loss: 2.7007939497629803, LR: 0.0007731699242873575, Duration: 51.68 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 15/15 [00:50<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.8991553703943889, Val Loss: 0.7231766581535339, Total Loss: 2.622332028547923, LR: 0.0007610090199233608, Duration: 51.74 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 15/15 [00:51<00:00,  3.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0280269145965577, Val Loss: 0.6833068132400513, Total Loss: 2.711333727836609, LR: 0.0007488295316208876, Duration: 51.93 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 15/15 [00:50<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.7484649022420247, Val Loss: 0.7237380146980286, Total Loss: 2.472202916940053, LR: 0.0007366351693700608, Duration: 51.66 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 15/15 [00:50<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.7490078926086425, Val Loss: 0.6863779425621033, Total Loss: 2.435385835170746, LR: 0.0007244296476917508, Duration: 51.98 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 15/15 [00:50<00:00,  3.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.4133824904759726, Val Loss: 0.7695233225822449, Total Loss: 3.1829058130582175, LR: 0.0007122166845060985, Duration: 51.90 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 15/15 [00:50<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.989539376894633, Val Loss: 0.7407202124595642, Total Loss: 2.730259589354197, LR: 0.0007, Duration: 51.72 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 15/15 [00:51<00:00,  3.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.04110627969106, Val Loss: 0.7122923731803894, Total Loss: 2.7533986528714496, LR: 0.0006877833154939015, Duration: 52.19 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 15/15 [00:50<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.9627915302912393, Val Loss: 0.7036316394805908, Total Loss: 2.6664231697718304, LR: 0.0006755703523082495, Duration: 51.79 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 15/15 [00:50<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.215097181002299, Val Loss: 0.7650821805000305, Total Loss: 2.9801793615023295, LR: 0.0006633648306299393, Duration: 51.62 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 15/15 [00:50<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.732415795326233, Val Loss: 0.6545288562774658, Total Loss: 2.3869446516036987, LR: 0.0006511704683791123, Duration: 51.93 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 15/15 [00:51<00:00,  3.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1403245528539023, Val Loss: 0.7416877150535583, Total Loss: 2.8820122679074607, LR: 0.0006389909800766392, Duration: 51.93 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 15/15 [00:50<00:00,  3.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.3462406635284423, Val Loss: 0.6923199892044067, Total Loss: 3.038560652732849, LR: 0.0006268300757126426, Duration: 51.52 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 15/15 [00:50<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.9328386584917705, Val Loss: 0.7024814486503601, Total Loss: 2.635320107142131, LR: 0.0006146914596163969, Duration: 51.69 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 15/15 [00:50<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.8812280178070069, Val Loss: 0.7228487730026245, Total Loss: 2.604076790809631, LR: 0.0006025788293279544, Duration: 51.58 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 15/15 [00:50<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.9970248063405356, Val Loss: 0.7064083814620972, Total Loss: 2.7034331878026325, LR: 0.0005904958744718383, Duration: 51.78 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 15/15 [00:50<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.5427856683731078, Val Loss: 0.6887787580490112, Total Loss: 2.231564426422119, LR: 0.0005784462756331488, Duration: 51.86 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 15/15 [00:50<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.8379464387893676, Val Loss: 0.8101152181625366, Total Loss: 2.648061656951904, LR: 0.0005664337032364186, Duration: 51.80 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 15/15 [00:50<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.21699279944102, Val Loss: 0.7475665211677551, Total Loss: 2.964559320608775, LR: 0.0005544618164275686, Duration: 51.82 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 15/15 [00:50<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.005228034655253, Val Loss: 0.6987837553024292, Total Loss: 2.7040117899576823, LR: 0.0005425342619592945, Duration: 51.70 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 15/15 [00:50<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0032418886820476, Val Loss: 0.7424818873405457, Total Loss: 2.745723776022593, LR: 0.0005306546730802327, Duration: 51.70 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 15/15 [00:50<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.8295075654983521, Val Loss: 0.6974217295646667, Total Loss: 2.526929295063019, LR: 0.0005188266684282354, Duration: 51.62 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 15/15 [00:50<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.776107676823934, Val Loss: 0.7711319923400879, Total Loss: 2.547239669164022, LR: 0.0005070538509281006, Duration: 51.64 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 15/15 [00:50<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.6546895662943522, Val Loss: 0.74345862865448, Total Loss: 2.398148194948832, LR: 0.0004953398066940844, Duration: 51.84 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 15/15 [00:50<00:00,  3.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0837464809417723, Val Loss: 0.7239661812782288, Total Loss: 2.807712662220001, LR: 0.0004836881039375369, Duration: 51.46 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 15/15 [00:50<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.1784119606018066, Val Loss: 0.6969752907752991, Total Loss: 2.8753872513771057, LR: 0.00047210229187999046, Duration: 51.80 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 15/15 [00:50<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.9118168989817301, Val Loss: 0.7019824385643005, Total Loss: 2.6137993375460304, LR: 0.0004605858996720319, Duration: 51.73 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|██████████| 15/15 [00:50<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.0050169944763185, Val Loss: 0.7253676056861877, Total Loss: 2.7303846001625063, LR: 0.0004491424353182898, Duration: 51.71 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|██████████| 15/15 [00:50<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.8579985698064168, Val Loss: 0.7260017991065979, Total Loss: 2.5840003689130144, LR: 0.0004377753846088615, Duration: 51.76 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|██████████| 15/15 [00:50<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.6451035817464192, Val Loss: 0.6968458294868469, Total Loss: 2.341949411233266, LR: 0.0004264882100575085, Duration: 51.80 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|██████████| 15/15 [00:50<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.5046961069107057, Val Loss: 0.7231835126876831, Total Loss: 2.2278796195983888, LR: 0.00041528434984693997, Duration: 51.93 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|██████████| 15/15 [00:50<00:00,  3.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.7769936362902323, Val Loss: 0.70062255859375, Total Loss: 2.477616194883982, LR: 0.00040416721678151053, Duration: 51.89 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|██████████| 15/15 [00:50<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.915289870897929, Val Loss: 0.7166441082954407, Total Loss: 2.6319339791933696, LR: 0.00039314019724764573, Duration: 51.78 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37: 100%|██████████| 15/15 [00:50<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.111906611919403, Val Loss: 0.7452526092529297, Total Loss: 2.8571592211723327, LR: 0.0003822066501823173, Duration: 51.54 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|██████████| 15/15 [00:50<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.8990045229593913, Val Loss: 0.7047130465507507, Total Loss: 2.6037175695101418, LR: 0.00037136990604987665, Duration: 51.67 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|██████████| 15/15 [00:50<00:00,  3.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.7554991563161215, Val Loss: 0.7184057235717773, Total Loss: 2.4739048798878986, LR: 0.0003606332658275641, Duration: 51.53 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40: 100%|██████████| 15/15 [00:50<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.9059346437454223, Val Loss: 0.703572154045105, Total Loss: 2.6095067977905275, LR: 0.00035000000000000016, Duration: 51.83 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41: 100%|██████████| 15/15 [00:50<00:00,  3.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.6268874049186706, Val Loss: 0.7178221940994263, Total Loss: 2.344709599018097, LR: 0.000339473347562962, Duration: 51.55 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42: 100%|██████████| 15/15 [00:50<00:00,  3.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.893630361557007, Val Loss: 0.7163046598434448, Total Loss: 2.6099350214004517, LR: 0.00032905651503675667, Duration: 51.51 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43: 100%|██████████| 15/15 [00:50<00:00,  3.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.9072073419888815, Val Loss: 0.7510281801223755, Total Loss: 2.658235522111257, LR: 0.00031875267548948103, Duration: 51.39 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44: 100%|██████████| 15/15 [00:50<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.6641966144243876, Val Loss: 0.7268722653388977, Total Loss: 2.391068879763285, LR: 0.0003085649675704773, Duration: 51.63 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45: 100%|██████████| 15/15 [00:50<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.7722946961720785, Val Loss: 0.7361909747123718, Total Loss: 2.5084856708844505, LR: 0.0002984964945542679, Duration: 51.63 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46: 100%|██████████| 15/15 [00:50<00:00,  3.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.9539666215578715, Val Loss: 0.6926761269569397, Total Loss: 2.646642748514811, LR: 0.0002885503233952689, Duration: 51.89 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47: 100%|██████████| 15/15 [00:50<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.8854046662648518, Val Loss: 0.7984238862991333, Total Loss: 2.683828552563985, LR: 0.00027872948379356616, Duration: 51.78 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48: 100%|██████████| 15/15 [00:50<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.3878305832544962, Val Loss: 0.7342281341552734, Total Loss: 2.12205871740977, LR: 0.0002690369672720392, Duration: 51.96 sec - model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 15/15 [00:50<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.8025549252827961, Val Loss: 0.7834681272506714, Total Loss: 2.5860230525334673, LR: 0.0002594757262651139, Duration: 51.59 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50: 100%|██████████| 15/15 [00:50<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.3593415141105651, Val Loss: 0.7738597393035889, Total Loss: 2.1332012534141542, LR: 0.00025004867321942243, Duration: 51.58 sec\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.860000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.880127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Recall</td>\n",
       "      <td>0.860000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F1 Score</td>\n",
       "      <td>0.852054</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Metric     Value\n",
       "0   Accuracy  0.860000\n",
       "1  Precision  0.880127\n",
       "2     Recall  0.860000\n",
       "3   F1 Score  0.852054"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch + 1}\")\n",
    "    \n",
    "    for _, data in pbar:\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        inputs, labels = mixup_fn(inputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # AutoCast 적용\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "        # 스케일링된 그라디언트 계산\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # 그라디언트 클리핑 전에 스케일링 제거\n",
    "        scaler.unscale_(optimizer)\n",
    "        clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "\n",
    "        # 옵티마이저 스텝 및 스케일러 업데이트\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "            \n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        lrs.append(lr)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)        \n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in valid_loader:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    val_loss /= len(valid_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    total_loss = epoch_loss + val_loss\n",
    "    \n",
    "    # 모델 저장\n",
    "    if total_loss < best_loss:\n",
    "        best_loss = total_loss\n",
    "        vit_save = True\n",
    "        if vit_save:\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_duration = time.time() - start_time\n",
    "    training_time += epoch_duration\n",
    "    \n",
    "    text = f'\\tLoss: {epoch_loss}, Val Loss: {val_loss}, Total Loss: {total_loss}, LR: {lr}, Duration: {epoch_duration:.2f} sec'\n",
    "    \n",
    "    if vit_save:\n",
    "        text += f' - model saved!'\n",
    "        vit_save = False\n",
    "\n",
    "    print(text)\n",
    "\n",
    "# 예측 수행 및 레이블 저장\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# 혼동 행렬 생성\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# 예측과 실제 레이블\n",
    "y_true = all_labels  # 실제 레이블\n",
    "y_pred = all_preds  # 모델에 의해 예측된 레이블\n",
    "\n",
    "# 전체 데이터셋에 대한 정확도\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# 평균 정밀도, 리콜, F1-Score ('weighted')\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "\n",
    "# 판다스 데이터프레임으로 결과 정리\n",
    "performance_metrics = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "    'Value': [accuracy, precision, recall, f1_score]\n",
    "})\n",
    "\n",
    "# 데이터프레임 출력\n",
    "performance_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 15/15 [00:50<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.8682246923446655, Val Loss: 0.7779513597488403, Total Loss: 2.646176052093506, LR: 0.000240758679706645, Duration: 51.68 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 15/15 [00:50<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.9862423857053122, Val Loss: 0.7408369183540344, Total Loss: 2.7270793040593464, LR: 0.00023160857554879947, Duration: 51.68 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 15/15 [00:50<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.7568724592526754, Val Loss: 0.765451192855835, Total Loss: 2.5223236521085104, LR: 0.00022260114795625115, Duration: 51.58 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 15/15 [00:50<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.6125226020812988, Val Loss: 0.7139967083930969, Total Loss: 2.3265193104743958, LR: 0.00021373914067870185, Duration: 51.80 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 15/15 [00:50<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.512954306602478, Val Loss: 0.7604970932006836, Total Loss: 2.273451399803162, LR: 0.00020502525316941678, Duration: 51.60 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 15/15 [00:50<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.6402256488800049, Val Loss: 0.7476833462715149, Total Loss: 2.3879089951515198, LR: 0.00019646213976294433, Duration: 51.63 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 15/15 [00:50<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.6622195839881897, Val Loss: 0.7394412159919739, Total Loss: 2.4016607999801636, LR: 0.00018805240886658067, Duration: 51.79 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 15/15 [00:50<00:00,  3.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.501959216594696, Val Loss: 0.7771209478378296, Total Loss: 2.279080164432526, LR: 0.00017979862216582396, Duration: 51.42 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 15/15 [00:50<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.9299867312113443, Val Loss: 0.703926682472229, Total Loss: 2.6339134136835733, LR: 0.0001717032938440596, Duration: 51.64 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 15/15 [00:50<00:00,  3.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 2.096792455514272, Val Loss: 0.8021771907806396, Total Loss: 2.8989696462949115, LR: 0.00016376888981671546, Duration: 51.53 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 15/15 [00:50<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.7682509620984395, Val Loss: 0.7559312582015991, Total Loss: 2.5241822203000384, LR: 0.00015599782698012037, Duration: 51.58 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 15/15 [00:50<00:00,  3.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.64539532661438, Val Loss: 0.7952052354812622, Total Loss: 2.440600562095642, LR: 0.00014839247247529466, Duration: 51.46 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 15/15 [00:50<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.8098185380299887, Val Loss: 0.7582208514213562, Total Loss: 2.5680393894513447, LR: 0.00014095514296689517, Duration: 51.83 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 15/15 [00:50<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.7548200289408367, Val Loss: 0.7385174036026001, Total Loss: 2.493337432543437, LR: 0.00013368810393753685, Duration: 51.62 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 15/15 [00:50<00:00,  3.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.7015121698379516, Val Loss: 0.7559743523597717, Total Loss: 2.4574865221977236, LR: 0.00012659356899770565, Duration: 51.43 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 15/15 [00:50<00:00,  3.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.7704113443692526, Val Loss: 0.7706521153450012, Total Loss: 2.5410634597142536, LR: 0.00011967369921147086, Duration: 51.53 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 15/15 [00:50<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.9385425845781963, Val Loss: 0.7434729337692261, Total Loss: 2.6820155183474226, LR: 0.00011293060243820324, Duration: 51.69 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 15/15 [00:50<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 1.601182222366333, Val Loss: 0.7555084824562073, Total Loss: 2.3566907048225403, LR: 0.00010636633269050183, Duration: 51.57 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19:   0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch + 1}\")\n",
    "    \n",
    "    for _, data in pbar:\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        inputs, labels = mixup_fn(inputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # AutoCast 적용\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "        # 스케일링된 그라디언트 계산\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # 그라디언트 클리핑 전에 스케일링 제거\n",
    "        scaler.unscale_(optimizer)\n",
    "        clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "\n",
    "        # 옵티마이저 스텝 및 스케일러 업데이트\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "            \n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        lrs.append(lr)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)        \n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in valid_loader:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    val_loss /= len(valid_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    total_loss = epoch_loss + val_loss\n",
    "    \n",
    "    # 모델 저장\n",
    "    if total_loss < best_loss:\n",
    "        best_loss = total_loss\n",
    "        vit_save = True\n",
    "        if vit_save:\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_duration = time.time() - start_time\n",
    "    training_time += epoch_duration\n",
    "    \n",
    "    text = f'\\tLoss: {epoch_loss}, Val Loss: {val_loss}, Total Loss: {total_loss}, LR: {lr}, Duration: {epoch_duration:.2f} sec'\n",
    "    \n",
    "    if vit_save:\n",
    "        text += f' - model saved!'\n",
    "        vit_save = False\n",
    "\n",
    "    print(text)\n",
    "\n",
    "# 예측 수행 및 레이블 저장\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# 혼동 행렬 생성\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# 예측과 실제 레이블\n",
    "y_true = all_labels  # 실제 레이블\n",
    "y_pred = all_preds  # 모델에 의해 예측된 레이블\n",
    "\n",
    "# 전체 데이터셋에 대한 정확도\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# 평균 정밀도, 리콜, F1-Score ('weighted')\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "\n",
    "# 판다스 데이터프레임으로 결과 정리\n",
    "performance_metrics = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "    'Value': [accuracy, precision, recall, f1_score]\n",
    "})\n",
    "\n",
    "# 데이터프레임 출력\n",
    "performance_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
