{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94bebad5",
   "metadata": {},
   "source": [
    "```\n",
    "GQA 유무에 따른 성능 비교\n",
    "with MoE\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20ec8bf0",
   "metadata": {
    "id": "20ec8bf0"
   },
   "outputs": [],
   "source": [
    "## Importing Libraries\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3390f4",
   "metadata": {
    "id": "5a3390f4"
   },
   "source": [
    "## Hyperparameters and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0c61f36",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1732703737281,
     "user": {
      "displayName": "Hugman Sangkeun Jung",
      "userId": "08689291704194029524"
     },
     "user_tz": -540
    },
    "id": "d0c61f36",
    "outputId": "45d9894c-a14d-4164-9f2d-699025751e72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Modified hyperparameters\n",
    "SEQUENCE_LENGTH = 64\n",
    "\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = EMBEDDING_DIM*2\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 1e-4\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c9b962",
   "metadata": {
    "id": "89c9b962"
   },
   "source": [
    "## Data Preparation\n",
    "\n",
    "We are using the TinyShakespeare dataset, a small character-level text corpus consisting of a subset of Shakespeare's plays. It's often used for testing sequence models, as it includes a rich set of vocabulary and provides a challenging task for next-character prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "089830e2",
   "metadata": {
    "id": "089830e2"
   },
   "outputs": [],
   "source": [
    "## Utility Functions\n",
    "\n",
    "def load_data(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "def create_char_mappings(text):\n",
    "    chars = sorted(list(set(text)))\n",
    "    char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "    idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "    return chars, char_to_idx, idx_to_char"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fc6452",
   "metadata": {
    "id": "30fc6452"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f07670d9",
   "metadata": {
    "id": "f07670d9"
   },
   "outputs": [],
   "source": [
    "class CharDataset(Dataset):\n",
    "    def __init__(self, text, seq_length, char_to_idx):\n",
    "        self.text = text\n",
    "        self.seq_length = seq_length\n",
    "        self.char_to_idx = char_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text) - self.seq_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = [self.char_to_idx[ch] for ch in self.text[idx:idx+self.seq_length]]\n",
    "        y = [self.char_to_idx[ch] for ch in self.text[idx+1:idx+self.seq_length+1]]\n",
    "        return torch.tensor(x), torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9172da13",
   "metadata": {
    "id": "9172da13"
   },
   "outputs": [],
   "source": [
    "def prepare_data(text, seq_length, batch_size, val_split):\n",
    "    chars, char_to_idx, idx_to_char = create_char_mappings(text)\n",
    "\n",
    "    # Split data into train and validation\n",
    "    val_size = int(len(text) * val_split)\n",
    "    train_text, val_text = text[:-val_size], text[-val_size:]\n",
    "\n",
    "    train_dataset = CharDataset(train_text, seq_length, char_to_idx)\n",
    "    val_dataset = CharDataset(val_text, seq_length, char_to_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=12, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=12, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader, chars, char_to_idx, idx_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "nEwKFB_8L6AG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3706,
     "status": "ok",
     "timestamp": 1732704246464,
     "user": {
      "displayName": "Hugman Sangkeun Jung",
      "userId": "08689291704194029524"
     },
     "user_tz": -540
    },
    "id": "nEwKFB_8L6AG",
    "outputId": "e75cdff9-3775-461e-d930-0ced534bf74d"
   },
   "outputs": [],
   "source": [
    "# !wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=19zosLuU0z4MxIMKbGVYEGlg52QyfbTIy' -O input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47d03398",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 668,
     "status": "ok",
     "timestamp": 1732704255324,
     "user": {
      "displayName": "Hugman Sangkeun Jung",
      "userId": "08689291704194029524"
     },
     "user_tz": -540
    },
    "id": "47d03398",
    "outputId": "f247f5b3-88ff-4ecb-e8a3-4b2e42a2820b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 1115394\n",
      "Vocabulary size: 65\n",
      "Train dataset size: 1003791\n",
      "Validation dataset size: 111475\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "text = load_data('./input.txt')\n",
    "train_loader, val_loader, chars, char_to_idx, idx_to_char = prepare_data(text, SEQUENCE_LENGTH, BATCH_SIZE, VALIDATION_SPLIT)\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(f\"Total characters: {len(text)}\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Train dataset size: {len(train_loader.dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9442b104",
   "metadata": {
    "id": "9442b104"
   },
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1f0a6a1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 916,
     "status": "ok",
     "timestamp": 1732704257555,
     "user": {
      "displayName": "Hugman Sangkeun Jung",
      "userId": "08689291704194029524"
     },
     "user_tz": -540
    },
    "id": "f1f0a6a1",
    "outputId": "9ef6eb5a-0015-40ad-f143-236e3d38fb7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([512, 64])\n",
      "Target shape: torch.Size([512, 64])\n",
      "Sample 1: ------------------------------\n",
      "Input sequence : oveThat I have borne your father?FLORIZEL:Very noblyHave yo\n",
      "Target sequence: veThat I have borne your father?FLORIZEL:Very noblyHave you\n",
      "\n",
      "Sample 2: ------------------------------\n",
      "Input sequence : patricians shall attend and shrug,I' the end admire, where ladi\n",
      "Target sequence: atricians shall attend and shrug,I' the end admire, where ladie\n",
      "\n",
      "Sample 3: ------------------------------\n",
      "Input sequence : uly, sir, I am a poor fellow that would live.ESCALUS:How woul\n",
      "Target sequence: ly, sir, I am a poor fellow that would live.ESCALUS:How would\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to convert index sequence to character sequence\n",
    "def indices_to_text(indices, idx_to_char):\n",
    "    return ''.join([idx_to_char[idx.item()] for idx in indices])\n",
    "\n",
    "# Get a batch of data\n",
    "dataiter = iter(train_loader)\n",
    "batch_x, batch_y = next(dataiter)\n",
    "\n",
    "print(f\"Input shape: {batch_x.shape}\")\n",
    "print(f\"Target shape: {batch_y.shape}\")\n",
    "\n",
    "# Print a few samples from the batch\n",
    "num_samples = 3\n",
    "for i in range(num_samples):\n",
    "    print(f\"Sample {i+1}: ------------------------------\" )\n",
    "    print(\"Input sequence :\", indices_to_text(batch_x[i], idx_to_char).replace('\\n',''))\n",
    "    print(\"Target sequence:\", indices_to_text(batch_y[i], idx_to_char).replace('\\n',''))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5486f73f",
   "metadata": {
    "id": "5486f73f"
   },
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61a52f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_vram_usage(device=\"cuda\"):\n",
    "    allocated = torch.cuda.memory_allocated(device) / (1024**2)  # in MB\n",
    "    reserved = torch.cuda.memory_reserved(device) / (1024**2)    # in MB\n",
    "    max_allocated = torch.cuda.max_memory_allocated(device) / (1024**2)  # in MB\n",
    "    print(f\"Allocated: {allocated:.2f} MB, Reserved: {reserved:.2f} MB, Max Allocated: {max_allocated:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59178a11",
   "metadata": {
    "id": "59178a11"
   },
   "outputs": [],
   "source": [
    "def validate(model, dataloader, criterion, device, epoch, step):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            output, _ = model(x)\n",
    "            loss = criterion(output.view(-1, output.size(-1)), y.view(-1))\n",
    "            losses.append((step, epoch, loss.item()))\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea5e61f5",
   "metadata": {
    "id": "ea5e61f5"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, dataloader, criterion, optimizer, device, epoch, step, moe_loss_weight=0.01):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    vram_usage = []\n",
    "    scaler = torch.amp.GradScaler('cuda')\n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch}\", leave=False)\n",
    "    for batch, (x, y) in enumerate(pbar):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.amp.autocast('cuda'):\n",
    "            output, moe_loss = model(x)\n",
    "            loss = criterion(output.view(-1, output.size(-1)), y.view(-1)) + moe_loss_weight * moe_loss\n",
    "            \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        step += 1\n",
    "        losses.append((step, epoch, loss.item()))\n",
    "        \n",
    "        allocated = torch.cuda.memory_allocated(device) / (1024**2)\n",
    "        vram_usage.append(allocated)\n",
    "        pbar.set_postfix(loss=f'{loss.item():.4f}', step=step, vram=f'{allocated:.2f} MB')\n",
    "    return losses, step, vram_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbedfd61",
   "metadata": {
    "id": "cbedfd61"
   },
   "source": [
    "## Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e91075e8",
   "metadata": {
    "id": "e91075e8"
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, device, epochs):\n",
    "    all_train_losses = []\n",
    "    all_val_losses = []\n",
    "    all_vram_usages = []\n",
    "    step = 0\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        # Training phase with tqdm updates\n",
    "        epoch_train_losses, step, vram_usage = train(model, train_loader, criterion, optimizer, device, epoch, step)\n",
    "        all_train_losses.extend(epoch_train_losses)\n",
    "        all_vram_usages.append(vram_usage)\n",
    "        \n",
    "        # Validation phase\n",
    "        epoch_val_losses = validate(model, val_loader, criterion, device, epoch, step)\n",
    "        all_val_losses.extend(epoch_val_losses)\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        print(f'Epoch {epoch}/{epochs}, Train Loss: {epoch_train_losses[-1][2]:.4f}, '\n",
    "              f'Val Loss: {epoch_val_losses[-1][2]:.4f}, Epoch Time: {epoch_time:.2f}s',\n",
    "              f'Average Vram Usage: {np.mean(vram_usage):.2f}MB')\n",
    "\n",
    "    train_losses_df = pd.DataFrame(all_train_losses, columns=['step', 'epoch', 'loss_value'])\n",
    "    val_losses_df = pd.DataFrame(all_val_losses, columns=['step', 'epoch', 'loss_value'])\n",
    "    # average_vram_usage = np.mean(all_vram_usages)\n",
    "    return model, train_losses_df, val_losses_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4535c886",
   "metadata": {
    "id": "4535c886"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, char_to_idx, idx_to_char, start_text, device, max_length=500):\n",
    "    model.eval()\n",
    "    current_text = start_text\n",
    "    hidden = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            x = torch.tensor([[char_to_idx[ch] for ch in current_text[-SEQUENCE_LENGTH:]]]).to(device)\n",
    "            output, hidden = model(x, hidden)\n",
    "            probs = torch.softmax(output[0, -1], dim=0)\n",
    "            next_char_idx = torch.multinomial(probs, 1).item()\n",
    "            next_char = idx_to_char[next_char_idx]\n",
    "            current_text += next_char\n",
    "\n",
    "    return current_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6aef1cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_comparison_dict = {}\n",
    "\n",
    "def add_loss_to_comparison(model_name, train_losses_df, val_losses_df):\n",
    "    \"\"\"\n",
    "    Adds training and validation losses from a model to the comparison dictionary.\n",
    "    \"\"\"\n",
    "    loss_comparison_dict[model_name] = {\n",
    "        'train': train_losses_df,\n",
    "        'val': val_losses_df\n",
    "    }\n",
    "\n",
    "def print_final_losses(loss_dict):\n",
    "    for model_name, losses in loss_dict.items():\n",
    "        train_df = losses['train']\n",
    "        val_df = losses['val']\n",
    "        final_train = train_df.groupby('epoch')['loss_value'].last().iloc[-1]\n",
    "        final_val = val_df.groupby('epoch')['loss_value'].last().iloc[-1]\n",
    "        print(f\"{model_name}: Final Train Loss: {final_train:.4f}, Final Val Loss: {final_val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "973b4a80",
   "metadata": {
    "id": "973b4a80"
   },
   "outputs": [],
   "source": [
    "# Function to plot loss curves\n",
    "def plot_loss(train_losses_df, val_losses_df):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    # Plot training losses\n",
    "    for epoch in train_losses_df['epoch'].unique():\n",
    "        epoch_train_losses = train_losses_df[train_losses_df['epoch'] == epoch]\n",
    "        plt.plot(epoch_train_losses['step'], epoch_train_losses['loss_value'],\n",
    "                 color='blue', alpha=0.3)\n",
    "\n",
    "    # scatter training loss at the end of each epoch\n",
    "    last_train_losses = train_losses_df.groupby('epoch').last().reset_index()\n",
    "    plt.scatter(last_train_losses['step'], last_train_losses['loss_value'],\n",
    "                color='blue')\n",
    "\n",
    "    # Plot and scatter validation loss at the end of each epoch\n",
    "    last_val_losses = val_losses_df.groupby('epoch').last().reset_index()\n",
    "    plt.plot(last_val_losses['step'], last_val_losses['loss_value'],\n",
    "             color='orange', label='Validation Loss')\n",
    "    plt.scatter(last_val_losses['step'], last_val_losses['loss_value'],\n",
    "                color='orange')\n",
    "\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Function to print final loss values\n",
    "def print_final_losses(train_losses_df, val_losses_df):\n",
    "    print(\"Final Training Loss:\", train_losses_df.groupby('epoch')['loss_value'].last().iloc[-1])\n",
    "    print(\"Final Validation Loss:\", val_losses_df.groupby('epoch')['loss_value'].last().iloc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5001e203",
   "metadata": {
    "id": "5001e203"
   },
   "outputs": [],
   "source": [
    "# Function to plot loss curves for multiple models stored in loss_comparison_dict\n",
    "def plot_loss_comparisons():\n",
    "    \"\"\"\n",
    "    Plots the training loss curves and average validation loss per epoch for multiple models added to the loss comparison dictionary.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    # Get the last model in the dictionary (for special final-point highlighting)\n",
    "    last_model_name = list(loss_comparison_dict.keys())[-1]\n",
    "\n",
    "    # Loop through each model in the loss dictionary\n",
    "    for model_name, losses in loss_comparison_dict.items():\n",
    "        train_losses_df = losses['train']\n",
    "        val_losses_df = losses['val']\n",
    "\n",
    "        # Plot training losses for each model\n",
    "        plt.plot(train_losses_df['step'], train_losses_df['loss_value'],\n",
    "                 label=f'{model_name} train', linestyle='-', alpha=0.7)\n",
    "\n",
    "        # Scatter training loss at the end of each epoch\n",
    "        last_train_losses = train_losses_df.groupby('epoch').last().reset_index()\n",
    "        plt.scatter(last_train_losses['step'], last_train_losses['loss_value'], marker='o', s=50)\n",
    "\n",
    "        # Compute average validation loss per epoch (using the last step of each epoch for x-axis)\n",
    "        avg_val_losses = val_losses_df.groupby('epoch').agg({'loss_value': 'mean', 'step': 'last'}).reset_index()\n",
    "        # Scatter the average validation loss for each epoch\n",
    "        plt.scatter(avg_val_losses['step'], avg_val_losses['loss_value'], marker='s', s=50,\n",
    "                    label=f'{model_name} val avg')\n",
    "\n",
    "        # For the last model, highlight the final training loss with a star\n",
    "        if model_name == last_model_name:\n",
    "            final_step = train_losses_df['step'].iloc[-1]\n",
    "            final_loss = train_losses_df['loss_value'].iloc[-1]\n",
    "            plt.scatter(final_step, final_loss, marker='*', s=100, color='red', zorder=5)\n",
    "\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Comparison')\n",
    "    plt.legend()  # Legend shows both training and validation average labels\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16777ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_separate_train_val(loss_dict):\n",
    "    \"\"\"\n",
    "    모델별 Training Loss와 Validation Loss를 각각 별도의 그래프로 그립니다.\n",
    "    단, Validation Loss는 에포크별 평균으로 계산합니다.\n",
    "    \"\"\"\n",
    "    # 1. Training Loss Plot (원본 그대로)\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    plt.subplot(1, 2, 1)  # 1행 2열 중 첫 번째\n",
    "    for model_name, losses in loss_dict.items():\n",
    "        train_df = losses['train']\n",
    "        steps_train = train_df['step'].values\n",
    "        loss_train = train_df['loss_value'].values\n",
    "        plt.plot(steps_train, loss_train, label=f'{model_name} Train')\n",
    "    plt.title('Training Loss Comparison')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # 2. Validation Loss Plot (에포크별 평균 처리)\n",
    "    plt.subplot(1, 2, 2)  # 1행 2열 중 두 번째\n",
    "    for model_name, losses in loss_dict.items():\n",
    "        val_df = losses['val']\n",
    "        # 에포크별 평균 loss와 마지막 step을 계산\n",
    "        val_avg = val_df.groupby('epoch').agg({'loss_value': 'mean', 'step': 'last'}).reset_index()\n",
    "        plt.plot(val_avg['step'], val_avg['loss_value'], label=f'{model_name} Val')\n",
    "    plt.title('Validation Loss (Epoch Avg) Comparison')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3cdc2b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_attention(model, char_to_idx, idx_to_char, start_text, device, max_length=500):\n",
    "    model.eval()\n",
    "    current_text = start_text\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            # Convert the last sequence of characters to indices and feed it to the model\n",
    "            x = torch.tensor([[char_to_idx[ch] for ch in current_text[-SEQUENCE_LENGTH:]]]).to(device)\n",
    "            output = model(x)[0]  # No hidden state needed for attention-based models\n",
    "            probs = torch.softmax(output[0, -1], dim=0)\n",
    "            next_char_idx = torch.multinomial(probs, 1).item()\n",
    "            next_char = idx_to_char[next_char_idx]\n",
    "            current_text += next_char\n",
    "\n",
    "    return current_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2abd2d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(model_desc, model):\n",
    "    # Initialize the model\n",
    "    model = model.to(device)\n",
    "    # Use the same optimizer and criterion\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    # Train the model\n",
    "    trained_model, train_losses_df, val_losses_df = train_model(\n",
    "        model, train_loader, val_loader, criterion, optimizer, device, EPOCHS\n",
    "    )\n",
    "\n",
    "    add_loss_to_comparison(model_desc, train_losses_df, val_losses_df)\n",
    "\n",
    "    # Plot loss comparisons including this model\n",
    "    plot_loss_comparisons()\n",
    "    \n",
    "    plot_separate_train_val(loss_comparison_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "708a1ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for Transformer\n",
    "NUM_LAYERS = 4\n",
    "NUM_HEADS = 4\n",
    "FFN_DIM = 352\n",
    "DROPOUT = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f169751c",
   "metadata": {},
   "source": [
    "## Model 1: Diff w/ MHA(w/o GQA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4b090898",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    DIM = EMBEDDING_DIM \n",
    "    FFN_DIM = FFN_DIM\n",
    "    NUM_HEADS = NUM_HEADS \n",
    "    NUM_LAYERS = NUM_LAYERS\n",
    "\n",
    "    NUM_KV_HEADS = NUM_HEADS\n",
    "    VOCAB_SIZE = vocab_size\n",
    "    NORM_EPS = 1e-5 # LLaMA: 1e-5\n",
    "    ROPE_THETA = 10000 # LLaMA: 10000\n",
    "\n",
    "    MAX_BATCH_SIZE = BATCH_SIZE\n",
    "    MAX_SEQ_LEN = SEQUENCE_LENGTH # depending on the DATASET\n",
    "    NUM_KV_HEAD_REP = NUM_HEADS // NUM_KV_HEADS\n",
    "\n",
    "    HEAD_DIM = DIM // NUM_HEADS\n",
    "    DROPOUT = DROPOUT\n",
    "    DEVICE = device\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "    def _norm(self, x: torch.Tensor):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.weight * self._norm(x.float()).type_as(x)\n",
    "    \n",
    "def precompute_freqs_cis(head_dim: int, seq_len: int, theta: float = 10000.0, device: str = \"cuda\"):\n",
    "    if head_dim % 2 != 0:\n",
    "        raise ValueError(\"head_dim must be even for rotary embeddings.\")\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, head_dim, 2).float() / head_dim)).to(device)\n",
    "    t = torch.arange(seq_len, device=device, dtype=torch.float32)\n",
    "    freqs = torch.outer(t, freqs)  # [seq_len, head_dim//2]\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
    "    return freqs_cis  # [seq_len, head_dim // 2]\n",
    "\n",
    "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
    "    L = x.shape[1]\n",
    "    return freqs_cis.view(1, L, 1, x.shape[-1] // 2)  # [1, L, 1, head_dim]\n",
    "\n",
    "def apply_rotary_emb(x: torch.Tensor, device: str = \"cuda\"):\n",
    "    # x: [B, L, 2*heads, D] & D is even\n",
    "    _, L, _, D = x.shape\n",
    "    x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2)) # [B, L, 2*heads, D//2, 2]\n",
    "    freqs_cis = precompute_freqs_cis(D, L)\n",
    "    freqs = reshape_for_broadcast(freqs_cis, x)\n",
    "    x_rotated = x_complex * freqs\n",
    "    x_out = torch.view_as_real(x_rotated).reshape(x.shape)\n",
    "    return x_out.type_as(x).to(device)\n",
    "\n",
    "def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    # x: [B, L, n_kv, d]\n",
    "    B, L, nk, d = x.shape\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "    return x[:, :, :, None, :].expand(B, L, nk, n_rep, d).reshape(B, L, nk * n_rep, d)\n",
    "\n",
    "class MoE(nn.Module):\n",
    "    def __init__(self, dim, num_experts=3, topk=2, noise_std=0.1):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.ffn_dim = dim * num_experts\n",
    "        self.topk = topk\n",
    "        self.noise_std = noise_std\n",
    "\n",
    "        self.gate = nn.Linear(dim, num_experts)\n",
    "\n",
    "        self.w1 = nn.Linear(dim, self.ffn_dim, bias=False)\n",
    "        self.w3 = nn.Linear(dim, self.ffn_dim, bias=False)\n",
    "        self.w2 = nn.Linear(self.ffn_dim, self.ffn_dim, bias=False)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.w1.weight)\n",
    "        nn.init.xavier_uniform_(self.w2.weight)\n",
    "        nn.init.xavier_uniform_(self.w3.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gate_logits = self.gate(x)\n",
    "\n",
    "        if self.noise_std > 0:\n",
    "            noise = torch.randn_like(gate_logits) * self.noise_std\n",
    "            gate_logits += noise\n",
    "\n",
    "        gate_probs = F.softmax(gate_logits, dim=-1)\n",
    "        load_balance_loss = ((gate_probs.mean(0) - (1.0 / self.num_experts)) ** 2).mean() * self.num_experts\n",
    "\n",
    "        topk_probs, topk_indices = torch.topk(gate_probs, self.topk, dim=-1)\n",
    "\n",
    "        B, L, D = x.size()\n",
    "\n",
    "        w1_out = F.silu(self.w1(x).view(B, L, self.num_experts, -1))\n",
    "        w3_out = self.dropout(self.w3(x).view(B, L, self.num_experts, -1))\n",
    "\n",
    "        expert_output = w1_out * w3_out\n",
    "        expert_output = self.w2(expert_output.view(B, L, -1)).view(B, L, self.num_experts, D)\n",
    "\n",
    "        selected_expert_outputs = torch.gather(\n",
    "            expert_output,\n",
    "            2,\n",
    "            topk_indices.unsqueeze(-1).expand(-1, -1, self.topk, D)\n",
    "        )\n",
    "\n",
    "        output = torch.einsum('blk,blkd->bld', topk_probs, selected_expert_outputs)\n",
    "\n",
    "        return output, load_balance_loss\n",
    "\n",
    "\n",
    "\n",
    "class DiffSelfAttention(nn.Module):\n",
    "    def __init__(self, n_heads, n_kv_heads, n_rep, dim, dropout, batch, seq_len, device, depth=0):\n",
    "        super().__init__()\n",
    "        self.n_heads_q = n_heads\n",
    "        self.n_kv_heads = n_kv_heads\n",
    "        self.n_rep = n_rep  # typically n_heads // n_kv_heads\n",
    "        self.head_dim = dim // n_heads  # standard head dimension\n",
    "\n",
    "\n",
    "        self.wq = nn.Linear(dim, 2 * n_heads * self.head_dim, bias=False)\n",
    "        self.wk = nn.Linear(dim, 2 * n_kv_heads * self.head_dim, bias=False)\n",
    "        self.wv = nn.Linear(dim, 2 * n_kv_heads * self.head_dim, bias=False)\n",
    "        # Output projection expects input dim = 2*n_heads*head_dim.\n",
    "        self.wo = nn.Linear(2 * n_heads * self.head_dim, dim, bias=False)\n",
    "        self.attn_dropout = dropout\n",
    "\n",
    "        # KV Cache (for inference with caching)\n",
    "        self.cache_k = torch.zeros(batch, 2 * n_heads, seq_len, self.head_dim, device=device)\n",
    "        self.cache_v = torch.zeros(batch, n_heads, seq_len, 2 * self.head_dim, device=device)\n",
    "\n",
    "        # Lambda parameter as in diff attention paper.\n",
    "        self.lambda_init = 0.8 - 0.6 * math.exp(-0.3 * depth)\n",
    "        self.lambda_q1 = nn.Parameter(torch.zeros(self.head_dim, dtype=torch.float32).normal_(mean=0, std=0.1))\n",
    "        self.lambda_k1 = nn.Parameter(torch.zeros(self.head_dim, dtype=torch.float32).normal_(mean=0, std=0.1))\n",
    "        self.lambda_q2 = nn.Parameter(torch.zeros(self.head_dim, dtype=torch.float32).normal_(mean=0, std=0.1))\n",
    "        self.lambda_k2 = nn.Parameter(torch.zeros(self.head_dim, dtype=torch.float32).normal_(mean=0, std=0.1))\n",
    "        \n",
    "        # groupnorm\n",
    "        self.norm = RMSNorm(2*self.head_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, start_pos: int, mask, return_attn=False):\n",
    "        B, L, _ = x.shape\n",
    "        src_len = trg_len = L\n",
    "        offset = start_pos # same with start_pos\n",
    "        \n",
    "        # Project and reshape: \n",
    "        xq = self.wq(x).view(B, trg_len, 2 * self.n_heads_q, self.head_dim)   # [B, L, 2*heads, head_dim]\n",
    "        xk = self.wk(x).view(B, src_len, 2 * self.n_kv_heads, self.head_dim)  # [B, L, 2*heads_kv, head_dim]\n",
    "        xv = self.wv(x).view(B, src_len, self.n_kv_heads, 2 * self.head_dim)  # [B, L, heads_kv, 2*head_dim]\n",
    "        \n",
    "        # Apply rotary embeddings on queries and keys.\n",
    "        xq = apply_rotary_emb(xq, device=x.device)\n",
    "        xk = apply_rotary_emb(xk, device=x.device)\n",
    "        \n",
    "        # Grouped Query Attention:\n",
    "        xq = xq.transpose(1, 2)                         # [B, 2*heads, L, head_dim]\n",
    "        xk = repeat_kv(xk, self.n_rep).transpose(1, 2)  # [B, 2*heads, L, head_dim]\n",
    "        xv = repeat_kv(xv, self.n_rep).transpose(1, 2)  # [B, 2*heads, L, head_dim]\n",
    "        \n",
    "        # Update KV cache.\n",
    "        self.cache_k[:B, :, offset:offset+L] = xk\n",
    "        self.cache_v[:B, :, offset:offset+L] = xv\n",
    "        \n",
    "        # scaling\n",
    "        scaling = 1 / math.sqrt(self.head_dim)\n",
    "        xq *= scaling\n",
    "        \n",
    "        attn_weights = torch.matmul(xq, xk.transpose(-1, -2))\n",
    "        \n",
    "        if mask is None:\n",
    "            mask = torch.triu(\n",
    "                torch.zeros([L, L])\n",
    "                .float()\n",
    "                .fill_(float(\"-inf\"))\n",
    "                .type_as(attn_weights),\n",
    "                1 + offset,\n",
    "            )\n",
    "        \n",
    "        attn_weights = torch.nan_to_num(attn_weights)\n",
    "        attn_weights += mask\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1).type_as(attn_weights)\n",
    "        attn_weights = attn_weights.view(B, self.n_heads_q, 2, trg_len, src_len)\n",
    "        \n",
    "        # Combine the two attention scores with lambda.\n",
    "        lambda_1 = torch.exp(torch.sum(self.lambda_q1 * self.lambda_k1, dim=-1).float()).type_as(xq)\n",
    "        lambda_2 = torch.exp(torch.sum(self.lambda_q2 * self.lambda_k2, dim=-1).float()).type_as(xq)\n",
    "        lambda_full = lambda_1 - lambda_2 + self.lambda_init\n",
    "        \n",
    "        attn1 = attn_weights[:,:,0]\n",
    "        attn2 = attn_weights[:,:,1]\n",
    "        \n",
    "        attn_weights = attn1 - lambda_full * attn2\n",
    "        attn_weights = F.dropout(attn_weights, self.attn_dropout, training=self.training)\n",
    "        \n",
    "        # Compute attention output.\n",
    "        attn = torch.matmul(attn_weights, xv)\n",
    "        attn = self.norm(attn)\n",
    "        attn = attn * (1-self.lambda_init)\n",
    "        attn = attn.transpose(1, 2).contiguous().view(B, L, -1)\n",
    "        attn_output = self.wo(attn)\n",
    "        \n",
    "        if return_attn:\n",
    "            # Return four maps:\n",
    "            # [첫 번째 맵(attn1), 두 번째 맵(attn2), 두 번째 맵에 람다를 곱한 결과, 최종 결과(attn_weights)]\n",
    "            return attn_output, [attn1, attn2, lambda_full*attn2, attn_weights]\n",
    "        else:\n",
    "            return attn_output\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, args, depth: int = 0, num_experts=5, topk=2, noise_std=0.1):\n",
    "        super().__init__()\n",
    "        self.attn_norm = RMSNorm(args.DIM, eps=args.NORM_EPS)\n",
    "        self.ffn_norm = RMSNorm(args.DIM, eps=args.NORM_EPS)\n",
    "        self.attention = DiffSelfAttention(\n",
    "            n_heads=args.NUM_HEADS,\n",
    "            n_kv_heads=args.NUM_KV_HEADS if args.NUM_KV_HEADS is not None else args.NUM_HEADS,\n",
    "            n_rep=args.NUM_KV_HEAD_REP,\n",
    "            dim=args.DIM,\n",
    "            dropout=args.DROPOUT,\n",
    "            batch=args.MAX_BATCH_SIZE,\n",
    "            seq_len=args.MAX_SEQ_LEN,\n",
    "            device=args.DEVICE,\n",
    "            depth=depth\n",
    "        )\n",
    "        \n",
    "        self.ffn = MoE(\n",
    "            dim=args.DIM,\n",
    "            num_experts=num_experts,\n",
    "            topk=topk,\n",
    "            noise_std=noise_std\n",
    "        )\n",
    "\n",
    "        self.res_dropout = nn.Dropout(args.DROPOUT)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, start_pos: int, mask, return_attn=False):\n",
    "        if return_attn:\n",
    "            attn_out, attn_map = self.attention(self.attn_norm(x), start_pos, mask, return_attn=True)\n",
    "            h = x + self.res_dropout(attn_out)\n",
    "            ffn_out, moe_loss = self.ffn(self.ffn_norm(h))\n",
    "            h = h + self.res_dropout(ffn_out)\n",
    "            return h, attn_map, moe_loss\n",
    "        else:\n",
    "            h = x + self.res_dropout(self.attention(self.attn_norm(x), start_pos, mask))\n",
    "            ffn_out, moe_loss = self.ffn(self.ffn_norm(h))\n",
    "            h = h + self.res_dropout(ffn_out)\n",
    "            return h, moe_loss\n",
    "        \n",
    "class DiffTransformer(nn.Module):\n",
    "    def __init__(self, args, num_experts=3, topk=2, noise_std=0.1):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.tok_embeddings = nn.Embedding(args.VOCAB_SIZE, args.DIM)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(args, depth=i, num_experts=num_experts, topk=topk, noise_std=noise_std)\n",
    "            for i in range(args.NUM_LAYERS)\n",
    "        ])\n",
    "        self.norm = RMSNorm(args.DIM, eps=args.NORM_EPS)\n",
    "        self.output = nn.Linear(args.DIM, args.VOCAB_SIZE, bias=False)\n",
    "        self.device = args.DEVICE\n",
    "\n",
    "    def forward(self, x: torch.Tensor, start_pos: int = 0, return_attn=False):\n",
    "        B, L = x.shape\n",
    "        h = self.tok_embeddings(x)\n",
    "        mask = None\n",
    "        if L > 1:\n",
    "            mask = torch.triu(torch.full((L, L), float('-inf'), device=self.device), diagonal=1)\n",
    "        attn_maps, moe_losses = [], []\n",
    "        for layer in self.layers:\n",
    "            if return_attn:\n",
    "                h, attn_map, moe_loss = layer(h, start_pos, mask, return_attn=True)\n",
    "                attn_maps.append(attn_map)\n",
    "                moe_losses.append(moe_loss)\n",
    "            else:\n",
    "                h, moe_loss = layer(h, start_pos, mask)\n",
    "                moe_losses.append(moe_loss)\n",
    "        logits = self.output(self.norm(h)).float()\n",
    "        total_moe_loss = sum(moe_losses)\n",
    "        if return_attn:\n",
    "            return logits, attn_maps, total_moe_loss\n",
    "        return logits, total_moe_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3c60df16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DiffTransformer(\n",
       "  (tok_embeddings): Embedding(65, 128)\n",
       "  (layers): ModuleList(\n",
       "    (0-3): 4 x TransformerBlock(\n",
       "      (attn_norm): RMSNorm()\n",
       "      (ffn_norm): RMSNorm()\n",
       "      (attention): DiffSelfAttention(\n",
       "        (wq): Linear(in_features=128, out_features=256, bias=False)\n",
       "        (wk): Linear(in_features=128, out_features=256, bias=False)\n",
       "        (wv): Linear(in_features=128, out_features=256, bias=False)\n",
       "        (wo): Linear(in_features=256, out_features=128, bias=False)\n",
       "        (norm): RMSNorm()\n",
       "      )\n",
       "      (ffn): MoE(\n",
       "        (gate): Linear(in_features=128, out_features=3, bias=True)\n",
       "        (w1): Linear(in_features=128, out_features=384, bias=False)\n",
       "        (w3): Linear(in_features=128, out_features=384, bias=False)\n",
       "        (w2): Linear(in_features=384, out_features=384, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (res_dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (norm): RMSNorm()\n",
       "  (output): Linear(in_features=128, out_features=65, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PARAMS = ModelArgs()\n",
    "diff1 = DiffTransformer(PARAMS).to(device)\n",
    "diff1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e1ecdc1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "DiffTransformer                          [512, 64, 65]             --\n",
       "├─Embedding: 1-1                         [512, 64, 128]            8,320\n",
       "├─ModuleList: 1-2                        --                        --\n",
       "│    └─TransformerBlock: 2-1             [512, 64, 128]            --\n",
       "│    │    └─RMSNorm: 3-1                 [512, 64, 128]            128\n",
       "│    │    └─DiffSelfAttention: 3-2       [512, 64, 128]            131,264\n",
       "│    │    └─Dropout: 3-3                 [512, 64, 128]            --\n",
       "│    │    └─RMSNorm: 3-4                 [512, 64, 128]            128\n",
       "│    │    └─MoE: 3-5                     [512, 64, 128]            246,147\n",
       "│    │    └─Dropout: 3-6                 [512, 64, 128]            --\n",
       "│    └─TransformerBlock: 2-2             [512, 64, 128]            --\n",
       "│    │    └─RMSNorm: 3-7                 [512, 64, 128]            128\n",
       "│    │    └─DiffSelfAttention: 3-8       [512, 64, 128]            131,264\n",
       "│    │    └─Dropout: 3-9                 [512, 64, 128]            --\n",
       "│    │    └─RMSNorm: 3-10                [512, 64, 128]            128\n",
       "│    │    └─MoE: 3-11                    [512, 64, 128]            246,147\n",
       "│    │    └─Dropout: 3-12                [512, 64, 128]            --\n",
       "│    └─TransformerBlock: 2-3             [512, 64, 128]            --\n",
       "│    │    └─RMSNorm: 3-13                [512, 64, 128]            128\n",
       "│    │    └─DiffSelfAttention: 3-14      [512, 64, 128]            131,264\n",
       "│    │    └─Dropout: 3-15                [512, 64, 128]            --\n",
       "│    │    └─RMSNorm: 3-16                [512, 64, 128]            128\n",
       "│    │    └─MoE: 3-17                    [512, 64, 128]            246,147\n",
       "│    │    └─Dropout: 3-18                [512, 64, 128]            --\n",
       "│    └─TransformerBlock: 2-4             [512, 64, 128]            --\n",
       "│    │    └─RMSNorm: 3-19                [512, 64, 128]            128\n",
       "│    │    └─DiffSelfAttention: 3-20      [512, 64, 128]            131,264\n",
       "│    │    └─Dropout: 3-21                [512, 64, 128]            --\n",
       "│    │    └─RMSNorm: 3-22                [512, 64, 128]            128\n",
       "│    │    └─MoE: 3-23                    [512, 64, 128]            246,147\n",
       "│    │    └─Dropout: 3-24                [512, 64, 128]            --\n",
       "├─RMSNorm: 1-3                           [512, 64, 128]            128\n",
       "├─Linear: 1-4                            [512, 64, 65]             8,320\n",
       "==========================================================================================\n",
       "Total params: 1,527,436\n",
       "Trainable params: 1,527,436\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 781.79\n",
       "==========================================================================================\n",
       "Input size (MB): 0.26\n",
       "Forward/backward pass size (MB): 2771.65\n",
       "Params size (MB): 6.11\n",
       "Estimated Total Size (MB): 2778.02\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# Describe the model\n",
    "summary(diff1.to(device), input_size=(BATCH_SIZE, SEQUENCE_LENGTH), dtypes=[torch.long])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc9c02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_test(\"DIFF1\", diff1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8775d573",
   "metadata": {},
   "source": [
    "## Model 2: Diff w/ GQA(groups=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af4220d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    DIM = EMBEDDING_DIM \n",
    "    FFN_DIM = FFN_DIM\n",
    "    NUM_HEADS = NUM_HEADS \n",
    "    NUM_LAYERS = NUM_LAYERS\n",
    "\n",
    "    NUM_KV_HEADS = NUM_HEADS // 2\n",
    "    VOCAB_SIZE = vocab_size\n",
    "    NORM_EPS = 1e-5 # LLaMA: 1e-5\n",
    "    ROPE_THETA = 10000 # LLaMA: 10000\n",
    "\n",
    "    MAX_BATCH_SIZE = BATCH_SIZE\n",
    "    MAX_SEQ_LEN = SEQUENCE_LENGTH # depending on the DATASET\n",
    "    NUM_KV_HEAD_REP = NUM_HEADS // NUM_KV_HEADS\n",
    "\n",
    "    HEAD_DIM = DIM // NUM_HEADS\n",
    "    DROPOUT = DROPOUT\n",
    "    DEVICE = device\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "    def _norm(self, x: torch.Tensor):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.weight * self._norm(x.float()).type_as(x)\n",
    "    \n",
    "def precompute_freqs_cis(head_dim: int, seq_len: int, theta: float = 10000.0, device: str = \"cuda\"):\n",
    "    if head_dim % 2 != 0:\n",
    "        raise ValueError(\"head_dim must be even for rotary embeddings.\")\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, head_dim, 2).float() / head_dim)).to(device)\n",
    "    t = torch.arange(seq_len, device=device, dtype=torch.float32)\n",
    "    freqs = torch.outer(t, freqs)  # [seq_len, head_dim//2]\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
    "    return freqs_cis  # [seq_len, head_dim // 2]\n",
    "\n",
    "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
    "    L = x.shape[1]\n",
    "    return freqs_cis.view(1, L, 1, x.shape[-1] // 2)  # [1, L, 1, head_dim]\n",
    "\n",
    "def apply_rotary_emb(x: torch.Tensor, device: str = \"cuda\"):\n",
    "    # x: [B, L, 2*heads, D] & D is even\n",
    "    _, L, _, D = x.shape\n",
    "    x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2)) # [B, L, 2*heads, D//2, 2]\n",
    "    freqs_cis = precompute_freqs_cis(D, L)\n",
    "    freqs = reshape_for_broadcast(freqs_cis, x)\n",
    "    x_rotated = x_complex * freqs\n",
    "    x_out = torch.view_as_real(x_rotated).reshape(x.shape)\n",
    "    return x_out.type_as(x).to(device)\n",
    "\n",
    "def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    # x: [B, L, n_kv, d]\n",
    "    B, L, nk, d = x.shape\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "    return x[:, :, :, None, :].expand(B, L, nk, n_rep, d).reshape(B, L, nk * n_rep, d)\n",
    "\n",
    "class MoE(nn.Module):\n",
    "    def __init__(self, dim, ffn_dim, num_experts=5, topk=2, noise_std=0.1):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.topk = topk\n",
    "        self.noise_std = noise_std\n",
    "\n",
    "        self.gate = nn.Linear(dim, num_experts)\n",
    "\n",
    "        self.w1 = nn.Linear(dim, ffn_dim * num_experts, bias=False)\n",
    "        self.w3 = nn.Linear(dim, ffn_dim * num_experts, bias=False)\n",
    "        self.w2 = nn.Linear(ffn_dim * num_experts, dim * num_experts, bias=False)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.w1.weight)\n",
    "        nn.init.xavier_uniform_(self.w2.weight)\n",
    "        nn.init.xavier_uniform_(self.w3.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gate_logits = self.gate(x)\n",
    "\n",
    "        if self.noise_std > 0:\n",
    "            noise = torch.randn_like(gate_logits) * self.noise_std\n",
    "            gate_logits += noise\n",
    "\n",
    "        gate_probs = F.softmax(gate_logits, dim=-1)\n",
    "\n",
    "        load_balance_loss = ((gate_probs.mean(0) - (1.0 / self.num_experts)) ** 2).mean() * self.num_experts\n",
    "\n",
    "        topk_probs, topk_indices = torch.topk(gate_probs, self.topk, dim=-1)\n",
    "\n",
    "        w1_out = F.silu(self.w1(x))\n",
    "        w3_out = self.dropout(self.w3(x))\n",
    "        expert_output = w1_out * w3_out\n",
    "        expert_output = self.w2(expert_output)\n",
    "\n",
    "        expert_output = expert_output.view(-1, self.num_experts, x.size(-1))\n",
    "\n",
    "        selected_expert_outputs = torch.gather(\n",
    "            expert_output,\n",
    "            1,\n",
    "            topk_indices.unsqueeze(-1).expand(-1, -1, x.size(-1))\n",
    "        )\n",
    "\n",
    "        output = torch.einsum('bk,bkd->bd', topk_probs, selected_expert_outputs)\n",
    "\n",
    "        return output, load_balance_loss\n",
    "\n",
    "\n",
    "class DiffSelfAttention(nn.Module):\n",
    "    def __init__(self, n_heads, n_kv_heads, n_rep, dim, dropout, batch, seq_len, device, depth=0):\n",
    "        super().__init__()\n",
    "        self.n_heads_q = n_heads\n",
    "        self.n_kv_heads = n_kv_heads\n",
    "        self.n_rep = n_rep  # typically n_heads // n_kv_heads\n",
    "        self.head_dim = dim // n_heads  # standard head dimension\n",
    "\n",
    "\n",
    "        self.wq = nn.Linear(dim, 2 * n_heads * self.head_dim, bias=False)\n",
    "        self.wk = nn.Linear(dim, 2 * n_kv_heads * self.head_dim, bias=False)\n",
    "        self.wv = nn.Linear(dim, 2 * n_kv_heads * self.head_dim, bias=False)\n",
    "        # Output projection expects input dim = 2*n_heads*head_dim.\n",
    "        self.wo = nn.Linear(2 * n_heads * self.head_dim, dim, bias=False)\n",
    "        self.attn_dropout = dropout\n",
    "\n",
    "        # KV Cache (for inference with caching)\n",
    "        self.cache_k = torch.zeros(batch, 2 * n_heads, seq_len, self.head_dim, device=device)\n",
    "        self.cache_v = torch.zeros(batch, n_heads, seq_len, 2 * self.head_dim, device=device)\n",
    "\n",
    "        # Lambda parameter as in diff attention paper.\n",
    "        self.lambda_init = 0.8 - 0.6 * math.exp(-0.3 * depth)\n",
    "        self.lambda_q1 = nn.Parameter(torch.zeros(self.head_dim, dtype=torch.float32).normal_(mean=0, std=0.1))\n",
    "        self.lambda_k1 = nn.Parameter(torch.zeros(self.head_dim, dtype=torch.float32).normal_(mean=0, std=0.1))\n",
    "        self.lambda_q2 = nn.Parameter(torch.zeros(self.head_dim, dtype=torch.float32).normal_(mean=0, std=0.1))\n",
    "        self.lambda_k2 = nn.Parameter(torch.zeros(self.head_dim, dtype=torch.float32).normal_(mean=0, std=0.1))\n",
    "        \n",
    "        # groupnorm\n",
    "        self.norm = RMSNorm(2*self.head_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, start_pos: int, mask, return_attn=False):\n",
    "        B, L, _ = x.shape\n",
    "        src_len = trg_len = L\n",
    "        offset = start_pos # same with start_pos\n",
    "        \n",
    "        # Project and reshape: \n",
    "        xq = self.wq(x).view(B, trg_len, 2 * self.n_heads_q, self.head_dim)   # [B, L, 2*heads, head_dim]\n",
    "        xk = self.wk(x).view(B, src_len, 2 * self.n_kv_heads, self.head_dim)  # [B, L, 2*heads_kv, head_dim]\n",
    "        xv = self.wv(x).view(B, src_len, self.n_kv_heads, 2 * self.head_dim)  # [B, L, heads_kv, 2*head_dim]\n",
    "        \n",
    "        # Apply rotary embeddings on queries and keys.\n",
    "        xq = apply_rotary_emb(xq, device=x.device)\n",
    "        xk = apply_rotary_emb(xk, device=x.device)\n",
    "        \n",
    "        # Grouped Query Attention:\n",
    "        xq = xq.transpose(1, 2)                         # [B, 2*heads, L, head_dim]\n",
    "        xk = repeat_kv(xk, self.n_rep).transpose(1, 2)  # [B, 2*heads, L, head_dim]\n",
    "        xv = repeat_kv(xv, self.n_rep).transpose(1, 2)  # [B, 2*heads, L, head_dim]\n",
    "        \n",
    "        # Update KV cache.\n",
    "        self.cache_k[:B, :, offset:offset+L] = xk\n",
    "        self.cache_v[:B, :, offset:offset+L] = xv\n",
    "        \n",
    "        # scaling\n",
    "        scaling = 1 / math.sqrt(self.head_dim)\n",
    "        xq *= scaling\n",
    "        \n",
    "        attn_weights = torch.matmul(xq, xk.transpose(-1, -2))\n",
    "        \n",
    "        if mask is None:\n",
    "            mask = torch.triu(\n",
    "                torch.zeros([L, L])\n",
    "                .float()\n",
    "                .fill_(float(\"-inf\"))\n",
    "                .type_as(attn_weights),\n",
    "                1 + offset,\n",
    "            )\n",
    "        \n",
    "        attn_weights = torch.nan_to_num(attn_weights)\n",
    "        attn_weights += mask\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1).type_as(attn_weights)\n",
    "        attn_weights = attn_weights.view(B, self.n_heads_q, 2, trg_len, src_len)\n",
    "        \n",
    "        # Combine the two attention scores with lambda.\n",
    "        lambda_1 = torch.exp(torch.sum(self.lambda_q1 * self.lambda_k1, dim=-1).float()).type_as(xq)\n",
    "        lambda_2 = torch.exp(torch.sum(self.lambda_q2 * self.lambda_k2, dim=-1).float()).type_as(xq)\n",
    "        lambda_full = lambda_1 - lambda_2 + self.lambda_init\n",
    "        \n",
    "        attn1 = attn_weights[:,:,0]\n",
    "        attn2 = attn_weights[:,:,1]\n",
    "        \n",
    "        attn_weights = attn1 - lambda_full * attn2\n",
    "        attn_weights = F.dropout(attn_weights, self.attn_dropout, training=self.training)\n",
    "        \n",
    "        # Compute attention output.\n",
    "        attn = torch.matmul(attn_weights, xv)\n",
    "        attn = self.norm(attn)\n",
    "        attn = attn * (1-self.lambda_init)\n",
    "        attn = attn.transpose(1, 2).contiguous().view(B, L, -1)\n",
    "        attn_output = self.wo(attn)\n",
    "        \n",
    "        if return_attn:\n",
    "            # Return four maps:\n",
    "            # [첫 번째 맵(attn1), 두 번째 맵(attn2), 두 번째 맵에 람다를 곱한 결과, 최종 결과(attn_weights)]\n",
    "            return attn_output, [attn1, attn2, lambda_full*attn2, attn_weights]\n",
    "        else:\n",
    "            return attn_output\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, args, depth: int = 0, num_experts=5, topk=2, noise_std=0.1):\n",
    "        super().__init__()\n",
    "        self.attn_norm = RMSNorm(args.DIM, eps=args.NORM_EPS)\n",
    "        self.ffn_norm = RMSNorm(args.DIM, eps=args.NORM_EPS)\n",
    "        self.attention = DiffSelfAttention(\n",
    "            n_heads=args.NUM_HEADS,\n",
    "            n_kv_heads=args.NUM_KV_HEADS if args.NUM_KV_HEADS is not None else args.NUM_HEADS,\n",
    "            n_rep=args.NUM_KV_HEAD_REP,\n",
    "            dim=args.DIM,\n",
    "            dropout=args.DROPOUT,\n",
    "            batch=args.MAX_BATCH_SIZE,\n",
    "            seq_len=args.MAX_SEQ_LEN,\n",
    "            device=args.DEVICE,\n",
    "            depth=depth\n",
    "        )\n",
    "        \n",
    "        self.ffn = MoE(\n",
    "            dim=args.DIM,\n",
    "            ffn_dim=args.FFN_DIM,\n",
    "            num_experts=num_experts,\n",
    "            topk=topk,\n",
    "            noise_std=noise_std\n",
    "        )\n",
    "\n",
    "        self.res_dropout = nn.Dropout(args.DROPOUT)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, start_pos: int, mask, return_attn=False):\n",
    "        if return_attn:\n",
    "            attn_out, attn_map = self.attention(self.attn_norm(x), start_pos, mask, return_attn=True)\n",
    "            h = x + self.res_dropout(attn_out)\n",
    "            ffn_out, moe_loss = self.ffn(self.ffn_norm(h))\n",
    "            h = h + self.res_dropout(ffn_out)\n",
    "            return h, attn_map, moe_loss\n",
    "        else:\n",
    "            h = x + self.res_dropout(self.attention(self.attn_norm(x), start_pos, mask))\n",
    "            ffn_out, moe_loss = self.ffn(self.ffn_norm(h))\n",
    "            h = h + self.res_dropout(ffn_out)\n",
    "            return h, moe_loss\n",
    "        \n",
    "class DiffTransformer(nn.Module):\n",
    "    def __init__(self, args, num_experts=5, topk=2, noise_std=0.1):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.tok_embeddings = nn.Embedding(args.VOCAB_SIZE, args.DIM)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(args, depth=i, num_experts=num_experts, topk=topk, noise_std=noise_std)\n",
    "            for i in range(args.NUM_LAYERS)\n",
    "        ])\n",
    "        self.norm = RMSNorm(args.DIM, eps=args.NORM_EPS)\n",
    "        self.output = nn.Linear(args.DIM, args.VOCAB_SIZE, bias=False)\n",
    "        self.device = args.DEVICE\n",
    "\n",
    "    def forward(self, x: torch.Tensor, start_pos: int = 0, return_attn=False):\n",
    "        B, L = x.shape\n",
    "        h = self.tok_embeddings(x)\n",
    "        mask = None\n",
    "        if L > 1:\n",
    "            mask = torch.triu(torch.full((L, L), float('-inf'), device=self.device), diagonal=1)\n",
    "        attn_maps, moe_losses = [], []\n",
    "        for layer in self.layers:\n",
    "            if return_attn:\n",
    "                h, attn_map, moe_loss = layer(h, start_pos, mask, return_attn=True)\n",
    "                attn_maps.append(attn_map)\n",
    "                moe_losses.append(moe_loss)\n",
    "            else:\n",
    "                h, moe_loss = layer(h, start_pos, mask)\n",
    "                moe_losses.append(moe_loss)\n",
    "        logits = self.output(self.norm(h)).float()\n",
    "        total_moe_loss = sum(moe_losses)\n",
    "        if return_attn:\n",
    "            return logits, attn_maps, total_moe_loss\n",
    "        return logits, total_moe_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123e5eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = ModelArgs()\n",
    "diff2 = DiffTransformer(PARAMS).to(device)\n",
    "diff2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11c6f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# Describe the model\n",
    "summary(diff2.to(device), input_size=(BATCH_SIZE, SEQUENCE_LENGTH), dtypes=[torch.long])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a0194f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_test(\"DIFF2\", diff2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9d1bc0",
   "metadata": {},
   "source": [
    "## Compare Attention Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c8a78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def compare_attention_maps(models_dict, dataloader, start_pos=0, layer=0, head=0):\n",
    "\n",
    "    x, _ = next(iter(dataloader))\n",
    "    device = 'cuda:0'\n",
    "    x = x.to(device)\n",
    "    \n",
    "    attn_results = {}\n",
    "    \n",
    "    for name, model in models_dict.items():\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        with torch.inference_mode():\n",
    "            _, _, attn_maps = model.forward(x, start_pos=start_pos, return_attn=True)\n",
    "            attns = attn_maps[layer]         \n",
    "            names = ['attn1', 'attn2', 'λ*attn2', 'attn1-λ*attn2']\n",
    "            names = [name+'_'+x for x in names]\n",
    "            for name, attn in zip(names, attns):\n",
    "                attn_results[name] = attn[0, head].detach().cpu().numpy()\n",
    "    \n",
    "    n_models = len(attn_results)\n",
    "    fig, axs = plt.subplots(1, n_models, figsize=(n_models * 4, 4))\n",
    "    if n_models == 1:\n",
    "        axs = [axs]\n",
    "\n",
    "    for ax, (name, attn) in zip(axs, attn_results.items()):\n",
    "        if 'attn1-λ*attn2' in name:\n",
    "            im = ax.imshow(attn, interpolation='nearest', cmap='seismic', vmin=-1, vmax=1)\n",
    "        else:\n",
    "            im = ax.imshow(attn, interpolation='nearest', cmap='Reds', vmin=np.min(attn), vmax=np.max(attn))\n",
    "        ax.set_title(f\"{name.upper()} \\nLayer {layer+1}, Head {head+1}\")\n",
    "        ax.set_xlabel(\"Key Token Position\")\n",
    "        ax.set_ylabel(\"Query Token Position\")\n",
    "        fig.colorbar(im, ax=ax)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87661b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"diff1\": diff1,\n",
    "    \"diff2\": diff2,\n",
    "}\n",
    "\n",
    "for i in range(3):\n",
    "    compare_attention_maps(models, train_loader, start_pos=0, layer=i, head=i)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
