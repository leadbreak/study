{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94bebad5",
   "metadata": {},
   "source": [
    "```\n",
    "LSTM vs Transformer vs LLaMA vs Diff\n",
    "```\n",
    "- 파라미터 수 78만 정도의 작은 모델 & without GQA\n",
    "- 작은 배치 사이즈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20ec8bf0",
   "metadata": {
    "id": "20ec8bf0"
   },
   "outputs": [],
   "source": [
    "## Importing Libraries\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3390f4",
   "metadata": {
    "id": "5a3390f4"
   },
   "source": [
    "## Hyperparameters and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0c61f36",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1732703737281,
     "user": {
      "displayName": "Hugman Sangkeun Jung",
      "userId": "08689291704194029524"
     },
     "user_tz": -540
    },
    "id": "d0c61f36",
    "outputId": "45d9894c-a14d-4164-9f2d-699025751e72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Modified hyperparameters\n",
    "SEQUENCE_LENGTH = 64\n",
    "\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = EMBEDDING_DIM*2\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 1e-4\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c9b962",
   "metadata": {
    "id": "89c9b962"
   },
   "source": [
    "## Data Preparation\n",
    "\n",
    "We are using the TinyShakespeare dataset, a small character-level text corpus consisting of a subset of Shakespeare's plays. It's often used for testing sequence models, as it includes a rich set of vocabulary and provides a challenging task for next-character prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "089830e2",
   "metadata": {
    "id": "089830e2"
   },
   "outputs": [],
   "source": [
    "## Utility Functions\n",
    "\n",
    "def load_data(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "def create_char_mappings(text):\n",
    "    chars = sorted(list(set(text)))\n",
    "    char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "    idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "    return chars, char_to_idx, idx_to_char"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fc6452",
   "metadata": {
    "id": "30fc6452"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f07670d9",
   "metadata": {
    "id": "f07670d9"
   },
   "outputs": [],
   "source": [
    "class CharDataset(Dataset):\n",
    "    def __init__(self, text, seq_length, char_to_idx):\n",
    "        self.text = text\n",
    "        self.seq_length = seq_length\n",
    "        self.char_to_idx = char_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text) - self.seq_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = [self.char_to_idx[ch] for ch in self.text[idx:idx+self.seq_length]]\n",
    "        y = [self.char_to_idx[ch] for ch in self.text[idx+1:idx+self.seq_length+1]]\n",
    "        return torch.tensor(x), torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9172da13",
   "metadata": {
    "id": "9172da13"
   },
   "outputs": [],
   "source": [
    "def prepare_data(text, seq_length, batch_size, val_split):\n",
    "    chars, char_to_idx, idx_to_char = create_char_mappings(text)\n",
    "\n",
    "    # Split data into train and validation\n",
    "    val_size = int(len(text) * val_split)\n",
    "    train_text, val_text = text[:-val_size], text[-val_size:]\n",
    "\n",
    "    train_dataset = CharDataset(train_text, seq_length, char_to_idx)\n",
    "    val_dataset = CharDataset(val_text, seq_length, char_to_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=12, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=12, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader, chars, char_to_idx, idx_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "nEwKFB_8L6AG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3706,
     "status": "ok",
     "timestamp": 1732704246464,
     "user": {
      "displayName": "Hugman Sangkeun Jung",
      "userId": "08689291704194029524"
     },
     "user_tz": -540
    },
    "id": "nEwKFB_8L6AG",
    "outputId": "e75cdff9-3775-461e-d930-0ced534bf74d"
   },
   "outputs": [],
   "source": [
    "# !wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=19zosLuU0z4MxIMKbGVYEGlg52QyfbTIy' -O input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47d03398",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 668,
     "status": "ok",
     "timestamp": 1732704255324,
     "user": {
      "displayName": "Hugman Sangkeun Jung",
      "userId": "08689291704194029524"
     },
     "user_tz": -540
    },
    "id": "47d03398",
    "outputId": "f247f5b3-88ff-4ecb-e8a3-4b2e42a2820b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 1115394\n",
      "Vocabulary size: 65\n",
      "Train dataset size: 1003791\n",
      "Validation dataset size: 111475\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "text = load_data('./input.txt')\n",
    "train_loader, val_loader, chars, char_to_idx, idx_to_char = prepare_data(text, SEQUENCE_LENGTH, BATCH_SIZE, VALIDATION_SPLIT)\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(f\"Total characters: {len(text)}\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Train dataset size: {len(train_loader.dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9442b104",
   "metadata": {
    "id": "9442b104"
   },
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1f0a6a1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 916,
     "status": "ok",
     "timestamp": 1732704257555,
     "user": {
      "displayName": "Hugman Sangkeun Jung",
      "userId": "08689291704194029524"
     },
     "user_tz": -540
    },
    "id": "f1f0a6a1",
    "outputId": "9ef6eb5a-0015-40ad-f143-236e3d38fb7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([512, 64])\n",
      "Target shape: torch.Size([512, 64])\n",
      "Sample 1: ------------------------------\n",
      "Input sequence : ORK:Should I do so, I should belie my thoughts:Comfort's in he\n",
      "Target sequence: RK:Should I do so, I should belie my thoughts:Comfort's in hea\n",
      "\n",
      "Sample 2: ------------------------------\n",
      "Input sequence : t not more than shame to shame it so?Landlord of England art th\n",
      "Target sequence:  not more than shame to shame it so?Landlord of England art tho\n",
      "\n",
      "Sample 3: ------------------------------\n",
      "Input sequence : specially.GREMIO:What's that, I pray?HORTENSIO:Marry, sir,\n",
      "Target sequence: pecially.GREMIO:What's that, I pray?HORTENSIO:Marry, sir, \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to convert index sequence to character sequence\n",
    "def indices_to_text(indices, idx_to_char):\n",
    "    return ''.join([idx_to_char[idx.item()] for idx in indices])\n",
    "\n",
    "# Get a batch of data\n",
    "dataiter = iter(train_loader)\n",
    "batch_x, batch_y = next(dataiter)\n",
    "\n",
    "print(f\"Input shape: {batch_x.shape}\")\n",
    "print(f\"Target shape: {batch_y.shape}\")\n",
    "\n",
    "# Print a few samples from the batch\n",
    "num_samples = 3\n",
    "for i in range(num_samples):\n",
    "    print(f\"Sample {i+1}: ------------------------------\" )\n",
    "    print(\"Input sequence :\", indices_to_text(batch_x[i], idx_to_char).replace('\\n',''))\n",
    "    print(\"Target sequence:\", indices_to_text(batch_y[i], idx_to_char).replace('\\n',''))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5486f73f",
   "metadata": {
    "id": "5486f73f"
   },
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61a52f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_vram_usage(device=\"cuda\"):\n",
    "    allocated = torch.cuda.memory_allocated(device) / (1024**2)  # in MB\n",
    "    reserved = torch.cuda.memory_reserved(device) / (1024**2)    # in MB\n",
    "    max_allocated = torch.cuda.max_memory_allocated(device) / (1024**2)  # in MB\n",
    "    print(f\"Allocated: {allocated:.2f} MB, Reserved: {reserved:.2f} MB, Max Allocated: {max_allocated:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59178a11",
   "metadata": {
    "id": "59178a11"
   },
   "outputs": [],
   "source": [
    "def validate(model, dataloader, criterion, device, epoch, step):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            output, _ = model(x)\n",
    "            loss = criterion(output.view(-1, output.size(-1)), y.view(-1))\n",
    "            losses.append((step, epoch, loss.item()))\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea5e61f5",
   "metadata": {
    "id": "ea5e61f5"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, dataloader, criterion, optimizer, device, epoch, step):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    vram_usage = []\n",
    "    scaler = torch.amp.GradScaler('cuda')\n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch}\", leave=False)\n",
    "    for batch, (x, y) in enumerate(pbar):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.amp.autocast('cuda'):\n",
    "            output, _ = model(x)\n",
    "            loss = criterion(output.view(-1, output.size(-1)), y.view(-1))\n",
    "            \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        step += 1\n",
    "        losses.append((step, epoch, loss.item()))\n",
    "        \n",
    "        # VRAM 사용량을 progress bar의 postfix로 업데이트\n",
    "        allocated = torch.cuda.memory_allocated(device) / (1024**2)\n",
    "        vram_usage.append(allocated)\n",
    "        pbar.set_postfix(loss=f'{loss.item():.4f}', step=step, vram=f'{allocated:.2f} MB')\n",
    "    return losses, step, vram_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbedfd61",
   "metadata": {
    "id": "cbedfd61"
   },
   "source": [
    "## Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c53d8c89",
   "metadata": {
    "id": "c53d8c89"
   },
   "outputs": [],
   "source": [
    "def ___generate(model, char_to_idx, idx_to_char, start_char, max_length, device):\n",
    "    model.eval()\n",
    "    hidden = None\n",
    "    current_char = start_char\n",
    "    generated_text = current_char\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        x = torch.tensor([[char_to_idx[current_char]]]).to(device)\n",
    "        output, hidden = model(x, hidden)\n",
    "        probs = torch.softmax(output[0, -1], dim=0)\n",
    "        next_char_idx = torch.multinomial(probs, 1).item()\n",
    "        current_char = idx_to_char[next_char_idx]\n",
    "        generated_text += current_char\n",
    "\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e91075e8",
   "metadata": {
    "id": "e91075e8"
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, device, epochs):\n",
    "    all_train_losses = []\n",
    "    all_val_losses = []\n",
    "    all_vram_usages = []\n",
    "    step = 0\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        # Training phase with tqdm updates\n",
    "        epoch_train_losses, step, vram_usage = train(model, train_loader, criterion, optimizer, device, epoch, step)\n",
    "        all_train_losses.extend(epoch_train_losses)\n",
    "        all_vram_usages.append(vram_usage)\n",
    "        \n",
    "        # Validation phase\n",
    "        epoch_val_losses = validate(model, val_loader, criterion, device, epoch, step)\n",
    "        all_val_losses.extend(epoch_val_losses)\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        print(f'Epoch {epoch}/{epochs}, Train Loss: {epoch_train_losses[-1][2]:.4f}, '\n",
    "              f'Val Loss: {epoch_val_losses[-1][2]:.4f}, Epoch Time: {epoch_time:.2f}s',\n",
    "              f'Average Vram Usage: {np.mean(vram_usage):.2f}')\n",
    "\n",
    "    train_losses_df = pd.DataFrame(all_train_losses, columns=['step', 'epoch', 'loss_value'])\n",
    "    val_losses_df = pd.DataFrame(all_val_losses, columns=['step', 'epoch', 'loss_value'])\n",
    "    # average_vram_usage = np.mean(all_vram_usages)\n",
    "    return model, train_losses_df, val_losses_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4535c886",
   "metadata": {
    "id": "4535c886"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, char_to_idx, idx_to_char, start_text, device, max_length=500):\n",
    "    model.eval()\n",
    "    current_text = start_text\n",
    "    hidden = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            x = torch.tensor([[char_to_idx[ch] for ch in current_text[-SEQUENCE_LENGTH:]]]).to(device)\n",
    "            output, hidden = model(x, hidden)\n",
    "            probs = torch.softmax(output[0, -1], dim=0)\n",
    "            next_char_idx = torch.multinomial(probs, 1).item()\n",
    "            next_char = idx_to_char[next_char_idx]\n",
    "            current_text += next_char\n",
    "\n",
    "    return current_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6aef1cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_comparison_dict = {}\n",
    "\n",
    "def add_loss_to_comparison(model_name, train_losses_df, val_losses_df):\n",
    "    \"\"\"\n",
    "    Adds training and validation losses from a model to the comparison dictionary.\n",
    "    \"\"\"\n",
    "    loss_comparison_dict[model_name] = {\n",
    "        'train': train_losses_df,\n",
    "        'val': val_losses_df\n",
    "    }\n",
    "\n",
    "def print_final_losses(loss_dict):\n",
    "    for model_name, losses in loss_dict.items():\n",
    "        train_df = losses['train']\n",
    "        val_df = losses['val']\n",
    "        final_train = train_df.groupby('epoch')['loss_value'].last().iloc[-1]\n",
    "        final_val = val_df.groupby('epoch')['loss_value'].last().iloc[-1]\n",
    "        print(f\"{model_name}: Final Train Loss: {final_train:.4f}, Final Val Loss: {final_val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "973b4a80",
   "metadata": {
    "id": "973b4a80"
   },
   "outputs": [],
   "source": [
    "# Function to plot loss curves\n",
    "def plot_loss(train_losses_df, val_losses_df):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    # Plot training losses\n",
    "    for epoch in train_losses_df['epoch'].unique():\n",
    "        epoch_train_losses = train_losses_df[train_losses_df['epoch'] == epoch]\n",
    "        plt.plot(epoch_train_losses['step'], epoch_train_losses['loss_value'],\n",
    "                 color='blue', alpha=0.3)\n",
    "\n",
    "    # scatter training loss at the end of each epoch\n",
    "    last_train_losses = train_losses_df.groupby('epoch').last().reset_index()\n",
    "    plt.scatter(last_train_losses['step'], last_train_losses['loss_value'],\n",
    "                color='blue')\n",
    "\n",
    "    # Plot and scatter validation loss at the end of each epoch\n",
    "    last_val_losses = val_losses_df.groupby('epoch').last().reset_index()\n",
    "    plt.plot(last_val_losses['step'], last_val_losses['loss_value'],\n",
    "             color='orange', label='Validation Loss')\n",
    "    plt.scatter(last_val_losses['step'], last_val_losses['loss_value'],\n",
    "                color='orange')\n",
    "\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Function to print final loss values\n",
    "def print_final_losses(train_losses_df, val_losses_df):\n",
    "    print(\"Final Training Loss:\", train_losses_df.groupby('epoch')['loss_value'].last().iloc[-1])\n",
    "    print(\"Final Validation Loss:\", val_losses_df.groupby('epoch')['loss_value'].last().iloc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5001e203",
   "metadata": {
    "id": "5001e203"
   },
   "outputs": [],
   "source": [
    "# Function to plot loss curves for multiple models stored in loss_comparison_dict\n",
    "def plot_loss_comparisons():\n",
    "    \"\"\"\n",
    "    Plots the training loss curves and average validation loss per epoch for multiple models added to the loss comparison dictionary.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    # Get the last model in the dictionary (for special final-point highlighting)\n",
    "    last_model_name = list(loss_comparison_dict.keys())[-1]\n",
    "\n",
    "    # Loop through each model in the loss dictionary\n",
    "    for model_name, losses in loss_comparison_dict.items():\n",
    "        train_losses_df = losses['train']\n",
    "        val_losses_df = losses['val']\n",
    "\n",
    "        # Plot training losses for each model\n",
    "        plt.plot(train_losses_df['step'], train_losses_df['loss_value'],\n",
    "                 label=f'{model_name} train', linestyle='-', alpha=0.7)\n",
    "\n",
    "        # Scatter training loss at the end of each epoch\n",
    "        last_train_losses = train_losses_df.groupby('epoch').last().reset_index()\n",
    "        plt.scatter(last_train_losses['step'], last_train_losses['loss_value'], marker='o', s=50)\n",
    "\n",
    "        # Compute average validation loss per epoch (using the last step of each epoch for x-axis)\n",
    "        avg_val_losses = val_losses_df.groupby('epoch').agg({'loss_value': 'mean', 'step': 'last'}).reset_index()\n",
    "        # Scatter the average validation loss for each epoch\n",
    "        plt.scatter(avg_val_losses['step'], avg_val_losses['loss_value'], marker='s', s=50,\n",
    "                    label=f'{model_name} val avg')\n",
    "\n",
    "        # For the last model, highlight the final training loss with a star\n",
    "        if model_name == last_model_name:\n",
    "            final_step = train_losses_df['step'].iloc[-1]\n",
    "            final_loss = train_losses_df['loss_value'].iloc[-1]\n",
    "            plt.scatter(final_step, final_loss, marker='*', s=100, color='red', zorder=5)\n",
    "\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Comparison')\n",
    "    plt.legend()  # Legend shows both training and validation average labels\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16777ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_separate_train_val(loss_dict):\n",
    "    \"\"\"\n",
    "    모델별 Training Loss와 Validation Loss를 각각 별도의 그래프로 그립니다.\n",
    "    단, Validation Loss는 에포크별 평균으로 계산합니다.\n",
    "    \"\"\"\n",
    "    # 1. Training Loss Plot (원본 그대로)\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    plt.subplot(1, 2, 1)  # 1행 2열 중 첫 번째\n",
    "    for model_name, losses in loss_dict.items():\n",
    "        train_df = losses['train']\n",
    "        steps_train = train_df['step'].values\n",
    "        loss_train = train_df['loss_value'].values\n",
    "        plt.plot(steps_train, loss_train, label=f'{model_name} Train')\n",
    "    plt.title('Training Loss Comparison')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # 2. Validation Loss Plot (에포크별 평균 처리)\n",
    "    plt.subplot(1, 2, 2)  # 1행 2열 중 두 번째\n",
    "    for model_name, losses in loss_dict.items():\n",
    "        val_df = losses['val']\n",
    "        # 에포크별 평균 loss와 마지막 step을 계산\n",
    "        val_avg = val_df.groupby('epoch').agg({'loss_value': 'mean', 'step': 'last'}).reset_index()\n",
    "        plt.plot(val_avg['step'], val_avg['loss_value'], label=f'{model_name} Val')\n",
    "    plt.title('Validation Loss (Epoch Avg) Comparison')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1eb8ad",
   "metadata": {
    "id": "aa1eb8ad"
   },
   "source": [
    "## Model 1: GRU Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e017e89b",
   "metadata": {
    "id": "e017e89b"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GRUDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=2, dropout=0.1):\n",
    "        super(GRUDecoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # Use multiple GRU layers with dropout for increased capacity.\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers=num_layers,\n",
    "                          dropout=dropout if num_layers > 1 else 0.0, batch_first=True)\n",
    "        # Additional fully connected layers to further increase parameters.\n",
    "        self.fc1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Layer normalization after GRU output.\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        # Embedding lookup\n",
    "        embed = self.embedding(x)\n",
    "        # GRU forward pass\n",
    "        output, hidden = self.gru(embed, hidden)\n",
    "        # Apply dropout and normalization\n",
    "        output = self.dropout(output)\n",
    "        output = self.layer_norm(output)\n",
    "        # Additional FC layer with non-linearity, then final output layer.\n",
    "        output = F.gelu(self.fc1(output))\n",
    "        output = self.fc2(output)\n",
    "        return output, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a3581e4",
   "metadata": {
    "id": "9a3581e4"
   },
   "outputs": [],
   "source": [
    "### Model Initialization\n",
    "gru = GRUDecoder(vocab_size, EMBEDDING_DIM, HIDDEN_DIM).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(gru.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2fe18507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "GRUDecoder                               [512, 64, 65]             --\n",
       "├─Embedding: 1-1                         [512, 64, 128]            8,320\n",
       "├─GRU: 1-2                               [512, 64, 256]            691,200\n",
       "├─Dropout: 1-3                           [512, 64, 256]            --\n",
       "├─LayerNorm: 1-4                         [512, 64, 256]            512\n",
       "├─Linear: 1-5                            [512, 64, 256]            65,792\n",
       "├─Linear: 1-6                            [512, 64, 65]             16,705\n",
       "==========================================================================================\n",
       "Total params: 782,529\n",
       "Trainable params: 782,529\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 22.70\n",
       "==========================================================================================\n",
       "Input size (MB): 0.26\n",
       "Forward/backward pass size (MB): 251.92\n",
       "Params size (MB): 3.13\n",
       "Estimated Total Size (MB): 255.31\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(gru, input_size=(BATCH_SIZE, SEQUENCE_LENGTH), dtypes=[torch.long])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec86dc8e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 121082,
     "status": "ok",
     "timestamp": 1732704395031,
     "user": {
      "displayName": "Hugman Sangkeun Jung",
      "userId": "08689291704194029524"
     },
     "user_tz": -540
    },
    "id": "ec86dc8e",
    "outputId": "d3a32ebf-e2ad-4e8b-fb04-f2651e0ac049"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    }
   ],
   "source": [
    "## Training Loop\n",
    "trained_model, train_losses_df, val_losses_df = train_model(gru, train_loader, val_loader, criterion, optimizer, device, EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70664998",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 489,
     "status": "ok",
     "timestamp": 1732704395512,
     "user": {
      "displayName": "Hugman Sangkeun Jung",
      "userId": "08689291704194029524"
     },
     "user_tz": -540
    },
    "id": "70664998",
    "outputId": "a970b003-a23c-47b7-900e-5ed705866838"
   },
   "outputs": [],
   "source": [
    "# Text generation using validation data\n",
    "val_sample, _ = next(iter(val_loader))\n",
    "start_text = ''.join([idx_to_char[idx.item()] for idx in val_sample[0][:SEQUENCE_LENGTH]])\n",
    "generated_text = generate_text(trained_model, char_to_idx, idx_to_char, start_text, device)\n",
    "print(f\"Generated text (starting with validation data [{start_text}]):\")\n",
    "print(\"-\"*50)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcca4e52",
   "metadata": {
    "id": "fcca4e52"
   },
   "outputs": [],
   "source": [
    "# After training a model (e.g., LSTM without RMSNorm), add its losses\n",
    "add_loss_to_comparison('GRU', train_losses_df, val_losses_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b72703",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1732704396861,
     "user": {
      "displayName": "Hugman Sangkeun Jung",
      "userId": "08689291704194029524"
     },
     "user_tz": -540
    },
    "id": "98b72703",
    "outputId": "ece1620a-8690-4750-97c1-ada645d59a08"
   },
   "outputs": [],
   "source": [
    "# Decoder Input/Output Example\n",
    "sample_input, _ = next(iter(val_loader))\n",
    "sample_input = sample_input[0].unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "trained_model.eval()\n",
    "with torch.no_grad():\n",
    "    output, _ = trained_model(sample_input)\n",
    "\n",
    "print(\"\\nSample Input:\")\n",
    "print(''.join([idx_to_char[idx.item()] for idx in sample_input[0]]))\n",
    "\n",
    "print(\"\\nModel Output (logits for next character prediction):\")\n",
    "print(output.shape)\n",
    "print(output[0, 0, :10])  # Print first 10 logits of the first timestep\n",
    "\n",
    "print(\"\\nPredicted next character:\")\n",
    "predicted_char_idx = torch.argmax(output[0, -1]).item()\n",
    "print(idx_to_char[predicted_char_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23a4aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_comparisons()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868a5338",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "executionInfo": {
     "elapsed": 899,
     "status": "ok",
     "timestamp": 1732704397758,
     "user": {
      "displayName": "Hugman Sangkeun Jung",
      "userId": "08689291704194029524"
     },
     "user_tz": -540
    },
    "id": "868a5338",
    "outputId": "06397d97-49fb-4f85-b2b3-2b27ca481fe0"
   },
   "outputs": [],
   "source": [
    "plot_separate_train_val(loss_comparison_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39178b7a",
   "metadata": {},
   "source": [
    "## Model 2: GPT-2(2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5095e1c",
   "metadata": {
    "id": "a5095e1c"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, ffn_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim, ffn_dim)\n",
    "        self.fc2 = nn.Linear(ffn_dim, dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.fc1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, ffn_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=dim, num_heads=num_heads, dropout=dropout, batch_first=True)\n",
    "        self.ln2 = nn.LayerNorm(dim)\n",
    "        self.ffn = FeedForward(dim, ffn_dim, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor = None, return_attn: bool = False):\n",
    "        # Self-attention with residual connection\n",
    "        x_norm = self.ln1(x)\n",
    "        attn_out, attn_weights = self.attn(x_norm, x_norm, x_norm, attn_mask=mask)\n",
    "        x = x + self.dropout(attn_out)\n",
    "        # Feed-forward network with residual connection\n",
    "        x = x + self.dropout(self.ffn(self.ln2(x)))\n",
    "        if return_attn:\n",
    "            return x, attn_weights\n",
    "        return x\n",
    "\n",
    "class ClassicTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, max_seq_len, dim, num_layers, num_heads, ffn_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, dim)\n",
    "        self.pos_embedding = nn.Embedding(max_seq_len, dim)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(dim, num_heads, ffn_dim, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(dim)\n",
    "        self.output = nn.Linear(dim, vocab_size, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, return_attn: bool = False):\n",
    "        # x: [B, L]\n",
    "        B, L = x.shape\n",
    "        pos_ids = torch.arange(0, L, device=x.device).unsqueeze(0).expand(B, L)\n",
    "        # Sum token and positional embeddings\n",
    "        h = self.token_embedding(x) + self.pos_embedding(pos_ids)\n",
    "        h = self.dropout(h)\n",
    "        \n",
    "        # Causal mask (upper triangular with -inf)\n",
    "        if L > 1:\n",
    "            mask = torch.triu(torch.full((L, L), float('-inf'), device=x.device), diagonal=1)\n",
    "        else:\n",
    "            mask = None\n",
    "        \n",
    "        attn_maps = []\n",
    "        for layer in self.layers:\n",
    "            if return_attn:\n",
    "                h, attn_map = layer(h, mask, return_attn=True)\n",
    "                attn_maps.append(attn_map)\n",
    "            else:\n",
    "                h = layer(h, mask)\n",
    "            \n",
    "        h = self.ln_f(h)\n",
    "        logits = self.output(h)\n",
    "        if return_attn:\n",
    "            return logits, attn_maps\n",
    "        return logits, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d8a352",
   "metadata": {
    "id": "a2d8a352"
   },
   "outputs": [],
   "source": [
    "def generate_text_attention(model, char_to_idx, idx_to_char, start_text, device, max_length=500):\n",
    "    model.eval()\n",
    "    current_text = start_text\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            # Convert the last sequence of characters to indices and feed it to the model\n",
    "            x = torch.tensor([[char_to_idx[ch] for ch in current_text[-SEQUENCE_LENGTH:]]]).to(device)\n",
    "            output = model(x)[0]  # No hidden state needed for attention-based models\n",
    "            probs = torch.softmax(output[0, -1], dim=0)\n",
    "            next_char_idx = torch.multinomial(probs, 1).item()\n",
    "            next_char = idx_to_char[next_char_idx]\n",
    "            current_text += next_char\n",
    "\n",
    "    return current_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c936b1",
   "metadata": {
    "id": "32c936b1"
   },
   "outputs": [],
   "source": [
    "def train_and_test(model_desc, model, start_text):\n",
    "    # Initialize the model\n",
    "    model = model.to(device)\n",
    "    # Use the same optimizer and criterion\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    # Train the model\n",
    "    trained_model, train_losses_df, val_losses_df = train_model(\n",
    "        model, train_loader, val_loader, criterion, optimizer, device, EPOCHS\n",
    "    )\n",
    "\n",
    "    # Generate text\n",
    "    generated_text = generate_text_attention(trained_model, char_to_idx, idx_to_char, start_text, device)\n",
    "    print(f\"Generated text [{start_text}]:\")\n",
    "    print(\"-\"*50)\n",
    "    print(generated_text)\n",
    "\n",
    "    add_loss_to_comparison(model_desc, train_losses_df, val_losses_df)\n",
    "\n",
    "    # Plot loss comparisons including this model\n",
    "    plot_loss_comparisons()\n",
    "    \n",
    "    plot_separate_train_val(loss_comparison_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5f9f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for Transformer\n",
    "NUM_LAYERS = 4\n",
    "NUM_HEADS = 4\n",
    "FFN_DIM = 480\n",
    "DROPOUT = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2f0392",
   "metadata": {},
   "outputs": [],
   "source": [
    "classic = ClassicTransformer(vocab_size, SEQUENCE_LENGTH, EMBEDDING_DIM, NUM_LAYERS, NUM_HEADS, FFN_DIM, DROPOUT).to(device)\n",
    "classic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12dcf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# Describe the model\n",
    "summary(classic.to(device), input_size=(BATCH_SIZE, SEQUENCE_LENGTH), dtypes=[torch.long])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528c01d4",
   "metadata": {
    "id": "528c01d4",
    "outputId": "3f2819ba-5a87-4666-d333-ccfbb1f72305"
   },
   "outputs": [],
   "source": [
    "train_and_test(\"CLASSIC\", classic, start_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39911f55",
   "metadata": {
    "id": "39911f55"
   },
   "source": [
    "## Model 3: Modern Transformer(LLaMA - 2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc30f0f8",
   "metadata": {
    "id": "fc30f0f8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    DIM = EMBEDDING_DIM \n",
    "    # FFN_DIM = HIDDEN_DIM\n",
    "    FFN_DIM = 480\n",
    "    NUM_HEADS = NUM_HEADS \n",
    "    NUM_LAYERS = NUM_LAYERS - 1\n",
    "\n",
    "    NUM_KV_HEADS = NUM_HEADS \n",
    "    VOCAB_SIZE = vocab_size\n",
    "    NORM_EPS = 1e-5 # LLaMA: 1e-5\n",
    "    ROPE_THETA = 10000 # LLaMA: 500000, ROPOMER: 10000\n",
    "\n",
    "    MAX_BATCH_SIZE = BATCH_SIZE\n",
    "    MAX_SEQ_LEN = SEQUENCE_LENGTH # depending on the DATASET\n",
    "    NUM_KV_HEAD_REP = NUM_HEADS // NUM_KV_HEADS\n",
    "\n",
    "    HEAD_DIM = DIM // NUM_HEADS\n",
    "    DROPOUT = DROPOUT\n",
    "    DEVICE = device\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "    def _norm(self, x: torch.Tensor):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.weight * self._norm(x.float()).type_as(x)\n",
    "    \n",
    "def precompute_freqs_cis(head_dim: int, seq_len: int, theta: float = 10000.0, device: str = \"cuda\"):\n",
    "    if head_dim % 2 != 0:\n",
    "        raise ValueError(\"head_dim must be even for rotary embeddings.\")\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, head_dim, 2).float() / head_dim)).to(device)\n",
    "    t = torch.arange(seq_len, device=device, dtype=torch.float32)\n",
    "    freqs = torch.outer(t, freqs)\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
    "    return freqs_cis\n",
    "\n",
    "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
    "    L = x.shape[1]\n",
    "    return freqs_cis.view(1, L, 1, x.shape[-1] // 2)\n",
    "\n",
    "def apply_rotary_emb(x: torch.Tensor, freqs_cis: torch.Tensor):\n",
    "    x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
    "    freqs = reshape_for_broadcast(freqs_cis, x)\n",
    "    x_rotated = x_complex * freqs\n",
    "    x_out = torch.view_as_real(x_rotated).reshape(x.shape)\n",
    "    return x_out.type_as(x)\n",
    "\n",
    "def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    B, L, nk, d = x.shape\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "    return x[:, :, :, None, :].expand(B, L, nk, n_rep, d).reshape(B, L, nk * n_rep, d)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, ffn_dim, dropout):\n",
    "        super().__init__()\n",
    "        hidden_dim = ffn_dim\n",
    "        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n",
    "        self.w3 = nn.Linear(dim, hidden_dim, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x: [B, L, D]\n",
    "        return self.w2(F.silu(self.w1(x)) * self.dropout(self.w3(x)))\n",
    "    \n",
    "# GQA with KV cache and attention map output option\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, n_heads, n_kv_heads, n_rep, dim, dropout, batch, seq_len, device):\n",
    "        super().__init__()\n",
    "        self.n_heads_q = n_heads\n",
    "        self.n_kv_heads = n_kv_heads\n",
    "        self.n_rep = n_rep\n",
    "        self.head_dim = dim // n_heads\n",
    "\n",
    "        self.wq = nn.Linear(dim, n_heads * self.head_dim, bias=False)\n",
    "        self.wk = nn.Linear(dim, n_kv_heads * self.head_dim, bias=False)\n",
    "        self.wv = nn.Linear(dim, n_kv_heads * self.head_dim, bias=False)\n",
    "        self.wo = nn.Linear(n_heads * self.head_dim, dim, bias=False)\n",
    "        self.attn_dropout = dropout\n",
    "        \n",
    "        # KV Cache\n",
    "        self.cache_k = torch.zeros(batch, seq_len, n_kv_heads, self.head_dim, device=device)\n",
    "        self.cache_v = torch.zeros(batch, seq_len, n_kv_heads, self.head_dim, device=device)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, start_pos, freqs_cis: torch.Tensor, mask, return_attn=False):\n",
    "        B, L, _ = x.shape\n",
    "        xq = self.wq(x).view(B, L, self.n_heads_q, self.head_dim)\n",
    "        xk = self.wk(x).view(B, L, self.n_kv_heads, self.head_dim)\n",
    "        xv = self.wv(x).view(B, L, self.n_kv_heads, self.head_dim)\n",
    "        \n",
    "        # Apply rotary embeddings\n",
    "        xq = apply_rotary_emb(xq, freqs_cis)\n",
    "        xk = apply_rotary_emb(xk, freqs_cis)\n",
    "        \n",
    "        # Update KV cache        \n",
    "        self.cache_k[:B, start_pos:start_pos+L] = xk\n",
    "        self.cache_v[:B, start_pos:start_pos+L] = xv\n",
    "        \n",
    "        # GQA: Adjust dimensions for attention computation\n",
    "        xq = xq.transpose(1, 2)   # shape: [B, n_heads, L, head_dim]\n",
    "        xk = torch.repeat_interleave(xk, repeats=self.n_rep, dim=2).transpose(1, 2)  # shape: [B, n_heads, L, head_dim]\n",
    "        xv = torch.repeat_interleave(xv, repeats=self.n_rep, dim=2).transpose(1, 2)  # shape: [B, n_heads, L, head_dim]\n",
    "\n",
    "        # Compute scaled dot-product attention manually to capture attention weights\n",
    "        scores = torch.matmul(xq, xk.transpose(-2, -1)) / math.sqrt(self.head_dim)  # [B, n_heads, L, L]\n",
    "        if mask is not None:\n",
    "            scores = scores + mask.unsqueeze(0).unsqueeze(0)  # Broadcast mask\n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        attn_weights = F.dropout(attn_weights, self.attn_dropout, training=self.training)\n",
    "        attn_output = torch.matmul(attn_weights, xv)  # [B, n_heads, L, head_dim]\n",
    "        \n",
    "        # Reshape attention output and project\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(B, L, -1)\n",
    "        output = self.wo(attn_output)  # [B, L, D]\n",
    "        if return_attn:\n",
    "            return output, attn_weights\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, args: 'ModelArgs'):\n",
    "        super().__init__()\n",
    "        self.attention = SelfAttention(\n",
    "            args.NUM_HEADS, \n",
    "            args.NUM_KV_HEADS, \n",
    "            args.NUM_KV_HEAD_REP, \n",
    "            args.DIM, \n",
    "            args.DROPOUT, \n",
    "            args.MAX_BATCH_SIZE, \n",
    "            args.MAX_SEQ_LEN, \n",
    "            args.DEVICE\n",
    "        )\n",
    "        self.ffn = FeedForward(args.DIM, args.FFN_DIM, args.DROPOUT)\n",
    "        self.attention_norm = RMSNorm(args.DIM, args.NORM_EPS)\n",
    "        self.ffn_norm = RMSNorm(args.DIM, args.NORM_EPS)\n",
    "        self.res_dropout = nn.Dropout(args.DROPOUT)\n",
    "    def forward(self, x: torch.Tensor, start_pos, freqs_cis: torch.Tensor, mask, return_attn=False):\n",
    "        if return_attn:\n",
    "            attn_out, attn_map = self.attention(self.attention_norm(x), start_pos, freqs_cis, mask, return_attn=True)\n",
    "            h = x + self.res_dropout(attn_out)\n",
    "            h = h + self.res_dropout(self.ffn(self.ffn_norm(h)))\n",
    "            return h, attn_map\n",
    "        else:\n",
    "            h = x + self.res_dropout(self.attention(self.attention_norm(x), start_pos, freqs_cis, mask))\n",
    "            h = h + self.res_dropout(self.ffn(self.ffn_norm(h)))\n",
    "            return h\n",
    "\n",
    "class LlamaTransformer(nn.Module):\n",
    "    def __init__(self, args: 'ModelArgs'):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.tok_embeddings = nn.Embedding(args.VOCAB_SIZE, args.DIM)\n",
    "        self.layers = nn.ModuleList([TransformerBlock(args) for _ in range(args.NUM_LAYERS)])\n",
    "        self.norm = RMSNorm(args.DIM, args.NORM_EPS)\n",
    "        self.output = nn.Linear(args.DIM, args.VOCAB_SIZE, bias=False)\n",
    "        self.freqs_cis = precompute_freqs_cis(args.HEAD_DIM, args.MAX_SEQ_LEN, args.ROPE_THETA, args.DEVICE)\n",
    "        self.device = args.DEVICE\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, start_pos=0, return_attn=False):\n",
    "        B, L = x.shape\n",
    "        h = self.tok_embeddings(x)  # [B, L, D]\n",
    "        freqs_cis = self.freqs_cis[start_pos:start_pos+L]\n",
    "        \n",
    "        mask = None\n",
    "        if L > 1:\n",
    "            mask = torch.full((L, L), float('-inf'), device=self.device)\n",
    "            mask = torch.triu(mask, 1)\n",
    "        attn_maps = []\n",
    "        for layer in self.layers:\n",
    "            if return_attn:\n",
    "                h, attn_map = layer(h, start_pos, freqs_cis, mask, return_attn=True)\n",
    "                attn_maps.append(attn_map)\n",
    "            else:\n",
    "                h = layer(h, start_pos, freqs_cis, mask)\n",
    "        logits = self.output(self.norm(h)).float()\n",
    "        if return_attn:\n",
    "            return logits, attn_maps\n",
    "        return logits, _\n",
    "        \n",
    "    @torch.inference_mode()\n",
    "    def forward_inference(self, x: torch.Tensor, start_pos=0, return_attn=False):\n",
    "        B, L = x.shape\n",
    "        h = self.tok_embeddings(x)  # [B, L, D]\n",
    "        freqs_cis = self.freqs_cis[start_pos:start_pos+L]\n",
    "        \n",
    "        mask = None\n",
    "        if L > 1:\n",
    "            mask = torch.full((L, L), float('-inf'), device=self.device)\n",
    "            mask = torch.triu(mask, 1)\n",
    "        attn_maps = []\n",
    "        for layer in self.layers:\n",
    "            if return_attn:\n",
    "                h, attn_map = layer(h, start_pos, freqs_cis, mask, return_attn=True)\n",
    "                attn_maps.append(attn_map)\n",
    "            else:\n",
    "                h = layer(h, start_pos, freqs_cis, mask)\n",
    "        logits = self.output(self.norm(h)).float()\n",
    "        if return_attn:\n",
    "            return logits, attn_maps\n",
    "        return logits, _\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b4f114",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = ModelArgs()\n",
    "llama = LlamaTransformer(PARAMS).to(device)\n",
    "llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087c3927",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# Describe the model\n",
    "summary(llama.to(device), input_size=(BATCH_SIZE, SEQUENCE_LENGTH), dtypes=[torch.long])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b361db8b",
   "metadata": {
    "id": "b361db8b",
    "outputId": "33c6269c-5979-4100-b6e7-a89b6b4ed962"
   },
   "outputs": [],
   "source": [
    "train_and_test(\"LLaMA\", llama, start_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8775d573",
   "metadata": {},
   "source": [
    "## Model 4: Diff Transformer(2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af4220d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArgs:\n",
    "    DIM = EMBEDDING_DIM \n",
    "    # FFN_DIM = HIDDEN_DIM\n",
    "    FFN_DIM = 320\n",
    "    NUM_HEADS = NUM_HEADS \n",
    "    NUM_LAYERS = NUM_LAYERS - 1\n",
    "\n",
    "    NUM_KV_HEADS = NUM_HEADS \n",
    "    VOCAB_SIZE = vocab_size\n",
    "    NORM_EPS = 1e-5 # LLaMA: 1e-5\n",
    "    ROPE_THETA = 10000 # LLaMA: 500000, ROPOMER: 10000\n",
    "\n",
    "    MAX_BATCH_SIZE = BATCH_SIZE\n",
    "    MAX_SEQ_LEN = SEQUENCE_LENGTH # depending on the DATASET\n",
    "    NUM_KV_HEAD_REP = NUM_HEADS // NUM_KV_HEADS\n",
    "\n",
    "    HEAD_DIM = DIM // NUM_HEADS\n",
    "    DROPOUT = DROPOUT\n",
    "    DEVICE = device\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "    def _norm(self, x: torch.Tensor):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.weight * self._norm(x.float()).type_as(x)\n",
    "\n",
    "# Precompute rotary embedding frequencies.\n",
    "def precompute_freqs_cis(head_dim: int, seq_len: int, theta: float = 10000.0, device: str = \"cuda\"):\n",
    "    if head_dim % 2 != 0:\n",
    "        raise ValueError(\"head_dim must be even for rotary embeddings.\")\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, head_dim, 2).float() / head_dim)).to(device)\n",
    "    t = torch.arange(seq_len, device=device, dtype=torch.float32)\n",
    "    freqs = torch.outer(t, freqs)  # [seq_len, head_dim//2]\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
    "    return freqs_cis  # [seq_len, head_dim // 2]\n",
    "\n",
    "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
    "    L = x.shape[1]\n",
    "    return freqs_cis.view(1, L, 1, x.shape[-1] // 2)  # [1, L, 1, head_dim]\n",
    "\n",
    "def apply_rotary_emb(x: torch.Tensor, freqs_cis: torch.Tensor, device: str = \"cuda\"):\n",
    "    # x: [..., 2*d]\n",
    "    x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
    "    freqs = reshape_for_broadcast(freqs_cis, x)\n",
    "    x_rotated = x_complex * freqs\n",
    "    x_out = torch.view_as_real(x_rotated).reshape(x.shape)\n",
    "    return x_out.type_as(x).to(device)\n",
    "\n",
    "def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    # x: [B, L, n_kv, d]\n",
    "    B, L, nk, d = x.shape\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "    return x[:, :, :, None, :].expand(B, L, nk, n_rep, d).reshape(B, L, nk * n_rep, d)\n",
    "\n",
    "# FeedForward network.\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, ffn_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(dim, ffn_dim, bias=False)\n",
    "        self.w2 = nn.Linear(ffn_dim, dim, bias=False)\n",
    "        self.w3 = nn.Linear(dim, ffn_dim, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.w2(F.silu(self.w1(x)) * self.dropout(self.w3(x)))\n",
    "\n",
    "# Diffusion-based Self-Attention.\n",
    "class DiffSelfAttention(nn.Module):\n",
    "    def __init__(self, n_heads, n_kv_heads, n_rep, dim, dropout, batch, seq_len, device, depth=0):\n",
    "        super().__init__()\n",
    "        self.n_heads_q = n_heads\n",
    "        self.n_kv_heads = n_kv_heads\n",
    "        self.n_rep = n_rep  # typically n_heads // n_kv_heads\n",
    "        self.head_dim = dim // n_heads  # standard head dimension\n",
    "\n",
    "        # For diff attention, project Q to 2*n_heads*head_dim,\n",
    "        # and K, V to 2*n_kv_heads*head_dim.\n",
    "        self.wq = nn.Linear(dim, 2 * n_heads * self.head_dim, bias=False)\n",
    "        self.wk = nn.Linear(dim, 2 * n_kv_heads * self.head_dim, bias=False)\n",
    "        self.wv = nn.Linear(dim, 2 * n_kv_heads * self.head_dim, bias=False)\n",
    "        # Output projection expects input dim = 2*n_heads*head_dim.\n",
    "        self.wo = nn.Linear(2 * n_heads * self.head_dim, dim, bias=False)\n",
    "        self.attn_dropout = dropout\n",
    "\n",
    "        # KV Cache (for inference with caching)\n",
    "        self.cache_k = torch.zeros(batch, seq_len, n_kv_heads, 2 * self.head_dim, device=device)\n",
    "        self.cache_v = torch.zeros(batch, seq_len, n_kv_heads, 2 * self.head_dim, device=device)\n",
    "\n",
    "        # Lambda parameter as in diff attention paper.\n",
    "        self.lamdba = 0.8 - 0.6 * math.exp(-0.3 * depth)\n",
    "        # (Lambda parameters below could be used for further modulation.)\n",
    "        self.lambda_q1 = nn.Parameter(torch.zeros(self.head_dim, dtype=torch.float32).normal_(mean=0, std=0.1))\n",
    "        self.lambda_k1 = nn.Parameter(torch.zeros(self.head_dim, dtype=torch.float32).normal_(mean=0, std=0.1))\n",
    "        self.lambda_q2 = nn.Parameter(torch.zeros(self.head_dim, dtype=torch.float32).normal_(mean=0, std=0.1))\n",
    "        self.lambda_k2 = nn.Parameter(torch.zeros(self.head_dim, dtype=torch.float32).normal_(mean=0, std=0.1))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, start_pos: int, freqs_cis: torch.Tensor, mask, return_attn=False):\n",
    "        B, L, _ = x.shape\n",
    "        # Project and reshape:\n",
    "        xq = self.wq(x).view(B, L, self.n_heads_q, 2 * self.head_dim)\n",
    "        xk = self.wk(x).view(B, L, self.n_kv_heads, 2 * self.head_dim)\n",
    "        xv = self.wv(x).view(B, L, self.n_kv_heads, 2 * self.head_dim)\n",
    "        \n",
    "        # Apply rotary embeddings on queries and keys.\n",
    "        xq = apply_rotary_emb(xq, freqs_cis, device=x.device)\n",
    "        xk = apply_rotary_emb(xk, freqs_cis, device=x.device)\n",
    "        \n",
    "        # Update KV cache.\n",
    "        self.cache_k[:B, start_pos:start_pos+L] = xk\n",
    "        self.cache_v[:B, start_pos:start_pos+L] = xv\n",
    "        \n",
    "        # Grouped Query Attention:\n",
    "        xq = xq.transpose(1, 2)\n",
    "        xk = torch.repeat_interleave(xk, repeats=self.n_rep, dim=2).transpose(1, 2)\n",
    "        xv = torch.repeat_interleave(xv, repeats=self.n_rep, dim=2).transpose(1, 2)\n",
    "        \n",
    "        # Split Q and K along last dimension into two halves.\n",
    "        Q1, Q2 = xq.chunk(2, dim=-1)\n",
    "        K1, K2 = xk.chunk(2, dim=-1)\n",
    "        \n",
    "        s = 1 / math.sqrt(self.head_dim)\n",
    "        scores1 = torch.matmul(Q1, K1.transpose(-2, -1)) * s\n",
    "        scores2 = torch.matmul(Q2, K2.transpose(-2, -1)) * s\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores1 = scores1 + mask.unsqueeze(0).unsqueeze(0)\n",
    "            scores2 = scores2 + mask.unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        attn1 = F.softmax(scores1, dim=-1)\n",
    "        attn2 = F.softmax(scores2, dim=-1)\n",
    "        \n",
    "        # Combine the two attention scores with lambda.\n",
    "        lambda_1 = torch.exp(torch.sum(self.lambda_q1 * self.lambda_k1, dim=-1).float()).type_as(xq)\n",
    "        lambda_2 = torch.exp(torch.sum(self.lambda_q2 * self.lambda_k2, dim=-1).float()).type_as(xq)\n",
    "        lambda_full = lambda_1 - lambda_2 + self.lambda_init\n",
    "        attn_weights = attn1 - lambda_full * attn2\n",
    "        attn_weights = F.dropout(attn_weights, self.attn_dropout, training=self.training)\n",
    "        \n",
    "        # Compute attention output.\n",
    "        attn_output = torch.matmul(attn_weights, xv)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(B, L, -1)\n",
    "        \n",
    "        if return_attn:\n",
    "            # Return four maps:\n",
    "            # [첫 번째 맵(attn1), 두 번째 맵(attn2), 두 번째 맵에 람다를 곱한 결과, 최종 결과(attn_weights)]\n",
    "            return self.wo(attn_output), [attn1, attn2, self.lamdba * attn2, attn_weights]\n",
    "        else:\n",
    "            return self.wo(attn_output)\n",
    "\n",
    "# Transformer Block.\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, args, depth: int = 0):\n",
    "        super().__init__()\n",
    "        self.attn_norm = RMSNorm(args.DIM, eps=args.NORM_EPS)\n",
    "        self.ffn_norm = RMSNorm(args.DIM, eps=args.NORM_EPS)\n",
    "        self.attention = DiffSelfAttention(\n",
    "            n_heads=args.NUM_HEADS,\n",
    "            n_kv_heads=args.NUM_KV_HEADS if args.NUM_KV_HEADS is not None else args.NUM_HEADS,\n",
    "            n_rep=args.NUM_KV_HEAD_REP,\n",
    "            dim=args.DIM,\n",
    "            dropout=args.DROPOUT,\n",
    "            batch=args.MAX_BATCH_SIZE,\n",
    "            seq_len=args.MAX_SEQ_LEN,\n",
    "            device=args.DEVICE,\n",
    "            depth=depth\n",
    "        )\n",
    "        self.ffn = FeedForward(args.DIM, args.FFN_DIM, args.DROPOUT)\n",
    "        self.res_dropout = nn.Dropout(args.DROPOUT)\n",
    "    def forward(self, x: torch.Tensor, start_pos: int, freqs_cis: torch.Tensor, mask, return_attn=False):\n",
    "        if return_attn:\n",
    "            attn_out, attn_map = self.attention(self.attn_norm(x), start_pos, freqs_cis, mask, return_attn=True)\n",
    "            h = x + self.res_dropout(attn_out)\n",
    "            h = h + self.res_dropout(self.ffn(self.ffn_norm(h)))\n",
    "            return h, attn_map\n",
    "        else:\n",
    "            h = x + self.res_dropout(self.attention(self.attn_norm(x), start_pos, freqs_cis, mask))\n",
    "            h = h + self.res_dropout(self.ffn(self.ffn_norm(h)))\n",
    "            return h\n",
    "\n",
    "# Diffusion Transformer.\n",
    "class DiffTransformer(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.tok_embeddings = nn.Embedding(args.VOCAB_SIZE, args.DIM)\n",
    "        self.layers = nn.ModuleList([TransformerBlock(args, depth=i) for i in range(args.NUM_LAYERS)])\n",
    "        self.norm = RMSNorm(args.DIM, eps=args.NORM_EPS)\n",
    "        self.output = nn.Linear(args.DIM, args.VOCAB_SIZE, bias=False)\n",
    "        # IMPORTANT: For diff attention, queries/keys have dimension 2*head_dim.\n",
    "        # Therefore, precompute freqs_cis with head_dim = 2 * (DIM // NUM_HEADS).\n",
    "        self.freqs_cis = precompute_freqs_cis(2 * (args.DIM // args.NUM_HEADS), args.MAX_SEQ_LEN, args.ROPE_THETA, device=args.DEVICE)\n",
    "        self.device = args.DEVICE\n",
    "\n",
    "    def forward(self, x: torch.Tensor, start_pos: int = 0, return_attn=False):\n",
    "        B, L = x.shape\n",
    "        h = self.tok_embeddings(x)  # [B, L, DIM]\n",
    "        freqs_cis = self.freqs_cis[start_pos:start_pos+L]\n",
    "        mask = None\n",
    "        if L > 1:\n",
    "            mask = torch.triu(torch.full((L, L), float('-inf'), device=self.device), diagonal=1)\n",
    "        attn_maps = []\n",
    "        for layer in self.layers:\n",
    "            if return_attn:\n",
    "                h, attn_map = layer(h, start_pos, freqs_cis, mask, return_attn=True)\n",
    "                attn_maps.append(attn_map)\n",
    "            else:\n",
    "                h = layer(h, start_pos, freqs_cis, mask)\n",
    "        logits = self.output(self.norm(h)).float()\n",
    "        if return_attn:\n",
    "            return logits, attn_maps\n",
    "        return logits, _\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def forward_inference(self, x: torch.Tensor, start_pos: int = 0, return_attn=False):\n",
    "        B, L = x.shape\n",
    "        h = self.tok_embeddings(x)\n",
    "        freqs_cis = self.freqs_cis[start_pos:start_pos+L]\n",
    "        mask = None\n",
    "        if L > 1:\n",
    "            mask = torch.triu(torch.full((L, L), float('-inf'), device=self.device), diagonal=1)\n",
    "        attn_maps = []\n",
    "        for layer in self.layers:\n",
    "            if return_attn:\n",
    "                h, attn_map = layer(h, start_pos, freqs_cis, mask, return_attn=True)\n",
    "                attn_maps.append(attn_map)\n",
    "            else:\n",
    "                h = layer(h, start_pos, freqs_cis, mask)\n",
    "        logits = self.output(self.norm(h)).float()\n",
    "        if return_attn:\n",
    "            return logits, attn_maps\n",
    "        return logits, _\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123e5eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = ModelArgs()\n",
    "diff = DiffTransformer(PARAMS).to(device)\n",
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11c6f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# Describe the model\n",
    "summary(diff.to(device), input_size=(BATCH_SIZE, SEQUENCE_LENGTH), dtypes=[torch.long])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a0194f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_test(\"DIFF\", diff, start_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9d1bc0",
   "metadata": {},
   "source": [
    "## Compare Attention Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c8a78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "def compare_attention_maps(models_dict, dataloader, start_pos=0, layer=0, head=0):\n",
    "\n",
    "    # Fetch a single batch from the dataloader.\n",
    "    x, _ = next(iter(dataloader))\n",
    "    device = 'cuda:0'\n",
    "    x = x.to(device)\n",
    "    \n",
    "    attn_results = {}\n",
    "    \n",
    "    # For each model, perform a forward pass to obtain attention maps.\n",
    "    for name, model in models_dict.items():\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        with torch.inference_mode():\n",
    "            # For classic model, assume its forward signature differs (doesn't need start_pos)\n",
    "            if name == 'classic':\n",
    "                _, attn_maps = model.forward(x, return_attn=True)\n",
    "                attn = attn_maps[layer][head] \n",
    "                attn_results[name] = attn.detach().cpu().numpy()\n",
    "            elif name == 'llama':\n",
    "                _, attn_maps = model.forward(x, start_pos=start_pos, return_attn=True)\n",
    "                attn = attn_maps[layer][0, head] \n",
    "                attn_results[name] = attn.detach().cpu().numpy()\n",
    "            elif name == 'diff':\n",
    "                _, attn_maps = model.forward(x, start_pos=start_pos, return_attn=True)\n",
    "                attns = attn_maps[layer]         \n",
    "                names = ['diff1', 'diff2', 'λ*diff2', 'diff1-λ*diff2']\n",
    "                for name, attn in zip(names, attns):\n",
    "                    attn_results[name] = attn[0, head].detach().cpu().numpy()\n",
    "    \n",
    "    # Plot attention maps side-by-side.\n",
    "    n_models = len(attn_results)\n",
    "    fig, axs = plt.subplots(1, n_models, figsize=(n_models * 4, 4))\n",
    "    if n_models == 1:\n",
    "        axs = [axs]\n",
    "    for ax, (name, attn) in zip(axs, attn_results.items()):\n",
    "        im = ax.imshow(attn, interpolation='nearest', cmap='viridis')\n",
    "        ax.set_title(f\"{name.upper()} \\nLayer {layer+1}, Head {head+1}\")\n",
    "        ax.set_xlabel(\"Key Token Position\")\n",
    "        ax.set_ylabel(\"Query Token Position\")\n",
    "        fig.colorbar(im, ax=ax)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87661b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"classic\": classic,\n",
    "    \"llama\": llama,\n",
    "    \"diff\": diff\n",
    "}\n",
    "\n",
    "for i in range(4):\n",
    "    compare_attention_maps(models, train_loader, start_pos=0, layer=2, head=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14892e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_attention_maps_scaled(models_dict, dataloader, start_pos=0, layer=0, head=0):\n",
    "\n",
    "    # Fetch a single batch from the dataloader.\n",
    "    x, _ = next(iter(dataloader))\n",
    "    device = 'cuda:0'\n",
    "    x = x.to(device)\n",
    "    \n",
    "    attn_results = {}\n",
    "    \n",
    "    # For each model, perform a forward pass to obtain attention maps.\n",
    "    for name, model in models_dict.items():\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        with torch.inference_mode():\n",
    "            # For classic model, assume its forward signature differs (doesn't need start_pos)\n",
    "            if name == 'classic':\n",
    "                _, attn_maps = model.forward(x, return_attn=True)\n",
    "                attn = attn_maps[layer][head] \n",
    "                attn_results[name] = attn.detach().cpu().numpy()\n",
    "            elif name == 'llama':\n",
    "                _, attn_maps = model.forward(x, start_pos=start_pos, return_attn=True)\n",
    "                attn = attn_maps[layer][0, head] \n",
    "                attn_results[name] = attn.detach().cpu().numpy()\n",
    "            elif name == 'diff':\n",
    "                _, attn_maps = model.forward(x, start_pos=start_pos, return_attn=True)\n",
    "                attns = attn_maps[layer]         \n",
    "                names = ['diff1', 'diff2', 'λ*diff2', 'diff1-λ*diff2']\n",
    "                for name, attn in zip(names, attns):\n",
    "                    attn = torch.clamp(attn[0, head], min=0.3)\n",
    "                    attn_results[name] = attn.detach().cpu().numpy()\n",
    "    \n",
    "    # Plot attention maps side-by-side.\n",
    "    n_models = len(attn_results)\n",
    "    fig, axs = plt.subplots(1, n_models, figsize=(n_models * 4, 4))\n",
    "    if n_models == 1:\n",
    "        axs = [axs]\n",
    "    for ax, (name, attn) in zip(axs, attn_results.items()):\n",
    "        im = ax.imshow(attn, interpolation='nearest', cmap='viridis')\n",
    "        ax.set_title(f\"{name.upper()} \\nLayer {layer+1}, Head {head+1}\")\n",
    "        ax.set_xlabel(\"Key Token Position\")\n",
    "        ax.set_ylabel(\"Query Token Position\")\n",
    "        fig.colorbar(im, ax=ax)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9797da77",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"classic\": classic,\n",
    "    \"llama\": llama,\n",
    "    \"diff\": diff\n",
    "}\n",
    "\n",
    "for i in range(4):\n",
    "    compare_attention_maps_scaled(models, train_loader, start_pos=0, layer=2, head=i)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
