{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_theta_pos_frequencies(head_dim:int, seq_len:int, device:str, theta:float=10000.0):\n",
    "    assert head_dim % 2 == 0, \"Dimension (divided with head) must be divisible by 2\"\n",
    "    \n",
    "    theta_numerator = torch.arange(0, head_dim, 2).float()\n",
    "    theta = 1.0 / (theta ** (theta_numerator / head_dim)).to(device)\n",
    "    m = torch.arange(seq_len, device=device)\n",
    "    freqs = torch.outer(m, theta).float() # (seq_len) outer product* (head_dim / 2) -> (seq_len, head_dim/2)\n",
    "    \n",
    "    # torch.polar = abs⋅cos(angle)+abs⋅sin(angle)⋅j\n",
    "    freqs_complex = torch.polar(torch.ones_like(freqs), freqs)\n",
    "    return freqs_complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_embeddings(x: torch.Tensor, freqs_complex:torch.Tensor, device:str):\n",
    "    # (B, seq_len, num_heads, head_dim) -> (B, seq_len, num_heads, head_dim/2)\n",
    "    x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
    "    # (seq_len, head_dim/2) -> (1, seq_len, 1, head_dim/2)\n",
    "    freqs_complex = freqs_complex.unsqueeze(0).unsqueeze(2)\n",
    "    # (B, seq_len, num_heads, head_dim/2) * (1, seq_len, 1, head_dim/2) = (B, seq_len, num_heads, head_dim/2)\n",
    "    x_rotated = x_complex * freqs_complex\n",
    "    # (B, seq_len, num_heads, head_dim/2) -> (B, seq_len, num_heads, head_dim/2, 2)\n",
    "    x_out = torch.view_as_real(x_rotated)\n",
    "    # (B, seq_len, num_heads, head_dim/2, 2) -> (B, seq_len, num_heads, head)\n",
    "    x_out = x_out.reshape(*x.shape)\n",
    "    return x_out.type_as(x).to(device)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-8):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "        \n",
    "    def _reciprocal_rms(self, x: torch.Tensor):\n",
    "        '''\n",
    "        rms(x) = sqrt(mean(x^2))\n",
    "        reciprocal_rms(x) = x / rms(x)\n",
    "        '''\n",
    "        \n",
    "        # (B, seq_len, dim) * (B,. seq_len, 1) -> (B, seq_len, dim)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(dim=self.dim, keepdim=True) + self.eps)\n",
    "    \n",
    "    def forward(self, x:torch.Tensor):        \n",
    "        # (dim) * (B, seq_len, dim) -> (B, seq_len, dim)\n",
    "        return self.weight * self._reciprocal_rms(x).type_as(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArgs:\n",
    "    dim: int = 768      # llama: 4096, gpt: 768\n",
    "    n_layers: int = 4   # llama: 32, gpt: 12\n",
    "    n_heads: int = 4    # llama: 16, gpt: 12\n",
    "    n_kv_heads: Optional[int] = None\n",
    "    vocal_size: int = -1\n",
    "    multiple_of: int = 4 # llama: 256, gpt: 64\n",
    "    ffn_dim_multiplier: Optional[float] = None\n",
    "    norm_eps: float = 1e-6\n",
    "    \n",
    "    # needed for KV cache\n",
    "    max_batch_size: int = 32\n",
    "    max_seq_len: int = 1024   # llama: 2048, gpt: 1024\n",
    "    \n",
    "    device: str = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    '''\n",
    "    x: (B, seq_len, dim) -> (B, seq_len, n_rep, dim)\n",
    "    '''\n",
    "    \n",
    "    batch_size, seq_len, n_kv_heads, head_dim = x.shape\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "    else:\n",
    "        return (\n",
    "            x[:, :, :, None, :]\n",
    "            .expand(batch_size, seq_len, n_kv_heads, n_rep, head_dim)\n",
    "            .reshape(batch_size, seq_len, n_kv_heads * n_rep, head_dim)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        \n",
    "        # calculate n_heads of Grouped Attention with KV cache\n",
    "        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n",
    "        self.n_heads_q = args.n_heads\n",
    "        self.n_rep = self.n_heads_q // self.n_kv_heads\n",
    "        self.head_dim = args.dim // self.n_heads_q      \n",
    "        \n",
    "        self.wq = nn.Linear(args.dim, self.n_heads_q * self.head_dim, bias=False)       \n",
    "        self.wk = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.wv = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.wo = nn.Linear(args.dim, args.dim, bias=False) \n",
    "        \n",
    "        self.cache_k = torch.zeros((args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim), device=args.device)\n",
    "        self.cache_v = torch.zeros((args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim), device=args.device)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, start_pos: int, freqs_complex: torch.Tensor):\n",
    "        batch_size, seq_len, _ = x.size() # (B, seq_len, dim)\n",
    "        \n",
    "        xq = self.wq(x) # (B, seq_len, dim) -> (B, seq_len, n_heads_q * head_dim)\n",
    "        xk = self.wk(x) # (B, seq_len, dim) -> (B, seq_len, n_heads_kv * head_dim)\n",
    "        xv = self.wv(x) # (B, seq_len, dim) -> (B, seq_len, n_heads_kv * head_dim)\n",
    "        \n",
    "        xq = xq.view(batch_size, seq_len, self.n_heads_q, self.head_dim) # (B, seq_len, n_heads_q * head_dim) -> (B, seq_len, n_heads_q, head_dim)\n",
    "        xk = xk.view(batch_size, seq_len, self.n_kv_heads, self.head_dim) # (B, seq_len, n_heads_kv * head_dim) -> (B, seq_len, n_heads_kv, head_dim) \n",
    "        xv = xv.view(batch_size, seq_len, self.n_kv_heads, self.head_dim) # (B, seq_len, n_heads_kv * head_dim) -> (B, seq_len, n_heads_kv, head_dim)\n",
    "        \n",
    "        # position encoding for query, key\n",
    "        xq = apply_rotary_embeddings(xq, freqs_complex, x.device)\n",
    "        xk = apply_rotary_embeddings(xk, freqs_complex, x.device)\n",
    "        \n",
    "        # Replace the 4entry in the cache with the new values\n",
    "        self.cache_k[:batch_size, start_pos:start_pos+seq_len] = xk\n",
    "        self.cache_v[:batch_size, start_pos:start_pos+seq_len] = xv\n",
    "        \n",
    "        # (B, seq_len_kv, n_heads_kv, head_dim) \n",
    "        keys = self.cache_k[:batch_size, :start_pos+seq_len]\n",
    "        values = self.cache_v[:batch_size, :start_pos+seq_len]\n",
    "        \n",
    "        # repeat the heads of K and V to match the number of heads of Q\n",
    "        keys = repeat_kv(keys, self.n_rep)\n",
    "        values = repeat_kv(values, self.n_rep)\n",
    "        \n",
    "        # (B, seq_len, n_heads_q, head_dim) -> (B, n_heads_q, seq_len, head_dim)\n",
    "        xq = xq.transpose(1, 2)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "        \n",
    "        # (B, n_heads_q, seq_len, head_dim) * (B, n_heads_q, head_dim, seq_len_kv) -> (B, n_heads_q, seq_len, seq_len_kv)\n",
    "        scores = torch.matmul(xq, keys.transpose(2, 3) / math.sqrt(self.head_dim))\n",
    "        scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "        \n",
    "        # (B, n_heads_q, seq_len, seq_len_kv) * (B, n_heads_q, seq_len_kv, head_dim) -> (B, n_heads_q, seq_len, head_dim)\n",
    "        out = torch.matmul(scores, values)\n",
    "        \n",
    "        # (B, n_heads_q, seq_len, head_dim) -> (B, seq_len, n_heads_q, head_dim) -> (B, seq_len, dim)\n",
    "        out = (out.transpose(1, 2).contiguous().view(batch_size, seq_len, self.n_heads_q * self.head_dim))\n",
    "        # (B, seq_len, dim) -> (B, seq_len, dim)\n",
    "        out = self.wo(out) \n",
    "        return out "
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "\n",
    "        hidden_dim = args.hidden_dim\n",
    "\n",
    "        self.w1 = nn.Linear(args.dim, hidden_dim, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_dim, args.dim, bias=False)\n",
    "        self.w3 = nn.Linear(args.dim, hidden_dim, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # (B, Seq_Len, Dim) --> (B, Seq_Len, Hidden_Dim)\n",
    "        swish = F.silu(self.w1(x))\n",
    "        # (B, Seq_Len, Dim) --> (B, Seq_Len, Hidden_Dim)\n",
    "        x_V = self.w3(x)\n",
    "        # (B, Seq_Len, Hidden_Dim) * (B, Seq_Len, Hidden_Dim) --> (B, Seq_Len, Hidden_Dim)\n",
    "        x = swish * x_V\n",
    "        # (B, Seq_Len, Hidden_Dim) --> (B, Seq_Len, Dim)\n",
    "        x = self.w2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        args: ModelArgs\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        hidden_dim = 4 * args.dim\n",
    "        hidden_dim = int(2 * hidden_dim / 3)\n",
    "        if args.ffn_dim_multiplier is not None:\n",
    "            hidden_dim = int(args.ffn_dim_multiplier * hidden_dim)\n",
    "        # Round the hidden_dim to the nearest multiple of the multiple_of parameter\n",
    "        hidden_dim = args.multiple_of * ((hidden_dim + args.multiple_of - 1) // args.multiple_of)\n",
    "\n",
    "        self.w1 = nn.Linear(args.dim, hidden_dim, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_dim, args.dim, bias=False)\n",
    "        self.w3 = nn.Linear(args.dim, hidden_dim, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # (B, Seq_Len, Dim) --> (B, Seq_Len, Hidden_Dim)\n",
    "        swish = F.silu(self.w1(x))\n",
    "        # (B, Seq_Len, Dim) --> (B, Seq_Len, Hidden_Dim)\n",
    "        x_V = self.w3(x)\n",
    "        # (B, Seq_Len, Hidden_Dim) * (B, Seq_Len, Hidden_Dim) --> (B, Seq_Len, Hidden_Dim)\n",
    "        x = swish * x_V\n",
    "        # (B, Seq_Len, Hidden_Dim) --> (B, Seq_Len, Dim)\n",
    "        x = self.w2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.n_heads = args.n_heads\n",
    "        self.dim = args.dim\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "        \n",
    "        self.attention = SelfAttention(args)\n",
    "        self.feed_forward = FeedForward(args)\n",
    "        \n",
    "        # PreNorm\n",
    "        self.attention_norm = RMSNorm(self.dim, eps=args.norm_eps)\n",
    "        self.ffn_norm = RMSNorm(self.dim, eps=args.norm_eps)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, start_pos: int, freqs_complex: torch.Tensor):        \n",
    "        # attention\n",
    "        ## (B, seq_len, dim) + (B, seq_len, dim) -> (B, seq_len, dim)\n",
    "        y = self.attention_norm(x + self.attention(x, start_pos, freqs_complex))\n",
    "        \n",
    "        # feed forward\n",
    "        ## (B, seq_len, dim) -> (B, seq_len, dim)\n",
    "        y = self.ffn_norm(y + self.feed_forward(y))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "\n",
    "        assert args.vocab_size != -1, \"Vocab size must be set\"\n",
    "\n",
    "        self.args = args\n",
    "        self.vocab_size = args.vocab_size\n",
    "        self.n_layers = args.n_layers\n",
    "        self.tok_embeddings = nn.Embedding(self.vocab_size, args.dim)\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        for layer_id in range(args.n_layers):\n",
    "            self.layers.append(Block(args))\n",
    "\n",
    "        self.norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "        self.output = nn.Linear(args.dim, self.vocab_size, bias=False)\n",
    "\n",
    "        self.freqs_complex = precompute_theta_pos_frequencies(self.args.dim // self.args.n_heads, self.args.max_seq_len * 2, device=self.args.device)\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor, start_pos: int):\n",
    "        # (B, Seq_Len)\n",
    "        batch_size, seq_len = tokens.shape\n",
    "        # assert seq_len == 1, \"Only one token at a time can be processed\"\n",
    "\n",
    "        # (B, Seq_Len) -> (B, Seq_Len, Dim)\n",
    "        h = self.tok_embeddings(tokens)\n",
    "\n",
    "        # Retrieve the pairs (m, theta) corresponding to the positions [start_pos, start_pos + seq_len]\n",
    "        freqs_complex = self.freqs_complex[start_pos:start_pos + seq_len]\n",
    "        \n",
    "        # Consecutively apply all the encoder layers\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, start_pos, freqs_complex)\n",
    "        h = self.norm(h)\n",
    "        output = self.output(h).float()\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentencepiece import SentencePieceProcessor\n",
    "\n",
    "tokenizer = SentencePieceProcessor()\n",
    "tokenizer.Load('llama/tokenizer.model')\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = ModelArgs()\n",
    "args.vocab_size = 1024\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.device == 'cuda':\n",
    "    torch.set_default_tensor_type(torch.cuda.HalfTensor)  \n",
    "else:\n",
    "    torch.set_default_tensor_type(torch.BFloat16Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(args).to(args.device)\n",
    "x_dummy = torch.rand((args.max_batch_size, args.max_seq_len))\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dummy = torch.randint(low=0, high=args.max_seq_len, size=(args.max_batch_size, args.max_seq_len), device=args.device)\n",
    "\n",
    "x_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "cur_iterator = tqdm(range(1, args.max_seq_len), desc=\"Generating tokens\")\n",
    "for cur_pos in cur_iterator:\n",
    "    with torch.no_grad():\n",
    "        logits = model.forward(x_dummy[:, cur_pos-1:cur_pos], cur_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INFERENCE CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import torch\n",
    "import time\n",
    "from pathlib import Path\n",
    "import json\n",
    "from sentencepiece import SentencePieceProcessor\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import torch\n",
    "import time\n",
    "from pathlib import Path\n",
    "import json\n",
    "from sentencepiece import SentencePieceProcessor\n",
    "from tqdm import tqdm\n",
    "\n",
    "class LLaMA:\n",
    "\n",
    "    def __init__(self, model: Transformer, tokenizer: SentencePieceProcessor, model_args: ModelArgs):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.args = model_args\n",
    "\n",
    "    @staticmethod\n",
    "    def build(checkpoints_dir: str, tokenizer_path: str, load_model: bool, max_seq_len: int, max_batch_size: int, device: str):\n",
    "        prev_time = time.time()\n",
    "        if load_model:\n",
    "            checkpoints = sorted(Path(checkpoints_dir).glob(\"*.pth\"))\n",
    "            assert len(checkpoints) > 0, f\"no checkpoint files found in {checkpoints_dir}\"\n",
    "            ckpt_path = checkpoints[0]\n",
    "            print(f'Loading checkpoint \"{ckpt_path}\"')\n",
    "            checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "            print(f\"Loaded checkpoint in {time.time() - prev_time:.2f}s\")\n",
    "            prev_time = time.time()\n",
    "        with open(Path(checkpoints_dir) / \"params.json\", \"r\") as f:\n",
    "            params = json.loads(f.read())\n",
    "\n",
    "        model_args: ModelArgs = ModelArgs(\n",
    "            max_seq_len=max_seq_len,\n",
    "            max_batch_size=max_batch_size,\n",
    "            device=device,\n",
    "            **params\n",
    "        )\n",
    "\n",
    "        tokenizer = SentencePieceProcessor()\n",
    "        tokenizer.load(tokenizer_path)\n",
    "        model_args.vocab_size = tokenizer.vocab_size()\n",
    "        \n",
    "        if device == \"cuda\":\n",
    "            torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
    "        else:\n",
    "            torch.set_default_tensor_type(torch.BFloat16Tensor)\n",
    "        \n",
    "        model = Transformer(model_args).to(device)\n",
    "\n",
    "        if load_model:\n",
    "            # The only unmatched key in the checkpoint is rope.freqs. Remove it\n",
    "            del checkpoint['rope.freqs']\n",
    "            model.load_state_dict(checkpoint, strict=True)\n",
    "            print(f\"Loaded state dict in {time.time() - prev_time:.2f}s\")\n",
    "        \n",
    "        return LLaMA(model, tokenizer, model_args)\n",
    "\n",
    "    def text_completion(self, prompts: list[str], temperature: float = 0.6, top_p: float = 0.9, max_gen_len: Optional[int] = None):\n",
    "        if max_gen_len is None:\n",
    "            max_gen_len = self.args.max_seq_len - 1\n",
    "        # Convert each prompt into tokens\n",
    "        prompt_tokens = [self.tokenizer.encode(prompt, out_type=int, add_bos=True, add_eos=False) for prompt in prompts]\n",
    "        # Make sure the batch size is not too large\n",
    "        batch_size = len(prompt_tokens)\n",
    "        assert batch_size <= self.args.max_batch_size, f\"batch size must be less than or equal to {self.args.max_batch_size}\"\n",
    "        max_prompt_len = max(len(prompt) for prompt in prompt_tokens)\n",
    "        # Make sure the prompt length is not larger than the maximum sequence length\n",
    "        assert max_prompt_len <= self.args.max_seq_len, f\"prompt length must be less than or equal to {self.args.max_seq_len}\"\n",
    "        total_len = min(self.args.max_seq_len, max_gen_len + max_prompt_len)\n",
    "\n",
    "        # Create the list that will contain the generated tokens, along with the initial prompt tokens\n",
    "        pad_id = self.tokenizer.pad_id()\n",
    "        tokens = torch.full((batch_size, total_len), pad_id, dtype=torch.long, device=device)\n",
    "        for k, t in enumerate(prompt_tokens):\n",
    "            # Populate the initial tokens with the prompt tokens\n",
    "            tokens[k, : len(t)] = torch.tensor(t, dtype=torch.long, device=device)\n",
    "        \n",
    "        eos_reached = torch.tensor([False] * batch_size, device=device)\n",
    "        prompt_tokens_mask = tokens != pad_id # True if the token is a prompt token, False otherwise\n",
    "        cur_iterator = tqdm(range(1, total_len), desc=\"Generating tokens\")\n",
    "        for cur_pos in cur_iterator:\n",
    "            with torch.no_grad():\n",
    "                logits = self.model.forward(tokens[:, cur_pos-1:cur_pos], cur_pos)\n",
    "            if temperature > 0:\n",
    "                # The temperature is applied before the softmax\n",
    "                probs = torch.softmax(logits[:, -1] / temperature, dim=-1)\n",
    "                next_token = self._sample_top_p(probs, top_p)\n",
    "            else:\n",
    "                # Greedily select the token with the max probability\n",
    "                next_token = torch.argmax(logits[:, -1], dim=-1)\n",
    "\n",
    "            next_token = next_token.reshape(-1)\n",
    "            # Only replace token if it is a padding token\n",
    "            next_token = torch.where(prompt_tokens_mask[:, cur_pos], tokens[:, cur_pos], next_token)\n",
    "            tokens[:, cur_pos] = next_token\n",
    "            # EOS is reached only if we found an EOS token for a padding position\n",
    "            eos_reached |= (~prompt_tokens_mask[:, cur_pos]) & (next_token == self.tokenizer.eos_id)\n",
    "            if all(eos_reached):\n",
    "                break\n",
    "\n",
    "        out_tokens = []\n",
    "        out_text = []\n",
    "        for prompt_index, current_prompt_tokens in enumerate(tokens.tolist()):\n",
    "            # Cut to the EOS token, if present\n",
    "            if self.tokenizer.eos_id in current_prompt_tokens:\n",
    "                eos_idx = current_prompt_tokens.index(self.tokenizer.eos_id)\n",
    "                current_prompt_tokens = current_prompt_tokens[:eos_idx]\n",
    "            out_tokens.append(current_prompt_tokens)\n",
    "            out_text.append(self.tokenizer.decode(current_prompt_tokens))\n",
    "        return (out_tokens, out_text)\n",
    "    \n",
    "    def _sample_top_p(self, probs, p):\n",
    "        # (B, vocab_size)\n",
    "        probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n",
    "        # (B, vocab_size)\n",
    "        probs_sum = torch.cumsum(probs_sort, dim=-1)\n",
    "        # (B, vocab_size)\n",
    "        # (Substracting \"probs_sort\" shifts the cumulative sum by 1 position to the right before masking)\n",
    "        mask = probs_sum - probs_sort > p \n",
    "        # Zero out all the probabilities of tokens that are not selected by the Top P\n",
    "        probs_sort[mask] = 0.0 \n",
    "        # Redistribute the probabilities so that they sum up to 1.\n",
    "        probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
    "        # Sample a token (its index) from the top p distribution\n",
    "        next_token = torch.multinomial(probs_sort, num_samples=1)\n",
    "        # Get the token position in the vocabulary corresponding to the sampled index\n",
    "        next_token = torch.gather(probs_idx, -1, next_token) \n",
    "        return next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "allow_cuda = False\n",
    "device = 'cuda' if torch.cuda.is_available() and allow_cuda else 'cpu'\n",
    "\n",
    "prompts = [\n",
    "    \"Simply put, the theory of relativity states that \",\n",
    "    \"If Google was an Italian company founded in Milan, it would\",\n",
    "    # Few shot promt\n",
    "    \"\"\"Translate English to French:\n",
    "    \n",
    "    sea otter => loutre de mer\n",
    "    peppermint => menthe poivrée\n",
    "    plush girafe => girafe peluche\n",
    "    cheese =>\"\"\",\n",
    "    # Zero shot prompt\n",
    "    \"\"\"Tell me if the following person is actually Doraemon disguised as human:\n",
    "    Name: Umar Jamil\n",
    "    Decision: \n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "model = LLaMA.build(\n",
    "    checkpoints_dir='llama/llama-2-7b/',\n",
    "    tokenizer_path='llama/tokenizer.model',\n",
    "    load_model=False,\n",
    "    max_seq_len=1024,\n",
    "    max_batch_size=len(prompts),\n",
    "    device=device\n",
    ")\n",
    "\n",
    "out_tokens, out_texts = (model.text_completion(prompts, max_gen_len=64))\n",
    "assert len(out_texts) == len(prompts)\n",
    "for i in range(len(out_texts)):\n",
    "    print(f'{out_texts[i]}')\n",
    "    print('-' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "llama = LLaMA.build(\"llama-2-7b/\",\n",
    "                    \"tokenizer.model\", \n",
    "                    load_model=True, \n",
    "                    max_seq_len=1024, \n",
    "                    max_batch_size=3, \n",
    "                    device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = ModelArgs()\n",
    "args.vocab_size = 1024\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Modern_Transformer(args)\n",
    "x_dummy = torch.rand((args.max_batch_size, args.max_seq_len))\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dummy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(x_dummy, 1023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# This software may be used and distributed in accordance with the terms of the Llama 3 Community License Agreement.\n",
    "\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import fairscale.nn.model_parallel.initialize as fs_init\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from fairscale.nn.model_parallel.layers import (\n",
    "    ColumnParallelLinear,\n",
    "    RowParallelLinear,\n",
    "    VocabParallelEmbedding,\n",
    ")\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    dim: int = 4096\n",
    "    n_layers: int = 32\n",
    "    n_heads: int = 32\n",
    "    n_kv_heads: Optional[int] = None\n",
    "    vocab_size: int = -1\n",
    "    multiple_of: int = 256  # make SwiGLU hidden layer size multiple of large power of 2\n",
    "    ffn_dim_multiplier: Optional[float] = None\n",
    "    norm_eps: float = 1e-5\n",
    "    rope_theta: float = 500000\n",
    "\n",
    "    max_batch_size: int = 32\n",
    "    max_seq_len: int = 2048\n",
    "\n",
    "\n",
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight\n",
    "\n",
    "\n",
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    t = torch.arange(end, device=freqs.device, dtype=torch.float32)\n",
    "    freqs = torch.outer(t, freqs)\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    return freqs_cis\n",
    "\n",
    "\n",
    "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
    "    ndim = x.ndim\n",
    "    assert 0 <= 1 < ndim\n",
    "    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n",
    "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
    "    return freqs_cis.view(*shape)\n",
    "\n",
    "\n",
    "def apply_rotary_emb(\n",
    "    xq: torch.Tensor,\n",
    "    xk: torch.Tensor,\n",
    "    freqs_cis: torch.Tensor,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
    "    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n",
    "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
    "    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
    "\n",
    "\n",
    "def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"torch.repeat_interleave(x, dim=2, repeats=n_rep)\"\"\"\n",
    "    bs, slen, n_kv_heads, head_dim = x.shape\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "    return (\n",
    "        x[:, :, :, None, :]\n",
    "        .expand(bs, slen, n_kv_heads, n_rep, head_dim)\n",
    "        .reshape(bs, slen, n_kv_heads * n_rep, head_dim)\n",
    "    )\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n",
    "        model_parallel_size = fs_init.get_model_parallel_world_size()\n",
    "        self.n_local_heads = args.n_heads // model_parallel_size\n",
    "        self.n_local_kv_heads = self.n_kv_heads // model_parallel_size\n",
    "        self.n_rep = self.n_local_heads // self.n_local_kv_heads\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "\n",
    "        self.wq = ColumnParallelLinear(\n",
    "            args.dim,\n",
    "            args.n_heads * self.head_dim,\n",
    "            bias=False,\n",
    "            gather_output=False,\n",
    "            init_method=lambda x: x,\n",
    "        )\n",
    "        self.wk = ColumnParallelLinear(\n",
    "            args.dim,\n",
    "            self.n_kv_heads * self.head_dim,\n",
    "            bias=False,\n",
    "            gather_output=False,\n",
    "            init_method=lambda x: x,\n",
    "        )\n",
    "        self.wv = ColumnParallelLinear(\n",
    "            args.dim,\n",
    "            self.n_kv_heads * self.head_dim,\n",
    "            bias=False,\n",
    "            gather_output=False,\n",
    "            init_method=lambda x: x,\n",
    "        )\n",
    "        self.wo = RowParallelLinear(\n",
    "            args.n_heads * self.head_dim,\n",
    "            args.dim,\n",
    "            bias=False,\n",
    "            input_is_parallel=True,\n",
    "            init_method=lambda x: x,\n",
    "        )\n",
    "\n",
    "        self.cache_k = torch.zeros(\n",
    "            (\n",
    "                args.max_batch_size,\n",
    "                args.max_seq_len,\n",
    "                self.n_local_kv_heads,\n",
    "                self.head_dim,\n",
    "            )\n",
    "        ).cuda()\n",
    "        self.cache_v = torch.zeros(\n",
    "            (\n",
    "                args.max_batch_size,\n",
    "                args.max_seq_len,\n",
    "                self.n_local_kv_heads,\n",
    "                self.head_dim,\n",
    "            )\n",
    "        ).cuda()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        start_pos: int,\n",
    "        freqs_cis: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor],\n",
    "    ):\n",
    "        bsz, seqlen, _ = x.shape\n",
    "        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\n",
    "\n",
    "        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
    "        xk = xk.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n",
    "        xv = xv.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n",
    "\n",
    "        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)\n",
    "\n",
    "        self.cache_k = self.cache_k.to(xq)\n",
    "        self.cache_v = self.cache_v.to(xq)\n",
    "\n",
    "        self.cache_k[:bsz, start_pos : start_pos + seqlen] = xk\n",
    "        self.cache_v[:bsz, start_pos : start_pos + seqlen] = xv\n",
    "\n",
    "        keys = self.cache_k[:bsz, : start_pos + seqlen]\n",
    "        values = self.cache_v[:bsz, : start_pos + seqlen]\n",
    "\n",
    "        # repeat k/v heads if n_kv_heads < n_heads\n",
    "        keys = repeat_kv(\n",
    "            keys, self.n_rep\n",
    "        )  # (bs, cache_len + seqlen, n_local_heads, head_dim)\n",
    "        values = repeat_kv(\n",
    "            values, self.n_rep\n",
    "        )  # (bs, cache_len + seqlen, n_local_heads, head_dim)\n",
    "\n",
    "        xq = xq.transpose(1, 2)  # (bs, n_local_heads, seqlen, head_dim)\n",
    "        keys = keys.transpose(1, 2)  # (bs, n_local_heads, cache_len + seqlen, head_dim)\n",
    "        values = values.transpose(\n",
    "            1, 2\n",
    "        )  # (bs, n_local_heads, cache_len + seqlen, head_dim)\n",
    "        scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "        if mask is not None:\n",
    "            scores = scores + mask  # (bs, n_local_heads, seqlen, cache_len + seqlen)\n",
    "        scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "        output = torch.matmul(scores, values)  # (bs, n_local_heads, seqlen, head_dim)\n",
    "        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)\n",
    "        return self.wo(output)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        hidden_dim: int,\n",
    "        multiple_of: int,\n",
    "        ffn_dim_multiplier: Optional[float],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(2 * hidden_dim / 3)\n",
    "        # custom dim factor multiplier\n",
    "        if ffn_dim_multiplier is not None:\n",
    "            hidden_dim = int(ffn_dim_multiplier * hidden_dim)\n",
    "        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
    "\n",
    "        self.w1 = ColumnParallelLinear(\n",
    "            dim, hidden_dim, bias=False, gather_output=False, init_method=lambda x: x\n",
    "        )\n",
    "        self.w2 = RowParallelLinear(\n",
    "            hidden_dim, dim, bias=False, input_is_parallel=True, init_method=lambda x: x\n",
    "        )\n",
    "        self.w3 = ColumnParallelLinear(\n",
    "            dim, hidden_dim, bias=False, gather_output=False, init_method=lambda x: x\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w2(F.silu(self.w1(x)) * self.w3(x))\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, layer_id: int, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.n_heads = args.n_heads\n",
    "        self.dim = args.dim\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "        self.attention = Attention(args)\n",
    "        self.feed_forward = FeedForward(\n",
    "            dim=args.dim,\n",
    "            hidden_dim=4 * args.dim,\n",
    "            multiple_of=args.multiple_of,\n",
    "            ffn_dim_multiplier=args.ffn_dim_multiplier,\n",
    "        )\n",
    "        self.layer_id = layer_id\n",
    "        self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "        self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        start_pos: int,\n",
    "        freqs_cis: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor],\n",
    "    ):\n",
    "        h = x + self.attention(self.attention_norm(x), start_pos, freqs_cis, mask)\n",
    "        out = h + self.feed_forward(self.ffn_norm(h))\n",
    "        return out\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, params: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.params = params\n",
    "        self.vocab_size = params.vocab_size\n",
    "        self.n_layers = params.n_layers\n",
    "\n",
    "        self.tok_embeddings = VocabParallelEmbedding(\n",
    "            params.vocab_size, params.dim, init_method=lambda x: x\n",
    "        )\n",
    "\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for layer_id in range(params.n_layers):\n",
    "            self.layers.append(TransformerBlock(layer_id, params))\n",
    "\n",
    "        self.norm = RMSNorm(params.dim, eps=params.norm_eps)\n",
    "        self.output = ColumnParallelLinear(\n",
    "            params.dim, params.vocab_size, bias=False, init_method=lambda x: x\n",
    "        )\n",
    "\n",
    "        self.freqs_cis = precompute_freqs_cis(\n",
    "            params.dim // params.n_heads,\n",
    "            params.max_seq_len * 2,\n",
    "            params.rope_theta,\n",
    "        )\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def forward(self, tokens: torch.Tensor, start_pos: int):\n",
    "        _bsz, seqlen = tokens.shape\n",
    "        h = self.tok_embeddings(tokens)\n",
    "        self.freqs_cis = self.freqs_cis.to(h.device)\n",
    "        freqs_cis = self.freqs_cis[start_pos : start_pos + seqlen]\n",
    "\n",
    "        mask = None\n",
    "        if seqlen > 1:\n",
    "            mask = torch.full((seqlen, seqlen), float(\"-inf\"), device=tokens.device)\n",
    "\n",
    "            mask = torch.triu(mask, diagonal=1)\n",
    "\n",
    "            # When performing key-value caching, we compute the attention scores\n",
    "            # only for the new sequence. Thus, the matrix of scores is of size\n",
    "            # (seqlen, cache_len + seqlen), and the only masked entries are (i, j) for\n",
    "            # j > cache_len + i, since row i corresponds to token cache_len + i.\n",
    "            mask = torch.hstack(\n",
    "                [torch.zeros((seqlen, start_pos), device=tokens.device), mask]\n",
    "            ).type_as(h)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, start_pos, freqs_cis, mask)\n",
    "        h = self.norm(h)\n",
    "        output = self.output(h).float()\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelArgs(dim=768, n_layers=4, n_heads=4, n_kv_heads=4, vocab_size=1024, multiple_of=4, ffn_dim_multiplier=None, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=2048)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = ModelArgs()\n",
    "args.vocab_size = 1024\n",
    "args.n_layers = 4\n",
    "args.dim = 768\n",
    "args.n_heads = 4\n",
    "args.n_kv_heads = 4\n",
    "args.multiple_of = 4\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "model parallel group is not initialized",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(args\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m      2\u001b[0m x_dummy \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand((args\u001b[38;5;241m.\u001b[39mmax_batch_size, args\u001b[38;5;241m.\u001b[39mmax_seq_len))\n\u001b[1;32m      4\u001b[0m model\n",
      "Cell \u001b[0;32mIn[1], line 258\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m params\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_layers \u001b[38;5;241m=\u001b[39m params\u001b[38;5;241m.\u001b[39mn_layers\n\u001b[0;32m--> 258\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtok_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mVocabParallelEmbedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModuleList()\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(params\u001b[38;5;241m.\u001b[39mn_layers):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/fairscale/nn/model_parallel/layers.py:118\u001b[0m, in \u001b[0;36mVocabParallelEmbedding.__init__\u001b[0;34m(self, num_embeddings, embedding_dim, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse, init_method)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# Divide the weight matrix along the vocaburaly dimension.\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_start_index, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_end_index \u001b[38;5;241m=\u001b[39m VocabUtility\u001b[38;5;241m.\u001b[39mvocab_range_from_global_vocab_size(\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_embeddings, \u001b[43mget_model_parallel_rank\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, get_model_parallel_world_size()\n\u001b[1;32m    119\u001b[0m )\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_embeddings_per_partition \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_end_index \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_start_index\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# Allocate weights.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/fairscale/nn/model_parallel/initialize.py:157\u001b[0m, in \u001b[0;36mget_model_parallel_rank\u001b[0;34m()\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_model_parallel_rank\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m    156\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return my rank for the model parallel group.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdistributed\u001b[38;5;241m.\u001b[39mget_rank(group\u001b[38;5;241m=\u001b[39m\u001b[43mget_model_parallel_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/fairscale/nn/model_parallel/initialize.py:128\u001b[0m, in \u001b[0;36mget_model_parallel_group\u001b[0;34m()\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_model_parallel_group\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mdistributed\u001b[38;5;241m.\u001b[39mProcessGroup:\n\u001b[1;32m    127\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the model parallel group the caller rank belongs to.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m _MODEL_PARALLEL_GROUP \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel parallel group is not initialized\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _MODEL_PARALLEL_GROUP\n",
      "\u001b[0;31mAssertionError\u001b[0m: model parallel group is not initialized"
     ]
    }
   ],
   "source": [
    "model = Transformer(args).to(args.device)\n",
    "x_dummy = torch.rand((args.max_batch_size, args.max_seq_len))\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
