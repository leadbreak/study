{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reference\n",
    "- [[Hands-On] Mastering LLama — LLama1 바닥부터 구현하기(1/3)](https://medium.com/@hugmanskj/hands-on-mastering-llama-llama1-%EB%B0%94%EB%8B%A5%EB%B6%80%ED%84%B0-%EA%B5%AC%ED%98%84%ED%95%98%EA%B8%B0-1-3-ba92f504d37b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing Libraries\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Modified hyperparameters\n",
    "SEQUENCE_LENGTH = 64\n",
    "\n",
    "EMBEDDING_DIM = 256\n",
    "HIDDEN_DIM = 256\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 0.001\n",
    "PRINT_EVERY = 1000\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utility Functions\n",
    "\n",
    "def load_data(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "def create_char_mappings(text):\n",
    "    chars = sorted(list(set(text)))\n",
    "    char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "    idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "    return chars, char_to_idx, idx_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDataset(Dataset):\n",
    "    def __init__(self, text, seq_length, char_to_idx):\n",
    "        self.text = text\n",
    "        self.seq_length = seq_length\n",
    "        self.char_to_idx = char_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text) - self.seq_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = [self.char_to_idx[ch] for ch in self.text[idx:idx+self.seq_length]]\n",
    "        y = [self.char_to_idx[ch] for ch in self.text[idx+1:idx+self.seq_length+1]]\n",
    "        return torch.tensor(x), torch. Tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(text, seq_length, batch_size, val_split):\n",
    "    chars, char_to_idx, idx_to_char = create_char_mappings(text)\n",
    "\n",
    "    # Split data into train and validation\n",
    "    val_size = int(len(text) * val_split)\n",
    "    train_text, val_text = text[:-val_size], text[-val_size:]\n",
    "\n",
    "    train_dataset = CharDataset(train_text, seq_length, char_to_idx)\n",
    "    val_dataset = CharDataset(val_text, seq_length, char_to_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, chars, char_to_idx, idx_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "text = load_data('./input.txt')\n",
    "train_loader, val_loader, chars, char_to_idx, idx_to_char = prepare_data(text, SEQUENCE_LENGTH, BATCH_SIZE, VALIDATION_SPLIT)\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(f\"Total characters: {len(text)}\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Train dataset size: {len(train_loader.dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert index sequence to character sequence\n",
    "def indices_to_text(indices, idx_to_char):\n",
    "    return ''.join([idx_to_char[idx.item()] for idx in indices])\n",
    "\n",
    "# Get a batch of data\n",
    "dataiter = iter(train_loader)\n",
    "batch_x, batch_y = next(dataiter)\n",
    "\n",
    "print(f\"Input shape: {batch_x.shape}\")\n",
    "print(f\"Target shape: {batch_y.shape}\")\n",
    "\n",
    "# Print a few samples from the batch\n",
    "num_samples = 3\n",
    "for i in range(num_samples):\n",
    "    print(f\"Sample {i+1}: ------------------------------\" )\n",
    "    print(\"Input sequence :\", indices_to_text(batch_x[i], idx_to_char).replace('\\n',''))\n",
    "    print(\"Target sequence:\", indices_to_text(batch_y[i], idx_to_char).replace('\\n',''))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer, device, epoch, step, log_interval=10):\n",
    "    model.train()\n",
    "    losses = []\n",
    "\n",
    "    for batch, (x, y) in enumerate(dataloader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output, _ = model(x)\n",
    "        loss = criterion(output.view(-1, output.size(-1)), y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        step += 1\n",
    "        losses.append((step, epoch, loss.item()))\n",
    "\n",
    "        if (batch + 1) % log_interval == 0:\n",
    "            print(f'Epoch {epoch}, Step {step}, Batch {batch+1}/{len(dataloader)}, Loss: {loss.item():.4f}')\n",
    "\n",
    "    return losses, step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, criterion, device, epoch, step):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            output, _ = model(x)\n",
    "            loss = criterion(output.view(-1, output.size(-1)), y.view(-1))\n",
    "            losses.append((step, epoch, loss.item()))\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, device, epochs, log_interval=10):\n",
    "    all_train_losses = []\n",
    "    all_val_losses = []\n",
    "    step = 0\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"Epoch {epoch}/{epochs}\")\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        # Training phase\n",
    "        epoch_train_losses, step = train(model, train_loader, criterion, optimizer, device, epoch, step, log_interval)\n",
    "        all_train_losses.extend(epoch_train_losses)\n",
    "\n",
    "        # Validation phase\n",
    "        epoch_val_losses = validate(model, val_loader, criterion, device, epoch, step)\n",
    "        all_val_losses.extend(epoch_val_losses)\n",
    "\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        print(f'Epoch {epoch}/{epochs}, '\n",
    "              f'Train Loss: {epoch_train_losses[-1][2]:.4f}, '\n",
    "              f'Val Loss: {epoch_val_losses[-1][2]:.4f}, '\n",
    "              f'Epoch Time: {epoch_time:.2f}s')\n",
    "\n",
    "    # Create DataFrames after all epochs\n",
    "    train_losses_df = pd.DataFrame(all_train_losses, columns=['step', 'epoch', 'loss_value'])\n",
    "    val_losses_df = pd.DataFrame(all_val_losses, columns=['step', 'epoch', 'loss_value'])\n",
    "\n",
    "    return model, train_losses_df, val_losses_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, char_to_idx, idx_to_char, start_text, device, max_length=500):\n",
    "    model.eval()\n",
    "    current_text = start_text\n",
    "    hidden = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            x = torch.tensor([[char_to_idx[ch] for ch in current_text[-SEQUENCE_LENGTH:]]]).to(device)\n",
    "            output, hidden = model(x, hidden)\n",
    "            probs = torch.softmax(output[0, -1], dim=0)\n",
    "            next_char_idx = torch.multinomial(probs, 1).item()\n",
    "            next_char = idx_to_char[next_char_idx]\n",
    "            current_text += next_char\n",
    "\n",
    "    return current_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utiltiy functions\n",
    "\n",
    "# Dictionary to store the losses from different models/configurations\n",
    "loss_comparison_dict = {}\n",
    "\n",
    "# Function to add and store losses from different models\n",
    "def add_loss_to_comparison(model_name, train_losses_df, val_losses_df):\n",
    "    \"\"\"\n",
    "    Adds training and validation losses from a model to the comparison dictionary.\n",
    "\n",
    "    Args:\n",
    "    - model_name (str): A name to identify the model (e.g., 'LSTM', 'LSTM with RMSNorm')\n",
    "    - train_losses_df (DataFrame): DataFrame containing the training losses.\n",
    "    - val_losses_df (DataFrame): DataFrame containing the validation losses.\n",
    "    \"\"\"\n",
    "    loss_comparison_dict[model_name] = {\n",
    "        'train': train_losses_df,\n",
    "        'val': val_losses_df\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot loss curves\n",
    "def plot_loss(train_losses_df, val_losses_df):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    # Plot training losses\n",
    "    for epoch in train_losses_df['epoch'].unique():\n",
    "        epoch_train_losses = train_losses_df[train_losses_df['epoch'] == epoch]\n",
    "        plt.plot(epoch_train_losses['step'], epoch_train_losses['loss_value'],\n",
    "                 color='blue', alpha=0.3)\n",
    "\n",
    "    # scatter training loss at the end of each epoch\n",
    "    last_train_losses = train_losses_df.groupby('epoch').last().reset_index()\n",
    "    plt.scatter(last_train_losses['step'], last_train_losses['loss_value'],\n",
    "                color='blue')\n",
    "\n",
    "    # Plot and scatter validation loss at the end of each epoch\n",
    "    last_val_losses = val_losses_df.groupby('epoch').last().reset_index()\n",
    "    plt.plot(last_val_losses['step'], last_val_losses['loss_value'],\n",
    "             color='orange', label='Validation Loss')\n",
    "    plt.scatter(last_val_losses['step'], last_val_losses['loss_value'],\n",
    "                color='orange')\n",
    "\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Function to print final loss values\n",
    "def print_final_losses(train_losses_df, val_losses_df):\n",
    "    print(\"Final Training Loss:\", train_losses_df.groupby('epoch')['loss_value'].last().iloc[-1])\n",
    "    print(\"Final Validation Loss:\", val_losses_df.groupby('epoch')['loss_value'].last().iloc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot loss curves for multiple models stored in loss_comparison_dict\n",
    "def plot_loss_comparisons():\n",
    "    \"\"\"\n",
    "    Plots the training and validation loss curves for multiple models added to the loss comparison dictionary.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    # Get the last model in the dictionary\n",
    "    last_model_name = list(loss_comparison_dict.keys())[-1]\n",
    "\n",
    "    # Loop through each model in the loss dictionary\n",
    "    for model_name, losses in loss_comparison_dict.items():\n",
    "        train_losses_df = losses['train']\n",
    "        val_losses_df = losses['val']\n",
    "\n",
    "        # Plot training losses for each model\n",
    "        plt.plot(train_losses_df['step'], train_losses_df['loss_value'],\n",
    "                 label=f'{model_name}', linestyle='-', alpha=0.7)\n",
    "\n",
    "        # scatter training loss at the end of each epoch\n",
    "        last_train_losses = train_losses_df.groupby('epoch').last().reset_index()\n",
    "\n",
    "        # Scatter normal points for all models\n",
    "        plt.scatter(last_train_losses['step'], last_train_losses['loss_value'], marker='o', s=50)\n",
    "\n",
    "        # If this is the last model, highlight the last point with a star (no label in legend)\n",
    "        if model_name == last_model_name:\n",
    "            final_step = train_losses_df['step'].iloc[-1]\n",
    "            final_loss = train_losses_df['loss_value'].iloc[-1]\n",
    "            plt.scatter(final_step, final_loss, marker='*', s=100, color='red', zorder=5)  # Reduced size to s=100\n",
    "\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Comparison')\n",
    "    plt.legend()  # Legend will only show colors, no final star\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
