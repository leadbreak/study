{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Model A (Combined nn.Linear):\n",
      "  Total Time for 100 iterations: 26.97 ms\n",
      "  Peak GPU Memory Usage: 12.02 MB\n",
      "\n",
      "Model B (Sparse MoE with Separate nn.Linear):\n",
      "  Total Time for 100 iterations: 257.81 ms\n",
      "  Peak GPU Memory Usage: 11.79 MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "# GPU 사용 가능 여부 확인\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 게이팅 네트워크 정의\n",
    "class GatingNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, num_experts, add_noise=False, noise_std=1.0):\n",
    "        super(GatingNetwork, self).__init__()\n",
    "        self.layer = nn.Linear(input_dim, num_experts)\n",
    "        self.add_noise = add_noise\n",
    "        self.noise_std = noise_std\n",
    "\n",
    "    def forward(self, x):\n",
    "        gate_logits = self.layer(x)  # [batch_size, num_experts]\n",
    "        if self.add_noise and self.training:\n",
    "            noise = torch.randn_like(gate_logits) * self.noise_std\n",
    "            gate_logits = gate_logits + noise\n",
    "        gate_probs = torch.softmax(gate_logits, dim=-1)  # [batch_size, num_experts]\n",
    "        return gate_probs  # [batch_size, num_experts]\n",
    "\n",
    "# MoE with separate nn.Linear layers for each expert (True Sparse MoE)\n",
    "class MoE_Sparse_True(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_experts, topk=2, add_noise=False, noise_std=1.0):\n",
    "        super(MoE_Sparse_True, self).__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.topk = topk\n",
    "\n",
    "        # 전문가들을 개별 nn.Linear 모듈로 정의\n",
    "        self.experts = nn.ModuleList([nn.Linear(input_dim, output_dim).to(device) for _ in range(num_experts)])\n",
    "        for expert in self.experts:\n",
    "            nn.init.xavier_uniform_(expert.weight)\n",
    "            nn.init.zeros_(expert.bias)\n",
    "\n",
    "        self.gate = GatingNetwork(input_dim, num_experts, add_noise=add_noise, noise_std=noise_std)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # 1. 게이팅 네트워크를 통해 전문가 선택 확률 계산\n",
    "        gate_probs = self.gate(x)  # [batch_size, num_experts]\n",
    "\n",
    "        # 2. Topk 전문가 선택\n",
    "        topk_probs, topk_indices = torch.topk(gate_probs, self.topk, dim=-1)  # [batch_size, topk]\n",
    "\n",
    "        # 3. 초기 출력 텐서 생성\n",
    "        output = torch.zeros(batch_size, self.experts[0].out_features, device=x.device)\n",
    "\n",
    "        # 4. 배치와 Topk를 평탄화\n",
    "        flat_indices = topk_indices.view(-1)  # [batch_size * topk]\n",
    "        flat_probs = topk_probs.view(-1)      # [batch_size * topk]\n",
    "\n",
    "        # 5. 고유한 전문가 식별\n",
    "        unique_experts = torch.unique(flat_indices)\n",
    "\n",
    "        # 6. 각 고유 전문가별로 출력을 계산하고 누적\n",
    "        for expert_id in unique_experts:\n",
    "            # 해당 전문가가 선택된 위치 마스크 생성\n",
    "            mask = (flat_indices == expert_id)  # [batch_size * topk]\n",
    "            if mask.sum() == 0:\n",
    "                continue\n",
    "            indices = mask.nonzero(as_tuple=False).view(-1)  # [count]\n",
    "\n",
    "            # 배치 인덱스 계산 (0부터 batch_size-1까지)\n",
    "            batch_indices = (indices // self.topk).long()  # [count]\n",
    "\n",
    "            # 선택된 입력과 확률 추출\n",
    "            x_selected = x[batch_indices]               # [count, input_dim]\n",
    "            probs_selected = flat_probs[indices].unsqueeze(1)  # [count, 1]\n",
    "\n",
    "            # 전문가 출력 계산\n",
    "            expert_output = self.experts[expert_id](x_selected)  # [count, output_dim]\n",
    "\n",
    "            # 출력 누적 (배치 인덱스를 사용하여 직접 인덱싱)\n",
    "            output[batch_indices] += expert_output * probs_selected  # [batch_size, output_dim]\n",
    "\n",
    "        return output  # [batch_size, output_dim]\n",
    "\n",
    "# MoE with a single combined nn.Linear layer (for reference)\n",
    "class MoE_Combined(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_experts, topk=2, add_noise=False, noise_std=1.0):\n",
    "        super(MoE_Combined, self).__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.topk = topk\n",
    "\n",
    "        # 모든 전문가를 하나의 큰 nn.Linear 레이어로 통합\n",
    "        self.experts = nn.Linear(input_dim, output_dim * num_experts)\n",
    "        nn.init.xavier_uniform_(self.experts.weight)\n",
    "        nn.init.zeros_(self.experts.bias)\n",
    "\n",
    "        self.gate = GatingNetwork(input_dim, num_experts, add_noise=add_noise, noise_std=noise_std)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # 1. 게이팅 네트워크를 통해 전문가 선택 확률 계산\n",
    "        gate_probs = self.gate(x)  # [batch_size, num_experts]\n",
    "\n",
    "        # 2. Topk 전문가 선택\n",
    "        topk_probs, topk_indices = torch.topk(gate_probs, self.topk, dim=-1)  # [batch_size, topk]\n",
    "\n",
    "        # 3. 전문가 출력 계산 (모든 전문가의 출력을 한 번에 계산)\n",
    "        expert_outputs = self.experts(x)  # [batch_size, num_experts * output_dim]\n",
    "        expert_outputs = expert_outputs.view(batch_size, self.num_experts, -1)  # [batch_size, num_experts, output_dim]\n",
    "\n",
    "        # 4. 선택된 Topk 전문가의 출력 추출\n",
    "        batch_indices = torch.arange(batch_size).unsqueeze(1).expand(-1, self.topk).to(x.device)  # [batch_size, topk]\n",
    "        selected_expert_outputs = expert_outputs[batch_indices, topk_indices]  # [batch_size, topk, output_dim]\n",
    "\n",
    "        # 5. 전문가 출력에 확률 가중치 적용\n",
    "        topk_probs = topk_probs.unsqueeze(-1)  # [batch_size, topk, 1]\n",
    "        weighted_expert_outputs = selected_expert_outputs * topk_probs  # [batch_size, topk, output_dim]\n",
    "\n",
    "        # 6. 최종 출력 계산 (Topk 전문가의 가중합)\n",
    "        output = weighted_expert_outputs.sum(dim=1)  # [batch_size, output_dim]\n",
    "\n",
    "        return output  # [batch_size, output_dim]\n",
    "\n",
    "# GPU 메모리 사용량 측정 함수\n",
    "def get_gpu_memory():\n",
    "    return torch.cuda.memory_allocated(device) / (1024 ** 2)  # MB 단위로 변환\n",
    "\n",
    "# GPU 메모리 초기화 함수\n",
    "def reset_gpu_memory():\n",
    "    torch.cuda.reset_peak_memory_stats(device)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# 모델 비교 테스트 함수\n",
    "def compare_models(model_a, model_b, input_dim, batch_size, num_iterations=100):\n",
    "    # 모델을 GPU로 이동\n",
    "    model_a.to(device)\n",
    "    model_b.to(device)\n",
    "\n",
    "    # 모델을 평가 모드로 설정\n",
    "    model_a.eval()\n",
    "    model_b.eval()\n",
    "\n",
    "    # 더미 입력 데이터 생성\n",
    "    x = torch.randn(batch_size, input_dim).to(device)\n",
    "\n",
    "    # 워밍업\n",
    "    with torch.no_grad():\n",
    "        model_a(x)\n",
    "        model_b(x)\n",
    "\n",
    "    # GPU 메모리 초기화\n",
    "    reset_gpu_memory()\n",
    "\n",
    "    # 모델 A (Combined) 측정\n",
    "    start_event_a = torch.cuda.Event(enable_timing=True)\n",
    "    end_event_a = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    start_event_a.record()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_iterations):\n",
    "            output_a = model_a(x)\n",
    "    end_event_a.record()\n",
    "    torch.cuda.synchronize()\n",
    "    elapsed_time_a = start_event_a.elapsed_time(end_event_a)  # ms 단위\n",
    "    peak_mem_a = torch.cuda.max_memory_allocated(device) / (1024 ** 2)  # MB 단위\n",
    "\n",
    "    # GPU 메모리 초기화\n",
    "    reset_gpu_memory()\n",
    "\n",
    "    # 모델 B (Sparse) 측정\n",
    "    start_event_b = torch.cuda.Event(enable_timing=True)\n",
    "    end_event_b = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    start_event_b.record()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_iterations):\n",
    "            output_b = model_b(x)\n",
    "    end_event_b.record()\n",
    "    torch.cuda.synchronize()\n",
    "    elapsed_time_b = start_event_b.elapsed_time(end_event_b)  # ms 단위\n",
    "    peak_mem_b = torch.cuda.max_memory_allocated(device) / (1024 ** 2)  # MB 단위\n",
    "\n",
    "    # 결과 출력\n",
    "    print(f\"Model A (Combined nn.Linear):\")\n",
    "    print(f\"  Total Time for {num_iterations} iterations: {elapsed_time_a:.2f} ms\")\n",
    "    print(f\"  Peak GPU Memory Usage: {peak_mem_a:.2f} MB\\n\")\n",
    "\n",
    "    print(f\"Model B (Sparse MoE with Separate nn.Linear):\")\n",
    "    print(f\"  Total Time for {num_iterations} iterations: {elapsed_time_b:.2f} ms\")\n",
    "    print(f\"  Peak GPU Memory Usage: {peak_mem_b:.2f} MB\\n\")\n",
    "\n",
    "# 메인 함수\n",
    "def main():\n",
    "    # 파라미터 설정\n",
    "    input_dim = 128\n",
    "    output_dim = 256\n",
    "    num_experts = 10\n",
    "    batch_size = 32\n",
    "    topk = 2\n",
    "    add_noise = False  # 공정한 비교를 위해 노이즈 비활성화\n",
    "    noise_std = 1.0\n",
    "    num_iterations = 100\n",
    "\n",
    "    # 모델 인스턴스화\n",
    "    model_combined = MoE_Combined(input_dim, output_dim, num_experts, topk=topk, add_noise=add_noise, noise_std=noise_std).to(device)\n",
    "    model_sparse = MoE_Sparse_True(input_dim, output_dim, num_experts, topk=topk, add_noise=add_noise, noise_std=noise_std).to(device)\n",
    "\n",
    "    # 모델 비교 실행\n",
    "    compare_models(model_combined, model_sparse, input_dim, batch_size, num_iterations=num_iterations)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
