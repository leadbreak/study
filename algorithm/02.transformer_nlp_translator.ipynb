{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data & Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_excel('./대화체.xlsx')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 그래프에서 한글이 깨지지 않게 폰트 설치. \n",
    "# 맨처음 실행 후 런타임 다시시작해야 반영됨\n",
    "# colab이라면 cell에서, Linux 등의 환경이라면 터미널 통해서 아래 코드 실행\n",
    "'''\n",
    "!sudo apt-get install -y fonts-nanum\n",
    "!sudo fc-cache -fv\n",
    "!rm ~/.cache/matplotlib -rf\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# plt.rc('font', family='NanumBarunGothic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "!pip install transformers\n",
    "!pip install sentencepiece # MarianTokenizer 불러올 때 필요\n",
    "!pip install sacremoses # MarianMTModel 에서 불러올 때 warning 뜨는 것 방지\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128 # 논문에선 2.5만 token이 한 batch에 담기게 했다고 함.\n",
    "EPOCH = 15 \n",
    "max_len = 512 # model.model.encoder.embed_positions 를 보면 512로 했음을 알 수 있다.\n",
    "d_model = 512\n",
    "\n",
    "warmup_steps = 1500 # 데이터 수 * EPOCH / BS = 총 step 수 인것 고려 \n",
    "LR_scale = 0.5 # Noam scheduler에 peak LR 값 조절을 위해 곱해질 녀석 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data.loc[idx, '원문'], self.data.loc[idx, '번역문']\n",
    "\n",
    "data = pd.read_excel('대화체.xlsx')\n",
    "custom_DS = CustomDataset(data)\n",
    "\n",
    "train_DS, val_DS, test_DS, _ = torch.utils.data.random_split(custom_DS, [95000, 2000, 1000, len(custom_DS)-95000-2000-1000])\n",
    "\n",
    "train_DL = torch.utils.data.DataLoader(train_DS, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_DL = torch.utils.data.DataLoader(val_DS, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_DL = torch.utils.data.DataLoader(test_DS, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(len(train_DS))\n",
    "print(len(val_DS))\n",
    "print(len(test_DS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotly.graph_objs as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-ko-en')\n",
    "\n",
    "eos_idx = tokenizer.eos_token_id\n",
    "pad_idx = tokenizer.pad_token_id\n",
    "print(\"eos_idx = \", eos_idx)\n",
    "print(\"pad_idx = \", pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.vocab_size\n",
    "print(f'tokenizer의 사전 크기: {vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = 'Tokenizer Test is Started with Hugginface MarianTokenizer'\n",
    "print(f\"original : {text}\")\n",
    "print(f\"token : {tokenizer.tokenize(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = '허깅페이스 마리안 토크나이저로 수행하는 토크나이저 테스트'\n",
    "print(f\"original : {text}\")\n",
    "print(f\"token : {tokenizer.tokenize(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = '문장을 넣으면 토크나이즈해서 숫자로 바꿔줍니다.'\n",
    "\n",
    "tokenized = tokenizer.tokenize(text)\n",
    "encoded_tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "encoded_tokens_end = tokenizer.encode(text, add_special_tokens=True)\n",
    "\n",
    "print(tokenized)\n",
    "print(encoded_tokens)\n",
    "print(encoded_tokens_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(tokenizer.decode([0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(tokenizer.decode([13774]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.decode(encoded_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Scheduler & Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Noam Scheduler 공식\n",
    "\n",
    "### $\\text{Learning Rate} = \\frac{1}{\\sqrt{d_{\\text{model}}}} \\times \\min\\left(\\frac{1}{\\sqrt{\\text{step\\_num}}}, \\frac{\\text{step\\_num}}{\\text{warmup\\_steps}^{1.5}}\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_params(model):\n",
    "    num = sum([p.numel() for p in model.parameters() if p.requires_grad])\n",
    "    return num\n",
    "\n",
    "class NoamScheduler:\n",
    "    def __init__(self, optimizer, d_model, warmup_steps, LR_scale = 1):\n",
    "        self.optimizer = optimizer\n",
    "        self.current_step = 0\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.LR_scale = LR_scale\n",
    "\n",
    "    def step(self):\n",
    "        self.current_step += 1\n",
    "        lrate = self.LR_scale * (self.d_model ** -0.5) * min(self.current_step ** -0.5, self.current_step * self.warmup_steps ** -1.5)\n",
    "        self.optimizer.param_groups[0]['lr'] = lrate\n",
    "        \n",
    "def plot_scheduler(scheduler_name, optimizer, scheduler, total_steps): # LR curve 보기\n",
    "    lr_history = []\n",
    "    steps = range(1, total_steps)\n",
    "\n",
    "    for _ in steps: # base model -> 10만 steps (12시간), big model -> 30만 steps (3.5일) 로 훈련했다고 함\n",
    "        lr_history += [optimizer.param_groups[0]['lr']]\n",
    "        scheduler.step()\n",
    "\n",
    "    plt.figure()\n",
    "    if total_steps == 100000:\n",
    "        plt.plot(steps, (512 ** -0.5) * torch.tensor(steps) ** -0.5, 'g--', linewidth=1, label=r\"$d_{\\mathrm{model}}^{-0.5} \\cdot \\mathrm{step}^{-0.5}$\")\n",
    "        plt.plot(steps, (512 ** -0.5) * torch.tensor(steps) * 4000 ** -1.5, 'r--', linewidth=1, label=r\"$d_{\\mathrm{model}}^{-0.5} \\cdot \\mathrm{step} \\cdot \\mathrm{warmup\\_steps}^{-1.5}$\")    \n",
    "    plt.plot(steps, lr_history, 'b', linewidth=2, alpha=0.35, label=\"Learning Rate\")\n",
    "\n",
    "    plt.ylim([-0.1*max(lr_history), 1.2*max(lr_history)])\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(nn.Linear(1, 1).parameters(), lr=0) # 테스트용 optimizer\n",
    "scheduler = NoamScheduler(optimizer, d_model=512, warmup_steps=4000) # 논문 값\n",
    "plot_scheduler(scheduler_name = 'Noam', optimizer = optimizer, scheduler = scheduler, total_steps = 100000)\n",
    "\n",
    "optimizer = optim.Adam(nn.Linear(1, 1).parameters(), lr=0)\n",
    "scheduler = NoamScheduler(optimizer, d_model=d_model, warmup_steps=warmup_steps, LR_scale=LR_scale)\n",
    "plot_scheduler(scheduler_name = 'Noam', optimizer = optimizer, scheduler = scheduler, total_steps = int(len(train_DS)*EPOCH/BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. Regularization - Label Smoothing Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def smooth_label(targets: torch.Tensor, classes: int, smoothing=0.1):\n",
    "    assert 0 <= smoothing < 1\n",
    "    confidence = 1.0 - smoothing\n",
    "    label_shape = torch.Size((targets.size(0), classes))\n",
    "    with torch.no_grad():\n",
    "        smooth_labels = torch.empty(size=label_shape, device=targets.device)\n",
    "        smooth_labels.fill_(smoothing / (classes - 1))\n",
    "        smooth_labels.scatter_(1, targets.data.unsqueeze(1), confidence)\n",
    "    return smooth_labels\n",
    "\n",
    "def custom_cross_entropy(input, target, smoothing=0.1, ignore_index=-100):\n",
    "    log_probs = F.log_softmax(input, dim=-1)\n",
    "    target = smooth_label(target, input.size(-1), smoothing)\n",
    "    \n",
    "    if ignore_index >= 0:\n",
    "        mask = target != ignore_index\n",
    "        target = target[mask]\n",
    "        log_probs = log_probs[mask]\n",
    "\n",
    "    loss = (-target * log_probs).sum(dim=-1)\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input = torch.randn(3, 5, requires_grad=True) # 임의의 예측값\n",
    "target = torch.tensor([1, 0, 4])  # 실제 레이블\n",
    "label = smooth_label(target, input.size(-1), 0.0)\n",
    "label_smoothing = smooth_label(target, input.size(-1), 0.1)\n",
    "\n",
    "loss = custom_cross_entropy(input, target, smoothing=0.0, ignore_index=pad_idx)\n",
    "loss_smoothing = custom_cross_entropy(input, target, smoothing=0.1, ignore_index=pad_idx)\n",
    "input, label, label_smoothing, loss, loss_smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LabelSmoothingCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, smoothing=0.1, ignore_index=-100):\n",
    "        super(LabelSmoothingCrossEntropyLoss, self).__init__()\n",
    "        self.smoothing = smoothing\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        log_probs = F.log_softmax(input, dim=-1)\n",
    "        n_classes = input.size(-1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Create a smoothed label distribution\n",
    "            true_dist = torch.full_like(log_probs, self.smoothing / (n_classes - 1))\n",
    "            ignore = target == self.ignore_index\n",
    "            target = target.masked_fill(ignore, 0)\n",
    "            true_dist.scatter_(1, target.unsqueeze(1), 1.0 - self.smoothing)\n",
    "            true_dist.masked_fill_(ignore.unsqueeze(1), 0)\n",
    "\n",
    "            # Mask to avoid calculating loss for ignored indices\n",
    "            mask = ~ignore\n",
    "\n",
    "        # Apply mask and compute loss\n",
    "        loss = -true_dist * log_probs\n",
    "        loss = loss.masked_select(mask.unsqueeze(1)).mean()\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = LabelSmoothingCrossEntropyLoss(smoothing=0.1, ignore_index=pad_idx)\n",
    "# criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1) MHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MHA(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        assert d_model % n_heads == 0, f'd_model ({d_model})은 n_heads ({n_heads})로 나누어 떨어져야 합니다.'\n",
    "\n",
    "        self.head_dim = d_model // n_heads  # int 형변환 제거\n",
    "\n",
    "        # 쿼리, 키, 값에 대한 선형 변환\n",
    "        self.fc_q = nn.Linear(d_model, d_model) \n",
    "        self.fc_k = nn.Linear(d_model, d_model) \n",
    "        self.fc_v = nn.Linear(d_model, d_model)\n",
    "        self.fc_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # 어텐션 점수를 위한 스케일 요소\n",
    "        self.scale = torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        batch_size = Q.shape[0]\n",
    "\n",
    "        # 쿼리, 키, 값에 대한 선형 변환 수행\n",
    "        Q = self.fc_q(Q) \n",
    "        K = self.fc_k(K)\n",
    "        V = self.fc_v(V)\n",
    "\n",
    "        # 멀티 헤드 어텐션을 위해 텐서 재구성 및 순서 변경\n",
    "        Q = Q.reshape(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.reshape(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.reshape(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "        # 스케일드 닷-프로덕트 어텐션 계산\n",
    "        attention_score = Q @ K.permute(0, 1, 3, 2) / self.scale\n",
    "\n",
    "        # 마스크 적용 (제공된 경우)\n",
    "        if mask is not None:\n",
    "            attention_score = attention_score.masked_fill(mask, -1e10)\n",
    "\n",
    "        # 소프트맥스를 사용하여 어텐션 확률 계산\n",
    "        attention_dist = torch.softmax(attention_score, dim=-1)\n",
    "\n",
    "        # 어텐션 결과\n",
    "        attention = attention_dist @ V\n",
    "\n",
    "        # 어텐션 헤드 재조립\n",
    "        x = attention.permute(0, 2, 1, 3).reshape(batch_size, -1, self.d_model)\n",
    "\n",
    "        # 최종 선형 변환\n",
    "        x = self.fc_o(x)\n",
    "\n",
    "        return x, attention_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Feed Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, drop_p):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear = nn.Sequential(nn.Linear(d_model, d_ff),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Dropout(drop_p),       ## ADD Dropout\n",
    "                                    nn.Linear(d_ff, d_model))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Encoder Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, drop_p):\n",
    "        \"\"\"\n",
    "        EncoderLayer 클래스의 초기화 메소드입니다.\n",
    "        :param d_model: 모델의 차원 크기\n",
    "        :param n_heads: 어텐션 헤드의 개수\n",
    "        :param d_ff: 피드 포워드 네트워크의 내부 차원\n",
    "        :param drop_p: 드롭아웃 비율\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_atten = MHA(d_model, n_heads)\n",
    "        self.self_atten_LN = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.FF = FeedForward(d_model, d_ff, drop_p)\n",
    "        self.FF_LN = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "    \n",
    "    def forward(self, x, enc_mask):\n",
    "        \"\"\"\n",
    "        EncoderLayer 클래스의 순전파 메소드입니다.\n",
    "        :param x: 입력 텐서\n",
    "        :param enc_mask: 인코더 마스크\n",
    "        \"\"\"\n",
    "        x_norm = self.self_atten_LN(x) ## Pre-LN\n",
    "        \n",
    "        # 멀티헤드 어텐션과 잔차 연결\n",
    "        residual, atten_enc = self.self_atten(x_norm, x_norm, x_norm, enc_mask)\n",
    "        x = x + self.dropout(residual)\n",
    "\n",
    "        # 레이어 정규화 적용\n",
    "        x_norm = self.FF_LN(x)\n",
    "        # 피드 포워드 네트워크와 잔차 연결\n",
    "        residual = self.FF(x_norm)\n",
    "        x = x + self.dropout(residual)\n",
    "\n",
    "        return x, atten_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_embedding, max_len, d_model, n_heads, n_layers, d_ff, drop_p):\n",
    "        \"\"\"\n",
    "        Encoder 클래스의 초기화 메소드입니다.\n",
    "        :param input_embedding: 입력 임베딩 레이어\n",
    "        :param max_len: 입력 시퀀스의 최대 길이\n",
    "        :param d_model: 모델의 차원 크기\n",
    "        :param n_heads: 멀티헤드 어텐션의 헤드 수\n",
    "        :param n_layers: 인코더 레이어의 수\n",
    "        :param d_ff: 피드 포워드 네트워크의 내부 차원\n",
    "        :param drop_p: 드롭아웃 비율\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # 스케일링 팩터\n",
    "        self.scale = torch.sqrt(torch.tensor(d_model, dtype=torch.float32))\n",
    "        self.input_embedding = input_embedding\n",
    "        self.pos_embedding = nn.Embedding(max_len, d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "\n",
    "        # 인코더 레이어를 n_layers만큼 생성\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, n_heads, d_ff, drop_p) for _ in range(n_layers)])        \n",
    "\n",
    "    def forward(self, src, mask, atten_map_save=False):\n",
    "        \"\"\"\n",
    "        Encoder 클래스의 순전파 메소드입니다.\n",
    "        :param src: 입력 소스\n",
    "        :param mask: 인코더 마스크\n",
    "        :param atten_map_save: 어텐션 맵 저장 여부\n",
    "        \"\"\"\n",
    "        pos = torch.arange(src.shape[1], device=src.device).repeat(src.shape[0], 1) # 위치 임베딩 생성\n",
    "\n",
    "        x = self.scale * self.input_embedding(src) + self.pos_embedding(pos)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        atten_encs = []\n",
    "        for layer in self.layers:\n",
    "            x, atten_enc = layer(x, mask)\n",
    "            if atten_map_save:\n",
    "                atten_encs.append(atten_enc[0].unsqueeze(0))\n",
    "\n",
    "        if atten_map_save:\n",
    "            atten_encs = torch.cat(atten_encs, dim=0)\n",
    "\n",
    "        return x, atten_encs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4) Decoder Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, drop_p):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_atten = MHA(d_model, n_heads)\n",
    "        self.self_atten_LN = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.enc_dec_atten = MHA(d_model, n_heads)\n",
    "        self.enc_dec_atten_LN = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.FF = FeedForward(d_model, d_ff, drop_p)\n",
    "        self.FF_LN = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "\n",
    "    def forward(self, x, enc_out, dec_mask, enc_dec_mask):\n",
    "\n",
    "        residual, atten_dec = self.self_atten(x, x, x, dec_mask)\n",
    "        residual = self.dropout(residual)\n",
    "        x = self.self_atten_LN(x + residual)\n",
    "\n",
    "        residual, atten_enc_dec = self.enc_dec_atten(x, enc_out, enc_out, enc_dec_mask) # Q는 디코더로부터 K,V는 인코더로부터!!\n",
    "        residual = self.dropout(residual)\n",
    "        x = self.enc_dec_atten_LN(x + residual)\n",
    "\n",
    "        residual = self.FF(x)\n",
    "        residual = self.dropout(residual)\n",
    "        x = self.FF_LN(x + residual)\n",
    "\n",
    "        return x, atten_dec, atten_enc_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, drop_p):\n",
    "        \"\"\"\n",
    "        DecoderLayer 클래스의 초기화 메소드입니다.\n",
    "        :param d_model: 모델의 차원 크기\n",
    "        :param n_heads: 멀티헤드 어텐션의 헤드 수\n",
    "        :param d_ff: 피드 포워드 네트워크의 내부 차원\n",
    "        :param drop_p: 드롭아웃 비율\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Self-Attention 레이어\n",
    "        self.self_atten = MHA(d_model, n_heads)\n",
    "        self.self_atten_LN = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Encoder-Decoder Attention 레이어\n",
    "        self.enc_dec_atten = MHA(d_model, n_heads)\n",
    "        self.enc_dec_atten_LN = nn.LayerNorm(d_model)\n",
    "\n",
    "        # 피드 포워드 네트워크\n",
    "        self.FF = FeedForward(d_model, d_ff, drop_p)\n",
    "        self.FF_LN = nn.LayerNorm(d_model)\n",
    "\n",
    "        # 드롭아웃\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "\n",
    "#     def forward(self, x, enc_out, dec_mask, enc_dec_mask):\n",
    "#         \"\"\"\n",
    "#         DecoderLayer 클래스의 순전파 메소드입니다.\n",
    "#         :param x: 디코더의 입력\n",
    "#         :param enc_out: 인코더의 출력\n",
    "#         :param dec_mask: 디코더 마스크\n",
    "#         :param enc_dec_mask: 인코더-디코더 마스크\n",
    "#         \"\"\"\n",
    "#         # 디코더 Self-Attention\n",
    "#         residual, atten_dec = self.self_atten(x, x, x, dec_mask)\n",
    "#         residual = self.dropout(residual)\n",
    "#         x = self.self_atten_LN(x + residual)\n",
    "\n",
    "#         # Encoder-Decoder Attention\n",
    "#         # Q from Decoder + K,V from Encoder\n",
    "#         residual, atten_enc_dec = self.enc_dec_atten(x, enc_out, enc_out, enc_dec_mask)\n",
    "#         residual = self.dropout(residual)\n",
    "#         x = self.enc_dec_atten_LN(x + residual)\n",
    "\n",
    "#         # 피드 포워드 네트워크\n",
    "#         residual = self.FF(x)\n",
    "#         residual = self.dropout(residual)\n",
    "#         x = self.FF_LN(x + residual)\n",
    "\n",
    "#         return x, atten_dec, atten_enc_dec\n",
    "\n",
    "    def forward(self, x, enc_out, dec_mask, enc_dec_mask):\n",
    "        \"\"\"\n",
    "        DecoderLayer 클래스의 순전파 메소드입니다.\n",
    "        :param x: 디코더의 입력\n",
    "        :param enc_out: 인코더의 출력\n",
    "        :param dec_mask: 디코더 마스크\n",
    "        :param enc_dec_mask: 인코더-디코더 마스크\n",
    "        \"\"\"\n",
    "        x, atten_dec = self.process_sublayer(x, self.self_atten, self.self_atten_LN, dec_mask)\n",
    "        x, atten_enc_dec = self.process_sublayer(x, self.enc_dec_atten, self.enc_dec_atten_LN, enc_dec_mask, enc_out)\n",
    "        x, _ = self.process_sublayer(x, self.FF, self.FF_LN)\n",
    "\n",
    "        return x, atten_dec, atten_enc_dec\n",
    "\n",
    "    def process_sublayer(self, x, sublayer, norm_layer, mask=None, enc_out=None):\n",
    "        \"\"\"\n",
    "        디코더의 서브레이어 처리를 위한 함수.\n",
    "        :param x: 입력 텐서\n",
    "        :param sublayer: 서브레이어 (어텐션 또는 피드 포워드)\n",
    "        :param norm_layer: 레이어 정규화\n",
    "        :param mask: 마스크 (디코더 또는 인코더-디코더 마스크)\n",
    "        :param enc_out: 인코더의 출력 (인코더-디코더 어텐션에만 필요)\n",
    "        \"\"\"\n",
    "        x_norm = norm_layer(x)\n",
    "        if isinstance(sublayer, MHA):\n",
    "            # 멀티헤드 어텐션 레이어의 경우\n",
    "            if enc_out is not None:\n",
    "                # 인코더-디코더 어텐션\n",
    "                residual, atten = sublayer(x_norm, enc_out, enc_out, mask)\n",
    "            else:\n",
    "                # 자기 어텐션\n",
    "                residual, atten = sublayer(x_norm, x_norm, x_norm, mask)\n",
    "        elif isinstance(sublayer, FeedForward):\n",
    "            # 피드 포워드 레이어의 경우\n",
    "            residual = sublayer(x_norm)\n",
    "            atten = None  # 피드 포워드 레이어는 어텐션 맵을 반환하지 않음\n",
    "        else:\n",
    "            raise TypeError(\"Unsupported sublayer type\")\n",
    "\n",
    "        return x + self.dropout(residual), atten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_embedding, max_len, d_model, n_heads, n_layers, d_ff, drop_p):\n",
    "        \"\"\"\n",
    "        Decoder 클래스의 초기화 메소드.\n",
    "        :param input_embedding: 입력 임베딩 레이어\n",
    "        :param max_len: 입력 시퀀스의 최대 길이\n",
    "        :param d_model: 모델의 차원 크기\n",
    "        :param n_heads: 멀티헤드 어텐션의 헤드 수\n",
    "        :param n_layers: 디코더 레이어의 수\n",
    "        :param d_ff: 피드 포워드 네트워크의 내부 차원\n",
    "        :param drop_p: 드롭아웃 비율\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.scale = torch.sqrt(torch.tensor(d_model, dtype=torch.float32))\n",
    "        self.input_embedding = input_embedding\n",
    "        self.pos_embedding = nn.Embedding(max_len, d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, n_heads, d_ff, drop_p) for _ in range(n_layers)])\n",
    "\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, trg, enc_out, dec_mask, enc_dec_mask, atten_map_save=False):\n",
    "        \"\"\"\n",
    "        Decoder 클래스의 순전파 메소드.\n",
    "        :param trg: 타깃 입력\n",
    "        :param enc_out: 인코더의 출력\n",
    "        :param dec_mask: 디코더 마스크\n",
    "        :param enc_dec_mask: 인코더-디코더 마스크\n",
    "        :param atten_map_save: 어텐션 맵 저장 여부\n",
    "        \"\"\"\n",
    "        pos = torch.arange(trg.shape[1], device=trg.device).repeat(trg.shape[0], 1)\n",
    "\n",
    "        x = self.scale * self.input_embedding(trg) + self.pos_embedding(pos)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        atten_decs = []\n",
    "        atten_enc_decs = []\n",
    "        for layer in self.layers:\n",
    "            x, atten_dec, atten_enc_dec = layer(x, enc_out, dec_mask, enc_dec_mask)\n",
    "            if atten_map_save:\n",
    "                atten_decs.append(atten_dec[0].unsqueeze(0))\n",
    "                atten_enc_decs.append(atten_enc_dec[0].unsqueeze(0))\n",
    "\n",
    "        if atten_map_save:\n",
    "            atten_decs = torch.cat(atten_decs, dim=0)\n",
    "            atten_enc_decs = torch.cat(atten_enc_decs, dim=0)\n",
    "\n",
    "        x = self.fc_out(x)\n",
    "        \n",
    "        return x, atten_decs, atten_enc_decs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, max_len, d_model, n_heads, n_layers, d_ff, drop_p):\n",
    "        super().__init__()\n",
    "\n",
    "        input_embedding = nn.Embedding(vocab_size, d_model) \n",
    "        self.encoder = Encoder(input_embedding, max_len, d_model, n_heads, n_layers, d_ff, drop_p)\n",
    "        self.decoder = Decoder(input_embedding, max_len, d_model, n_heads, n_layers, d_ff, drop_p)\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        # for m in self.modules():\n",
    "        #     if hasattr(m,'weight') and m.weight.dim() > 1: # layer norm에 대해선 initial 안하겠다는 뜻\n",
    "        #         nn.init.kaiming_uniform_(m.weight) # He의 분산은 2/Nin\n",
    "\n",
    "        for m in self.modules():\n",
    "            if hasattr(m,'weight') and m.weight.dim() > 1: # 인풋 임베딩은 그대로 쓰기 위함 \n",
    "                nn.init.xavier_uniform_(m.weight) # xavier의 분산은 2/(Nin+Nout) 즉, 분산이 더 작다. => 그래서 sigmoid/tanh에 적합한 것! (vanishing gradient 막기 위해)\n",
    "\n",
    "    def make_enc_mask(self, src): # src.shape = 개단\n",
    "        \n",
    "        enc_mask = (src == pad_idx).unsqueeze(1).unsqueeze(2) # 개11단\n",
    "        enc_mask = enc_mask.repeat(1, self.n_heads, src.shape[1], 1) # 개헤단단   \n",
    "        \"\"\" src pad mask\n",
    "        F F T T\n",
    "        F F T T\n",
    "        F F T T\n",
    "        F F T T\n",
    "        \"\"\"\n",
    "        return enc_mask\n",
    "\n",
    "    def make_dec_mask(self, trg): # trg.shape = 개단\n",
    "\n",
    "        trg_pad_mask = (trg.to('cpu') == pad_idx).unsqueeze(1).unsqueeze(2) # 개11단\n",
    "        trg_pad_mask = trg_pad_mask.repeat(1, self.n_heads, trg.shape[1], 1) # 개헤단단\n",
    "        \"\"\" trg pad mask\n",
    "        F F F T T\n",
    "        F F F T T\n",
    "        F F F T T \n",
    "        F F F T T\n",
    "        F F F T T\n",
    "        \"\"\"\n",
    "        trg_dec_mask = torch.tril(torch.ones(trg.shape[0], self.n_heads, trg.shape[1], trg.shape[1]))==0 # 개헤단단\n",
    "        \"\"\" trg future mask\n",
    "        F T T T T\n",
    "        F F T T T\n",
    "        F F F T T \n",
    "        F F F F T\n",
    "        F F F F F\n",
    "        \"\"\"\n",
    "        dec_mask = trg_pad_mask | trg_dec_mask # dec_mask.shape = 개헤단단\n",
    "        \"\"\" decoder mask\n",
    "        F T T T T\n",
    "        F F T T T\n",
    "        F F F T T \n",
    "        F F F T T\n",
    "        F F F T T\n",
    "        \"\"\"\n",
    "        return dec_mask\n",
    "\n",
    "    def make_enc_dec_mask(self, src, trg):\n",
    "\n",
    "        enc_dec_mask = (src == pad_idx).unsqueeze(1).unsqueeze(2) # 개11단\n",
    "        enc_dec_mask = enc_dec_mask.repeat(1, self.n_heads, trg.shape[1], 1) # 개헤단단   \n",
    "        \"\"\" src pad mask\n",
    "        F F T T\n",
    "        F F T T\n",
    "        F F T T\n",
    "        F F T T \n",
    "        F F T T \n",
    "        \"\"\"\n",
    "        return enc_dec_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "\n",
    "        enc_mask = self.make_enc_mask(src)\n",
    "        dec_mask = self.make_dec_mask(trg)\n",
    "        enc_dec_mask = self.make_enc_dec_mask(src, trg)\n",
    "\n",
    "        enc_out, atten_encs = self.encoder(src, enc_mask)\n",
    "        out, atten_decs, atten_enc_decs = self.decoder(trg, enc_out, dec_mask, enc_dec_mask)\n",
    "\n",
    "        return out, atten_encs, atten_decs, atten_enc_decs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, max_len, d_model, n_heads, n_layers, d_ff, drop_p):\n",
    "        \"\"\"\n",
    "        Transformer 클래스의 초기화 메소드.\n",
    "        :param vocab_size: 어휘 사전의 크기\n",
    "        :param max_len: 입력 시퀀스의 최대 길이\n",
    "        :param d_model: 모델의 차원 크기\n",
    "        :param n_heads: 멀티헤드 어텐션의 헤드 수\n",
    "        :param n_layers: 인코더 및 디코더 레이어의 수\n",
    "        :param d_ff: 피드 포워드 네트워크의 내부 차원\n",
    "        :param drop_p: 드롭아웃 비율\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        input_embedding = nn.Embedding(vocab_size, d_model) \n",
    "        self.encoder = Encoder(input_embedding, max_len, d_model, n_heads, n_layers, d_ff, drop_p)\n",
    "        self.decoder = Decoder(input_embedding, max_len, d_model, n_heads, n_layers, d_ff, drop_p)\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        # 파라미터 초기화\n",
    "        for m in self.modules():\n",
    "            if hasattr(m, 'weight') and m.weight.dim() > 1: \n",
    "                nn.init.xavier_uniform_(m.weight) \n",
    "\n",
    "    def make_enc_mask(self, src):\n",
    "        \"\"\"\n",
    "        인코더 마스크 생성.\n",
    "        :param src: 입력 소스 (batch_size, src_len)\n",
    "        :return: 인코더 마스크 (batch_size, 1, 1, src_len)\n",
    "                 - pad_idx에 해당하는 위치는 True, 그 외는 False\n",
    "        \"\"\"\n",
    "        enc_mask = (src == pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        return enc_mask.repeat(1, self.n_heads, src.shape[1], 1).to(src.device)\n",
    "\n",
    "    def make_dec_mask(self, trg):\n",
    "        \"\"\"\n",
    "        디코더 마스크 생성 (패딩 마스크 및 미래 토큰 마스킹).\n",
    "        :param trg: 타깃 입력 (batch_size, trg_len)\n",
    "        :return: 디코더 마스크 (batch_size, 1, trg_len, trg_len)\n",
    "                 - 패딩 위치 및 미래 위치는 True, 그 외는 False\n",
    "        \"\"\"\n",
    "        trg_pad_mask = (trg == pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        trg_pad_mask = trg_pad_mask.repeat(1, self.n_heads, trg.shape[1], 1).to(trg.device)\n",
    "        trg_dec_mask = torch.tril(torch.ones(trg.shape[0], self.n_heads, trg.shape[1], trg.shape[1], device=trg.device))==0\n",
    "        dec_mask = trg_pad_mask | trg_dec_mask\n",
    "        return dec_mask\n",
    "\n",
    "    def make_enc_dec_mask(self, src, trg):\n",
    "        \"\"\"\n",
    "        인코더-디코더 마스크 생성.\n",
    "        :param src: 입력 소스 (batch_size, src_len)\n",
    "        :param trg: 타깃 입력 (batch_size, trg_len)\n",
    "        :return: 인코더-디코더 마스크 (batch_size, 1, trg_len, src_len)\n",
    "                 - 소스의 pad_idx 위치는 True, 그 외는 False\n",
    "        \"\"\"\n",
    "        enc_dec_mask = (src == pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        return enc_dec_mask.repeat(1, self.n_heads, trg.shape[1], 1).to(src.device)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        enc_mask = self.make_enc_mask(src)\n",
    "        dec_mask = self.make_dec_mask(trg)\n",
    "        enc_dec_mask = self.make_enc_dec_mask(src, trg)\n",
    "\n",
    "        enc_out, atten_encs = self.encoder(src, enc_mask)\n",
    "        out, atten_decs, atten_enc_decs = self.decoder(trg, enc_out, dec_mask, enc_dec_mask)\n",
    "\n",
    "        return out, atten_encs, atten_decs, atten_enc_decs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_path = './traslator.pt'\n",
    "save_history_path = './translator_history.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer & input embedding layer & last fc layer\n",
    "tokenizer = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-ko-en')\n",
    "model = MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-ko-en')\n",
    "\n",
    "eos_idx = tokenizer.eos_token_id\n",
    "pad_idx = tokenizer.pad_token_id\n",
    "print(\"eos_idx = \", eos_idx)\n",
    "print(\"pad_idx = \", pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda:2'\n",
    "\n",
    "BATCH_SIZE = 128 # 논문에선 2.5만 token이 한 batch에 담기게 했다고 함.\n",
    "EPOCH = 15 \n",
    "max_len = 512 # model.model.encoder.embed_positions 를 보면 512로 했음을 알 수 있다.\n",
    "\n",
    "scheduler_name = 'Noam'\n",
    "# scheduler_name = 'Cos'\n",
    "#### Noam ####\n",
    "# toatl_steps = 95000 // BATCH_SIZE\n",
    "# warmup_steps = 4000 # 이건 논문에서 제시한 값 (총 10만 step의 4%)\n",
    "warmup_steps = 1500 # 데이터 수 * EPOCH / BS = 총 step 수 인것 고려 # 저장된 모델\n",
    "# warmup_steps = 1300 \n",
    "LR_scale = 0.5 # Noam scheduler에 peak LR 값 조절을 위해 곱해질 녀석 # 저장된 모델\n",
    "# LR_scale = 0.4\n",
    "#### Cos ####\n",
    "LR_init = 5e-4\n",
    "T0 = 1500 # 첫 주기\n",
    "T_mult = 2 # 배 만큼 주기가 길어짐 (1보다 큰 정수여야 함)\n",
    "#############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.vocab_size\n",
    "print(vocab_size)\n",
    "\n",
    "# 논문에 나오는 base 모델 (많은 train loss를 많이 줄이려면 많은 Epoch이 요구됨, 또, test 성능도 좋으려면 더 많은 데이터 요구)\n",
    "d_model = 512\n",
    "n_heads = 8\n",
    "n_layers = 6\n",
    "d_ff = 2048\n",
    "drop_p = 0.1\n",
    "\n",
    "# 좀 사이즈 줄인 모델 (훈련된 input_embedding, fc_out 사용하면 사용 불가)\n",
    "# d_model = 256\n",
    "# n_heads = 8\n",
    "# n_layers = 3\n",
    "# d_ff = 512\n",
    "# drop_p = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_DS 테스트\n",
    "i = 5\n",
    "idx = test_DS.indices[i]\n",
    "print(idx) # 엑셀 파일에서 idx번째 문장에 들어있음을 확인할 수 있다\n",
    "src_text, trg_text = custom_DS.__getitem__(idx)\n",
    "print(src_text)\n",
    "print(trg_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_DL 테스트\n",
    "src_texts, trg_texts = next(iter(train_DL))\n",
    "\n",
    "print(src_texts)\n",
    "print(trg_texts)\n",
    "print(len(src_texts))\n",
    "print(len(trg_texts))\n",
    "\n",
    "# 여러 문장에 대해서는 tokenizer.encode() 가 아닌 그냥 tokenizer()\n",
    "src = tokenizer(src_texts, padding=True, truncation=True, max_length = max_len, return_tensors='pt').input_ids \n",
    "# add_special_tokens = True (default)면 마지막 토큰에 <eos> 가 붙어 나옴\n",
    "# truncation = True: max_len 보다 길면 끊고 <eos> 집어넣어버림 \n",
    "# <eos>가 반드시 있는 게 좋은 건지는 알 수 없지만 그냥 붙여봤어요..\n",
    "trg_texts = ['</s> ' + s for s in trg_texts]\n",
    "trg = tokenizer(trg_texts, padding=True, truncation=True, max_length = max_len, return_tensors='pt').input_ids # pt: pytorch tensor로 변환\n",
    "\n",
    "print(src[:2])\n",
    "print(trg[:2])\n",
    "print(src.shape)\n",
    "print(trg.shape)\n",
    "print(trg[:,-1]) # 가장 마지막 단어를 보니 어떤 문장은 <eos> 로 끝이 났고 나머지는 <pad> 로 끝이 났다는 걸 볼 수 있음\n",
    "print(tokenizer.decode(trg[trg[:,-1]==eos_idx,:][0])) # 가장 긴 문장 중 첫 번째 문장 관찰\n",
    "print(trg[5,:-1]) # 디코더 입력\n",
    "print(trg[5,1:]) # 디코더 출력\n",
    "# 그런데 [:,:-1] 로 주면 패딩된 문장은 eos도 넣는 셈 아닌가? 맞다! 하지만 괜찮다. 어차피 출력으로 pad token이 기다리고 있으니.. (loss에서 ignore됨)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train(model, train_DL, val_DL, criterion, optimizer):\n",
    "    loss_history = {\"train\": [], \"val\": []}\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    for ep in range(EPOCH):\n",
    "        start_time = time.time()  # 에포크 시작 시간 기록\n",
    "\n",
    "        # 학습 모드\n",
    "        model.train()\n",
    "        train_loss = loss_epoch(model, train_DL, criterion, optimizer=optimizer, max_len=max_len, DEVICE=DEVICE, tokenizer=tokenizer)\n",
    "        loss_history[\"train\"].append(train_loss)\n",
    "\n",
    "        # 평가 모드\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = loss_epoch(model, val_DL, criterion, max_len=max_len, DEVICE=DEVICE, tokenizer=tokenizer)\n",
    "            loss_history[\"val\"].append(val_loss)\n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                torch.save({\"model\": model, \"ep\": ep, \"optimizer\": optimizer.state_dict()}, save_model_path)\n",
    "\n",
    "        # 에포크 소요 시간 계산\n",
    "        end_time = time.time()\n",
    "        epoch_time = end_time - start_time\n",
    "\n",
    "        # 로그 출력\n",
    "        print(f\"Epoch{ep+1}/{EPOCH}|train loss:{train_loss:.5f} val loss:{val_loss:.5f} current_LR:{optimizer.param_groups[0]['lr']:.8f} time:{epoch_time:.2f}s\")\n",
    "        # print(\"-\" * 20)\n",
    "\n",
    "    # 모든 학습이 끝난 후 시각화\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=list(range(1, EPOCH + 1)), y=loss_history[\"train\"], mode='lines+markers', name='Train Loss'))\n",
    "    fig.add_trace(go.Scatter(x=list(range(1, EPOCH + 1)), y=loss_history[\"val\"], mode='lines+markers', name='Validation Loss'))\n",
    "    fig.update_layout(title='Loss History', xaxis_title='Epoch', yaxis_title='Loss', showlegend=True)\n",
    "    fig.show()\n",
    "\n",
    "    torch.save({\"loss_history\": loss_history, \"EPOCH\": EPOCH, \"BATCH_SIZE\": BATCH_SIZE}, save_history_path)\n",
    "\n",
    "    \n",
    "def Test(model, test_DL, criterion, max_len, DEVICE, tokenizer):\n",
    "    model.eval() # test mode로 전환\n",
    "    with torch.no_grad():\n",
    "        test_loss = loss_epoch(model, test_DL, criterion, max_len=max_len, DEVICE=DEVICE, tokenizer=tokenizer)\n",
    "    print(f\"Test loss: {round(test_loss, 3)} | Test PPL: {round(math.exp(test_loss), 3)}\")\n",
    "\n",
    "def loss_epoch(model, DL, criterion, optimizer=None, max_len=None, DEVICE=None, tokenizer=None):\n",
    "    N = len(DL.dataset) # 데이터 수\n",
    "\n",
    "    rloss = 0\n",
    "    for src_texts, trg_texts in tqdm(DL, leave=False):\n",
    "        src = tokenizer(src_texts, padding=True, truncation=True, max_length=max_len, return_tensors='pt').input_ids.to(DEVICE)\n",
    "        trg_texts = ['</s> ' + s for s in trg_texts]\n",
    "        trg = tokenizer(trg_texts, padding=True, truncation=True, max_length=max_len, return_tensors='pt').input_ids.to(DEVICE)\n",
    "        # 추론\n",
    "        y_hat = model(src, trg[:, :-1])[0] # 모델 통과 시킬 때 trg의 <eos>는 제외!\n",
    "        # y_hat.shape = 개단채 즉, 훈련 땐 문장이 한번에 튀어나옴\n",
    "        # 손실\n",
    "        loss = criterion(y_hat.permute(0, 2, 1), trg[:, 1:]) # 손실 계산 시 <sos> 는 제외!\n",
    "        # 개단채 -> 개채단으로 바꿔줌 (1D segmentation으로 생각)\n",
    "        # 업데이트\n",
    "        if optimizer is not None:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        # 손실 누적\n",
    "        loss_b = loss.item() * src.shape[0]\n",
    "        rloss += loss_b\n",
    "    loss_e = rloss / N\n",
    "    return loss_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Transformer(vocab_size, max_len, d_model, n_heads, n_layers, d_ff, drop_p).to(DEVICE)\n",
    "\n",
    "# params = [p for p in model.parameters() if p.requires_grad] # 사전 학습된 layer를 사용할 경우\n",
    "params = model.parameters()\n",
    "\n",
    "# 논문에서 제시한 beta와 eps 사용 & 맨 처음 step 의 LR=0으로 출발 (warm-up)\n",
    "optimizer = optim.Adam(params, \n",
    "                       lr=0, \n",
    "                       betas=(0.9, 0.98), \n",
    "                       eps=1e-9) \n",
    "scheduler = NoamScheduler(optimizer, d_model=d_model, warmup_steps=warmup_steps, LR_scale=LR_scale)\n",
    "\n",
    "\n",
    "Train(model, train_DL, val_DL, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
