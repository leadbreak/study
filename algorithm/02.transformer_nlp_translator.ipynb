{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Prework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 그래프에서 한글이 깨지지 않게 폰트 설치. \n",
    "# 맨처음 실행 후 런타임 다시시작해야 반영됨\n",
    "# colab이라면 cell에서, Linux 등의 환경이라면 터미널 통해서 아래 코드 실행\n",
    "'''\n",
    "!sudo apt-get install -y fonts-nanum\n",
    "!sudo fc-cache -fv\n",
    "!rm ~/.cache/matplotlib -rf\n",
    "'''\n",
    "\n",
    "'''\n",
    "!pip install transformers\n",
    "!pip install sentencepiece # MarianTokenizer 불러올 때 필요\n",
    "!pip install sacremoses # MarianMTModel 에서 불러올 때 warning 뜨는 것 방지\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data & Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_excel('./대화체.xlsx')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128 ## 논문에선 2.5만 token이 한 batch에 담기게 했다고 함.\n",
    "EPOCH = 20 ## 논문에선 약 560 에포크 진행\n",
    "max_len = 512\n",
    "d_model = 512\n",
    "\n",
    "warmup_steps = 1500 ## 논문에선 4,000 스탭 \n",
    "LR_scale = 1 # Noam scheduler에 peak LR 값 조절을 위해 곱해질 녀석 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data.loc[idx, '원문'], self.data.loc[idx, '번역문']\n",
    "\n",
    "data = pd.read_excel('대화체.xlsx')\n",
    "custom_DS = CustomDataset(data)\n",
    "\n",
    "train_DS, val_DS, test_DS, _ = torch.utils.data.random_split(custom_DS, [95000, 2000, 1000, len(custom_DS)-95000-2000-1000])\n",
    "\n",
    "train_DL = torch.utils.data.DataLoader(train_DS, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_DL = torch.utils.data.DataLoader(val_DS, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_DL = torch.utils.data.DataLoader(test_DS, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(len(train_DS))\n",
    "print(len(val_DS))\n",
    "print(len(test_DS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotly.graph_objs as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-ko-en')\n",
    "\n",
    "eos_idx = tokenizer.eos_token_id\n",
    "pad_idx = tokenizer.pad_token_id\n",
    "print(\"eos_idx = \", eos_idx)\n",
    "print(\"pad_idx = \", pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.vocab_size\n",
    "print(f'tokenizer의 사전 크기: {vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = 'Tokenizer Test is Started with Hugginface MarianTokenizer'\n",
    "print(f\"original : {text}\")\n",
    "print(f\"token : {tokenizer.tokenize(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = '허깅페이스 마리안 토크나이저로 수행하는 토크나이저 테스트'\n",
    "print(f\"original : {text}\")\n",
    "print(f\"token : {tokenizer.tokenize(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = '문장을 넣으면 토크나이즈해서 숫자로 바꿔줍니다.'\n",
    "\n",
    "tokenized = tokenizer.tokenize(text)\n",
    "encoded_tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "encoded_tokens_end = tokenizer.encode(text, add_special_tokens=True)\n",
    "\n",
    "print(tokenized)\n",
    "print(encoded_tokens)\n",
    "print(encoded_tokens_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(tokenizer.decode([0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(tokenizer.decode([13774]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.decode(encoded_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Scheduler & Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Noam Scheduler 공식\n",
    "\n",
    "### $\\text{Learning Rate} = \\frac{1}{\\sqrt{d_{\\text{model}}}} \\times \\min\\left(\\frac{1}{\\sqrt{\\text{step\\_num}}}, \\frac{\\text{step\\_num}}{\\text{warmup\\_steps}^{1.5}}\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_params(model):\n",
    "    num = sum([p.numel() for p in model.parameters() if p.requires_grad])\n",
    "    return num\n",
    "\n",
    "class NoamScheduler:\n",
    "    def __init__(self, optimizer, d_model, warmup_steps, LR_scale=1):\n",
    "        self.optimizer = optimizer  # 최적화할 옵티마이저\n",
    "        self.step_count = 0  # 현재까지 진행된 스텝 수\n",
    "        self.d_model = d_model  # 모델의 차원 수\n",
    "        self.warmup_steps = warmup_steps  # 웜업 단계에서의 스텝 수\n",
    "        self.LR_scale = LR_scale  # 학습률 스케일 인자\n",
    "        self._d_model_factor = self.LR_scale * (self.d_model ** -0.5)  # 모델 차원에 대한 계수를 미리 계산\n",
    "\n",
    "    def step(self):\n",
    "        self.step_count += 1  # 스텝 수 증가\n",
    "        lr = self.calculate_learning_rate()  # 새 학습률 계산\n",
    "        self.optimizer.param_groups[0]['lr'] = lr  # 옵티마이저의 학습률 갱신\n",
    "\n",
    "    def calculate_learning_rate(self):\n",
    "        # 초기 웜업 단계에서는 학습률을 서서히 증가시키고, 이후에는 감소시키는 방식으로 계산\n",
    "        minimum_factor = min(self.step_count ** -0.5, self.step_count * self.warmup_steps ** -1.5)\n",
    "        return self._d_model_factor * minimum_factor\n",
    "        \n",
    "def plot_scheduler(scheduler_name, optimizer, scheduler, total_steps): # LR curve 보기\n",
    "    lr_history = []\n",
    "    steps = range(1, total_steps)\n",
    "\n",
    "    for _ in steps: # base model -> 10만 steps (12시간), big model -> 30만 steps (3.5일) 로 훈련했다고 함\n",
    "        lr_history += [optimizer.param_groups[0]['lr']]\n",
    "        scheduler.step()\n",
    "\n",
    "    plt.figure()\n",
    "    if total_steps == 100000:\n",
    "        plt.plot(steps, (512 ** -0.5) * torch.tensor(steps) ** -0.5, 'g--', linewidth=1, label=r\"$d_{\\mathrm{model}}^{-0.5} \\cdot \\mathrm{step}^{-0.5}$\")\n",
    "        plt.plot(steps, (512 ** -0.5) * torch.tensor(steps) * 4000 ** -1.5, 'r--', linewidth=1, label=r\"$d_{\\mathrm{model}}^{-0.5} \\cdot \\mathrm{step} \\cdot \\mathrm{warmup\\_steps}^{-1.5}$\")    \n",
    "    plt.plot(steps, lr_history, 'b', linewidth=2, alpha=0.35, label=\"Learning Rate\")\n",
    "\n",
    "    plt.ylim([-0.1*max(lr_history), 1.2*max(lr_history)])\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(nn.Linear(1, 1).parameters(), lr=0) # 테스트용 optimizer\n",
    "scheduler = NoamScheduler(optimizer, d_model=512, warmup_steps=4000) # 논문 값\n",
    "plot_scheduler(scheduler_name = 'Noam', optimizer = optimizer, scheduler = scheduler, total_steps = 100000)\n",
    "\n",
    "optimizer = optim.Adam(nn.Linear(1, 1).parameters(), lr=0)\n",
    "scheduler = NoamScheduler(optimizer, d_model=d_model, warmup_steps=warmup_steps, LR_scale=LR_scale)\n",
    "plot_scheduler(scheduler_name = 'Noam', optimizer = optimizer, scheduler = scheduler, total_steps = int(len(train_DS)*EPOCH/BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. Regularization - Label Smoothing Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def smooth_label(targets: torch.Tensor, classes: int, smoothing=0.1):\n",
    "    assert 0 <= smoothing < 1\n",
    "    confidence = 1.0 - smoothing\n",
    "    label_shape = torch.Size((targets.size(0), classes))\n",
    "    with torch.no_grad():\n",
    "        smooth_labels = torch.empty(size=label_shape, device=targets.device)\n",
    "        smooth_labels.fill_(smoothing / (classes - 1))\n",
    "        smooth_labels.scatter_(1, targets.data.unsqueeze(1), confidence)\n",
    "    return smooth_labels\n",
    "\n",
    "def custom_cross_entropy(input, target, smoothing=0.1, ignore_index=-100):\n",
    "    log_probs = F.log_softmax(input, dim=-1)\n",
    "    target = smooth_label(target, input.size(-1), smoothing)\n",
    "    \n",
    "    if ignore_index >= 0:\n",
    "        mask = target != ignore_index\n",
    "        target = target[mask]\n",
    "        log_probs = log_probs[mask]\n",
    "\n",
    "    loss = (-target * log_probs).sum(dim=-1)\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input = torch.randn(3, 5, requires_grad=True) # 임의의 예측값\n",
    "target = torch.tensor([1, 0, 4])  # 실제 레이블\n",
    "label = smooth_label(target, input.size(-1), 0.0)\n",
    "label_smoothing = smooth_label(target, input.size(-1), 0.1)\n",
    "\n",
    "loss = custom_cross_entropy(input, target, smoothing=0.0, ignore_index=pad_idx)\n",
    "loss_smoothing = custom_cross_entropy(input, target, smoothing=0.1, ignore_index=pad_idx)\n",
    "input, label, label_smoothing, loss, loss_smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LabelSmoothingCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, smoothing=0.1, ignore_index=65000):\n",
    "        super(LabelSmoothingCrossEntropyLoss, self).__init__()\n",
    "        # 스무딩 파라미터 설정. 0에 가까울수록 일반 크로스 엔트로피에 가까움\n",
    "        self.smoothing = smoothing\n",
    "        # 무시할 레이블(패딩)의 인덱스. 이 인덱스에 해당하는 레이블은 손실 계산에서 제외\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        # 입력 텍스트에 대한 로그 소프트맥스를 적용하여 모델의 예측 로그 확률을 계산\n",
    "        log_probs = F.log_softmax(input, dim=-1)\n",
    "        # 출력 언어의 어휘 크기를 계산 - 일반적인 분류 문제에서는 클래스의 수\n",
    "        n_classes = input.size(-1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # 스무딩된 레이블 분포를 생성. 각 클래스(어휘)에 작은 확률을 할당해 다양한 번역을 고려하도록 \n",
    "            true_dist = torch.full_like(log_probs, self.smoothing / (n_classes - 1))\n",
    "            # 무시할 레이블을 처리합니다. -> 패딩 토큰\n",
    "            ignore = target == self.ignore_index\n",
    "            # 무시할 레이블을 0으로 설정\n",
    "            target = target.masked_fill(ignore, 0)\n",
    "            # 실제 레이블 위치에 (1 - 스무딩) 값을 할당\n",
    "            true_dist.scatter_(1, target.unsqueeze(1), 1.0 - self.smoothing)\n",
    "            # 무시할 레이블의 위치에 0을 할당\n",
    "            true_dist.masked_fill_(ignore.unsqueeze(1), 0)\n",
    "\n",
    "            # 무시할 인덱스에 대한 마스크를 생성\n",
    "            mask = ~ignore\n",
    "\n",
    "        # 손실을 계산합니다. 마스크를 적용하여 무시할 인덱스를 제외\n",
    "        loss = -true_dist * log_probs\n",
    "        # 최종 손실을 평균내어 반환\n",
    "        loss = loss.masked_select(mask.unsqueeze(1)).mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = LabelSmoothingCrossEntropyLoss(smoothing=0.1, ignore_index=pad_idx)\n",
    "# criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1) MHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MHA(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        assert d_model % n_heads == 0, f'd_model ({d_model})은 n_heads ({n_heads})로 나누어 떨어져야 합니다.'\n",
    "\n",
    "        self.head_dim = d_model // n_heads  # int 형변환 제거\n",
    "\n",
    "        # 쿼리, 키, 값에 대한 선형 변환\n",
    "        self.fc_q = nn.Linear(d_model, d_model) \n",
    "        self.fc_k = nn.Linear(d_model, d_model) \n",
    "        self.fc_v = nn.Linear(d_model, d_model)\n",
    "        self.fc_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # 어텐션 점수를 위한 스케일 요소\n",
    "        self.scale = torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        batch_size = Q.shape[0]\n",
    "\n",
    "        # 쿼리, 키, 값에 대한 선형 변환 수행\n",
    "        Q = self.fc_q(Q) \n",
    "        K = self.fc_k(K)\n",
    "        V = self.fc_v(V)\n",
    "\n",
    "        # 멀티 헤드 어텐션을 위해 텐서 재구성 및 순서 변경\n",
    "        Q = Q.reshape(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.reshape(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.reshape(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "        # 스케일드 닷-프로덕트 어텐션 계산\n",
    "        attention_score = Q @ K.permute(0, 1, 3, 2) / self.scale\n",
    "\n",
    "        # 마스크 적용 (제공된 경우)\n",
    "        if mask is not None:\n",
    "            attention_score = attention_score.masked_fill(mask, -1e10)\n",
    "\n",
    "        # 소프트맥스를 사용하여 어텐션 확률 계산\n",
    "        attention_dist = torch.softmax(attention_score, dim=-1)\n",
    "\n",
    "        # 어텐션 결과\n",
    "        attention = attention_dist @ V\n",
    "\n",
    "        # 어텐션 헤드 재조립\n",
    "        x = attention.permute(0, 2, 1, 3).reshape(batch_size, -1, self.d_model)\n",
    "\n",
    "        # 최종 선형 변환\n",
    "        x = self.fc_o(x)\n",
    "\n",
    "        return x, attention_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Feed Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, drop_p):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear = nn.Sequential(nn.Linear(d_model, d_ff),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Dropout(drop_p),       ## ADD Dropout\n",
    "                                    nn.Linear(d_ff, d_model))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Encoder Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, drop_p):\n",
    "        \"\"\"\n",
    "        EncoderLayer 클래스의 초기화 메소드입니다.\n",
    "        :param d_model: 모델의 차원 크기\n",
    "        :param n_heads: 어텐션 헤드의 개수\n",
    "        :param d_ff: 피드 포워드 네트워크의 내부 차원\n",
    "        :param drop_p: 드롭아웃 비율\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_atten = MHA(d_model, n_heads)\n",
    "        self.FF = FeedForward(d_model, d_ff, drop_p)\n",
    "        self.LN = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "    \n",
    "    def forward(self, x, enc_mask):\n",
    "        \"\"\"\n",
    "        EncoderLayer 클래스의 순전파 메소드입니다.\n",
    "        :param x: 입력 텐서\n",
    "        :param enc_mask: 인코더 마스크\n",
    "        \"\"\"\n",
    "        x_norm = self.LN(x) ## Pre-LN\n",
    "        \n",
    "        # 멀티헤드 어텐션과 잔차 연결\n",
    "        output, atten_enc = self.self_atten(x_norm, x_norm, x_norm, enc_mask)\n",
    "        x = x + self.dropout(output)\n",
    "\n",
    "        # 레이어 정규화 적용\n",
    "        x_norm = self.LN(x)\n",
    "        # 피드 포워드 네트워크와 잔차 연결\n",
    "        output = self.FF(x_norm)\n",
    "        x = x_norm + self.dropout(output)\n",
    "        x = self.LN(x)\n",
    "\n",
    "        return x, atten_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_embedding, max_len, d_model, n_heads, n_layers, d_ff, drop_p):\n",
    "        \"\"\"\n",
    "        Encoder 클래스의 초기화 메소드입니다.\n",
    "        :param input_embedding: 입력 임베딩 레이어\n",
    "        :param max_len: 입력 시퀀스의 최대 길이\n",
    "        :param d_model: 모델의 차원 크기\n",
    "        :param n_heads: 멀티헤드 어텐션의 헤드 수\n",
    "        :param n_layers: 인코더 레이어의 수\n",
    "        :param d_ff: 피드 포워드 네트워크의 내부 차원\n",
    "        :param drop_p: 드롭아웃 비율\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # 스케일링 팩터\n",
    "        self.scale = torch.sqrt(torch.tensor(d_model, dtype=torch.float32))\n",
    "        self.input_embedding = input_embedding\n",
    "        self.pos_embedding = nn.Embedding(max_len, d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "\n",
    "        # 인코더 레이어를 n_layers만큼 생성\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, n_heads, d_ff, drop_p) for _ in range(n_layers)])        \n",
    "\n",
    "    def forward(self, src, mask, atten_map_save=False):\n",
    "        \"\"\"\n",
    "        Encoder 클래스의 순전파 메소드입니다.\n",
    "        :param src: 입력 소스\n",
    "        :param mask: 인코더 마스크\n",
    "        :param atten_map_save: 어텐션 맵 저장 여부\n",
    "        \"\"\"\n",
    "        pos = torch.arange(src.shape[1], device=src.device).repeat(src.shape[0], 1) # 위치 임베딩 생성\n",
    "\n",
    "        x = self.scale * self.input_embedding(src) + self.pos_embedding(pos)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        atten_encs = []\n",
    "        for layer in self.layers:\n",
    "            x, atten_enc = layer(x, mask)\n",
    "            if atten_map_save:\n",
    "                atten_encs.append(atten_enc[0].unsqueeze(0))\n",
    "\n",
    "        if atten_map_save:\n",
    "            atten_encs = torch.cat(atten_encs, dim=0)\n",
    "\n",
    "        return x, atten_encs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale Effect of Input Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 임베딩 벡터와 d_model 설정\n",
    "d_model = 512\n",
    "embedding_vector = torch.randn(100, d_model)\n",
    "\n",
    "# 스케일링 팩터 적용\n",
    "scale = torch.sqrt(torch.tensor(d_model, dtype=torch.float32))\n",
    "scaled_embedding_vector = scale * embedding_vector\n",
    "\n",
    "# 임베딩 벡터의 분포를 히스토그램으로 시각화\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(embedding_vector.numpy().flatten(), bins=30, color='blue', alpha=0.7)\n",
    "plt.title(\"Original Embedding Distribution\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(scaled_embedding_vector.numpy().flatten(), bins=30, color='red', alpha=0.7)\n",
    "plt.title(\"Scaled Embedding Distribution\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, scale=None):\n",
    "    \"\"\"\n",
    "    간단한 어텐션 메커니즘을 구현한 함수입니다.\n",
    "    :param query: 쿼리 벡터\n",
    "    :param key: 키 벡터\n",
    "    :param scale: 스케일링 팩터\n",
    "    :return: 어텐션 스코어\n",
    "    \"\"\"\n",
    "    d_k = query.size(-1)\n",
    "    \n",
    "    # query와 key의 내적 계산\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1))\n",
    "\n",
    "    if scale:\n",
    "        scores = scores / scale\n",
    "\n",
    "    return torch.softmax(scores, dim=-1)\n",
    "\n",
    "# 임의의 쿼리, 키, 밸류 생성\n",
    "query = torch.randn(10, d_model)\n",
    "key = torch.randn(10, d_model)\n",
    "\n",
    "# 스케일링 적용 전후의 어텐션 스코어 계산\n",
    "attention_scores_without_scaling = attention(query, key)\n",
    "attention_scores_with_scaling = attention(query, key, scale)\n",
    "\n",
    "# 어텐션 스코어의 분포를 히스토그램으로 시각화\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(attention_scores_without_scaling.numpy().flatten(), bins=30, color='blue', alpha=0.7)\n",
    "plt.title(\"Attention Scores Without Scaling\")\n",
    "plt.xlabel(\"Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(attention_scores_with_scaling.numpy().flatten(), bins=30, color='red', alpha=0.7)\n",
    "plt.title(\"Attention Scores With Scaling\")\n",
    "plt.xlabel(\"Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4) Decoder Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, drop_p):\n",
    "        \"\"\"\n",
    "        DecoderLayer 클래스의 초기화 메소드입니다.\n",
    "        :param d_model: 모델의 차원 크기\n",
    "        :param n_heads: 멀티헤드 어텐션의 헤드 수\n",
    "        :param d_ff: 피드 포워드 네트워크의 내부 차원\n",
    "        :param drop_p: 드롭아웃 비율\n",
    "        \"\"\"\n",
    "        super().__init__()        \n",
    "        self.atten = MHA(d_model, n_heads) # Attention for Self & Cross\n",
    "        self.FF = FeedForward(d_model, d_ff, drop_p) # ff network\n",
    "        self.LN = nn.LayerNorm(d_model) # Layer Normalization\n",
    "        self.dropout = nn.Dropout(drop_p) # Dropout\n",
    "\n",
    "    def forward(self, x, enc_out, dec_mask, enc_dec_mask):\n",
    "        \"\"\"\n",
    "        DecoderLayer 클래스의 순전파 메소드입니다.\n",
    "        :param x: 디코더의 입력\n",
    "        :param enc_out: 인코더의 출력\n",
    "        :param dec_mask: 디코더 마스크\n",
    "        :param enc_dec_mask: 인코더-디코더 마스크\n",
    "        \"\"\"\n",
    "        x, atten_dec = self.process_sublayer(x, self.atten, self.LN, dec_mask)\n",
    "        x, atten_enc_dec = self.process_sublayer(x, self.atten, self.LN, enc_dec_mask, enc_out)\n",
    "        x, _ = self.process_sublayer(x, self.FF, self.LN)\n",
    "\n",
    "        return x, atten_dec, atten_enc_dec\n",
    "\n",
    "    def process_sublayer(self, x, sublayer, norm_layer, mask=None, enc_out=None):\n",
    "        \"\"\"\n",
    "        디코더의 서브레이어 처리를 위한 함수.\n",
    "        :param x: 입력 텐서\n",
    "        :param sublayer: 서브레이어 (어텐션 또는 피드 포워드)\n",
    "        :param norm_layer: 레이어 정규화\n",
    "        :param mask: 마스크 (디코더 또는 인코더-디코더 마스크)\n",
    "        :param enc_out: 인코더의 출력 (인코더-디코더 어텐션에만 필요)\n",
    "        \"\"\"\n",
    "        x_norm = norm_layer(x)\n",
    "        if isinstance(sublayer, MHA): # mha case\n",
    "            if enc_out is not None: # encoder-decoder attention\n",
    "                residual, atten = sublayer(x_norm, enc_out, enc_out, mask)\n",
    "            else: # self attention\n",
    "                residual, atten = sublayer(x_norm, x_norm, x_norm, mask)\n",
    "        elif isinstance(sublayer, FeedForward): # ff network\n",
    "            residual = sublayer(x_norm)\n",
    "            atten = None  # 피드 포워드 레이어는 어텐션 맵을 반환하지 않음\n",
    "        else:\n",
    "            raise TypeError(\"Unsupported sublayer type\")\n",
    "\n",
    "        return x + self.dropout(residual), atten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_embedding, max_len, d_model, n_heads, n_layers, d_ff, drop_p):\n",
    "        \"\"\"\n",
    "        Decoder 클래스의 초기화 메소드.\n",
    "        :param input_embedding: 입력 임베딩 레이어\n",
    "        :param max_len: 입력 시퀀스의 최대 길이\n",
    "        :param d_model: 모델의 차원 크기\n",
    "        :param n_heads: 멀티헤드 어텐션의 헤드 수\n",
    "        :param n_layers: 디코더 레이어의 수\n",
    "        :param d_ff: 피드 포워드 네트워크의 내부 차원\n",
    "        :param drop_p: 드롭아웃 비율\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.scale = torch.sqrt(torch.tensor(d_model, dtype=torch.float32))\n",
    "        self.input_embedding = input_embedding\n",
    "        self.pos_embedding = nn.Embedding(max_len, d_model)\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, n_heads, d_ff, drop_p) for _ in range(n_layers)])\n",
    "\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, trg, enc_out, dec_mask, enc_dec_mask, atten_map_save=False):\n",
    "        \"\"\"\n",
    "        Decoder 클래스의 순전파 메소드.\n",
    "        :param trg: 타깃 입력\n",
    "        :param enc_out: 인코더의 출력\n",
    "        :param dec_mask: 디코더 마스크\n",
    "        :param enc_dec_mask: 인코더-디코더 마스크\n",
    "        :param atten_map_save: 어텐션 맵 저장 여부\n",
    "        \"\"\"\n",
    "        pos = torch.arange(trg.shape[1], device=trg.device).repeat(trg.shape[0], 1)\n",
    "\n",
    "        x = self.scale * self.input_embedding(trg) + self.pos_embedding(pos)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        atten_decs = []\n",
    "        atten_enc_decs = []\n",
    "        for layer in self.layers:\n",
    "            x, atten_dec, atten_enc_dec = layer(x, enc_out, dec_mask, enc_dec_mask)\n",
    "            if atten_map_save:\n",
    "                atten_decs.append(atten_dec[0].unsqueeze(0))\n",
    "                atten_enc_decs.append(atten_enc_dec[0].unsqueeze(0))\n",
    "\n",
    "        if atten_map_save:\n",
    "            atten_decs = torch.cat(atten_decs, dim=0)\n",
    "            atten_enc_decs = torch.cat(atten_enc_decs, dim=0)\n",
    "\n",
    "        x = self.fc_out(x)\n",
    "        \n",
    "        return x, atten_decs, atten_enc_decs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, max_len, d_model, n_heads, n_layers, d_ff, drop_p):\n",
    "        \"\"\"\n",
    "        Transformer 클래스의 초기화 메소드.\n",
    "        :param vocab_size: 어휘 사전의 크기\n",
    "        :param max_len: 입력 시퀀스의 최대 길이\n",
    "        :param d_model: 모델의 차원 크기\n",
    "        :param n_heads: 멀티헤드 어텐션의 헤드 수\n",
    "        :param n_layers: 인코더 및 디코더 레이어의 수\n",
    "        :param d_ff: 피드 포워드 네트워크의 내부 차원\n",
    "        :param drop_p: 드롭아웃 비율\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        input_embedding = nn.Embedding(vocab_size, d_model) \n",
    "        self.encoder = Encoder(input_embedding, max_len, d_model, n_heads, n_layers, d_ff, drop_p)\n",
    "        self.decoder = Decoder(input_embedding, max_len, d_model, n_heads, n_layers, d_ff, drop_p)\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        # 파라미터 초기화\n",
    "        for m in self.modules():\n",
    "            if hasattr(m, 'weight') and m.weight.dim() > 1: \n",
    "                nn.init.xavier_uniform_(m.weight) \n",
    "\n",
    "    def make_enc_mask(self, src):\n",
    "        \"\"\"\n",
    "        인코더 마스크 생성.\n",
    "        :param src: 입력 소스 (batch_size, src_len)\n",
    "        :return: 인코더 마스크 (batch_size, 1, 1, src_len)\n",
    "                 - pad_idx에 해당하는 위치는 True, 그 외는 False\n",
    "        \"\"\"\n",
    "        enc_mask = (src == pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        return enc_mask.repeat(1, self.n_heads, src.shape[1], 1).to(src.device)\n",
    "\n",
    "    def make_dec_mask(self, trg):\n",
    "        \"\"\"\n",
    "        디코더 마스크 생성 (패딩 마스크 및 미래 토큰 마스킹).\n",
    "        :param trg: 타깃 입력 (batch_size, trg_len)\n",
    "        :return: 디코더 마스크 (batch_size, 1, trg_len, trg_len)\n",
    "                 - 패딩 위치 및 미래 위치는 True, 그 외는 False\n",
    "        \"\"\"\n",
    "        trg_pad_mask = (trg == pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        trg_pad_mask = trg_pad_mask.repeat(1, self.n_heads, trg.shape[1], 1).to(trg.device)\n",
    "        trg_dec_mask = torch.tril(torch.ones(trg.shape[0], self.n_heads, trg.shape[1], trg.shape[1], device=trg.device))==0\n",
    "        dec_mask = trg_pad_mask | trg_dec_mask\n",
    "        return dec_mask\n",
    "\n",
    "    def make_enc_dec_mask(self, src, trg):\n",
    "        \"\"\"\n",
    "        인코더-디코더 마스크 생성.\n",
    "        :param src: 입력 소스 (batch_size, src_len)\n",
    "        :param trg: 타깃 입력 (batch_size, trg_len)\n",
    "        :return: 인코더-디코더 마스크 (batch_size, 1, trg_len, src_len)\n",
    "                 - 소스의 pad_idx 위치는 True, 그 외는 False\n",
    "        \"\"\"\n",
    "        enc_dec_mask = (src == pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        return enc_dec_mask.repeat(1, self.n_heads, trg.shape[1], 1).to(src.device)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        enc_mask = self.make_enc_mask(src)\n",
    "        dec_mask = self.make_dec_mask(trg)\n",
    "        enc_dec_mask = self.make_enc_dec_mask(src, trg)\n",
    "\n",
    "        enc_out, atten_encs = self.encoder(src, enc_mask)\n",
    "        out, atten_decs, atten_enc_decs = self.decoder(trg, enc_out, dec_mask, enc_dec_mask)\n",
    "\n",
    "        return out, atten_encs, atten_decs, atten_enc_decs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_path = './translator_ls.pt'\n",
    "save_history_path = './translator_history_ls.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda:0' ## 8대의 GPU 없음\n",
    "\n",
    "# BATCH_SIZE = 128 ## 논문에선 2.5만 token이 한 batch에 담기게 했다고 함.\n",
    "# EPOCH = 20 ## 논문에선 약 560 에포크 진행\n",
    "# max_len = 512 \n",
    "\n",
    "# warmup_steps = 1500 ## 논문에선 4,000 스탭 \n",
    "# LR_scale = 1 # Noam scheduler에 peak LR 값 조절을 위해 곱해질 Scale "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 논문에 나오는 base 모델\n",
    "d_model = 512\n",
    "n_heads = 8\n",
    "n_layers = 6\n",
    "d_ff = 2048\n",
    "drop_p = 0.1\n",
    "\n",
    "# 좀 사이즈 줄인 모델 \n",
    "# base model params와 맞춤 : 65 mil\n",
    "d_model = 400\n",
    "n_heads = 8\n",
    "n_layers = 4\n",
    "d_ff = 1200\n",
    "drop_p = 0.1\n",
    "\n",
    "# 사이즈 더 줄인 모델 \n",
    "d_model = 256\n",
    "n_heads = 8\n",
    "n_layers = 4\n",
    "d_ff = 512\n",
    "drop_p = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_DS 테스트\n",
    "i = 5\n",
    "idx = test_DS.indices[i]\n",
    "print(idx) # 엑셀 파일에서 idx번째 문장에 들어있음을 확인할 수 있다\n",
    "src_text, trg_text = custom_DS.__getitem__(idx)\n",
    "print(src_text)\n",
    "print(trg_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train(model, train_DL, val_DL, criterion, optimizer):\n",
    "    history = {\"train\": [], \"val\": [], \"lr\":[]}\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    for ep in range(EPOCH):\n",
    "        start_time = time.time()  # 에포크 시작 시간 기록\n",
    "\n",
    "        # 학습 모드\n",
    "        model.train()\n",
    "        train_loss = loss_epoch(model, train_DL, criterion, optimizer=optimizer, max_len=max_len, DEVICE=DEVICE, tokenizer=tokenizer)\n",
    "        history[\"train\"].append(train_loss)\n",
    "\n",
    "        # 현재 학습률 기록\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        history[\"lr\"].append(current_lr)\n",
    "        \n",
    "        # 평가 모드\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = loss_epoch(model, val_DL, criterion, max_len=max_len, DEVICE=DEVICE, tokenizer=tokenizer)\n",
    "            history[\"val\"].append(val_loss)\n",
    "            epoch_time = time.time() - start_time\n",
    "\n",
    "            # 로그 출력\n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                torch.save({\"model\": model, \"ep\": ep, \"optimizer\": optimizer.state_dict(), 'loss':val_loss}, save_model_path)\n",
    "                print(f\"| Epoch {ep+1}/{EPOCH} | train loss:{train_loss:.5f} val loss:{val_loss:.5f} current_LR:{optimizer.param_groups[0]['lr']:.8f} time:{epoch_time:.2f}s => Model Saved!\")\n",
    "            else :\n",
    "                print(f\"| Epoch {ep+1}/{EPOCH} | train loss:{train_loss:.5f} val loss:{val_loss:.5f} current_LR:{optimizer.param_groups[0]['lr']:.8f} time:{epoch_time:.2f}s\")\n",
    "\n",
    "    torch.save({\"loss_history\": history, \"EPOCH\": EPOCH, \"BATCH_SIZE\": BATCH_SIZE}, save_history_path)\n",
    "    \n",
    "    show_history(loss_history=history)\n",
    "    \n",
    "def show_history(history, save_path='train_history_ls'):\n",
    "    # train loss, val loss 시각화\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=list(range(1, EPOCH + 1)), y=history[\"train\"], mode='lines+markers', name='Train Loss'))\n",
    "    fig.add_trace(go.Scatter(x=list(range(1, EPOCH + 1)), y=history[\"val\"], mode='lines+markers', name='Validation Loss'))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title='Training History',\n",
    "        xaxis_title='Epoch',\n",
    "        yaxis=dict(title='Loss'),\n",
    "        showlegend=True\n",
    "    )\n",
    "    fig.write_image(save_path+\".png\")\n",
    "    fig.show()\n",
    "    \n",
    "    # learning rate 시각화\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=list(range(1, EPOCH + 1)), y=history['lr'], mode='lines+markers', name='Learning Rate'))\n",
    "\n",
    "    # 레이아웃 업데이트\n",
    "    fig.update_layout(\n",
    "        title='Training History',\n",
    "        xaxis_title='Epoch',\n",
    "        yaxis=dict(title='Learning Rate'),\n",
    "        showlegend=True\n",
    "    )\n",
    "    fig.write_image(save_path+\"_lr.png\")\n",
    "    fig.show()\n",
    "    \n",
    "\n",
    "def loss_epoch(model, DL, criterion, optimizer=None, max_len=None, DEVICE=None, tokenizer=None):\n",
    "    N = len(DL.dataset) # 데이터 수\n",
    "\n",
    "    rloss = 0\n",
    "    for src_texts, trg_texts in tqdm(DL, leave=False):\n",
    "        src = tokenizer(src_texts, padding=True, truncation=True, max_length=max_len, return_tensors='pt').input_ids.to(DEVICE)\n",
    "        trg_texts = ['</s> ' + s for s in trg_texts]\n",
    "        trg = tokenizer(trg_texts, padding=True, truncation=True, max_length=max_len, return_tensors='pt').input_ids.to(DEVICE)\n",
    "        \n",
    "        # inference\n",
    "        y_hat = model(src, trg[:, :-1])[0] # 모델 통과 시킬 때 trg의 <eos>는 제외!\n",
    "        loss = criterion(y_hat.permute(0, 2, 1), trg[:, 1:]) # 손실 계산 시 <sos> 는 제외!\n",
    "        if optimizer is not None:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        \n",
    "        # loss accumulation\n",
    "        loss_b = loss.item() * src.shape[0]\n",
    "        rloss += loss_b\n",
    "    loss_e = rloss / N\n",
    "    return loss_e\n",
    "\n",
    "def Test(model, test_DL, criterion, max_len, DEVICE, tokenizer):\n",
    "    model.eval() # test mode로 전환\n",
    "    with torch.no_grad():\n",
    "        test_loss = loss_epoch(model, test_DL, criterion, max_len=max_len, DEVICE=DEVICE, tokenizer=tokenizer)\n",
    "    print(f\"Test loss: {round(test_loss, 3)} | Test PPL: {round(math.exp(test_loss), 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(vocab_size, max_len, d_model, n_heads, n_layers, d_ff, drop_p).to(DEVICE)\n",
    "\n",
    "# # 모델의 레이어와 파라미터 출력\n",
    "# for name, module in model.named_modules():\n",
    "#     print(name, module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 총 파라미터 수 계산\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = model.parameters()\n",
    "\n",
    "# 논문에서 제시한 beta와 eps 사용 & 맨 처음 step 의 LR=0으로 출발 (warm-up)\n",
    "optimizer = optim.Adam(params, \n",
    "                       lr=0, \n",
    "                       betas=(0.9, 0.98), \n",
    "                       eps=1e-9) \n",
    "scheduler = NoamScheduler(optimizer, d_model=d_model, warmup_steps=warmup_steps, LR_scale=LR_scale)\n",
    "\n",
    "\n",
    "Train(model, train_DL, val_DL, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = torch.load('translator.pt', map_location=DEVICE)\n",
    "load_model = loaded['model']\n",
    "ep = loaded['ep']\n",
    "optimizer = loaded['optimizer']\n",
    "\n",
    "print(loaded.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translation(model, src_text, atten_map_save=False, extra_token_length=50):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        src = tokenizer.encode(src_text, return_tensors='pt').to(DEVICE) \n",
    "        enc_mask = model.make_enc_mask(src)\n",
    "        enc_out, atten_enc = model.encoder(src, enc_mask, atten_map_save)\n",
    "\n",
    "        # 입력 시퀀스의 길이 계산 및 출력 시퀀스의 최대 길이 설정\n",
    "        max_output_length = src.shape[1] + extra_token_length\n",
    "\n",
    "        pred = tokenizer.encode('</s>', return_tensors='pt', add_special_tokens=False).to(DEVICE)\n",
    "        for _ in range(max_output_length):\n",
    "            dec_mask = model.make_dec_mask(pred)\n",
    "            enc_dec_mask = model.make_enc_dec_mask(src, pred)\n",
    "            out, atten_dec, atten_enc_dec = model.decoder(pred, enc_out, dec_mask, enc_dec_mask, atten_map_save)\n",
    "\n",
    "            pred_word = out.argmax(dim=2)[:,-1].unsqueeze(0) \n",
    "            pred = torch.cat([pred, pred_word], dim=1) \n",
    "\n",
    "            if tokenizer.decode(pred_word.item()) == '</s>':\n",
    "                break\n",
    "\n",
    "        translated_text = tokenizer.decode(pred[0])\n",
    "\n",
    "    return translated_text, atten_enc, atten_dec, atten_enc_dec\n",
    "\n",
    "\n",
    "def show_attention(atten, Query, Key, n):\n",
    "    plt.rc('font', family='NanumBarunGothic')\n",
    "    atten = atten.cpu()\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=3, figsize=[atten.shape[3]*1.5,atten.shape[2]])\n",
    "    for i in range(3):\n",
    "        ax[i].set_yticks(range(atten.shape[2]))\n",
    "        ax[i].set_yticklabels(Query, rotation=45)\n",
    "        ax[i].set_xticks(range(atten.shape[3]))\n",
    "        ax[i].set_xticklabels(Key, rotation=60)\n",
    "        ax[i].imshow(atten[n][i], cmap='bone') # h 번째 layer, 앞 세 개의 헤드만 plot\n",
    "        # ax[i].xaxis.tick_top()  # x축 레이블을 위쪽으로 이동"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역해보기\n",
    "i = 1\n",
    "idx = test_DS.indices[i]\n",
    "src_text, trg_text = custom_DS.__getitem__(idx)\n",
    "print(f\"입력: {src_text}\")\n",
    "print(f\"정답: {trg_text}\")\n",
    "\n",
    "translated_text, atten_enc, atten_dec, atten_enc_dec = translation(load_model, src_text, atten_map_save = True)\n",
    "print(f\"AI의 번역: {translated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_input = tokenizer.tokenize(src_text+' </s>') # <eos> 붙여서 학습 시켰기 때문에 여기도 붙여줘야\n",
    "dec_tokens = tokenizer.tokenize(translated_text) \n",
    "dec_input = dec_tokens[:-1] # 디코더 입력으로 들어가는 문장(sos 는 있고 eos는 없고)\n",
    "dec_output = dec_tokens[1:] # 디코더 출력으로 나간 문장\n",
    "\n",
    "show_attention(atten_enc, enc_input, enc_input, n = -1)\n",
    "show_attention(atten_dec, dec_input, dec_input, n = -1)\n",
    "show_attention(atten_enc_dec, dec_output, enc_input, n = -1) # 이 map을 해석할 때는 \"이 단어가 나오게끔 뭘 주목했느냐\" 로 해석해줘야 함 (ytick에 들어가는 단어가 아닌 예측한 단어를 썼기 때문)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "# trgs = [[['훌륭한', '강사와', '훌륭한', '수강생이','만나면','명강의가', '탄생한다']]]\n",
    "# preds = [['훌륭한', '강사와', '훌륭한', '수강생이','함께라면','명강의가','만들어진다']]\n",
    "# preds = [['만들어진다', '강사와', '훌륭한', '명강의가','훌륭한','수강생이','함께라면']]\n",
    "# preds = [['훌륭한', '강사와', '훌륭한', '수강생이','훌륭한','강사와','훌륭한','강의를', '만든다']]\n",
    "# preds = [['수강생이', '만나면', '명강의가', '탄생한다']]\n",
    "\n",
    "trgs = [[['훌륭한', '강사와', '훌륭한', '수강생이','만나면','명강의가', '탄생한다']], [['이것은', '두','번째','문장입니다']]]\n",
    "preds = [['훌륭한', '강사와', '훌륭한', '수강생이','훌륭한','강의를','만든다'], ['이것은','문장입니다']]\n",
    "\n",
    "bleu_score(preds, trgs, max_n = 4, weights = [0.25,0.25,0.25,0.25]) # default\n",
    "# bleu_score(preds, trgs, max_n = 1, weights = [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_bleu_score(model, DS):\n",
    "    trgs = []\n",
    "    preds = []\n",
    "\n",
    "    for i, (src_text, trg_text) in enumerate(DS):\n",
    "        \n",
    "        translated_text, _, _, _ = translation(load_model, src_text)\n",
    "\n",
    "        trg = tokenizer.tokenize(trg_text)\n",
    "        translated_tok = tokenizer.tokenize(translated_text)[1:-1] # <sos> & <eos> 제외\n",
    "\n",
    "        trgs += [[trg]]\n",
    "        preds += [translated_tok] \n",
    "        \n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"[{i + 1}/{len(DS)}]\")\n",
    "            print(f\"입력: {src_text}\")\n",
    "            print(f\"정답: {trg_text}\")\n",
    "            print(f\"AI의 번역: {translated_text[5:-4]}\") # 문자열에서 </s> 안보이게 하려고..\n",
    "\n",
    "    bleu = bleu_score(preds, trgs)\n",
    "    print()\n",
    "    print(f'Total BLEU Score = {bleu*100:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_bleu_score(load_model, test_DS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 내 번역기 써보기!\n",
    "src_text = \"안녕하세요! 이 강의 정말 열심히 준비 했어요.\"\n",
    "print(f\"입력: {src_text}\")\n",
    "\n",
    "translated_text, _, _, _ = translation(load_model, src_text)\n",
    "print(f\"AI의 번역: {translated_text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
